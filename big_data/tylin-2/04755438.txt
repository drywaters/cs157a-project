Using Rough Set to support Investment Strategies of Rule-based trading with Real-Time Data in Futures Market  Suk Jun Lee Department of Information and Industrial Engineering Yonsei University Seoul, Korea lsj77@yonsei ac.kr   Jae Joon Ahn Department of Information and Industrial Engineering Yonsei University Seoul, Korea resamzang@y onsei.ac.kr   Kyong Joo Oh 
Department of Information and Industrial Engineering Yonsei University Seoul, Korea johanoh@yon sei.ac.kr   Tae Yoon Kim Department of Statistics Keimyung University Daegu, Korea tykim@kmu.a c.kr    Hyoung Yong Lee Department of Business Administratio n, Hansung University Seoul, Korea leemit@hansu 
ng.ac.kr   Chi Woo Song Department of Information and Industrial Engineering Yonsei University Seoul, Korea chiwoo17@na te.com    Abstract  Investment strategies in stock market have gained unprecedented popularity in major financial markets around the world. However, it is a very difficult problem because of the fluctua tion of the stock market This study presents usefulness of rough set on the rule base to develop real-time investment strategies using technical analysis in futures market. This study consists of four phases. In the first phase, meaningful 
technical indicators are selected to reflect market movements. In the second phase, rough set is used to extract trading rules for identification of buy and sell patterns in the stock market. In the third phase, the investment strategies are developed in order to apply selected trading rules using rule-based reasoning to unpredictable stock market Finally, investment strategies on the basis of rule base are evaluated by real-time trading. This study then examines the profitability of the proposed model     1. Introduction  Determining investment strategies in the stock market is quite difficult since many factors, including political events, general economic conditions, and 
investors expectations, influence the stock market The stock market is essentially a non-linear, nonparameter system, and it is extremely hard to model with any reasonable accuracy. Although there have been numerous attempts in the past to predict the next trend, the best performers have traditionally been the ones who posses considerable knowledge of the markets. However, they are only human and are very limited in their capacity to assimilate information and spot subtle trends in the information, which may be the indicators of an imminent change in the value of 
market stock. For this reason, several researchers in finance and investment have begun to use information system fields, including expert systems and artificial intelligence technologies for predicting the stock market. Others have also predicted price movements in the stock market by using artificial neural networks [11  o r ex a m pl e, L e e an d Jo \(1 990  e l ope d an expert system for predicting the best stock market timing, when to buy and sell, using a candlestick chart They reported that the average hit ratio of applied rules was 72 As the Korean futures market has become larger 
and more mercurial, traders and investors in the market have come to need powerful supporters in their investment decision since human capability in analyzing all the data could not satisfy most expectations. This study proposes the real-time investment strategies by trading rules generated through rough set based on the rule base using technical analysis in futures market. It also presents a procedure for constructing efficient real-time investment strategies which uses technical analysis and rough set analysis in futures market. Meaningful 
technical indicators are selected to reflect market movements through the technical analysis. Then, rough set is used to extract trading rules based on selected technical indicators for identification of buy and sell patterns in the market. Two analyses, also known as ensemble based systems in data mining  w e re u s ed  as a classifier for decision making, which is better than a single classifier. One of the earliest studies on ensemble system is Dasarathy and Sheela s 1979 work Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 1 978-0-7695-3450-3/09 $25.00 © 2009 IEEE 


10 w h ich d i scu sses p a rtiti o n in g th e f e at u r e sp ace  using two or more classifiers Moreover, this study uses real-time data. Real-time data denotes information that is delivered immediately after collection, and thus is provided instantly. On the other hand, delayed or historical data is delivered after some time, usually from about 10 to 30 minutes making the information not up-to-date. Also, historical data is usually adjusted after being combined with realtime data, so the figures are somewhat inaccurate Croushore and Stark \(2003\9 earch ed in to a realtime data set for macroeconomists and Gerberding et al. \(2005 s t u d ied a realt i m e data s e t f o r Germ a n  macroeconomic variables The rest of the paper is organized as follows Section 2 reviews the concept of futures market and technical analysis, and the rough set theory. Section 3 describes the rule generation procedure using technical analysis and rough set analysis. In Section 4, the research data and experiments are described, and the empirical results are summarized and discussed Finally, the concluding remarks are presented in Section 5  2. Background  2.1. Futures market and technical analysis  Futures market is an opportunity where one takes a marginal profit that buys when the bull market is forecasted and sells when the bear market is forecasted Therefore, it is a market which offers the possibility of making profit in both bull market and bear market Namely, it is a trading market that predicted the directivity of price fluctuations Stocks are influenced by the intrinsic value evaluation, technical analysis theory of the enterprise and the current price fluctuation. However, since futures are transactions of goods which are extracted from the abstraction of price, the current price fluctuation prediction is possible in only technical analysis without the complicated enterprise of intrinsic value evaluation. Technical analysis studies the historical data relevant to price and volume movements of the stock by using charts as a primary tool to forecast possible price movements is  considered by many to be the original form of investment analysis, as it has been used since the 1800s. According to early research, future and past stock prices were deemed as irrelevant. As a result, it was believed that using past data to predict the future stock price was impossible, and that it would only have abnormal profits. However, recent findings have proven that there was, indeed, a relationship between the past and future return rates. Furthermore arguments have been made that by using past return rates, future return rates could also be forecasted There are various kinds of technical indicators used in futures market as we T h is s t u d y  us ed 26 tech n i cal  indicators for technical analysis. For more detailed references, see Murphy \(1986; 1999\21, 22 c h e li s 1995\1 o l b y 2003 8   2.2. Rough sets  The concept of rough sets, introduced by Pawlak 1982; 1997\[23, 24 i n a ll y  i s a m a t h e m atica l  approach to manage uncertain data and conditions Through this theory, correlation of attributes can be found, the importance of certain attributes can be grasped, and inconsistent data can More detailed discussion about the process of rough set theory can refer to Slowinski \(1997 an d Di m itras 1999\ [1  O n e c e n t r a l c o nce p t i n r o ugh se t s a n a l ysi s  is the notion of indiscernibility. Indiscernib ility arises from the inability to distinguish objects in a distinct set with respect to all of the object s significant features In a way, indiscernibility is related to similarity Objects characterized by the same information are indiscernible \(or similar\s of the available information about them Any set of indiscernible objects is called an elementary set. Nonetheless, rough set enables objects in elementary sets to be clearly distinguished in terms of the available information or knowledge. However since sets of objects will most likely be determined ambiguously by elementary set, objects are to be labeled roughly through a pair of sets known as lower and upper approximations. The lower approximation contains all objects that entirely belong to a certain category while the upper approximation consists of objects that have the possibility of belonging to the category. The boundary region is the group of objects that cannot be decisively assigned as being either a member or a non-member of that category. A rough set is any subset defined through its lower and upper approximation, and Figure 1 shows a graphical representation of this concept. Each indiscernibility set is displayed by a pixel. The subset of objects that needs to be approximated is drawn as a dashed line that crosses pixel boundaries, and cannot be defined in a crisp manner. The lower and upper approximations are drawn as thick gridlin  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 2 


  Upper approximation Subset Lower approximation  Figure 1. Lower and upper approximations of sets  An important advantage of the rough set approach is that it can deal with a set of inconsistent examples i.e., objects indiscernible by condition attributes but discernible by decision attributes. Furthermore, it provides useful information in the role of particular attributes and their subsets in the approximation of decision classes. It also prepares the foundation for generation of decision rules involving relevant attributes. In large data sets, some attributes may be superfluous and can be eliminated without losing essential classificatory information. The reduct and core are two additional fundamental rough set concepts that can be used for knowledge reduction. Reduct is the most concise way to discern object classes. In other words, reduct is the minimal subset that can provide the same object classification as the full set of attributes. The intersection of all reducts is called the core. Accordingly, the core is the class of all necessary attributes and without the core attributes the classification of the objects becomes less precise. As a data mining technique, one of the most important reasons for applying rough sets is the generation of decision rules, often presented in an IF condition\(s THEN decision\(s format. The decision rule reflects a relationship of a set of conditions with a conclusion or a decision. In fact, the generation of decision rules is combining the reducts with the values of the data. Such a rule may be exact, if the combination of the values of the condition attributes in that rule implies only one single combination of the values of the decision attributes, or approximate, if more than one combination of values of the decision attributes corresponds to the same values of the condition attribut Many studies have relied on technical analysis for successful stock market prediction [3, 4, 23  Several studies are mainly focused on artificial intelligence applications to stock market prediction [7  e v e r, les s res e arch  has f o cus e d on th e fu t u res market. Therefore, in this study, the futures market using the technical analysis and rough set analysis is investigated  3. Methodology  In this section, the architecture and characteristics of the proposed model are discussed. Figure 2 shows the architecture of the model which consists of three phases. The first phase is input data generation based on technical analysis; the second phase is rough set modeling; in the final phase, the rule retrieval procedure used in rule-based reasoning is applied [13 14, 18  New Period Target Rule Solution Rule Rule Base Adapted Rule Cre ation of Re duc ts Phase 2 Rough Set Analysis Disc retization Rule Gene ration Phase 1 Technical Analysis Tre nd Groups Ge ne ration           Phase 4 Rule-Based Trading  New Period Target Rule Solution Rule Rule Base Adapted Rule Cre ation of Re duc ts Phase 2 Rough Set Analysis Disc retization Rule Gene ration Trading Rule Applic ation Trading Rule Applic ation Phase 1 Input data generation based on Technical Analysis Tre nd Groups Ge neration           Phase 3 Rule-Based Trading   Figure 2. Architecture of the proposed model  Phase 1: Input data generation based on technical analysis At this phase we establish input data for each of the six cases characteristic by their trends, i.e., short-term ascending trend \(SAT\hort-term descending trend SDT\g-term ascending trend \(LAT\g-term descending trend \(LDT\lat top \(FT\, and flat bottom FB\Table 1 shows its detailed description. Then, the performances of technical indicators \(e.g., it consists of opening, high, low, closing price and trading volume Refer to Section 4 for their precise definitions mentioned in Appendix 1 for each training period are evaluated and, then, profitable technical indicators are selected in each trend group. One trend group consists of five technical indicators with the highest return rates in a given training period. The selected five technical indicators in each trend group are input data In this study, return rates means yearly average profit rates which is calculated in the ratio of the capital an year after the initial capital. Yearly average profit is defined as a value to exclude all transaction costs and slippages \(mentioned in Section 4\rom yearly gross profit. It then calculates the difference between yearly average short position and yearly average long position for total number of trades Finally six trend groups with profitable five technical indicators for a given each training period are generated     Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 3 


Table 1. Trend period definition Trend Definition SAT Short-term ascending trend High of stock price rises continuously for two to three weeks SDT Short-term descending trend Low of stock price falls continuously for two to three weeks LAT Long-term ascending trend Continuous rise of High for over 6 months LDT Long-term descending trend Continuous fall of Low for over 6 months FT Flat top After an ascending trend High and Low stay unmoved FB Flat bottom After a descending trend High and Low stay unmoved  Phase 2: Rough set modeling Consisting of three steps, this phase is mainly concerned about rough set modeling process applied to each trend group. In the first step, prior to the analysis of the data, an exploration and cleaning of the technical indicators data extracted in Phase 1 are conducted This effort could lead to much better results. Data cleaning may exist in removing obvious outliers and in data completion \(replacing or deleting blanks\o improve the overall quality of the discovered information, data transformation is usually conducted by means of discretization which basically corresponds to making the attribute s value sets smaller It is possible either computationally or mutually to consider several discretization functions such as discernibility preservation, entropy minimization equal frequency binning and various naive methods Equal frequency binning method is used in this study The second step is creation of reducts which is the computation of the reducts. Creation of reducts is a very important process in rough set analysis since core information by this is extracted in the concrete rule from a given data set. For each trend group, three reducts are created by the combination of three technical indicators randomly selected among five profitable indicators established in Phase 1, i.e., one trend group has three reducts. Various methods are available for this data reduction process, e.g., genetic algorithms, manual reducer, dynamic reducts approximate hitting set approaches, etc. Among these methods, the manual reducer method is used in this study The final step is rule generation for each trend Based on the reducts made in Step 2, patterns could be generated in the form of IF THEN production rules by combining the condition values with the decision values. The generated reducts could then be filtered according to some criteria as like coverage and accuracy, attribute cost, advanced quality measures or classificatory performance on external holdout data sets [31  I n  ge ne r a l  t h e c o nn e c t i o n o f c o nd i t i o n va l u e s  or input variables\ and decision values \(or output variables\ is based on conjunction. An exemplary form of the generated pattern could be  IF the first technical indicator values  AND the second technical indicator values  AND AND the th n technical indicator values  THEN BUY or SELL   For practical application of the decision rules stated above, successive application of the rules or the number of positions to hold can be considered.  For example, one may employ the following implementation rule to limit the number of positions to hold  IF today s signal is BUY And IF the previous day s signal is BUY THEN HOLD ELSE SELL IF today s signal is SELL And IF the previous day s signal is SELL THEN HOLD ELSE BUY   Phase 3: Rule-based trading The core in this phase is the construction of rule base which stores a collection of cases or memories from the past. Rules of each trend groups generated in Phase 2 are stored in the rule base. At this time, rules include values of 20 days back from the trading starting date. Then, if a new trading day occurs, the distance between the rules with technical indicators  value of 20 days back in the rule base and a new trading day with technical indicators values of 20 days back from the trading starting date is measured by the square of the difference function. Return rates are calculated by feeding back the nearest neighborhood trend group in the Phase 1  4. Empirical Study  This empirical study for constructing the proposed model is done by taking the Korean Stock Price Index 200 \(KOSPI 200\ the underlying asset \(or base index\ in the Korean futures market. The underlying asset is the asset for which the price of derivative is derived. For an empirical example of proposed model for the derivative, we consider the period from July Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 4 


1996 to December 2006 and divide it into training period \(July 1996 to June 2000\ and testing period July 2000 to December 2006\igure 3 depicts the stream of KOSPI 200 for the entire period                                              Figure 3. Overall flow of KOSPI 200 mm/dd/yyyy  For real-time data 10, 30, 60-minute and daily  time intervals \(or frequencies\ are available, i.e., to each frequency the corresponding data set composed of opening price, high price, low price, closing price, and trading\ volume are available. As the default settings of the system trading, the initial capital is 1,000,000 interest rate is 5.00%, transaction cost is 10,000, and slippage is 25,000. $1 is worth of 900, and slippage is the amount by which the trading target price is missed. For evaluation of trading system, return rates are calculated by the underlying asset contrast, which can be is validated as the difference between futures index and KOSPI 200. The equivalent condition applies to all the later trading. When five technical indicators with the highest return rate are selected in Phase 1, it excludes indicators which belong to the 5 highest high during a trade and the 5% lowest low during the same or consecutive trades. This is a kind of outlier detection process which removes technical indicators having extreme profits in a given trading peri   4.1. Preliminary analysis  Prior to the experiment, we conducted the selection of appropriate frequency for analyzing futures market through technical analysis. Table 2 shows return rates of the five most profitable technical indicators during the training period \(July 1996 to June 2000\ This period includes all trends of Korean futures market as shown in Figure 3. 10-minute has relatively lower return rates because of frequent trading. Daily data is relatively lower which has a value of 28.57% since it cannot solve the upward gap and downward gap. This is due to the differences of today s opening price and the previous day s closing price, i.e., the upward gap occurs when the today s opening price is above the previous day s closing price while the downward gap occurs when the today s opening price is below the previous day s closing price. These gaps are hard upon improving the performance of daily trading. The analysis from 30-minute data has higher return rates as it has recorded 186.38% in total. For that reason, 30minute data is used in the later experiment  Table 2. Return rates indicators applied to real time data at various frequencies during July 1996 to December 2006 Frequency Return rates Technical indicators employed 10-minute 7.16 CO, DMI, TRIX William s %R, PO 30-minute 186.38 CO, NCO, ROC, SMI Momentum 60-minute 133.53 SMI, NCO, ROC, CO SMA Daily 28.57 MI, VROC, DMI Band %b, PO  4.2. Usefulness of trend group  For construction of the rule-based trading system with the training period data, input data are generated at Phase 1. For this, the six trend \(or case\ods characterized by their own distinctive features \(SAT SDT, LAT, LDT, FT and FB\ obtained from the entire period by the help of the futures market experts Note that Table 3 shows training and testing period with respect to each trend group  Table 3. Training and t esting periods of trends Training period Testing period Trend Starting date Ending date Starting date Ending date SAT Dec. 26 1997 Mar. 05 1998 Dec. 26 2000 Jan. 22 2001 SDT Mar. 06 1998 Jun. 15 1998 Jul. 14 2000 Sep. 22 2000 LAT Oct. 01 1998 Jul. 09 1999 Apr. 02 2003 Apr. 23 2004 LDT Sep. 09 1996 Dec. 24 1997 Apr. 24 2002 Apr. 01 2003 FT Jul. 12 1999 Feb. 03 2000 Apr. 28 2004 Dec. 30 2004 FB Jun. 16 1998 Sep. 30 1998 Sep. 25 2000 Dec. 22 2000 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 5 


Table 4. Five most profitable indicators of six trend groups and return rates training period us ing 30-minute data  a\ATG Indicators Return rates RSI 623.24 WMA 479.86 Parabolic SAR 271.98 CCI 155.05 SMA 134.18 b\DTG Indicators Return rates NCO 536.95 ROC 536.95 Momentum 458.84 Parabolic SAR 210.74 DMI 152.04 c\ATG Indicators Return rates Momentum 570.66 MACD Osc 258.33 ROC 254.86 NCO 254.86 SMI 249.28 d\DTG Indicators Return rates SMI 863.78 WMA 630.24 EMA 500.00 MACD Osc 480.94 CCI 394.89 e\TG Indicator Return rates SMI 333.78 RSI 161.83 ROC 146.23 NCO 146.23 Momentum 56.98 f\BG Indicators Return rates RSI 242.73 SMI 135.42 Stochastic 95.91 ROC 17.88 NCO 17.88  Phase 1 is conducted in six trend groups. Five profitable technical indicators are selected through simulation in the training periods of each trend. Return rates of each indicator on a given specific training period are extracted by a system trading tool Tradestation 2000i Table 4 shows five most profitable indicators for each trend groups and their return rates by 30-minute data. Trend periods can be defined and categorized by groups which are known as short-term ascending trend group \(SATG\hort-term descending trend group \(SDTG\, long-term ascending trend group LATG\g-term descending trend group \(LDTG flat top group \(FTG\d flat bottom group \(FBG Recall that trend group is defined as the five most profitable indicators in a given trend period\ Most return rates of indicators of trend groups are highly profitable though they are obtained in the training period. Six trend groups are well furnished in order to perform real-time trading. Figure 4 provides an example of the series of process mentioned above discretization, creation of reducts, and rule generation in rough set modeling by using ROSETTA-software  a\cretization   b\reation of reducts        Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 


c\ule generation  Figure 4. An example of discretization creation of reducts, and rule generation for a trend group, SATG, in ROSETTA-software  Technical indicators belonging to each trend group during training period are utilized as condition attributes in rough set modeling. At this time, up and down signals as decision attributes corresponds to the values of the condition attributes as mentioned in Section 2.2. Three reducts are produced by three indicators randomly selected among five indicators. In this study, manual reducer method is used as a creation method of reduct. Table 5 demonstrates return rates of trend groups in the testing period. Return rates are relatively high when the trend groups are coordinated with the corresponding trend periods. On the other hand, when trend groups do not correspond with their respective trend periods, the return rates have low values. This result means higher return rates are obtained by trend groups suitable for trend periods Therefore, it can be concluded that the acquisition of market timing based on setting up trend groups is useful to support investment strategies of real-time trading. In other words, six trend groups applied to this study can be used as useful trading rule sets. In realtime trading, these six trend groups play a role of rule portfolio  Table 5. Return rate s \(%\end groups in the testing period Trend groups Trend SATG SDTG LATG LDTG FTG FBG SAT 77.76 -47.63 2.59 -62.40 -14.71 5.18 SDT -97.31 44.97 16.60 5.99 2.74 7.80 LAT -16.23 -8.36 144.63 28.97 -18.97 25.48 LDT -8.25 0.58 5.92 42.52 -13.06 1.52 FT 9.78 -34.29 -29.86 -67.60 13.55 1.84 FB 24.65 -10.10 -5.93 18.18 6.87 113.15 Shade regions are the return rates when the trend groups are coordinated with the corresponding trend periods  4.3. Real-time trading using rule base       100 125 150 175 200 01/03/2005 02/24/2005 04/18/2005 06/08/2005 07/27/2005 09/15/2005 11/07/2005 12/26/2005 02/15/2006 04/06/2006 05/29/2006 07/21/2006 09/11/2006 11/02/2006 12/21/2006 Jan. 24, 2005 Mar. 10, 2005 Apr.22, 2005 Aug. 02, 2005 Oct. 06, 2005 Jan. 20, 2006 Apr. 24, 2006 May. 25, 2006 Sep. 29, 2006 Nov. 15, 2006 100 125 150 175 200 01/03/2005 02/24/2005 04/18/2005 06/08/2005 07/27/2005 09/15/2005 11/07/2005 12/26/2005 02/15/2006 04/06/2006 05/29/2006 07/21/2006 09/11/2006 11/02/2006 12/21/2006 Jan. 24, 2005 Mar. 10, 2005 Apr.22, 2005 Aug. 02, 2005 Oct. 06, 2005 Jan. 20, 2006 Apr. 24, 2006 May. 25, 2006 Sep. 29, 2006 Nov. 15, 2006  Figure 5. KOSPI 200 from Jan., 2005 to Dec 2006 with ten starting dates  Real-time trading is conducted in the testing periods For this, ten starting dates to decide new periods in trading are randomly selected from Jan. 3, 2005 to Dec. 28, 2006 as shown in Figure 5. When a new period is determined, rule-based reasoning is activated i.e., the distance between 20-day historical data of trend groups and new period data is calculated. Table 6 shows new periods and nearest neighborhood trend groups, and the distance between them  Table 6. Euclidian distances between new periods and nearest neighborhood trend groups Starting date Trend groups Distance Jan. 24 2005 SATG 894.09 LATG 897.38 SDTG 897.60 FBG 897.63 FTG 897.91 LDTG 898.85 Mar. 10 2005 SDTG 931.81 FBG 932.53 SATG 933.04 LATG 933.20 FTG 934.01 LDTG 934.77 Apr.22 2005 FBG 867.59 LATG 869.63 SATG 870.62 SDTG 870.69 LDTG 871.18 FTG 873.73 Aug. 02 2005 SDTG 930.85 FBG 931.11 SATG 932.44 FTG 933.10 LATG 933.22 LDTG 933.50 Oct. 06 2005 SDTG 941.96 SATG 942.47 FBG 943.07 LDTG 943.25 FTG 944.29 LATG 945.41 Jan. 20 2006 SATG 977.11 FBG 978.22 LATG 978.67 FTG 981.89 LDTG 992.72 SDTG 994.45 Apr. 24 2006 SATG 940.83 SDTG 944.66 LATG 945.40 FBG 945.54 LDTG 945.70 FTG 948.16 May 25, 2006 SATG 940.83 SDTG 944.66 LATG 945.40 FBG 945.54 LDTG 945.70 FTG 948.16 Sep. 29 2006 SDTG 916.35 LATG 916.60 SATG 916.93 FBG 918.95 LDTG 919.14 FTG 919.49 Nov. 15 2006 SDTG 935.18 SATG 936.50 FBG 937.20 LDTG 937.52 LATG 938.67 FTG 938.94  The number of trend groups is made by adding in one nearest trend group at a time. For example, if 1 is Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 


designated as the number of trend groups, it would signify that only the nearest trend group was used. If 2 is used, it can be interpreted that two nearest trend groups are formed, and so on. Table 7 provides return rates according to the number of trend groups that fill the role of rule portfolio  Table 7. Average return rates \(%\ of the specifically determined, in order of shortest distance, combinations of trend groups Number of the combination of trend groups Starting date 1 2 3 4 5 6 Jan. 24 2005 8.01 5.67 4.32 2.81 1.64 -0.36 Mar. 10 2005 7.56 6.25 21.54 -5.36 -2.86 -0.96 Apr. 22 2005 6.32 13.62 11.77 0.84 -5.43 -10.37 Aug. 02 2005 15.07 9.35 11.09 19.13 -2.17 6.33 Oct. 06 2005 12.50 12.87 14.00 15.99 1.23 2.92 Jan. 20 2006 15.34 15.40 16.84 9.65 0.67 3.13 Apr. 24 2006 7.26 5.95 7.97 -0.29 -6.47 -0.40 May. 25 2006 19.04 17.86 21.52 -5.61 7.05 2.31 Sep. 29 2006 6.09 16.06 7.08 0.02 1.24 -4.09 Nov. 15 2006 5.11 6.38 10.84 -4.20 5.12 -1.53  Sharpe ratio is used for measuring portfolio performance of trend groups. The Sharpe ratio, defined as the ratio of the expected return \(i.e., the difference of return rates between a given portfolio and risk-free asset\ a portfolio over the standard deviation of the return series, has been widely cited and used in literature since the original work of Sh measure for evaluating portfolio performance. As neither the expected return nor its standard deviation is observable, they have to be estimated in some fashion usually by the sample average return and by the sample standard deviation, respectively. Consequently, the performance of different investment strategies must be compared on the basis of the estimated Sharpe ratio which inevitably contains some estimation error. Table 8 illustrates average return rates and Sharpe ratios due to the number of trend groups of ten new periods     Table 8. Average return rates \(%\ and Sharpe ratio of trend group portfolio Number of the combination of trend groups Measure 1 2 3 4 5 6 Average return rates 10.23 10.94 12.70 3.30 -1.41 1.39 Sharpe ratio 1.23 1.42 1.47 -0.15 -1.56 1.43  Return rates of Treasury bills with 3 years maturity are used instead of return rates of risk-free asset applied for calculating Sharpe ratio. Their average return rates from 2005 to 2006 are 4.55%. Sharpe ratio increases to 1.23, 1.42, and 1.47 by the third trend group, and then plunges dramatically from fourth trend group. As a result, when rule portfolio is made up by the number of trend groups, the portfolio has the best performance if it organizes three trend groups. Despite the average return rates being 12.70%, the trading using rough set analysis can be profitable compared to the average 5% of the open market interest rates   5. Concluding Remarks  The purpose of this study is to prove the usefulness of rough set and rule base to develop investment strategies using technical analysis in futures market Profitable indicators in technical analysis and trading rules with rough set analysis for the stock index futures were discovered. To apply appropriate trading rules to unpredictable stock market, trading simulation through rule-based reasoning process is performed by searching trend groups suitable for trends. The best performance is represented when the extracted distance based on rule base makes up three combinations of the nearest neighborhood trend groups. On the basis of rule base consisting of trend groups, technical indicators with high return rates can be found. In analyzing rule base despite the return rate being 12.70%, the trading using rule base can be profitable compared to the average 5 of the open market interest rates This study remains further studies. It only incorporates some basic tools within rough sets. A more elaborate study including other reduction techniques may lead to better results. It may be expected that improved performances could be produced through other reduction techniques such as genetic algorithms, dynamic reducts and approximate hitting set approaches    Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


6. References   A c h e lis S  B  T ech n i cal an alys is from A to Z, IL: Probus Publishing, Chicago, 1995  2  G   A r m a no, M. Ma rc he s i a nd A  Murr u A hybrid genetic-neural architecture for stock indexes forecasting  Information Sciences, vol. 170, 2005, pp. 3-33   R M  Ben  and H C R o chest e r  Is technical analysis profitable on a stock market which has characteristics that suggest it may be inefficient Research in International Business and Finance, vol. 19, 2005, pp. 384-398   L  Bl u m e D E a sl ey  an d M  O Hara Market statistics and technical analysis: the role of volume Journal of Finance, vol. 49, 1994, pp. 153 181  5 W  B r oc k  J  L a k onis h ok a n d B  L e ba ron  Simple technical trading rules and the stochastic properties of stock returns Journal of Finance, vol. 47, 1992, pp. 1731 1764  6 F  Bru i n sm a P  Ni j k a m p  an d R V r eek e r  A comparative industrial profile analysis of urban regions in western Europe: An application of rough set classification  Tijdschrift voor Economische en Sociale, 2002   S  H Chu n  an d Y J  P a rk  Dynamic adaptive ensemble case-based reasoning: application to stock market prediction  Expert Systems with Applications, vol. 28, 2005, pp. 435443  8 C o l b y  R  W 200 3 h e Enc y c l ope dia of Te c hnic a l  Market Indicators. New York: McGraw-Hill   D Crou sho r e an d T  S t ark  A real-time data set for macroeconomists: Does th e data vintage matter Review of Economics and Statistics, vol. 85, 2003, pp. 605 617  1 B V  Dasarat h y  an d B V  S h eel a  Composite classifier system design: Concepts and methodology  Proceedings of the IEEE vol. 67\(5\79, pp. 708 713  11 E  D a v i d, a nd T  S u r a pha n The use of data mining and neural networks for forecasting stock market returns Expert Systems with Applications, vol 29, 2005, pp. 927-940  12 A  I. Dim itra s R. Slow in sk i, R. Susm a g a  a nd C Zopounidis Business failure prediction using rough sets  European Journal of Operational Research, vol. 144, 1999 pp. 263 280  13 E D u p u it M  F  P o ue t O  T hom a s a nd J  B ourg o i s    Decision support methodology using rule-based reasoning coupled to non-parametric measurement for industrial wastewater network management Environmental Modeling Software, vol. 22, 2006, pp. 1153-1163  14 R.J Dz e n g  a nd H  Y. L e e   Critiquing contractors  scheduling by integrating rule-based and case-based reasoning Automation in Construction, vol. 13, 2004, pp 665-678  15  P. G e org e a nd R.H  J o nh  Build ing W i ning T r a d ing  Systems with TradeStation, John Wiley & Sons, Inc., New Jersey, 2003 1 C  G e rb erd i n g  Kaat z M    F  S e i t z  an d A  W o rm s  A real-time data set for German macroeconomic variables  Journal of Applied Social Science Studies, vol. 125, 2005, pp 337 346  17 A  G  J a ck s on, Z  P a w l a k a nd S  R  L e C l a i r Rough sets applied to the discovery of materials knowledge Journal of Alloys and Compounds, vol. 279, 1998, pp. 14 21  1 L  Jon e s a n d G  J Ko eh l e r  Combinatorial auctions using rule-based bids Decision Support Systems, vol. 34 2002, pp. 59 74  1 H L e e an d G  S  Jo   Expert system for predicting stock market timing using a candlestick chart Expert Systems with Applications, vol. 16, 1990, pp. 357-364  20 A  W  L o H  Ma m a y s ky a nd J  W a ng  Foundation of technical analysis: computations, algorithms, statistical inference, and empirical implementation Journal of Finance, vol. 55, 2000, pp. 1705 1765  2 J J M u r p h y  T e ch n i cal an al ysi s o f t h e F u t u res M a rket s   A Comprehensive Guide to Trading Methods and Applications, Prentice-Hall, New York, 1986  22 J  J   Mur phy T e c hnic a l a n a l y s is of the f i na nc ia l m a r k e t s   Prentice-Hall, New York, 1999  23 Z   P a w l ak  Rough set International Journal of Computer and Information Science, vol. 11, 1982, pp. 341 356  24 Z   P a w l ak  Rough set approach to knowledge-based decision support European Journal of Operational Research, vol. 99, 1997, pp. 48-57  2  P o likar, \(20 0 6  E n sem b l e b a sed sy ste m s in d ecision making. IEEE Circuits and Systems Magazine, 2006  26 C  Q i ng B.L  K a ry l, a nd J  S. Ma rc  A comparison between Fama and French's model and artificial neural networks in predicting the Chinese stock market Computers Operations Research. vol. 32, 2005, pp. 2499-2512  27 W  F Sha r pe  Mutual fund performance Journal of Business, vol. 39, 1966, pp. 119 138  28 R. Sl ow insk i, C. Zop o u n idis, a n d A  I. Dim itra s   Prediction of company acquisition in Greece by mean of the rough set approach European Journal of Operational Research, vol. 100, 1997, pp. 1-15  29 H  W e i, N  Y o s h ite r u a n d W  Shou-Y a ng  Forecasting stock market movement direction with support vector Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


machine Computers & Operations Research, vol. 32, 2005 pp. 2513-2522  30 L  W illia m  Na v a l, M P  R u sse ll, a n d R. T o m   Stock market trading rule discovery using technical charting heuristics Expert Systems with Applications, vol. 23, 2002 pp. 155-159  31 A  hrn Discernibility and rough sets in medicine Tools and applications PhD Thesis, Trondheim: Norwegian University of Science and Technology, 1999        Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


California Mexicali Mexico He is currently completing the requirements for the Master's in Science in Electrical Engineering at California State University Long Beach 11 


 12 7  C ONCLUSIONS AND F UTURE W ORK  This research task started with an all-FORTRAN implementation of the FTIR spectrometry algorithm converted it to C code, and developed a number of H W/SW systems on the V4FX60 hybrid-FPGA The execution ti me of the all software C implementation of the FTIR spectrometry algorithm was recorded and used for comparison as a base case Two software-based optimizations were applied that reduced the executi on time by more than 4.5x These included modifying the cod e to use all single-precision math library functions no n-ANSI when dealing with single-precision data and utilizi ng the IBM Performance Libraries  Perflib  to improve the speed of all single and double-precision arithmetic The idea of using a DP-only Perflib was introduced and then used in conjunction with the single-precision APU-FPU to fu rther improve system performance The bulk of the research dealt with looking into ha rdwarebased improvements to the FTIR spectrometry system  These included the Xilinx APU-FPU, and a single-pre cision dot-product co-processor The APU-FPU delivered significant speedup for all single-precision floati ng-point operations The dot-product co-processor although ineffective in the FTIR spectrometry system due to poor spatial locality of the data, showed nearly a 2x im provement over the APU-FPU when working with smaller, sequent ially accessed data sets Furthermore it was implemented  as a load/store-based APU-connected FCM thus establishin g a reference for creating similar APU co-processors T he design of a non-system-bused CPU-coupled co-process or is frequently overlooked in design guides yet it is a  very effective way to offload software routines to hardw are implementation The ML410 development board on which all of this w ork was conducted, hosts the V4FX60 hybrid-FPGA contain ing two PPC405 processors This research task focused o n optimizing the performance of the FTIR spectrometry  algorithm on a single PPC405 core, however, the des ign can be extended to utilize both available cores. Figure 16 on the following page shows a dual-core design that can be  implemented on the ML410 board The two PPC405 processors each have dedicated PLB interfaces but s hare a common OPB. On the common OPB, the processors need to negotiate access to the RS232 UART and SystemACE CF  controller. This negotiation can be done through du al-ported shared BRAM accessible by each processor from thei r respective PLB. The ML410 board has two external me mory interfaces that are both utilized in this concept PPC405 CPU0 uses the DDR2 DIMM 256 Mbytes while PPC405 CPU1 utilizes the DDR on-board component memory 64  Mbytes\. Each of the processors has some dedicated on-chip memory OCM connected through OCM interface The instruction side OCM is particularly necessary so e ach processor can store its own boot code in its own on chip memory as booting both processors from external mem ory may not be possible Both processors have their own FPUs connected on dedicated FCB interfaces Since the processing of individual interferograms is a comple tely independent task an up to 2x reduction in executio n time may be possible with a dual-core system However o ne bottleneck that may limit the speedup is negotiatin g access to the shared CF card controller Additional improvement to the overall performance o f the FTIR spectrometry system may be possible by rewriti ng the software in C. The automatic conversion from FORTRA N to C using f2c most likely does not produce optimal code, and it is certainly not appealing to read Some functio ns may also need to be rewritten with an optimized pattern  of data access This can help in cases such as the dot-prod uct coprocessor Further performance improvement may be achieved by trying a different compiler one that is specifical ly targeted for the embedded PPC405 processor A V2P performanc e study done at the NASA Goddard Space Flight Center concluded that using the WindRiver Diab DCC 5.2 com piler provides a 38 performance increase over the GNU-GC C 3.4 compiler The comparison was based on running a  Dhrystone benchmark application on a 400 MHz PPC405 design The GNU-GCC compiler achieved 458 DMIPS while the WindRiver Diab DCC achieved 628 DMIPS as  reported by Xilinx\ [10   Implementing additional hardware co-processors may result in the further reduction of execution time Using t he dotproduct design as a reference the FFT function fo r example can be implemented in the hardware This w ill help in the spectrum computation component of the s oftware processing. It may be necessary to re-arrange the d ata access pattern for optimal co-processor performance to av oid the pitfall seen when deploying the dot-product core Finally, no embedded processing system is complete without an OS. Linux is a good choice and is supported by X ilinx in EDK It is important to first finalize the hardware  design prior to deploying the OS Support for the APU may be lacking in Linux and getting the OS to recognize th e hardware FPU may be a project in itself For the FTIR spectrometry algorithm this research task started the process of moving from an all software system to a mixed HW/SW implementation on the V4FX60 hybridFPGA In the best case a more than 8x speedup was achieved compared to the FTIR base system This implementation, although nearly 2x faster the V2P s ystem at NASA JPL still lags behind the current state-of-th e-art space processor  the BAE RAD750 However the marg in between the two was narrowed down significantly and with further research as suggested above will most lik ely be eliminated altogether Directly benefiting from the  work presented in this paper is a 3-year JPL technology 


 13 development task that will support the MATMOS on-bo ard processing implementation for a future flight i nstrument   Fig. 16 Dual-core concept targeting ML410 development boar d 


 14 R EFERENCES  1  D L Bekker Hardware and Software Optimization of Fourier Transform Infrared Spectrometry on HybridFPGAs  MS Thesis Rochester Institute of Technology Rochester NY August 2007 Available http://hdl.handle.net/1850/4805  2  P J Pingree J.-F L Blavier G C Toon and D  L Bekker An FPGA/SoC Approach to On-Board Data Processing  Enabling New Mars Science with Smart Payloads in IEEE Aerospace Conference 2007  Big Sky MT March 2007 Available http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?isnu mber=4 144550&arnumber=4161501  3  S I Feldman D M Gay M W Maimone and N L Schryer, “A Fortran-to-C Converter,” Computing Scie nce Technical Report 149, AT&T Bell Laboratories, Murra y Hill NJ March 22 1995 Available http://www.netlib.org/f2c/f2c.pdf  4  IBM PowerPC Embedded Processor Performance Libraries tech rep IBM Microelectronics Divisi on Hopewell Junction, NY, December 12 2003 5  Virtex-4 Data Sheet: DC and Switching Characterist ics Datasheet DS302 Xilinx Inc San Jose CA March 27 2007 Available http://www.xilinx.com/bvdocs/publications/ds302.pdf  6  ML410 Embedded Development Platform  Xilinx Inc San Jose CA March 6 2007 Available http://www.xilinx.com/bvdocs/userguides/ug085.pdf  7  G Toon J.-F Blavier M McAuley and A Kiely Advanced On-Board Science Data Processing System for a Mars-orbiting FTIR Spectrometer R&TD Task 01STCR  R.05.023.048 NASA Jet Propulsion Laboratory Pasadena, CA, 2005 8  APU Floating-Point Unit v3.0 product specificati on Xilinx Inc San Jose CA January 26 2007 Availa ble http://www.xilinx.com/bvdocs/ipcenter/data_sheet/ap u_fp u.pdf  9  PowerPC 405 Processor Block Reference Guide Xilinx Inc San Jose CA July 20 2005 Available www.xilinx.com/bvdocs/userguides/ug018.pdf  10  D. Petrick, “Analyzing the Xilinx Virtex-II Pro Pow erPC with the Dhrystone Benchmark Applications,” tech. r ep NASA Goddard Space Flight Center Greenbelt Maryland Available http://klabs.org/DEI/Processor/PowerPC/v2pro_ppc_pe rf ormance_petrick.doc  B IOGRAPHY  Dmitriy Bekker has just completed his Masters Degree in Computer Engineering at the Rochester Institute of Technology in Rochester NY His areas of interest include FPGAs embedded systems digital signal processing and system architecture. He has coop  internship experience working at Brookhaven National Laboratory Syracuse Research Corporation NASA Dryden Flight Research Center, and the Jet Propulsion Laboratory. He recen tly won in the 2006 IEEE Student Design Contest for his pro ject in autonomous vehicle navigation. He is a member of IE EE Dr Lukowiak is an assistant professor in the Computer Engineering Department at Rochester Institute of Technology in Rochester NY His research interests are concentrated in the area of multidisciplinary projects that require modeling and hardware implementations FPGA and ASIC of data processing systems Dr Marcin Lukowi ak obtained his Ph.D in Technical Sciences from the P oznan University of Technology in October 2001 Muhammad Shaaban is an associate professor of computer engineering at the Rochester Institute of Technology His research interests include high performance computing processor microarchitecture heterogeneous and reconfigurable computing Shaaban has a PhD in Computer Engineering from the University of Souther n California. He is a senior member of the IEEE Dr. Blavier first joined the JPLMkIV Team in August 1985 as a contractor from Ball Aerospace He participated in the MkIV campaigns in McMurdo Antarctica groundbased and from Punta Arenas Chile NASA DC-8 In late 1987, he started graduate work with Profs Delbouille and Dubois at the University  of Liège Belgium his research tasks included install ing the 


 15 Fourier transform spectrometers at the Internationa l Scientific Station of the Jungfraujoch Switzerland  for atmospheric measurements and at the Institute of Astrophysics in Liège for laboratory measurements He was hired by JPL in August 1990 as MkIV cognizant engin eer and participated in all the MkIV campaigns since th en \(one DC-8 campaign 19 balloon campaigns Dr J.-F Bla vier obtained his Ph.D in Physics from the University o f Liège in July 1998 Paula Pingree is a Senior Engineer in the Instruments and Science Data Systems Division at JPL She has been involved in the design integration test and operation of several JPL flight projects most recently Deep Impact DI She has worked on the Tunable Laser Spectrometer development for the 2009 Mars Rover and is presently the Electronics CogE for the Juno Mission s Microwave Radiometer She also enjoys research and technology development for Smart Payloads in her s pare time Paula has a Bachelor of Engineering degree i n Electrical Engineering from Stevens Institute of Te chnology in Hoboken, NJ, and an MSEE degree from California State University Northridge.  She is a member of IEEE 


  16  Figure 15. AIRS-AMSRE differences as a function of AIRS error estimate over one day  AIRS has an error estimate of the total water vapor value that it calculates. The diffe rences between AIRS and AMSR-E are shown as a function of this estimate in figure 15 and very little correlation is found 11  R ELEVANT W ORK  Merged A-Train Level 2 Data A merged product that preserves the relationship of observed atmospheric water properties facilitates the hydrological studies by enabling scientists to get directly at the model data without worrying about the logistics of finding, collecting, and coordinating the measured quantities from different instruments. Previously there did not exist a capability to discover and access data from the A-Train\222s multiple instruments as merged multi-parameter data sets Enabling Orchestratable Service Workflows Our distributed service-oriented approach of loosely coupled services also enable s a higher level of reusability and orchestration with other services. Increasing numbers of workflow engines are already supporting Web Services as components/operators, which can then be orchestrated together into higher-level meta/virtual services SciFlo, a Scientific Dataflow Execution Environment, is a workflow engine that already supports assembling reusable SOAP Services, native execu tables, local command-line scripts, and codes into a distributed computing flow \(a graph of operators\8 SciFlo can u tilize o u r g en eric SOAP services as part of a larger coordinated data flow The Taverna Workbench is a free software tool for designing and executing workflows. Like SciFlo, it can orchestrate SOAP-based Web Services as components within a workflow. Taverna provides a visual editor to construct and edit the sequence of services in the workflow We have found that Taverna can dynamically introspect a given WSDL and construct the workflow component interface representing it Giovanni Giovanni, an acronym for the Goddard Earth Sciences Data and Information Services Cent er, or GES DISC, Interactive Online Visualization and Analys is Infrastructure, is a webbased tool to help visualize Earth science data  It  provides a simple and intuitive way to visualize, analyze and access vast amounts of Eart h science remote sensing data without having to download the data. Similar to the services developed here, it addresses the difficulties of traditional data acquisition and analysis methods by moving the complexity to the server-side Giovanni provides multiple in terface instances based on instrument and measurement ty pes. For example, the \223ATrain Along CloudSat Track Inst ance\224 can provide plots of vertical profiles of clouds, temperature, humidity, cloud and aerosol classification across the multiple instruments of the A-Train A distinction between Givanni\222s A-Train data and the data set in this paper is that we are using a formal merged product of the A-Train. We leverage the NEWS effort that is based on error- and resolution-weighted mean of the input data sets, with associated uncertainty estimates. This provides a formal model of the collective A-Train observations rather than the collection of the individual instrument measurements Each of Giovanni\222s multiple interface instances provides a very simple and easy to use web interface. However, we recognized that sometimes scientists want more than the simple interfaces. Some scien tists may want to process Level 3 products using their own trusted code, or may want to perform variations of their own plots. With Giovanni, the individual scientist wanting more custom advanced capabilities must depend on the Giovanni development team Giovanni is based on the web portal paradigm where users visit a web page and use web tools to find and visualize data. Similar to Giovanni, our client APIs also make data acquisition more seamless. However, our services are based on the different paradigm were the power and flexibility of data analysis and processing are shifted back into the scientists own familiar computing environments. We realize that scientists generally want to perform \223exploratory computing\224 where they can sere ndipitously analyze the data using their own familiar and trusted code 


  17 Giovanni 2 was inherently synchronous where processing was bounded to a single http session. Long service running times still require the user to hold the same http session Similar to our asynchronous Web Service we discussed, the upcoming Giovanni 3 will be supporting asynchronous sessions. They will be using a RSS feed to monitor the service request. Version 3 will also be based on a servicesoriented architecture, wher e Giovanni services can be offered as a standard SOAP Web Services. This is similar to our approach, as well as SciFlo\222s services 12  C ONCLUSIONS  To achieve the science research goal of investigating longterm and global-scale trends in climate, water and energy cycle, and weather variability, we enhanced and improved on existing algorithms to work with distributed and heterogeneous data and information systems infrastructure By developing a service-oriented architecture for discovering, accessing, and mani pulating of NEWS merged A-Train data sets, we can strengthen the interconnectedness and reusability of these services across broader range of Earth science investigations The merged NEWS Level 2 data is a formal model containing the voluminous data from the AIRS, AMSR-E MLS, MODIS, and CloudSat instruments. Previously scientists wanting to perform long-term and global-scale studies encompassing simultaneous measured quantities would quickly face a data acce ss hurdle of first finding the data, then manually downloading them, and finally merging the data into a cohesive model\227before starting their analysis. Additionally the voluminous nature of the data particularly because of the MODIS data\each scientist potentially downloading the same data resulting in redundancy of reprocessing on the client sides. Our paradigm pushes more of the commonly repeated processing onto the server side. Moreover, this avoids repeated downloading of the same data among the science users. We can deliver customi zed averaged, subsetted, and summarized data of the merged A-Train observations to the scientists for them to immediately begin their analysis work We recognized that scientists also often want to perform 223exploratory computing\224 where they can freely explore the aspects of the data and run serendipitous exploration in their own familiar environment. We developed client-side distributed APIs in popular analysis environments such as Matlab, IDL, and Python. Our APIs hide the complexity of Web Services and allow the service capabilities to be embedded in the scientists own computing environments By purposely avoiding the \223web portal\224 paradigm and providing the suite of platform specific APIs in each of these language platforms, we enable the scientists to remain within their own familiar environments to select, process and download the data seamlessly into their environment for their own further analysis. Alternative methods involving web portals force the scientists to leave the environment and manually interact with the web portal to search and download the data We can examine not only long-term changes in amplitude of a single variable but also those among multiple variables Our L3Q clustering method was specifically designed to preserve information about the covariability of multiple observations, such as those from the A-Train.  Weather and climate variability is characterized by changes among atmospheric observables, but those changes have been limited by a lack of observations and analytical techniques We are not aware of any multi-parameter analyses to date The full potential of the A-Train climate record will not be realized until the multi-parameter climatology is understood. The work presented is one method of approaching this difficult problem Our service tool addresses several objectives of the NASA Earth science data community including 1\mprove interoperability to facilitate the transparent access and manipulation of heterogeneous and distributed data by science users, 2\ransition and deploy existing Earth science research analysis tools and software using a 223Service Oriented Architecture\224 \(SOA\ to enhance their reuse potential for other science domains and improve overall awareness and access of these tools by a broad community, 3\ increase users\222 ability to customize their discovery, access, deliv ery, manipulation, and preferred format of data and information 12  F UTURE W ORK  On-demand Level 3T Summaries from Level 3Q We plan to develop services for creating custom summaries of the L3Q data into more refined Level 3T summaries L3T\create their own custom Level 3 products on demand from L3Q. The custom Level 3 products are the transformation of L3Q data based on user-specific objectives such as regression and correlation analyses. The cust om production will generate not only the transformed data but also the statistical estimation of the accuracy of the summarized data based on the distribution of L3Q and the quality of L3Q Delegating the Temporal-Spatial Data Querying Currently, our processing layer utilizes existing and legacy processing code that was developed in IDL, Matlab, and C++. Though the original intent was to be able to adapt existing code and wrap as a service, this meant maintaining its original form of accessing the source data for processing Small modifications were made to enable these codes to quickly access the data based on file path and file naming schemes. However, we want to decouple the file accessibility and processing roles 


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


