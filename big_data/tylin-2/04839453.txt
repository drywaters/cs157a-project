  1 Bayesian-Based Fusion of 2-D and 3-D LADAR Imagery Stephen Cain Air Force Institute of Technology 2950 Hobson Way WPAFB, OH  45433 937-255-3636 Stephen.Cain@afit.edu  Abstract 227Laser Radar imagers can be designed to provide 2-D or 3-D images of scenes.  The 2-D imagers generally possess superior spatial resolution while providing a rough estimate of the range to the target.  Newly developed 3-D systems possess the ability to form images of the scene as well as range to every pixel in the scene.  The spatial resolution of these systems is typically many times less than those of the 2-D systems.  It is the goal of this work to present a statistical method for fusing 2-D and 3-D LADAR imagery.  The method is used on simulated LADAR sensor 
data in order to demonstrate its potential for improving both the range resolution and spatial resolution over what could be achieved alone from either sensor T ABLE OF C ONTENTS  1  I NTRODUCTION 1  2  S ENSOR M ODELS 1  3  P ROPOSED S ENSOR FUSION ALGORITHM 3  4 RESULTS 4 
 5 CONCLUSIONS 5  6 REFERENCES 5  7 BIOGRAPHY 6  1  I NTRODUCTION  LADAR \(LAser Detection A nd Ranging\stems have potential applications across a wide range of fields including machine vision and long range reconnaissance The spatial resolution of LADAR imaging systems is ultimately limited by either the size of the aperture used to collect the light, the atmosphere in betw 1 een the sensor and 
the target or ratio of the pixel pitch to the focal length of the imaging system.  In modern 3-D LADAR imagers it is the pixel pitch of the image acquisition array that is generally the limiting factor.  2-D LADAR imagers are designed with denser focal planes than their 3-D counterparts.  This is due to the fact that the 2-D imag ing arrays can be fabricated with more pixels per unit 2 area because there are not other hardware components on the chip used in the high speed read out and integration function needed by the 3-D system  Because 2-D imagers can obtain high spatial resolution while 3-D imagers possess ranging capability at low spatial resolution a pote 3 ntial sensor fusion opportunity arises 1 
 1 U.S. Government work not protected by U.S. copyright 2 IEEEAC paper #1549, Version 3, Updated January 8, 2009  involving the fusion of these two types of sensors.  It is the goal of this research to fuse data from a 2-D sensor and a 3D sensor in order to obtain both higher spatial resolution and better range accuracy.  Secti on 2 of this paper describes the sensor models used to simulate the LADAR data used to test the proposed sensor fusion technique presented in Section 3.  Section 4 of this paper shows results obtained on simulated data using the proposed sensor fusion technique Section 5 of this paper is used to discuss conclusions drawn from these results 2  S ENSOR M ODELS  
The sensor models used in this study break down into two different categories.  The fi rst category is the 2-D LADAR model.  This model relates the perceived intensity of the target as a function of position in detector plane A\(x,y to the measured image in the 2-D focal plane array cam 
 212\212  N y N x ywxzhyxA wzi 11  2.1 In equation \(2.1\he point spread function h\(x,y is convolved with the amplitude of the target to produce the intensity in the focal plane i\(z,w 
The indices x,y,z,w refer to pixels in the focal plane array.  This simple model doesn\222t take into account the radiometry of the scenario The scene amplitude A\(x,y is the perceived intensity at the sensor receiver.  This paper is not concerned with estimating reflectivity of the target or other target parameters aside from the range to the target from the LADAR receiver The second sensor model is for the 3-D LADAR system This model contains elements of the 2-D LADAR model but adds the ranging component of the sensor.  The received pulse is modeled as a Gaussian 2 2 2 
 2   
002 002\003 yxrt k k e yxA tyxp 212 212  2.2 The received pulse p  x,y,t k  is a function of the range r\(x,y as a function of position in the scene.  It is also a function of the intensity of the scene.  The observed intensity in the 3-D LADAR system contains the e ffects of diffraction.  The intensity of the scene in the 3-D LADAR system I\(z,w,t k  is 


  2 computed via equation \(2.3 004\004  212  N z N w wzi wzd wzd ewzi dP 11 1   1    1 2.4  The joint probability of all the measurements taken by the 3-D sensor d 2 u,v is computed via equation \(2.5  212\212  N x N y k k yLvxLuhtyxp tvuI 11  2.3 In this equation t k  represents the times at which the LADAR return is measured by the 3-D sensor   L is the ratio between the pixel pitch of the 3-D sensor divided by the pixel pitch of the 2-D sensor.  This parameter captures the fact that the 3-D sensor possesses inferior spatial resolution to that of the 2-D sensor  The statistical model for the measured data for both the 2-D and 3-D LADAR systems assumes the noise in the measurement is governed by Poisson statistic If al l  t h e measurements d 1 z,w are statistically independent from each other, then the joint probability of the 2-D LADAR data is given by equation \(2.4  004\004\004  212  M u M vk k tvuItvud k tvud etvuI dP k k 11 20 1 2  2    2 2.5  The parameter M is the number of pixels in each dimension of the 3-D imaging focal plane.  In this paper it is assume that the ratio of the dimension of the 2-D array over the 3-D array dimension is equal to L This makes M=N/L The specific simulations generated to provide data for this research use a simple target model shown in Figure 1.  The 3-D sensor is assumed to measure 20 samples in range.  The target has uniform reflectivity, thus uniform amplitude A\(x,y The target region is represented with  40,000 pixels in a square region that is 200 by 200 pixels in size.  The simulated data generated in this paper uses a ratio of L=5  for the number of pixels in the 2-D imaging sensor  over the number of pixels in the 3-D imaging sensor.  This ratio is consistent with the pixel pitch ratio between a typical 3-D sensor \(100 micro-meters\hat of a typical 2-D sensor that might have a \(20 micro-meter\xel pitch  Figure 1: Target board model used to generate simulated data.  The front surface in white is at range sample number 7 in the gate while the darker regions are at a range of 11 samples in the range gate  Because of the target\222s uniformity in reflectivity and the fact that it is further assumed that the illuminating laser provides a uniform pattern of illumination at the target surface, the 2D data set is also uniform.  The 3-D observation of the target contains 20 images.  Figures 2 and 3 contain examples of what some of these images look like when viewing the target board shown in Figure 1   Figure 2: Image of the target board taken with the 3-D sensor at a range equal to th at of the front surface of the target board.  Notice how the loss of spatial resolution causes some of the bars to be significantly less visible  


 3.2 Here we can identify two components of the gradient that will be denoted as Q  and Q  which are defined in equations 3.3 3.4       0 0 11 2 yLvxLuh tvuI tvud yxQ M u M v k k oo   3  Figure 3: Image of the target board taken with the 3-D sensor at a range equal to th at of the back surface of the target board.  Notice how the loss of spatial resolution causes some of the bars to be significantly less visible  The images shown in Figures 2 and 3 possess 40 by 40 pixels, while the target board is simulated with 200 by 200 pixels.  This is due to the fact that the detector array under samples the images formed on it due to its large pixel pitch Figure 4 shows an image of the target board at a range corresponding to the front of the board  Figure 4: Image of the target board at a range of 7 samples in the range gate if the 3-D sensor had a pixel pitch of 20 micro-meters.  The 100 micro-meter pixel pitch causes a significant loss of spatial resolution as seen in Figures 2 and 3  Figure 5 shows the impulse response of the imaging system that is used for the function h\(x,y The blurring associated with this impulse response is due in part due to the diffraction effects of the op tical system.  The other contribution to the blurring is the spatial extent of the pixel integrating the light on the detector array.  It is assumed in this work that the array possesses a fill factor of 100 percent  Figure 5: Spatial impulse response of the 3-D imaging sensor  3  P ROPOSED S ENSOR FUSION ALGORITHM  The proposed algorithm is derived as a gradient ascent algorithm designed to maximize the log-likelihood of the 3D LADAR data.  The log-likelihood of the 3-D data is computed from the joint probability of the data shown in equation \(2.5 212\212  212    005 005 3.5    3.3   0 0 11 yvxuh yxQ M u M v oo  212 3.4    0 oo oo ko yxQyxQ tyxp rQ  212  M u M v K k k k k tvuItvuItvud rQ 111 2 ln  3.1 In order to maximize Q with respect to the range r we will differentiate Q with the respect to p in order to compute the gradient of the log-likelihood with respect to the pulse shape as shown in equation \(3.2\ith the gradient computed, a strategy will be devised for moving in the direction of the gradient in order to ascend the loglikelihood function 1      11 2 o o M u M v k k koo yLvxLuh tvuI tvud tyxp rQ 212\212  212\212\212  005 005 


  4  Since the gradient component in the denominator appears with a negative sign in front of it in equation \(3.2\t represents the propensity for the overall gradient to be negative. At the same time the numerator, being strictly positive and appearing in \(3.2\ponent to the gradient, represents the positive part of the gradient.  By forming a ratio of the positive part of the gradient over the negative part of the gradient and using this ratio to update the function p it becomes possible to form an update for the pulse function     11 2 o o M u M v k k koo old koo new yLvxLuh tvuI tvud tyxptyxp  3.6 With the new pulse function, p, estimated from the data by conducting a fixed number of iterations of equation 3.6 100 iterations is used in the results shown in Section 4\he range is estimated from the new pulse for each pixel \(x o y o  The range estimation algorithm is accomplished via a correlation algorithm that shifts the reference waveform in equation \(2.2\d calculates the correlation between the waveform and the new pulse p new x,y,t k  as function of that shift.  The shift that produces the maximum correlation is the estim this way the range can be updated for each pixel in the array.  With the updated range it is possible to repeat the process by using equation \(3.6\o produce a new estimated pulse shape per pixel and then to recover a range from that pulse.  This process is repeated for a fixed number of iterations.  In this study this iteration is completed 20 times; however, a means of choosing a number of iterations automatically from the data is desirable 4 RESULTS  Results are obtained from the simulated data set shown in this section through a series of figures and graphs that document the improvement in range resolution and spatial resolution obtained by processing the spatially under sampled 3-D LADAR data with the proposed algorithm Figure 6 shows the recovered range of the target board as a function of position in the scene.  Figure 7 shows the recovered range as a function of position from the raw unprocessed data.  These range estimates would be what a traditional range only processing technique would produce from the data.  Figure 8 shows an image recovered at a range corresponding to the location of the first surface in the target.  It is obvious from this image that the spatial features of the scene have b een largely restored by the proposed processing technique.  Figure 9 shows the true range image that was input into the simulation to generate the simulated sensor data.  Figure 10 shows the average range error across the entire scene as a function of the number of iterations used in the reconstruction.  The average range error in the first iteration is equal to the range error obtainable on the raw low-resolution data.  The improvement in spatial resolution appears to have a correspondingly positive effect on the range error.  This is due to the fact that the ra nging algorithm can better track the range to various places in the target scene when it has higher spatial resolution data to work with  Figure 6: Reconstructed image of the target using the proposed algorithm.  Along the top row it appears that two bars in the upper right and left corners appear to be partially reconstructed.  They are completely missing in Figure 7 which is the range image recovered from the raw unprocessed 3-D LADAR data  Figure 7: Range image computed from the raw sensor data Notice how the large bars near the bottom are thinner than in Figure 6 and Figure 8, which is the true range image This distortion of the bars is due to the spatial under sampling of the detector array 212\212  


  5  Figure 8:  True range image input into the simulation.  This range map is used to compare the recovered range found using the proposed algorithm in Figure 6 to the ranges obtained from the raw data found in Figure 7  Figure 9: 2-D Image of the target at a range at the front of the target.  Notice how the bars are much more similar in size and shape to those in Figure 4 \(non-under sampled view of the scene\han those found in Figure 2  Figure 10:  Range error as a function of iteration number using the proposed algorithm.  An average range error of 0.8 meters is obtained by ranging using the raw data.  The new algorithm takes that initial set of range estimates and improves on it through multiple iterations of the proposed algorithm.  This graph shows that the range error improves nearly 10 centimeters  5 CONCLUSIONS  The proposed ranging algorithm is shown in this paper to significantly improve both the spatial and range resolution of 3-D LADAR data.  The role of the 2-D LADAR data was to make the estimation of the amplitude of the scene A\(x,y unnecessary.  With a high-resolution map of the scene amplitude it has been shown that high resolution range estimates can be recovered from the coarsely sampled sensor data.  There are still a number of  issues to be dealt with in the application of this algorithm to measured sensor data.  One of the primary algorithmic concerns is that guidelines have yet to be developed for controlling the number of iterations to use in the image reconstruction The opinions and views expressed by the author are not necessarily those of the Unite d States Air Force or the Department of Defense 6 REFERENCES   J.W  Goodm an, Fourier Optics, New York: McGraw-Hill 1968   J.W Goodm an, St at i s t i cal Opt i c s, John W i l e y and Sons 1985  J. Khoury C L. W oods, J. Lorenzo, "R esol ut i on l i m i t s  for time-of-flight laser radar Proceedings of the SPIE  vol. 5816, pp. 270-276, 2005 


  6 7 BIOGRAPHY  Stephen Cain is an Associate Professor of Electrical Engineering at the Air Force Institute of Technology.  He received his B.S.E.E. from The University of Notre Dame, his M.S.E.E. from Michigan Technological University and his Ph.D. from the University of Dayton Dr. Cain has worked at Wyle Laboratories as a Senior Scientist and ITT Aerospace as a Senior Engineer.  He is currently working on LADAR imaging systems in the area of system characterization and ranging algorithm development   


number of products or processes covered by the application, though We also found no significance with regards to cost impacts of diversity-related complexity \(neither for DBMS nor for OS\ce, propositions P3.2 could not be supported  Turning to deviation-related AA complexity we found significant differences in terms of the products covered and at a lower significance level in terms of age and processes covered between those applications that do not deviate and those that deviate from the standard OS. Non-standard-compliant applications cover more products and also more processes. However, compliant applications are slightly older \(median: 3.2 years\ than non-compliant ones \(median: 2.2 years\hese results lend support to proposition P2.3 \(when measuring compliance to OS standards\terestingly, the results also indicate that the relation between age and deviation from proposition P1.3\ reverse to the proposition: older applications are more standard compliant Measuring the deviation of applications from the standard DBMS provides a somewhat different picture: significant differences only exist for the number of user departments involved in the application. Here, we find again a surprising result the standard-conformant applications have more userdepartments involved \(median: 3\an non-compliant ones \(median: 1 department; overall median: 2 departments\. This is inverse to the original proposition. We do not find support for propositions stating that with an increase in either age \(P1.3\r business requirements complexity \(P2.3 applications also increase in their deviation from standard technology \(in terms of DBMS As far as deviation-related impacts are concerned we found only one significant result: the group of applications that did not comply to DBMS standards exhibit lower maintenance cost than the compliant applications. In contrast to the original proposition 3.3, for DBMS-standard deviation, we observe significantly lower maintenance cost for noncompliant than for compliant applications  With regards to overlap-/redundancy-related AA complexity we found some support for proposition P2.4 stating that involvement of more users leads to more overlap of applications 8 No significant differences are found regarding the age of the applications, hence, the proposition that older   8 The variables for the coverage of products and processes were not included in this test as the overlap was computed using these two variables \(implying that there is a significant relation between the respective variables applications also exhibit a higher degree of overlap was not supported \(P1.4 However, interesting results were found for the impacts of overlap-redundancy-related AA complexity: it is striking to see that the applications with a medium level of overlaps have a lower median in operations cost than those with a low or high-level of overlap, implying a non-linear, U-shaped relation between overlap and operations cost. Interestingly the same holds true for maintenance cost. Hence, the original proposition \(P3.4\that applications with higher overlap also exhibit higher IT cost is not supported  5. Discussion  Up to now, the \(practical and academic discussion of AA complexity has lacked differentiation. Various kinds of AA complexity have been lumped together with little attempt to distinguish between them. The implicit propositions underlying the respective statements and recommendations have been that all these kinds of AA complexity increase with age and business complexity, and in turn cause higher IT costs In our case, however, the result \(Table 3\dicate that only interdependency-related AA complexity behaves as assumed by consultants and researchers with respect to causes and impact: older applications and those with more complex business requirements also exhibit more interfaces \(i.e. a higher degree of interdependency\; at the same time, more interdependent applications incur higher IT costs than less interdependent applications. An explanation for this is the intuitive assumption underlying the abovementioned propositions: on the one hand, when a business grows over time, applications are added, and these have to be connected to ever more applications In former times, more point-to-point interfaces might have been used, while newer applications try to reduce interfaces e.g. by relying on central middleware. On the other hand, it might also be more difficult to maintain and operate applications with many connections because it is not easy to keep track of all the interdependencies: changing one application means that all of the other applications connected to it have to be changed, as well, thus leading to higher costs for those interdependent applications. The advice of "doing more with less seems to hold true here. In this case, consultants would probably be right in advising their clients to either maintain the growth of or reduce the number of interfaces \(e.g. by introducing Enterprise Application Integration \(EAI\yers to eliminate point-to-point interfaces Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 


However, the other types of AA complexity diversity, deviation, and overlap\ot act according to the general propositions of researchers and consultants in our case. In the following paragraphs, we would like to discuss the deviations from the propositions stated in the introduction The diversity of applications with respect to OS does not seem to be related to any of the causes or impacts we looked at in our case. Concerning DBMS diversity seems to increase with age and the number of involved users. However, no relation to causes is found for DBMS-diversity either. There are multiple possible explanations for this. Firstly, looking at the data, we do not find many applications using multiple OS or DBMS in the company we chose for our case study \(e.g. there were only 20 applications using more than one DBMS\nce, the company might have already worked on the diversity issue Nevertheless, we would have expected some difference even among these few applications in the case of a strong relation. Secondly, we find differences in the support for the propositions depending on which level of the "technology stack we look at. If we think of the different types of technologies as being organized in a layered stack with the specificity of technologies increasing towards the top, OS would probably be on a low level and DBMS would be on a higher level \(because a DBMS is less 'general purpose', i.e. more constrained in what it can be used for\e might find the propositions supported only for 'more specific technologies, which we did not test for \(such as middleware, e.g. application servers, directory services, etc.\rther research might extend the proposition to include a broader coverage of the technology stack". Thirdly, it might well be that diversity \(i.e. the sheer number of technologies used by an application\ is indeed not an important complexity criterion on the level of applications However, it might still be important on more aggregate levels, such as application landscapes: If a business product \(such as derivatives\ makes use of applications that together employ a large number of different technologies on each layer of the technology stack, this combined diversity might very well have an impact on costs. Hence, we would have to look at higher levels of the IT architecture and could formulate the respective proposition for these levels even though they are not supported on the level of applications. Comparing complexity as well as its causes and impacts on different levels of IT architecture \(e.g. applications vs. application landscapes\would enable dissecting the impact that the combination of application has in contrast to single applications \(i.e. answer what the formation of a landscape is really contributing to the overall complexity\ch research could serve to further disentangle AA complexity by not only differentiating different types of AA complexity interdependency, diversity, deviation, and overlap but also by distinguishing different levels of AA complexity \(e.g. applications vs. application landscapes Regarding deviation, we again see differences depending on which level of the technology stack we look at: applications deviating from the standard OS also cover more investment banking products and processes. The reason for this might be that different investment products and processes were historically supported by applications on different platforms Hence applications that have been extended to also cover these products and processes had to be made compatible. This would also provide an explanation for why younger applications deviate more from the standard OS than older ones: introducing more connector" applications serving multiple products and processes will bear out this observation. This explanation makes us aware of the fact that the propositions would benefit from taking into account the previous complexity management activities conducted by the bank. A mediating construct of Complexity management activities and abilities might hence improve the conceptual model for future research. For this purpose, variables would have to be identified to measure the level of activities that address the respective AA complexity type \(e.g integration measures addressing interdependency standardization efforts addressing deviation, and consolidation efforts addressing overlap/redundancy Complexity management activities might mediate both the causes-complexity relation as well as the complexity-impact relation \(see Figure 2 for a refined conceptual model   Causes of IT complexity Age Business requirements  IT complexity Interdependency Diversity of technologies Deviation from technology standards Overlap/redundancy  Impact of IT complexity Cost Agility  Complexity management activities  Figure 2: Refined Conceptual Model  Regarding DBMS-related deviation, we observed two surprising relationships in our case: those applications that deviated more from the standard DBMS involved fewer user departments and also incurred less maintenance costs. The propositions suggested the inverse relationship. An explanation might be that the IT department refused to maintain the applications that were not standard-compliant Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


These applications might hence generate "hidden" IT costs somewhere else. These costs might then not be reported as IT cost, but e.g. as marketing cost in the case of a non-compliant marketing information system. This explanation would hint at a problem with allowing exceptions to standards if the user department itself takes care of the non-compliant application. In this case, official indicators might underestimate the costs incurred by non-compliance unless the decentralized cost is being tracked as well We cannot think of an intrinsic argument that might warrant the introduction of non-compliant technologies with the aim of decreasing maintenance costs. It should be noted that this might not be the case for other impact variables \(such as agility short-term agility might very well improve if applications with non-standard technologies are added, because it would allow business requirements to be fulfilled more quickly in the short-term However, as we did not include agility as a dependent variable in our study, this claim remains conjectural Concerning overlap/redundancy of applications we only see the proposition on increased user department involvement influencing a higher degree of overlap among applications. Interestingly, the degree of overlap among applications exhibited a Ushaped relation to both operations and maintenance costs, with medium-level overlapping applications incurring the least cost. The explanation for this might be that applications need to have some level of overlap in order to be integrated and should not only serve as silos that consume even more resources to keep them connected with other applications [4, p  th e oth e r e x tre m e, t hos e application s  w i t h a  very high overlap might be so redundant that they incur double work, which increases costs as well This explanation was confirmed when we discussed the results with the bank itself. It follows that at least in this case, reducing overlap is not always warranted, as there might be an optimal level of overlap that does not necessarily coincide with the lowest level of overlap. This reminds us that not all relationships have to be linear when it comes to complexity. Especially in relation to agility, for some types of complexity, a certain amount might be essential and even beneficial, but too much might again be detrimental. While this has been investigated to some degree in non-IT-related areas 1, 2 it h a s rem a i n ed on l y a clai m s o f a r i n th e field of IT complexity \(e.g ilit y i s  hy pot h e sized to  decrease at the extreme ends of the vague construct IT complexity", hence leaving ample room for future research  6. Conclusions  We have shown that the propositions underlying current research and \(consulting\ practice do not hold true for all types of AA complexity. We disentangled the rather broad and vague AA complexity concept by proposing four types of AA complexity interdependency-, diversity, deviation- and overlap/redundancy-related AA complexity. We would recommend against referring broadly to "AA complexity" without further differentiation in future research; we advise specifying which type of AA complexity \(interdependency, diversity, deviation or overlap\ really meant. Being more precise will enable us to add further to the body of knowledge in this highly relevant area of IS research. This proved to hold especially true as we found that the general propositions were only valid for interdependencyrelated AA complexity and not for the other types of AA complexity. Future research should clearly be based on multiple case studies rather than just on a single case study in order to allow for generalization As we found support for the propositions for interdependency-related AA complexity, we propose honing in on this type of AA complexity further, e.g by differentiating the degree or type of interdependency between applications \(see footnote 1\or example, a question of interest could be whether batch-oriented interfaces are more costly to maintain than online interfaces or interfaces facilitated by middleware We also pointed to potential differences for similar types of IT complexity on different architectural levels \(such as for deviation and diversity on the level of applications vs. deviation and diversity on the level of application landscapes which might warrant further research Based on the interpretation of our research findings, we also proposed several adjustments to the current propositions by referring to the technologystack levels for diversity and deviation, and by introducing AA complexity management activities as a mediating construct We hope that this research will serve as a foundation for future research in this area, which has so far in its entangled form been marked by assumptions rather than the subject of actual research       Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


7. References  1 A s hk e n a s R Simplicity-Minded Management Harvard Business Review, 2007 85 12\: p. 101-109 2 G o ttf re ds on, M. a nd K  A s pina ll Innovation vs Complexity Harvard Business Review, 2005 83 11\6271  S can t l e b u r y  S   et al    From IT Complexity to Commonality: Making Your Business More Nimble in Opportunities for Action in Information Technology 2004 The Boston Consulting Group 4 Ross J.W  P. W e ill, a n d D  C. Ro be rtso n Enterprise Architecture As Strategy 2006, Boston, MA: Harvard Business Scholl Press 5 Ma tte rn F S  Sc h nw  l de r, a n d W  Ste i n  Fighting Complexity in IT McKinsey Quarterly, 2003\(1\. 57-65 6 Child J  Parkinson's Progress: Accounting for the Number of Specialists in Organizations Administrative Science Quarterly, 1973 18 3\: p. 328 7 G e ll-Ma nn, M What is complexity Complexity, 1995 1 1\: p. 16-19 8 P a rk  R.E   Software size measurement: a framework for coutning source statements 1992, Software Engineering Institute, Pittsburg 9 Mc Ca be T  A complexity measure in Proceedings of Int'l Conf. Sofrware Engineering 1976 10  Mc Ca be T  a n d D   Sha r o n  Cyclomatic Complexity and the Year 2000 IEEE Software, 1996 13 3\ p. 115 1 Halstead  M  H   Elements of Software Science 1977 New York: Elsevier 12 r te ta B.M. a n d R  E. G i a c h e tti A measure of agility as the complexity of the enterprise system Robotics and Computer-Integrated Manufacturing, 2004 20 p. 495-503 1 S h p i l b erg D  et al    Avoiding the Alignment Trap in Information Technology. \(Cover story MIT Sloan Management Review, 2007 49 1\: p. 51-58 14 Bo h, W  F. a nd D. Y e llin Using Enterprise Architecture Standards in Managing Information Technology Journal of Management Information Systems 2007 23 3\: p. 163-207 15 Chil d P  e t a l  SMR Forum: The Management of Complexity Sloan Management Review, 1991 33 1\ p 73-80 16 K a is le r, S.H F  A r m our, a nd M. Va l i v u lla h  Enterprise Architecting: Critical Problems in Proceedings of the 38th Annual Hawaii International Conference on System Sciences \(HICSS'05 2005: Hawaii 17 X i a   F  Module Coupling: A Design Metric in AsiaPacific Software Engineering Conference \(APSEC'96  1996 18 K u bic e k H  The Organization Gap in Large-scale EDI systems in Scientific Research on EDI 1992: Alphen aan den Rijn 1 Hi tz M  and B  M o n t azeri  Measuring Coupling and Cohesion in Object-Oriented Systems in Proc. Int Symposium on Applied Corporate Computing 1995 2 Z ach m a n  J A    A framework for information systems architecture IBM Systems Journal, 1987 26 3\: p. 276292 2 J o hn so n G    Researchers on Complexity Ponder What It's All About in New York Times 1997. p. B4 22 Si ng h, K  The impact of technological complexity and interfirm cooperation on business survival Academy of Management Journal, 1997 40 2\: p. 339 23 A x e l rod, R  a nd M.D  C ohe n Harnessing Complexity organizational implications of a scientific frontier 2000 New York: Basic Books 24 Ke lly S. a nd M.A   A llison The Complexity Advantage 1999, New York: McGraw-Hill 25 S h e r m a n, H  a nd R Sc hultz  Open Boundaries creating business innovation through complexity 1998 New York: Perseus Books 26 Ca m pbe ll, D  J  Task Complexity: A Review and Analysis Academy of Management Review, 1988 13 1 p. 40-52 2 L e e O K  et al   IT-Enabled Organizational Agility and Firms' Sustainable Competitive Advantage in Proceedings of the Twenty Eighth International Conference on Information Systems 2007. Montral 28 r h out M  v E W a a r ts, a nd J.v   Hille g e rsbe rg   Change factors requiring agility and implications for IT  European Journal of Information Systems, 2006 15 p 132-145 29 W e bs te r, J  a n d R.T  W a ts on Analyzing the Past to Prepare for the Future: Writing a Literature Review MIS Quarterly, 2002 26 2\ p. xiii-xxiii 30 A I S MIS Journal Rankings 2007  [cited 2007 18 April 2007 v a ila ble  f r o m  http://www.isworld.org/csaunders/rankings.htm  31 Pruit t  S  Gartner issues 10 CIO resolutions for 2005  in InfoWorld 2004 32  Po rter M.E  an d  V  E  Mi lla r How information gives you competitive advantage Harvard Business Review 1985 63 4\: p. 149-160 33 T a ba c hnik  B.G  a nd L  S. Fide ll Using multivariate statistics 5th ed. 2007, Boston: Pearson 34 Marten so n s   A  Producing and Consuming Agility in Agile Information Systems - Conceptualization Construction, and Management K.C. DeSouza, Editor 2006, Butterworth-Heinemann. p. 41-51   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


Appendix: Detailed description of results  We structured the results of the analyses described in Section 3 according to the two different types of propositions, namely those related to the causes and those related to the impacts of AA complexity  A.1. Results from analyzing causes of AA complexity  Causes of interdependency-related AA complexity The Kruskal-Wallis test revealed a statistically significant difference in the age and number of user departments covered for the three different interdependency groups of applications \(see Table 4 for n and p values\he groups of applications with more interfaces record a higher median score in terms of age as well in terms of user departments involved \(see also  Table 4\The same is true at a lower significance level \(0.05 level\or the number of IB products and processes: the groups of applications with more interfaces also cover more products and processes \(see Table 4 for for n and p values\ce, we do find support for proposition P1.1, which states that the older an application, the higher its interdependency-complexity it is. We also find support for proposition P2.1, which asserts that the higher the complexity of business requirements the higher the interdependency-complexity of applications is  Causes of diversity-related AA complexity The Mann-Whitney U test revealed no significant difference in either the age or the number of user departments involved, or in the number of IB products or processes covered for applications with one OS versus applications with more than one OS see Table 5\. Hence, we do not find support for propositions P1.2 and P2.2, which state that \(OSrelated\ersity complexity is caused by age or business requirements-complexity We obtained similar results when measuring diversity-complexity in terms of the number of DBMS used by an application. The only significant difference at the .01 level was in the age of applications: The group of applications with more than one DBMS was also older on average than those applications with only one DBMS. This supports proposition P1.2, which states that the older the applications get, the more diverse they become \(in terms of DBMS\At a lower significance level \(.05 we see a significant difference in terms of the amount of user departments involved for those applications with one DBMS versus those with more than one DBMS: the former group was found to involve fewer user departments than the latter. This supports P2.2 which asserts that the more complex the business requirements are, the more diverse the applications become \(in terms of DBMS\. However, we do not find support for this proposition when looking at the number of products or processes covered by the application, though. See Table 5 for the detailed results of the analyses  Causes of deviation-related AA complexity A Mann-Whitney U Test \(Table 6\ealed significant differences in terms of the products covered and at a lower significance level in terms of age and processes covered between those applications that do not deviate from the standard OS and those that do. Nonstandard applications cover more products and also more processes \(median: 1 vs. 2\wever, standardcompliant applications at this firm are slightly older median: 3.2 years\ than the non-compliant applications \(median: 2.2 years\hese results lend support to proposition P2.3, which states that the higher the complexity of the business requirements in terms of product and process coverage\is, the more deviation from the standard \(in terms OS\s found in applications. Interestingly, the results also indicate that the relation between age and deviation from standard technology \(in terms of OS proposition P1.3\tradicts the proposition: older applications are more standard compliant Measuring the deviation of an application from the standard DBMS provides a somewhat different picture. A Mann-Whitney U test \(Table 6\ealed significant differences only for the number of user departments involved in the application \(at .05 level of significance\ain we find a surprising result in that the standard-conformant applications involve more user-departments \(median: 3\han the non-compliant ones \(median: 1 department; overall median: 2 departments\. We do not find support for the propositions stating that an increase in either age P1.3\siness requirements complexity \(P2.3 causes an increase in applications' deviation from standard technology \(in terms of DBMS\e see indications for an inverse relationship between the number of user departments involved and the deviation of applications from standards in terms of DBMS technology  Causes of overlap-/redundancy-related AA complexity A Kruskal-Wallis \(Table 7\ealed significant differences in terms of the number of user departments involved for those applications that have a low to medium overlap and those with a significant overlap \(the first two groups involve one user department and the last group involved 2.5 user Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 11 


departments on average\ignificant differences were found regarding the age of the applications. The variables for the coverage of the products and processes were not included in this test because the overlap was computed using these two variables implying that there is a significant relation between the respective variables\his lends some support to proposition P2.4 stating that involvement of more users leads to greater overlap of applications. The proposition that older applications also exhibit a higher degree of overlap was not supported \(P1.4  A.2. Results from analyzing impacts of AA complexity  Impacts of interdependency-related AA complexity A Kruskal-Wallis test \(Table 4\ealed a statistically significant difference in operations cost as well as maintenance cost across the three different interdependency-groups of applications. The more interdependent group \(i.e. applications with 3-7 or with 8 or more interfaces\igher median of operations \(Md=119,000 and 363,000 EUR respectively\ and maintenance costs \(Md=326,000 and 506,000 EUR respectively\ than the less interdependent group \(fewer than 3 interfaces applications \(Md=52,000 EUR operations costs and 64,000 EUR maintenance costs\. This supports the proposition \(P3.1\ that more interdependent applications also incur higher IT \(operations and maintenance\ts  Impacts of diversity-related AA complexity   Regarding OS-related diversity, a Mann-Whitney U test \(Table 5\howed no significant difference between the operations costs of more \(Md=93,890 n=105\d less diverse applications \(Md=131,540 n=27\09.5, z=-1.174, p=.24. The same holds for maintenance costs \(Md=166,400; n=121 vs Md=116,900; n =31\782, z=-.43; p=.668 To measure DBMS-related diversity, a MannWhitney U test was conducted \(Table 5\d revealed no significance difference in operations costs of more Md=129,985; n=85\d less diverse applications Md=154,777; n=16\5, z=-.237, p=.813. The same holds for maintenance costs \(Md=210,500 n=98 vs. Md=245,700; n=19\36, z=-.704 p=.481. Hence, the proposition \(P3.2\ that diversityrelated AA complexity leads to higher IT costs is not supported  Impacts of deviation-related AA complexity   Regarding deviation from the standard OS, a MannWhitney U test \(Table 6\revealed no significant difference between the maintenance costs and operations costs for standard-compliant \(Md=83,581 n=94 for operations cost and Md=148,300; n=113 for maintenance cost\nd non-compliant applications Md=180,147; n=38 for operations cost and Md=166,400; n=39 for maintenance cost z=-1.921, p=.055 for operations cost and U=2116 z=-.371, p=.711 for maintenance cost Concerning deviation from the standard DBMS, a Mann-Whitney U test \(Table 6\ealed a significant difference between maintenance costs for standardcompliant \(Md=373,100; n=59\d non-standardcompliant applications \(Md=170,200; n=58 U=1303, z=-2.231, p=.026. It is remarkable that the non-compliant applications had lower maintenance costs than the compliant applications. The difference between operations costs for compliant Md=109,978; n=51\d non-compliant applications Md=180,147; n=50\s not significant, U=1196.5 z=-.533, p=.594 Thus, the proposition \(P3.3\at application that deviate from technology standards incur higher IT costs is not supported. In contrast, for DBMSstandard deviation, we observed significantly lower maintenance cost for non-compliant than for compliant applications  Impacts of overlap/redundancy-related AA complexity a Kruskal-Wallis test \(Table 7\ showed significant differences in operations costs across the applications with a low \(less than 34 overlaps Md=90,442; n=37\edium \(35-79 overlaps Md=55,770; n=56\ and high level of overlap/redundancy \(more than 80 overlaps Md=129,985; n=61 6.862, p=.032. It is striking to see that the applications with medium overlaps have a lower median operations cost than those with a low or high-level of overlap, implying a non-linear U-shape relation between overlap and operations cost. Interestingly, the same holds true for maintenance cost. Applications with a low degree of overlap exhibited a median maintenance cost of 96,600 \(n=59\, those with a medium level of overlap incurred a median of 81,300 \(n=58\d highly overlapping applications a median of 248,700 \(n=67 9.791, p=.007. Hence, the proposition \(P3.4\at applications with a greater degree of overlap also exhibit higher IT costs is not supported  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 12 


Interdependency Number of interfaces \(gp3_y4_sum_intf  2 22 2 22 Df 047 000 000 016 000 000 Asymp. Sig 6.134 22.298 20.875 8.331 17.018 18.917 N Median 46 45 43 46 27 37 41 41 41 41 36 38 2 intf 8 intf 1.0000 2.2000 1.0000 1.0000 52.3020 64.4000 2.0000 7.8000 4.0000 2.0000 363.9885 506.3500 2 intf 8 intf Mean rank 55.89 47.03 44.13 56.13 33.67 37.77 2 intf 73.46 83.40 76.61 76.21 60.89 68.89 8 intf 40 39 33 40 30 34 3 7 intf 1.0000 3.2000 2.0000 1.0000 119.3940 326.2000 3 7 intf 63.63 59.97 56.50 60.54 42.33 58.22 3 7 intf  Business requirements Causes of complexity Impacts of complexity  No. of IB products covered y24_#_IB_ bankprod Application age y14_age No. of user departments y19_#_ user_dpts No. of IB process covered \(y26_ IB_bankproc Operations cost \(y36_ ops_cost Maintenance cost \(y37_ Maint_cost 2  Table 4: Results of Kruskal-Wallis test for causes and impacts of interdependency-related AA complexity    Diversity Number of OS/DBMS used by an application Operating systems \(gp2_y7a_OS DBMS \(gp2_y8a_DBMS 2,726.500 18,302.500 1.503 133 1,087.500 1,318.500 1.362 173 3,308.000 902 367 543.000 7,446.000 3.925 000 2,642.000 2,536.000 3,202.000 855 392 757.500 7,198.500 2.187 029 2,884.500 18,460.500 965 335 1,216.500 1,447.500 583 560 1,209.500 6,774.500 1,174 240 654.500 790.500 237 813 1,782.000 9,163.000 430 668 836.000 5,687.000 704 481 Wilcoxon W Z Asymp. Sig N Median Mean rank Mann-Whitney U Wilcoxon W Z Asymp. Sig Mann-Whitney U 21 1.0000 125 1.0000 75.30 62.79 20 8.0500 117 2.2000 63.64 100.35 19 3.0000 113 1.0000 63.70 83.13 21 1.0000 125 1.0000 74.27 68.93 16 154.7770 85 129.9850 51.30 49.41 19 245.7000 98 210.5000 58.03 64.00 N Median 1 DBMS 2 DBMS 1 DBMS 2 DBMS Mean rank 1 DBMS 2 DBMS Data not shown as no significance found  Business requirements Causes of complexity Impacts of complexity  No. of IB products covered \(y24_#_ IB_bankprod Application age y14_age No. of user departments y19_#_user_dpts No. of IB process covered \(y26_#_ IB_bankproc Operations cost \(y36_ ops_cost Maintenance cost \(y37_ Maint_cost  Table 5: Results of Mann-Whitney test for causes and impacts of diversity-related AA complexity  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 13 


 Deviation degree of deviation from standard OS/DBMS Operating systems \(gp2_y7b_OS_Dev DBMS \(gp2_y8b_DBMS_Dev 2,780.000 16,146.000 3.681 000 163 49 1.0000 2.0000 99.06 131.27 2,611.000 5,312.000 227 820 73 1.0000 73 1.0000 72.77 74.23 3,970.500 2.107 035 151 47 3.2000 2.2000 104.18 84.48 2,102.000 4,587.000 1.074 283 70 2.2000 67 3.5000 72.63 2,842.500 65.53 2,902.000 4,030.000 1.509 131 143 47 2.0000 1.0000 98.71 85.74 1,551.500 3,829.500 3.041 002 67 1.0000 65 3.0000 76.13 57.16 3,103.500 16,469.500 2.698 007 163 49 1.0000 2.0000 101.04 124.66 2,610.000 5,311.000 232 816 73 1.0000 73 1.0000 72.75 74.25 1,404.000 5,869.000 1.921 055 94 38 83.5815 180.1470 62.44 76.55 1,196.500 2,522.500 533 594 50 180.1470 51 109.9780 49.46 52.57 2,116.000 8,557.000 371 711 113 39 148.3000 166.4000 75.73 78.74 1,303.000 3,014.000 2.231 026 58 170.2000 59 373.1000 65.92 51.97  Wilcoxon W Z Asymp. Sig N Median No deviation \(1.0 Deviation 1.1 No deviation \(1.0 Deviation 1.1 Mean rank Mann-Whitney U No deviation \(1.0 Deviation 1.1 Wilcoxon W Z Asymp. Sig N Median No deviation \(1.0 Deviation \(>1.0 No deviation \(1.0 Deviation \(>1.0 Mean rank Mann-Whitney U No deviation \(1.0 Deviation \(>1.0  Business requirements Causes of complexity Impacts of complexity  No. of IB products covered \(y24_#_ IB_bankprod Application age y14_age No. of user departments y19_#_user_dpts No. of IB process covered \(y26_#_ IB_bankproc Operations cost \(y36_ ops_cost Maintenance cost \(y37_ Maint_cost  Table 6: Results of Mann-Whitney test for cause s and impacts of deviation-related AA complexity  Overlap/redundancy \(gp3_overlap_count 22 22 Df 216 001 032 007 Asymp. Sig 3.066 13.139 6.862 9.791 2 N Median 90 81 37 59 79 78 61 67 34 80 2.2000 1.0000 90.4420 96.6000 2.6000 2.5000 129.9850 248.7000 34 80 Mean rank 129.29 113.80 82.76 87.25 34 137.18 144.50 85.66 108.10 80 86 85 56 58 35 79 2.2000 1.0000 55.7705 81.3000 35 79 118.22 110.61 65.14 79.82 35 79  Not applicable Not applicable Business requirements Causes of complexity Impacts of complexity  No. of IB products covered \(y24_ _IB_bankprod Application age y14_age No. of user departments y19_#_ user_dpts No. of IB process covered \(y26_ _IB_bankproc Operations cost \(y36_ ops_cost Maintenance cost \(y37_ Maint_cost  Table 7: Results of Kruskal-Wallis test for causes and impacts of overlap-/redundancy-related AA complexity Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 14 


  15 R EFERENCES    http://www.w3.org/XML/Schema   eb Orchestration with BPEL\224 http://www.idealliance.org/pa pers/dx_xml03 papers/0406-01/04-06-01.html  Hi bernat e hom e page www.hibernate.org   Al l a rd, Dan and Hut c herson, Joe, \223C om m uni cat i ons Across Complex Space Networks\224, IEEE Aerospace Conference, March 1-8, 2008  W e b Servi ce Defi ni t i on Language http://www.w3.org/TR/wsdl   B a uer, C h ri st i a n and Ki ng Javi n Java Persi s t e nce for Hibernate, New York: Manning Publications, 2007 7] \223Software Agents An Overview\224 http://www.sce.carleton.ca/netm anage/docs/AgentsOverview ao.html  e thodology.org  http://www.riaspot.com artic les/entry/What-is-Ajax  http://www.json.org 11 h ttp to m cat.ap ach e.o r g   12] http://java.sun com/products/servlet  http://www.w3.org/Sty le/CSS    B IOGRAPHY  Dan Allard has worked as a software engineer at the Jet Propulsion Laboratory for the past 17 years.   He currently leads the development of core JPL accountability systems applications and infrastructure Other recent work includes the development of a message-based ground data system for the Mars Science Laboratory as well as research and development of ontologybased distributed communications     Dr. Charles D \(Chad\ards, Jr received his A.B degree in Physics from Princeton University in 1979 and his Ph.D. in Physics from the Calif ornia Institute of Technology in 1984.  Since then he has worked at NASA\222s Jet Propulsion Laboratory, where he currently serves as Manager of the Mars Network Office and as Chief Telecommunications Engineer for the Mars Exploration Program, leading the development of a dedicated orbiting infrastructure at Mars providing essential telecommunications and navi gation capabilities in support of Mars exploration.  Prior to that he managed the Telecommunications and Mission Operations Technology Office, overseeing a broad program of research and technology development in support of NASA\222s unique capabilities in deep space communications and mission operations.  Earlier in his career, Dr. Edwards worked in the Tracking Systems and Applications section at JPL where he carried out research on novel new radio tracking techniques in support of deep space navigation, planetary science, and radio astronomy  


  16  


Thank you Questions 


 18  Astronautical Congress Valencia, 2006 27  Bu reau  In tern atio n a l d e s Po ids et Mesures. \(2 008  August\SI Base Units. [On http://www.bipm.org/en/si/base_units   B IOGRAPHY  Author, Karl Strauss, has been employed by the Jet Propulsion Laboratory for over 22 years.  He has been in the Avionics Section from day One.  He is considered JPL\222s memory technology expert with projects ranging from hand-woven core memory \(for another employer\o high capacity solid state designs.  He managed the development of NASA\222s first Solid State Recorder, a DRAM-based 2 Gb design currently in use by the Cassini mission to Satu rn and the Chandra X-Ray observatory in Earth Orbit.  Karl was the founder, and seven-time chair of the IEEE NonVolatile Memory Technology Symposium, NVMTS, deciding that the various symposia conducted until then were too focused on one technology.  Karl is a Senior IEEE member and is active in the Nuclear and Plasma Scie nce Society, the Electron Device Society and the Aerospace Electronic Systems Society Karl is also an active member of SAE Karl thanks his wonderful wife of 28 years, Janet, for raising a spectacular family: three sons, Justin, Jeremy Jonathan.  Karl\222s passion is trains and is developing a model railroad based upon a four-day rail journey across Australia\222s Northern Outback   


 19 Bollobs, B. 2001. Random Graphs. Cambridge University Press; 2nd edition. 500pp  Cawley, G. C., B. L. C. Talbot, G. J. Janacek, and M. W Peck. 2006. Sparse Bayesian Ke rnel Survival Analysis for Modeling the Growth Domain of Microbial Pathogens  Chiang C. L. 1960. A stochastic study of life tables and its applications: I. Probability distribution of the biometric functions. Biometrics, 16:618-635  Cox,  D. R. 1972. Regression models and life tables J. R Stat. Soc. Ser. B 34:184-220  Cox, D. R. 1975.   Partial likelihood Biometrika 62:269276  Cox, D. R. & D. Oakes. 1984 Analysis of Survival Data  Chapman & Hall. London  Cressie, N. A. 1993 Statistics for Spatial Data John Wiley Sons. 900pp  Duchesne, T. 2005. Regression models for reliability given the usage accumulation history. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty Y. Armijo. pp.29-40. World Scientific, New Jersey  Eleuteri, A., R. Tagliaferri, L. Milano, G. Sansone, D D'Agostino, S. De Placido,  M. Laurentiis. 2003.  Survival analysis and neural networks. Proceedings of the International Joint Conference on Neural Networks, Vol. 4 20-24 July 2003 Page\(s\:2631 - 2636  Ellison, E., L. Linger, and M Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013, 1997  Fleming, T. R. & D. P. Harrington. 1991. Counting process and survival analysis. John Wiley & Sons. 429pp  Graver, J. and M. Sobel 2005. You may rely on the Reliability Polynomial for much more than you might think Communications in Statistics: Theory and Methods  34\(6\1411-1422  Graves, T. and M. Hamada. 2005. Bayesian methods for assessing system reliability: models and computation. In Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson, et al. pp.41-53  Grimmett, G. 2006 The Random-Cluster Model Springer  Grimmett, G. 1999 Percolation Springer  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis.  Springer. 481pp  Jin Z. 2005. Non-proportional semi-parametric regression models for censored data. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.279-292 World Scientific  Kalbfleisch, J. D. & R. L. Prentice. 1980 The Statistical Analysis of Failure Time Data John Wiley & Sons.  New York. 1980  Kalbfleisch, J. D. &  R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data.  Wiley-InterScience, 2nd ed 462pp  Lisboa, P. J. G. and H. Wong. 2001. Are neural networks best used to help logistic regression? Proceedings of International Joint Conference on Neural Networks, IJCNN 01. Volume 4, 15-19,  July 2001. Page\(s\:2472 - 2477 vol.4  Kauffman, R. J. and B. Wang. 2002. Duration in the Digital Economy. Proceedings of th e 36th Hawaii International Conference on System Sciences \(HICSS03\ Jan 2003  Kaplan, E. L. & P.  Meier.  1958.  Nonparametric estimation from incomplete observations J. Amer. Statist. Assoc  53:457-481  Klein, J. P. and P. K. Goel 1992. Survival Analysis: State of the Art.  Kluwer Academic Publishes. 450pp  Klein, J. P. and  M. L Moeschberger. 20 03. Survival analysis techniques for ce nsored and truncated data Springer  Krings, A. and Z. S. Ma. 2006.  "Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks MILCOM 2006, Military Communications Conference, 2325 October, 7 pages, 2006  Krings, A. W. 2008.  Survivable Systems.  in Information Assurance: Dependability and Security in Networked Systems Yi Qian, James Joshi, David Tipper, and Prashant Krishnamurthy, Morgan Kaufmann Publishers. \(in press  Lawless, J. F. 1982. Statistical models and methods for lifetime data.  John Wiley & Sons. 579pp  Lawless, J. F. 2003. Statistical models and methods for lifetime data.  John Wiley & Sons. 2nd ed. 630pp  Li, M. and P. Vitanyi. 1997. Introduction to  Kolmogorov Complexity and Its Applications. 2nd ed, Springer  Ma, Z. S. 1997.  Survival analysis and demography of Russian wheat aphid populations.  Ph.D dissertation, 307pp University of Idaho Moscow, Idaho, USA 


 20 Ma, Z. S., and E. J. Bechinski. 2008.  Developmental and Phenological Modeling of Russian Wheat Aphid Annals of Entomological Soc. Am In press  Ma, Z. S. and A. W. Krings. 2008a. The Competing Risks Analysis Approach to Reliability Survivability, and Prognostics and Health Management.  The 2008 IEEEAIAA AeroSpace Conference. BigSky, Montana, March 18, 2008. \(In Press, in the same volume  Ma, Z. S. and A. W. Krings 2008b. Multivariate Survival Analysis \(I\e Shared Frailty Approaches to Reliability and Dependence Modeling. The 2008 IEEE-AIAA AeroSpace Conference. BigSky Montana, March 1-8, 2008 In Press, in the same volume  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(II\ Multi-State Models in Biomedicine and Engineering Reliability. 2008 IEEE International Conference on Biomedical Engineering and Informatics BMEI 2008\27th-30th, 2008 Accepted   Mani, R., J. Drew, A. Betz, P. Datta. 1999. Statistics and Data Mining Techniques for Lifetime Value Modeling ACM Conf. on Knowledge Discovery and Data Mining  Mazzuchi, T. A., R Soyer., and R. V Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Meeker, W. Q. and L. A. Escobar. 1998. Statistical Methods for Reliability Data. Wiley-Interscience  Munson, J. C. 2003. Software Engineering Measurement Auerbach Publications  Nelson, W. 1969. Hazard plotting for incomplete failure data J. Qual. Tech 1:27-52  Nakagawa, T. 2006.  Shock and Damage Models in Reliability Theory. Springer  Osborn, B. 2005. Leveraging remote diagnostics data for predictive maintenance.   In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp. 353-363  Pena, E. A. and E. H. Slate. 2005. Dynamic modeling in reliability and survival analysis. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.55-71  Reineke, D. M., E. A. Pohl, and W. P. Murdock. 1998 Survival analysis and maintenance policies for a series system, with highly censore d data.  1998 Proceedings Annual Reliability and Maintainability Symposium. pp 182-188  Schabenberger, O. and C. A. Gotway. 2005. Statistical Methods for Spatial Data Analysis.  Chapman & Hall/CRC  Severini, T. A. 2000. Likelihood methods in statistics Oxford University Press  Shooman, M. L. 2002. Reliability of Computer Systems and Networks: Fault Tolerance, Analysis and Design. John Wiley and Sons. 551pp  Stillman, R. H. and M. S. Mack isack, B. Sharp, and C. Lee 1995. Case studies in survival analysis of overhead line components. IEE Conferen ce of the Reliability and Distribution Equipment. March 29-31, 1995. Conference Publication No. 406. pp210-215  Therneau, T. and P. Grambsch. 2000 Modeling Survival Data: Extending the Cox Model Springer  Wilson, A.  N. Limnios, S Kelly-McNulty, Y. Armijo 2005. Modern Statistical and Mathematical Methods in Reliability. World Scientific, New Jersey  Xie, M. 1991. Software Reliability Modeling. World Scientific Press    B IOGRAPHY   Zhanshan \(Sam\ Ma holds a Ph.D. in Entomology and is a Ph.D. candidate in Computer Science at the University of Idaho. He has published approximately 30 journal and 30 conference papers, mainly in the former field.  Prior to his recent return to academia, he worked as senior network/software engineers in software industry.  His current research interests include reliability and survivability of wireless sensor networks, fault tolerance survival analysis, evolutionary game theory, evolutionary computation and bioinformatics  Axel W. Krings is a professor of Computer Science at the University of Idaho.  He received his Ph.D. \(1993\ and M.S 1991\ degrees in Computer Science from the University of Nebraska - Lincoln, and his M.S. \(1982\ in Electrical Engineering from the FH-Aachen, Germany.  Dr. Krings has published extensively in the area of Computer Network Survivability, Security, Fault-Tolerance and Realtime Scheduling. In 2004/2005 he was a visiting professor at the Institut d'Informatique et Mathmatiques Appliques de Grenoble, at the Institut National Polytechnique de Grenoble, France.  His work has been funded by DoE/INL DoT/NIATT, DoD/OST and NIST 


