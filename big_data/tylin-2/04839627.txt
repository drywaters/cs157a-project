 1  Extending OSSE Beyond Numerical Weather Prediction to New Areas in Earth Observing Science  Charles D. Norton, Annmarie Eldering, Michael Turmon, and Jay Parker  Jet Propulsion Laboratory  California Institute of Technology  4800 Oak Grove Drive, Pasadena, CA 91109 8099  818 393 3920   Charles.D.Norton  Annmarie.Eldering  Michael.Turmon Jay.Parker}@jpl.nasa.gov   Abstract 321 An Observing System Simulation Experiment OSSE briefly stated is a computational system designed to quantitatively assess the impact of pr oposed scientific observations OSSEs allow one to examine how well specific science objectives can be met within a controlled 
environment where one can simulate the quality of data expected based on observation characteristics instrument parameters, data retrieval methods, associated uncertainties errors and trades among design constraints The numerical weather prediction community NWP has developed and utilized OSSEs to understand the impact of instrument designs and new measurements on numerical fo recasts over the last 40 years Now there is a growing interest in applying OSSEs as a mechanism for systematic analysis and science evaluation for future observations of interest to the Earth science community Examples include precise measurements of ea rth surface deformation ice dynamics ecosystem structure and atmospheric chemistry In this paper we introduce OSSE and describe the benefits and 
impact of the approach along with current work and future plans to apply OSSE for science analysis in vario us Earth science disciplines  1  2   T ABLE OF C ONTENTS  1  I NTRODUCTION    1  2  OSSE FOR S URFACE D EFORMATION S CIENCE  3  3  OSSE FOR A TMOSPHERIC C OMPOSITION S CIENCE  4  4  OSSE FOR I MAGING S PECTROSCOPY S CIENCE  7  5  C ONCLUDING 
C OMMENTS   8  6  A CKNOWLEDGEMENT   8  R EFERENCES    9  B IOGRAPHY    9  1  I NTRODUCTION  An Observing System Simulation Exp eriment OSSE is a software analysis system designed to quantitatively assess the impact of proposed scientific observations via high fidelity models OSSE\325s have been used in the numerical weather prediction community since the 1960s to perform 
design trades in mission and instrument parameters to satisfy science requirements 1   OSSE s are  essential to quantifying the various complex trades involved in maximizing and optimi zing how science  1    1 978 1 4244 2622 5/09/$25.00 \2512009 IEEE  2 IEEEAC paper #1107, Version 1, Updated Oct 18, 2008  requirements can be met given multiple instruments and science objectives Without a quantitative end to end sensitivity analysis coupled to the engineering aspects of the mission design, it\325s very difficult to know precisely how design d ecisions affect what can be achieved across various  disciplines In the Earth sciences given  NASA 
s  strategic needs for systematic analysis to evaluate science return for National Research Council \(NRC Decadal Survey missions  2   d e t e r m i n i n g  h o w  O S S E s  c a n be extended to new areas of observing science will impact planning and decision making for these missions  There are many components to an OSSE including orbit analyses sampling methods instrument models inversion methods statistical analyses and pe rformance sensitivity and uncertainty analysis techniques  Prior to examining these issues in the context of examples in the Earth sciences we set the current context and review the past development of this area  Historical Review of OSSE Development  The  National Resea rch Council\325s Decadal Survey 2 
  h a s  identified the Deformation Ecosystems and Dynamics of Ice DESDynI mission as the largest tier 1 mission concept ranging from 600 900 million with an estimated cost of 700 million dollars DESDynI w ill carry two instruments an L band interferometric synthetic aperture RADAR InSAR and a multibeam infrared LIDAR providing observational data on surface deformation terrestrial vegetation structure, and ice dynamics. These measurements will help solv e  science questions about the solid Earth ecosystems and climate relevant to NASA strategic objectives in Earth system science and societal needs   The Decadal Survey identifies high level mission goals from which the DESDynI Science Study Group has draft 
ed relevant science objectives suitable observations and specific measurements to establish the science requirements matrix Table 1 a  subset of that matrix shows  the flow down from high level goals to specific mea surements   Table 1  shows a variety of  requirements that must be met simultaneously across the solid Earth vegetation and ice dynamics disciplines  for DESDynI  Traditionally mission and science teams would find a point design solution to satisfy the measurement requirements but given multi ple disciplines and instruments how can one quantify how 


 2  design decisions will impact science objectives  in each discipline Can simulation reduce the risk in such decision making when evaluating the performance of proposed observing systems that will dr ive model forecasts used in decision support and model improvement   These kinds of questions are not new, but C. Newton 3  in 1954 is believed to be the first to suggest a systematic approach to resolving them He was interested in the placement of obser vation stations for improving numerical weather forecasts. Given the futility in adding and removing physical stations in an optimal way he suggested the alternative of experimenting with forecasts built partly from fictitious observations Although not mu ch work was performed over the next  10 years Alaka and Lewis 4  performed additional studies and identified the important role of error analysis when such approaches were used Sufficient benefit was seen that the Global Atmospheric Research Program GARP  was initiated as 322a national effort in computer simulation study of the predictive consequences of proposed observations systems to be known as 324OSSE\325\323 Over the following years numerous others performed studies to assess the impact of space based remote  sensing measurements on numerical weather prediction NWP Indeed by the mid 1980s it was concluded that the National Meteorological Center NMC should carry out simulation experiments to assess observational errors and performance characteristics of W INDSAT a proposed LIDAR instrument to measure the global wind field. By 1983, NMC, the Goddard Laboratory for Atmospheric Science \(GLAS\, and the European Centre for Medium Range Weather Forecasts \(ECMWF\ formed a new cooperative program for conducting OS SEs under the agreement that even given known uncertainties 322OSSEs offered the only available method for providing decision makers with the information necessary to determine whether a proposed observing system would be worth the investment\323 1    Understan ding the State of the Art  The state of the art work in OSSE analysis currently remains within the NWP community where Bob Atlas, and collaborators played a significant role in developing this capability at the Goddard Laboratory for Atmospheres GLA Thi s group and others demonstrated three key objectives of any complete OSSE system  1  A quantitative assessment of the potential impact of proposed observing systems on Earth system science data assimilation, and numerical forecasts \(in this case weather   2  Evaluation of new methodologies for the processing and assimilation of space based data  3  Evaluation of the tradeoffs in design and configuration of proposed observing systems e.g coverage resolution, accuracy, and data redundancy   This work has had a si gnificant impact on current weather satellite design and data processing, and OSSE analyses are on going for future systems such as the National Polar orbiting Operational Environment Satellite System NPOESS  Since the NWP community had proven the benefi ts of OSSE over many years NASA looked to this approach to help resolve questions during the formulation stage of the Hydros mission a pre cursor to the SMAP Decadal Study mission now in Phase A development The objective of Hydros was to perform spaceb orne global soil moisture measurements, but there was concern over the reliability of soil moisture retrievals in densely vegetated areas as well as the extent over which retrievals were possible  Table 1 Draft Science Requirements for Deformation, Ecosystem s, and Dynamics of Ice \(DESDynI\ Mission    Mission Goals  Science Objectives  Observations  Measurements  Deformation  Determine the likelihood of earthquakes, volcanic eruptions, and landslides and quantify the magnitude of events  Characterize the nature of  deformation at plate boundaries and the implications for earthquake hazards  Measure surface deformation  100 m imagery  Accurate to 5% of the rate of the deforming zone with a minimum of 1mm/yr   Ecosystems  Characterize the effects of changing climate and land use on species habitats and carbon budget  Develop globally consistent and spatially resolved estimates of above ground biomass and carbon stocks  Measure above ground biomass forest height and structure  100 m spatial resolution  Global vegetation c over  Ice Dynamics   Predict the response of ice sheets to climate change and impact on sea level   Quantify sea ice mass balance ice sheet surface mass balance and dynamics, and how it is changing   Measure surface height and displacements    Coverage ove r Arctic and southern oceans  Spatial resolution of 25 m   


 3  NASA directed the science team to resolve this concern to a ssess the impact of land surface heterogeneity instrument error and parameter uncertainty on soil moisture products The analysis investigated trade offs of resolution and accuracy as well as sensitivity to instrument noise and model error in determinin g if the 36 km product accuracy goals could be met. Prototype retrieval algorithms were also studied as applied to simulated observations on nature models The OSSE results confirmed that the 36 km retrieval goal of 4 volumetric soil moisture is feasible while also allowing assessment of eventual processing and retrieval strategies to mitigate errors [5   One reason why OSSE is so important throughout the entire mission lifecycle is that it provides a quantitative means to evaluate how changes in mission r equirements will impact science results In this paper, we review our progress toward this goal for areas relevant to Earth Science Decadal Survey missions  2  OSSE FOR S URFACE D EFORMATION S CIENCE  The primary goals of the DESDynI Radar/Lidar mission can b e summarized as follows  Determine the likelihood of earthquakes volcanic eruptions and landslide s  US annualized losses from earthquakes are 4.4B/yr  Characterize the effects of changing climate and land use on species habitats and carbon budget The structure of ecosystems is a key features that enables quantification of carbon storage  Predict the response of ice sheets to climate change and impact of sea level rise Ice sheets and glaciers are changing dramatically and remain one of the most under sampled domains as an indicator of climate change  The mission will carry two primary instruments, an L Band synthetic aperture radar operated as a repeat pass interferometer InSAR and a multi beam lidar These sensors will provide observations for solid  Earth ecosystems and climate measurements under the proposed 5 year mission with an 8 day revisit period The science steering committee has established priority 1 tasks relevant to OSSE analysis under mission requirements studies and mission configurat ion studies. These include  245  Validating that DESDynI will meet the science objectives  245  Assessing the quality of DESDynI science products  245  Configuration trades for the radar and lidar instruments to resolve a perceived incompatibility among observation needs to meet science goals  Addressing Science Questions  Sample science drivers where OSSE analyses can be applied include the following  Deformation  Can the current DESDynI mission configuration resolve tectonic deformation of the LA basin into individual de ep fault slip and a possible bulk component   Ecosystems  Under what conditions biomass density forest height topography etc can the current DESDynI mission measure the global distribution and changes of vegetation aboveground biomass and ecosystem stru cture with accuracy adequate to characterize the impact of ecosystem changes on the global carbon cycle, climate, and biodiversity  Ice Dynamics  Can contributions to sea level rise be separated into surface mass balance processes \(melting from atmospheric forcing\, dynamic ice flow response, and ocean forcing  Analysis Development  Development of component numerical models with verification against analytical results and validation against measurements where available will enable generation of simulated s cience observations for orbital sampling by the OSSE instrument models for deformation mass balance changes secular trends and recovery  To extend NWP based OSSE analyses to surface deformation this requires an ability to generate highly accurate surfac e deformations simulations for sampling by radar target simulators based on realistic orbit parameters Atmospheric effects must be included and processed by spacecraft instrument models to add noise to the measurement. The simulated measurements are run t hrough science retrieval algorithms to produce products that can be statistically compared to the original simulated deformations to capture the impact of instrument errors and sensitivities on the measurements. Metrics can be derived to capture how well t he combination of instrument design mission and orbital parameters and retrieval methods allow for relevant science questions to be answered  Figure 1 shows the kind of issues an OSSE can address in surface deformation science The InSAR will generate de formation interferograms where color bands represent the 3D\ surface deformation projected into the line of sight of the instrument This allows one to study questions in earthquake science but these fringe patterns are dependent on the look angle of the radar. By developing an OSSE we 


 4  can experiment numerically with issues regarding how well we can retrieve useful science results based on orbital repeat periods, look angle and other mission configuration issues  3  OSSE FOR A TMOSPHERIC C OMPOSITION S CIEN CE  The GEO CAPE and GACM Decadal Survey missions focus on the observation and transportation of global of air pollution identification of aerosols and ozone precursors For such missions, OSSE analysis can be applied to explore the mission design to ans wer science questions such as  245  What is the impact of long range transport of pollution from Asia on US CO and O 3  concentrations  245  Can we improve predictions of US boundary layer CO and O 3 concentrations  OSSE capabilities allow  exploration of a range of ins truments that might be used to study science problems focused on CO as well as instruments that would be used to study global and regional ozone  Analysis Development  OSSE development for atmospheric composition science is closely related to prior work in  numerical weather prediction NWP The majority of our efforts and success have been seen in this discipline 7   I n  t h i s  c a s e   t h e  components of the OSSE system applied in analysis for the aforementioned science questions can be categorized as follows  1  Phenomena Database The phenomena database is a collection of model s  and measurements that represent the Earth\325s atmospheric composition and other data needed for radiative transfer  calculations The current database includes model runs time  dependent 3 D  fields of all atmospheric species of interest\ from these models MOZART GEOS CHEM GMI We also have an aerosol climatology IMPACT and surface reflectivity data \(from MODIS measurements  2  Observation Scenario This tool  allows the user to select the o rbit parameters altitude inclination equator crossing time and sampling information samples per orbit, foot print size\ and the tool can be used to visualize the sampling and then generate a table of the observation times and locations An example of a selected observation scenario is shown in Figure 2   3  Input Signal Database Input signal database refers to the output of high spectral resolution \(monochromatic radiative transfer calculations that are carried out for each of the samples from the obse rvations scenario with radiative transfer inputs built from the selected phenomena database Atmospheric profiles are selected from the database, and then these are used to calculate optical depths in the atmosphere and surface properties The optical dep ths and surface properties are f ed into the radiative transfer c ode The use r may choose an  appropriate model such as LIDORT for scattering problems LBLRTM non scattering or CRTM \(microwave  4  Instrument Performance Requirements In this part of the model user selected instrument performance characteristics are applied to the monochromatic radiances The user selects the spectral resolution signal to noise ratio and instrument line shape function Ranges of performance parameters may be selected a llowing the users to simulate a suite of instruments rather than a single set of performance requirements   5  Measurement Database A measurement dataset contains the response of each instance of an instrument in the instrument list for a given sample list A virtual sensor  that can dynamically configure its performance based on the parametric performance specifications      Figure  1  Line of sight interferograms of surface deformation for Landers earthquake from 3D multi million finite element GeoFEST [6  m o d e l  s h o w i n g  h o w  fringe patterns vary based on look ang le of the radar Such simulations are critical to examining how sensitivities in observation modes, instrument parameters and retrieval methods work to form an OSSE analysis     Figure 2  An example of user selected sampling by choosing orbit altitude inclination and sampling space S amples take in o ne 24 hour period    


 5  simulates the instrument response The measurement database generates a unique key for each instrument case and tracks the completion statu s The measurement database is divided into two types of use cases observation and calibration The noise covariance matrix of the instrument is extracted employing the calibration measurement database  6  Trace Gas Retrieval This is the most computationall y expensive part of the OSSE A linearized optimal estimation approach is applied to take the measurement data constraint matrices user selected retrieval levels and Jacobians generated b y the radiative transfer model to perform a linearized retrieval The output of the retrieval for each instrument design at each observation sample is a retrieved profile estimated errors on the profile and an averaging kernel that reflects the retrieval sensitivity at each level   7  Trace Gas profile database The trac e gas profile database contains all of the output of the trace gas retrieval step  8  Prediction Accuracy Analysis The prediction accuracy analysis process has a number of steps The first part of the analysis is simple aggregation of data and calculation of  statistics We typically assess the bias and estimated errors of the simulated retrievals aggregating statistics in regional bins and for a number of different pressure levels We also examine the averaging kernels and degrees of freedom for signal in th e troposphere for regional bins  A more sophisticated step in the analysis is conducted with data assimilation. In the data assimilation step, the simulated retrieval s \(with data from a nature run are assimilated into a model that has been run with a diff erent emissions scenario As time progresses, more and more simulated measurements are integrated into the global model. For measurements with very good sensitivity and low bias the end of the assimilation results in a global field very similar to the nat ure run In cases where the instrument sensitivity is small the final model field will be a mixture of the nature run and the modified emissions run Figure 3  is an illustration of assimilation   The assimilated fields are then used to calculate science me trics For example if the science question were 322how does long range tran sport of ozone from Asia impact North America?\323  the science metric would be statistics of the difference in the ozone field for that region for a set of instrument designs. This aspe ct will continue to be matured in the future   Applying OSSE Simulations  This emerging  OSSE system capability was used for two science questions We studied a range of instruments that might be used to study science problems focused on CO and we looked at instruments that would be used to study global and regional ozone   A month long observation scenario was simulated at orbit altitude of 705 Km. For each day, more than 1000 samples were simulated over 14 orbits representing 1.5 min temporal resolution F or each sample three types of monochromatic radiance spectrum were simulated representing UV/VIS Near IR and Mid IR For IR simulation the trace gas phenomena model datasets were employed to represent the atmospheric state For UV/VIS simulation aeros ol and surface reflectance phenomena model datasets were utilized in addition to the trace gas phenomena datasets     Figure 3  As assimilation progresses you can see the low emissions field on the top  updated with simulated samples built from the regula r emissions field on the bottom  Corresponding to the above three types of input signal, three sets of instrument performance ranges were modeled and their responses were simulated to generate hypothetical measurement datasets For each type of instrument  three cases of SNR and two cases of spectral resolutions were simulated For each instrument case an observation database was generated for the month long observation scenario, resulting in over 31  000 measurements. In addition 


 6     Figure 4  A ssimilation example for ozone: Left panel shows the data field early in the assimilation, when data sampled from the regular emissions field \(right hand panel\ is assimilated into a low emission field       Figure 5  Comparison of requirements t o simulated performance given the mean error for simulations over 20 instrument configurations, where results in the yellow box match the requirements \(left\, and the distribution of error for the simulated instrument, orange circle, over the global observ ations \(right   to the observation measurements, a calibration database was also generated simulating the random noise in order to formulate the statistical distribution of the performance property   Of these simulations, one of the ozone cases was assimi lated into the gl obal model GEOS CHEM Figures 4  show s  the early stages of assimilating ozone. The left panel shows the low emission field, and the right panel shows the field from which the simulated data was sampled  The  bias of ozone in the surface laye r over the continental US was also  examined  as a function of time As the assimilation begins there is a bias of 60 ppb and by 20 days the bias is nearly zero Note that this assimilation utilized unrealistically good vertical sensitivity for the ozone sounding. In the next steps, more realistic vertical sensitivity will be used  This work  demonstrated  a new capability to examine the potential s cience return from new missions and to  analyze the trades in mission design instrument performance parameters  and science return An example is shown in Figure 5 where the mean error over simulations of 20 possible instrument configurations is compared against satisfying observation requirements marked by yellow box\. Each point in the figure represents contribu tions to the mean based on a large series of test cases run for a specific configuration The distribution of error for a simulated instrument is shown compared to the requirements allowing one to examine if outliers based on an instrument configuration impact the science question of interest  The current analysis focuses on Earth atmospheric composition measurements but the method of analysis is extendable to other measurements and planets   


 7    Figure 6  Illustration of end to end OSSE analysis capabili ty needed for spectroscopy science associated with canopy water content retrievals. Other relevant studies can be performed based on the nature models and retrieval algorithms applied   4  OSSE FOR I MAGING S PECTROSCOPY S CIENCE  Decadal survey missions in s pectroscopy science such as HyspIRI GEO CAPE and GACM have the capability to detect the surface and atmospheric response of systems due to climate variability and change In these regimes OSSE can help explore mission and instrument design for science questions such as  245  What is the spatial pattern of ecosystem and diversity distributions and how do ecosystems differ in their composition  245  What is the composition of the exposed terrestrial surface of the Earth and how does it respond to human and non huma n induced drivers  Development of OSSE capabilities also allows for the exploration of retrieval algorithms based on generation of scenes as observed from spectroscopy instruments This allows one to examine the impact of noise parameters from the instrume nt design and to quantify uncertainties based on sensitivities associated with such designs  Analysis Development  An example of the kind of analysis associated with spectroscopy such as vegetation water content retrievals and the impact of instrument para meters and algorithms on creating such retrievals, is shown in Figure 6. This consists of an integrated leaf/canopy model Prospect and SAIL a radiative transfer model Modtran a detector/instrument model and a  retrieval algorithm and a mechanism to explore instrument parameter sensitivities based on statistical analysis \(JMP  The Prospect model for leaf radiative transfer has a long provenance, dating from the work of Ja cquemoud and Baret 8   It considers chlorophyll, water, dry matt er content, and  leaf structure The SAIL model for canopy radiative transfer dates from NASA sponsored work  in the 1980s by L Alexander [9   It accounts for leaf density, soil reflectance diffuse skylight and illu mination and viewing geometry We used a Matlab impleme ntation of a contemporary integrated revision of these models called ProSAIL2 Additionally we have available as a component a separate version of these models called FlourMODleaf and F lourSAIL 10  which accounts for leaf fluorescence effects  For radia tive transfer we used the well known Modtran software running under IDL in the MODO environment The radiative transfer components use a spectral albedo reflectance generated by the leaf/canopy model  The spectroscopy instrument model is an Excel spr eadsheet that was developed for this purpose by the team.  It incorporates detector quantization and noise effects and takes as input the top of atmosphere radiance generated by Modtran  The canopy water  retrieval algorithm is a Matlab component developed  by the team based on well known band ratio techniques   Defining the infrastructure for integration of various models is a key aspect of supporting these kinds of analyses We used a Simple Object Access Protocol SOAP  web service approach for this work which allows for distributed model components to be included seamlessly in one model workflow.  This means that, for each component above, we built a web service interface which can be called from anywhere in the JPL intranet  To construct a specific data  flow based on these web services we used Taverna an open source workflow definition and enactment system This system provides a GUI interface for connecting inputs 


 8  to outputs to define a workflow  The workflow once defined, can be executed standalon e by Taverna, or from the GUI  Intermediate results can be retrieved and inspected Furthermore we also developed tools to iterate workflow execution over a table of parameters.  A spreadsheet or other tool can generate a comma separated value table tha t drives a workflow for parameter sweeps or randomized testing  As part of this work we also developed several other integration tools including spectral interpolators format converters and generic wrappers to turn Matlab and Excel components into SOAP enabled workflow elements   Applying OSSE Simulations  One typical result is the set of radiance spectra shown in Figure 7 This indicates the effect of changing canopy water content on observed spectra, including the effects of readout noise This was obt ained from the full  workflow system shown in Figure 6   A second typical resu lt is shown in Figure 8   This illustrates retrieval of canopy water  with accuracy around 10 of the true value while varying retrieval conditions including soil reflectivity and  leaf inclination angle   5  C ONCLUDING C OMMENTS  While OSSE capabilities are not yet fully established in these areas the potential exists to provide  a quantitative means to evaluate how changes in mission requirements will impact science results  There ar e a variety of questions related to generation of science products given multiple instruments observation strategies and retrieval methods where uncertainties can be reduced via OSSE analyses. The ability to integrate models to support design trades is k ey to this process and the ability to extend such capabilities to new areas of observing science can play a key role in the success of future Decadal Study mission efforts  6  A CKNOWLEDGEMENT  This research was carried out at the Jet Propulsion Laboratory  California Institute of Technology under a contract with the National Aeronautics and Space Administration The JPL Research and Technology Development Program also supported this work We also appreciate the support of the development teams Greg Lyzeng a, Margaret Glasscoe, Andrea Donnellan, Meemong Lee, Charles Miller, Harold Sobel, Hook Hua, Joseph Jacob Paul Springer, and Gary Block          Figure 8  The chart shows r et rieval of canopy water based on observed spectra Blue points sho w retrieval und er original canopy conditions Red points show ret rieval under other conditions Reflections from soil can complicate this retrieval      Figure 7  Observed radiance spectra for three values of canopy water content   


 9  R EFERENCES  1  C Arnold and C Dey Observing Systems Simulation Experiments Past Present and Future Bulle tin Amer Metero. Soc., 67\(6\, pp. 687 695. 1986  2  Earth Science and Applications from Space National Imperatives for the Next Decade and Beyond The National Academies Press. ISBN 13: 987 0 309 10397 9 Washington, D.C., 2007. 437 pp  3  C. W. Newton Analysis and Data Problems in Relation to Numerical Prediction Bulletin Amer Metero Soc 35 pp. 287 294. 1954  4  M A Alaka and F Lewis Numerical experiments lending to the design of Optimum Global Metrological Networks Tech Memo WBTM TDL 7 Environmental Science Services Administration Weather Bureau Washington D.C., 1967. 14 pp  5  W Crow et al An Observing System Simulation Experiment for Hydros Radiometer Only Soil Moisture Products IEEE Trans On Geoscience and Remote Sensing, Vol 43, No. 6, June 2005  6  J Parker G Lyzenga C Norton C Zuffada M Glasscoe J Lou and A Donnellan Geophysical Finite Element Simulation Tool GeoFEST Algorithms and Validation for Quasistatic Regional Faulted Crust Problems. Pure appl. geo phys. 165 \(2008\, pp. 1 25, 2008  7  M Lee R Weidner C Miller and K Bowman Senor Web Operations Explorer SOX for Earth Science Air Quality Missions Concepts. IEEE Aerospace Conference Big Sky, MT, 2008  8  Jacquemoud S. and Baret F. \(1990\, PRO SPECT: a model of leaf optical properties spectra Remote Sensing of Environment, 34:75 91   9  Alexander L 1983 SAIL Canopy Model Fortran Software Lyndon B Johnson Space Center NASA Technical Report JSC 18899  10  Miller J.R Berger M Alonso  L Cerovic Z Goulas Y Jacquemoud S Louis J Mohammed G Moya I Pedr\227s R Moreno J Verhoef W Zarco Tejada P.J Progress on the Development of an Integrated Canopy Fluorescence Model 2003 International Geoscience and Remote Sens ing Symposium IGARSS'03 pp 601 603 Vol.1 ISBN 0 7803 7929 2 0 7803 7930 6, Toulouse \(France\, 21 25 7 /2004    B IOGRAPHY  Charles D. Norton  i s a Principal Member of Technical Staff at the Jet Propulsion Laboratory California Institute of Technolog y He received his Ph.D in Computer Science from Rensselaer and his B.S.E in Electrical Engineering and Computer Science from Princeton University Prior to joining JPL he was a National Research Council resident scientist His work covers advanced scienti fic software for Earth and space science modeling with an emphasis on high performance computing and finite element adaptive methods Additionally he is leading efforts in development of smart payload instrument concepts. He has given 32 national and inter national keynote/invited talks published in numerous journals, conference proceedings, & book chapters. He is a member of the editorial board of the journal Scientific Programming the IEEE Technical Committee on Scalable Computing a Senior Member of IEE E AGU recipient of the JPL Lew Allen Award, and a NASA Exceptional Service Medal  Annmarie Eldering is a scientist and manager in the Earth Atmospheric Sciences section a t the Jet Propulsion Laboratory California Institute of Technology She received h er PhD from the California Institute of Technology and her B.E. From the Cooper Union for the Advancement of Science and Art. She is the Deputy Principal Investigator for the Tropospheric Emission Spectrometer on NASA\325s Aura satellite which measures verti cal profiles of tropospheric trace gases such as ozone and carbon monoxide Her work also involves simulations of designs for future tropospheric composition measurements missions  Michael Turmon is a group supervisor and principal member of the technical  staff at the Jet Propulsion Laboratory California Institute of Technology Michael's chief interest is in applications of pattern recognition and statistical inversion techniques to science data He has developed novel clustering and classification syste ms for object identification and tracking in terrestrial and space based solar imagery and he currently leads a task on data assimilation for highly nonlinear systems like ocean and atmospheres He received Bachelor's degrees in Computer Science and in El ectrical Engineering, and subsequently an M.S in Electrical Engineering from Washington University St Louis He obtained his Ph.D in Electrical Engineering from Cornell University in 1995. Michael was a National Science Foundation Graduate Fellow \(1987 90 won the NASA Exceptional Achievement Medal for his work on solar image analysis \(1999\, and received a Presidential 


 10  Early Career Award for Scientists and  Engineers PECASE in 2000 He has been co investigator on the NASA/ ESA SOHO spacecraft and the CNES Picard mission Michael is a member of the IEEE Computer Society the Institute for Mathematical Statistics and AAAS  Jay Parker is a Senior Scientist in the Satellite Geodesy and Geodynamics group of the Jet Propulsion Laboratory California Ins titute of Technology His graduate research used computer simulations to explain mesospheric ionization response to solar flares and dynamic instabilities Dr Parker's research subjects at the Jet Propulsion Laboratory include a variety of topics in remote  sensing analysis and modeling  These include supercomputing algorithms for electromagnetic scattering and radiation satellite geodesy and finite element simulation for earthquake related deformation and ocean sensing through GPS signal reflection He i s currently the software engineer and co investigator for the QuakeSim project which has developed a solid Earth science framework including a variety of simulation and analysis tools He also develops the SEASCRAPE software system for high fidelity simul ation and parametric retrieval of atmospheric infrared spectrometry at Remote Sensing Analysis Systems Inc of Altadena CA Dr Parker is a member of the American Geophysical Union and co chair of the Data Understanding and Assimilation working group of t he APEC Cooperation for Earthquake Simulation   


 11 This model specifies that covariates act multiplicatively on time t r than on the hazar d function.  That is, we assume a baseline hazard function to exist and that the effects of the covariates are to alter the rate at which an individual proceeds along the time axis.  In other words, the covariates z accelerates or decelerates the time to failure Kalbfleisch and Prentice 2002, Lawless 2003  It should be pointed out that the distribution-based regression models for exponential and Weibull distributions in the previous section are th e special cases of both PHM and AFT.  This correspondence is not necessarily true for models based on other distribu tions. Indeed, two-parameter Weibull distribution has the uniq ue property that it is closed under both multiplication of fa ilure time and multiplication of the hazard function by an arbitrary nonzero constant Lawless 2003, Kalbfleisch & Prentice 2002, Klein Moeschberger 2003  2.6. Counting Process and Survival Analysis   In the previous sections, we introduced censoring and survival analysis models th at can handle the censored information; however, we did not discuss how the censored information is processed.  Accommodating and maximally utilizing the partial information from the censored observations is the most challenging and also the most rewarding task in survival anal ysis.  This also establishes survival analysis as a unique fiel d in mathematical statistics Early statistical inferences for censored data in survival analysis were dependent on asymptotic likelihood theory Severini 2000\ Cox \(1972, 1975\ proposed partial likelihood as an extension to classical maximum likelihood estimation in the context of hi s proportional hazards model as a major contribution. Asymptotic likelihood has been and still is the dominant theory for developing survival analysis inference and hypothesis testing methods \(Klein and Moeschberger 2003, Severini 2000\. There are many monographs and textbooks of survival analysis containing sufficient details for applying survival analysis \(Cox and Oakes 1984, Kalbfleisch and Prentice 1980, 2002, Lawless 1982, 2003, Klein and Moeschberger, 2003\. A problem with traditional asymptotic lik elihood theory is that the resulting procedures can become very complicated when handling more complex censoring mechanisms \(Klein Moeschberger 2003\. A more elegant but requiring rigorous measure-theoretic probability theo ry is the approach with counting stochastic processes and the Martingale central limit theorems.  Indeed, this approach was used by Aalen 1975\ to set the rigorous mathematical foundation for survival analysis, and later further developed and summarized by Fleming and Harrington \(1991\, Andersen et al. \(1993\several research papers.  In reliability theory Aven and Jensen \(1999\ dem onstrated such an approach by developing a general failure model, which we briefly introduced in Section 1.2. However, the counting process and Martingale approach require measure theoretic treatments of probability and st ochastic processes, which is often not used in engineering or applied statistics.  A detailed introduction of the topic is obviously beyond the scope of this paper, and we only present a brief sketch of the most important concepts involved.  Readers are referred to the excellent monographs by Andersen et al. \(1993 Fleming and Harrington \(1991\ Aven and Jensen \(1999\ for comprehensive details, and Kal bfleisch and Prentice \(2002 Klein and Moeschberger \(2003\, Lawless \(2003\ for more application–oriented treatments The following discussion on this topic is drawn from Klein and Moeschberger \(2003  A counting stochastic process N  t  t 0 possesses the properties that N  0 ro and N  t   with probability one. The sample paths of N  t ht continuous and piecewise constant with jumps of size 1  step function In a right-censored sample, \(we assume only right censoring in this section N i  t  I  T i t  i   which keep the value 0 until individual i fails and then jump to 1  are counting processes. The accumulation of N i  t ocess     1 t N t N n i i is again a counting process, which counts the number of failures in the sample at or before time t   The counting process keeps track of the information on the occurrences of events,   for instance, the history information such as which individual was censored prior to time t and which individual died at or prior to time t as well as the covariates information This accumulated history information of the counting process at time t is termed filtration at time t denoted by F t For a given problem F t  rests on the observer of the counting process.  Thus, two observers with different recordings at different times will get different filtrations.  This is what Aven and Jensen 1999\ referred to as different information levels or the amount of actual available information about the state of a system may vary  If the failure times X i and censoring times C i  are independent,  then the probability of an event occurs at time t given the history just prior to t  F t\n be expressed as  t T if dt t h t C t X dt t C dt t X t P F dt t T t P i i i i i i r t i i r          1     t T if F dt t T t P i t i i r 0   1    51  Let dN  t be the change in the process N  t over a short time interval    t t t Ignoring the neglig ible chance of ties 1   t dN if a failure occurred and 0   t dN otherwise  Let Y  t denote the number of individuals with an observation time T i t Then the conditional expectation of dN  t   dt t h t Y F dt t C dt t X t with ns observatio of number E F t dN E t i i i t              52 


 12 The process  t  Y  t  h  t  is called the intensity process  of the counting process.  It is a stochastic process that is determined by the information contained in the history process F t via Y  t  Y  t  records the number of individuals experiencing the risk at a given time t   As it will become clear that   t is equivalent to the failure rate or hazard function in tr aditional reliability theory, but here it is treated as a stochastic process, the most general form one can assume for it. It is this generalization that encapsulates the power that counting stochastic process approach can offer to survival analysis  Let the cumulative intensity process H  t be defined as   t t ds s t H 0 0      53  It has the property that  E  N  t  F t  E  H  t  F t  H  t   54  This implies that filtration F t, is known, the value of Y  t  fixed and H  t es deterministic H  t is equivalent to the cumulative hazards in tr aditional reliability theory  A stochastic process with the property that its expectation at time t given history at time s < t is equal to its value at s is called a martingale That is M  t martingale if           t s s M F t M E s  55  It can be verified that the stochastic process         t H t N t M 56  is a martingale, and it is called the counting process martingale The increments of the counting process martingale have an expected value of zero, given its filtration F t That is  0      t F t dM E 57 The first part of the counting process martingale [Equation 56  N  t non-decreasing step function.  The second part H  t smooth process which is predictable in that its value at time t is fixed just prior to time t It is known as the compensator  of the counting process and is a random function. Therefore, the martingale can be considered as mean-zero noise and that is obtainable when one subtracts the smoothly varying compensator from the counting process  Another key component in the counting process and martingale theory for survival analysis is the notion of the predictable variation process of M  t oted by    t M It is defined as the compensator of process   2 t M  The term predictable variation process comes from the property that, for a martingale M  t it can be verified that the conditional variance of the increments of M  t  dM  t  h e inc r em ents of   t M That is          t M d F t dM Var t  58  To obtain      t F t dM Var  one needs the random variable N  t  variable with probability   t of having a jump of size 1 at time t The variance of N  t   t  1 t   since it follows b i nom ial distribution    Ignoring the ties in the censored data 2  t  is close to zero and Var  dM  t  F t    t  Y  t  h  t   This implies that the counting process N  t on the filtration F t behaves like a Poisson process with rate  t    Why do we need to convert the previous very intuitive concepts in survival analysis in to more abstract martingales The key is that many of the sta tistics in survival analysis can be derived as the stochastic integrals of the basic martingales described above The stochastic integral equations are mathematically well structured and some standard mathematical techniques for studying them can be adopted  Here, let   t K be a predictable  process An example of a predictable process is the process   t Y Over the interval 0 to t the stochastic integral of su ch a process, with respect to a martingale, is denoted by     0 u dM u K t It turns out that such stochastic integrals them selves are martingales as a function of t and their predictable variation process can be found from the predictable variation process of the original martingale by           0 2 0 u M d u K u dM u K t t 59 The above discussion was drawn from Klein and Moeschberger \(2003 also provide examples of how the above process is applied. In the following, we briefly introduce one of their exampl es — the derivation of the Nelson-Aalen cumulative hazard estimator  First, the model is formulated as           t dM dt t h t Y t dN  60  If   t Y is non-zero, then               t Y t dM t d t h t Y t dN 61  Let   t I be the indicator of whether   t Y  is positive and define 0/0 = 0, then integrating both sides of above \(61 One get 


 13                     0 0 0 u dM u Y u I u d u h u I u dN u Y u I t t t 62  The left side integral is the Nelson-Aalen estimator of H t           0  u dN u Y u I t H t 63  The second integral on the right side           0 u dM u Y u I t W t 64  is the stochastic integral of the predictable process      u Y u I with respect to a martingale, and hence is also a martingale  The first integral on the right side is a random quantity    t H   t du u h u I t H 0        65  For right-censored data it is equal to   t H  in the data range, if the stochastic uncertainty in the   t W is negligible Therefore, the statistic    t H is a nonparametric estimator of the random quantity    t H   We would like to mention one more advantage of the new approach, that is, the martingale central limit theorem.  The central limit theorem of martingales ensures certain convergence property and allows the derivations of confidence intervals for many st atistics.  In summary, most of the statistics developed with asymptotic likelihood theory in survival analysis can be derived as the stochastic integrals of some martingale.  The large sample properties of the statistics can be found by using the predictable variation process and martingale central limit theorem \(Klein and Moeschberger \(2003  2.7. Bayesian Survival Analysis  Like many other fields of statistics, survival analysis has also witnessed the rapid expansion of the Bayesian paradigm.  The introduction of the full-scale Bayesian paradigm is relative recent and occurred in the last decade however, the "invasion" has been thorough.  Until the recent publication of a monograph by Ibrahim, Chen and Sinhaet 2005\val anal ysis has been either missing or occupy at most one chapter in most survival analysis monographs and textbooks.  Ibrahim's et al. \(2005\ changed the landscape, with their comprehensive discussion of nearly every counterp arts of frequentist survival analysis from univariate to multivariate, from nonparametric, semiparametric to parametric models, from proportional to nonproportional hazards models, as well as the joint model of longitudinal and survival data.  It should be pointed out that Bayesian survival analysis has been studied for quite a while and can be traced back at least to the 1970s  A natural but fair question is what advantages the Bayesian approach can offer over the established frequentist survival analysis. Ibrahim et al 2005\entified two key advantages. First, survival models are generally very difficult to fit, due to the complex likelihood functions to accommodate censoring.  A Bayesi an approach may help by using the MCMC techniques and there is available software e.g., BUGS.  Second, the Bayesian paradigm can incorporate prior information in a natural way by using historical information, e.g., from clinical trials. The following discussion in this subsection draws from Ibrahim's et al. \(2005  The Bayesian paradigm is based on specifying a probability model for the observed data, given a vector of unknown parameters This leads to likelihood function L   D   Unlike in traditional statistics is treated as random and has a prior distribution denoted by   Inference concerning is then based on the posterior distribution which can be computed by Bayes theorem d D L D L D              66 where is the parameter space  The term    D is proportional to the likelihood    D L  which is the information from observed data, multiplied by the prior, which is quantified by   i.e          D L D 67 The denominator integral m  D s the normalizing constant of    D and often does not have an analytic closed form Therefore    D often has to be computed numerically The Gibbs sampler or other MCMC algorithms can be used to sample    D without knowing the normalizing constant m  D xist large amount of literature for solving the computational problems of m  D nd    D   Given that the general Bayesian algorithms for computing the posterior distributions should equally apply to Bayesian survival analysis, the specification or elicitation of informative prior needs much of the atte ntion.  In survival analysis with covariates such as Cox's proportional hazards model, the most popular choice of informative prior is the normal prior, and the most popular choice for noninformative is the uniform Non-informative prior is easy to use but they cannot be used in all applications, such as model selection or model comparison. Moreover, noninformative prior does not harness the real prior information. Therefore, res earch for informative prior specification is crucial for Bayesian survival analysis  


 14 Reliability estimation is influenced by the level of information available such as information on components or sub-systems. Bayesians approach is likely to provide such flexibility to accommodate various levels of information Graves and Hamada \(2005\roduced the YADAS a statistical modeling environment that implements the Bayesian hierarchical modeli ng via MCMC computation They showed the applications of YADES in reliability modeling and its flexibility in processing hierarchical information. Although this environment seems not designed with Bayesian surviv al analysis, similar package may be the direction if Bayesian survival analysis is applied to reliability modeling  2.8. Spatial Survival Analysis  To the best of our knowledge spatial survival analysis is an uncharted area, and there has been no spatial survival analysis reported with rigor ous mathematical treatment There are some applications of survival analysis to spatial data; however, they do not address the spatial process which in our opinion should be the essential aspect of any spatial survival analysis. To develop formal spatial survival analysis, one has to define the spatial process first  Recall, for survival analysis in the time domain, there is survival function   R t t T t S  Pr   68  where T is the random variable and S  t e cumulative probability that the lifetime will exceed time t In spatial domain, what is the counterpart of t One may wonder why do not we simply define the survival function in the spatial domain as   S  s  Pr S s  s R d  R > 0  69  where s is some metric for d-dimensional space R d and the space is restricted to the positive region S is the space to event measurement, e.g., the distance from some origin where we detect some point object.  The problem is that the metric itself is an attribute of space, rather than space itself Therefore, it appears to us that the basic entity for studying the space domain has to be broader than in the time domain This is probably why spatial process seems to be a more appropriate entity for studying  The following is a summary of descriptions of spatial processes and patterns, which intends to show the complexity of the issues involved.  It is not an attempt to define the similar survival function in spatial domain because we certainly unders tand the huge complexity involved. There are several monographs discussing the spatial process and patterns \(Schabenberger and Gotway 2005\.  The following discussion heavily draws from Cressie \(1993\ and Schabenberger and Gotway \(2005  It appears that the most widely adopted definition for spatial process is proposed in Cressie \(1993\, which defines a spatial process Z  s in d-dimensions as    Z  s  s D R d  70  Here Z  s denotes the attributes we observe, which are space dependent. When d = 2 the space R 2 is a plane  The problem is how to define randomness in this process According to Schabenberger and Gotway \(2005 Z  s an be thought of as located \(indexed\by spatial coordinates s  s 1  s 2  s n  the counterpart of time series Y  t  t   t 1  t 2  t n   indexed by time.  The spatial process is often called random field   To be more explicit, we denote Z  s  Z  s   to emphasize that Z is the outcome of a random experiment  A particular realization of produces a surface    s Z  Because the surface from whic h the samples are drawn is the result of the random experiment Z  s called a random function  One might ask what is a random experiment like in a spatial domain? Schabenberger and Gotway \(2005\ offered an imaginary example briefly described below.  Imagine pouring sand from a bucket on to a desktop surface, and one is interested in measuring the depth of the poured sand at various locations, denoted as Z  s The sand distributes on the surface according to the laws of physics.  With enough resources and patience, one can develop a deterministic model to predict exactly how the sand grains are distributed on the desktop surface.  This is the same argument used to determine the head-tail coin flipping experiment, which is well accepted in statistical scie nce. The probabilistic coinflip model is more parsimonious than the deterministic model that rests on the exact \(perfect\but hardly feasible representation of a coin's physics. Similarly deterministically modeling the placement of sand grains is equally daunting.  However, th e issue here is not placement of sand as a random event, as Schabenberger and Gotway 2005\phasized.  The issue is that the sand was poured only once regardless how many locations one measures the depth of the sand.  With that setting, the challenge is how do we define and compute the expectation of the random function Z  s Would E  Z  s   s  make sense  Schabenberger and Gotway \(2005\further raised the questions: \(1\to what distribution is the expectation being computed? \(2\ if the random experiment of sand pouring can only be counted once, ho w can the expectation be the long-term average  According to the definition of expectation in traditional statistics, one should repeat the process of sand pouring many times and consider the probability distributions of all surfaces generated from the repetitions to compute the expectation of Z  s is complication is much more serious 


 15 than what we may often reali ze. Especially, in practice many spatial data is obtained from one time space sample only. There is not any independent replication in the sense of observing several independent realizations of the spatial process \(Schabenberger and Gotway 2005  How is the enormous complexity in spatial statistics currently dealt with? The most commonly used simplification, which has also been vigorously criticized, is the stationarity assumption.  Opponents claim that stationarity often leads to erroneous inferences and conclusions.  Proponents counte r-argue that little progress can be made in the study of non-stationary process, without a good understanding of the stationary issues Schabenberger and Gotway 2005  The strict stationarity is a random field whose spatial distribution is invariant under translation of the coordination.  In other word s, the process repeats itself throughout its domain \(Schabenberger and Gotway 2005 There is also a second-order or weak\ of a random field  For random fields in the spatial domain, the model of Equation \(70  Z  s  s D R d  is still too general to allow statistical inference.  It can be decomposed into several sub-processes \(Cressie 1993             s s s W s s Z  D s  71  where  s  E  Z  is a deterministic mean structure called large-scale variation W  s is the zero-mean intrinsically stationary process, \(with second order derivative\nd it is called smooth small-scale variation   s is the zero-mean stationary process independent of W  s and is called microscale variation  s  is zero-mean white noise also called measurement error. This decomposition is not unique and is largely operational in nature \(Cressie 1993\he main task of a spatial algorithm is to determine the allocations of the large, small, and microscale components However, the form of the above equation is fixed Cressie 1993\, implying that it is not appropriate to sub-divide one or more of the items. Therefore, the key issue here is to obtain the deterministic  s  but in practice, especially with limited data, it is usually very difficult to get a unique  s   Alternative to the spatial domain decomposition approach the frequency domain methods or spectral analysis used in time series analysis can also be used in spatial statistics Again, one may refer to Schabenberger and Gotway \(2005  So, what are the imp lications of the general discussion on spatial process above to spatial survival analysis?  One point is clear, Equation \(69 S  s  Pr S s   s R d is simply too naive to be meaningful.  There seem, at least, four fundamental challenges when trying to develop survival analysis in space domain. \(1\ process is often multidimensional, while time pr ocess can always be treated as uni-dimensional in the sense that it can be represented as  Z  t  t R 1  The multidimensionality certainly introduces additional complications, but that is still not the only complication, perhaps not even the most significant 2\One of the fundamental complications is the frequent lack of independent replication in the sense of observing several independent realizations of the spatial process, as pointed out by Cressie \(1993\\(3\e superposition of \(1 and \(2\brings up even more complexity, since coordinates  s of each replication are a set of stochastic spatial process rather than a set of random variables. \(4\n if the modeling of the spatial process is separable from the time process, it is doubtful how useful the resulting model will be.  In time process modeling, if a population lives in a homogenous environment, the space can be condensed as a single point.  However, the freezing of time seems to leave out too much information, at least for survival analysis Since the traditional survival analysis is essentially a time process, therefore, it should be expanded to incorporate spatial coordinates into original survival function.  For example, when integrating space and time, one gets a spacetime process, such as          R t R D s t s Z d   where s is the spatial index \(coordinate t is time.  We may define the spatial-temporal survival function as   S  s  t  Pr T t  s D  72  where D is a subset of the d dimensional Euclidean space That is, the spatial-temporal survival function represents the cumulative probability that an individual will survive up to time T within hyperspace D which is a subset of the d dimensional Euclidian space.  One may define different scales for D or even divide D into a number of unit hyperspaces of measurement 1 unit  2.9. Survival Analysis and Artificial Neural Network   In the discussion on artificial neuron networks \(ANN Robert & Casella \(2004\noted, "Baring the biological vocabulary and the idealistic connection with actual neurons, the theory of neuron networks covers: \(1\odeling nonlinear relations between explanatory and dependent explained\ariables; \(2\mation of the parameters of these models based on a \(training\ sample."  Although Robert & Casella \(2004\ did not mentioned survival analysis, their notion indeed strike us in that the two points are also the essence of Cox s \(1972\roportional Hazards model  We argue that the dissimilarity might be superficial.  One of the most obvious differences is that ANN usually avoids probabilistic modeling, however, the ANN models can be analyzed and estimated from a statistical point of view, as demonstrated by Neal \(1999\, Ripley \(1994\, \(1996 Robert & Casella \(2004\What is more interesting is that the most hailed feature of ANN, i.e., the identifiability of model structure, if one review carefully, is very similar to the work done in survival anal ysis for the structure of the 


 16 Cox proportional hazards model. The multilayer ANN model, also known as back-propagation ANN model, is again very similar to the stratified proportional hazards model  There are at least two advantages of survival analysis over the ANN.  \(1\val analysis has a rigorous mathematical foundation. Counting stochastic processes and the Martingale central limit theory form the survival analysis models as stochastic integral s, which provide insight for analytic solutions. \(2\ ANN, simulation is usually necessary \(Robert & Casella \(2004\which is not the case in survival analysis  Our conjecture may explain a very interesting phenomenon Several studies have tried to integrate ANN with survival analysis.  As reviewed in the next section, few of the integrated survival analysis and ANN made significant difference in terms of model fittings, compared with the native survival analysis alone The indifference shows that both approaches do not complement each other.  If they are essentially different, the inte grated approach should produce some results that are signifi cantly different from the pure survival analysis alone, either positively or negatively Again, we emphasize that our discussion is still a pure conjecture at this stage   3   B RIEF C ASE R EVIEW OF S URVIVAL A NALYSIS A PPLICATIONS     3.1. Applications Found in IEEE Digital Library  In this section, we briefly review the papers found in the IEEE digital library w ith the keyword of survival analysis  search. The online search was conducted in the July of 2007, and we found about 40 papers in total. There were a few biomedical studies among the 40 survival-analysis papers published in IEEE publications. These are largely standard biomedical applications and we do not discuss these papers, for obvious reasons  Mazzuchi et al. \(1989\m ed to be the first to actively  advocate the use of Cox's \(1 972\ proportional hazards model \(PHM\in engineering re liability.  They quoted Cox's 1972\ original words industrial reliability studies and medical studies to show Cox's original motivation Mazzuchi et al \(1989 while this model had a significant impact on the biom edical field, it has received little attention in the reliability literature Nearly two decades after the introducti on paper of Mazzuchi et al 1989\ears that little significant changes have occurred in computer science and IEEE related engineering fields with regard to the proportional hazards models and survival analysis as a whole  Stillman et al. \(1995\used survival analysis to analyze the data for component maintenance and replacement programs Reineke et al. \(1998 ducted a similar study for determining the optimal maintenance by simulating a series system of four components Berzuini and Larizza \(1996 integrated time-series modeling with survival analysis for medical monitoring.  Kauffman and Wang \(2002\analyzed the Internet firm su rvival data from IPO \(initial public offer to business shutdown events, with survival analysis models  Among the 40 survival analysis papers, which we obtained from online search of the IEEE digital library, significant percentage is the integration of survival analysis with artificial neural networks \(ANN In many of these studies the objective was to utilize ANN to modeling fitting or parameter estimation for survival analysis.  The following is an incomplete list of the major ANN survival analysis integration papers found in IEEE digital library, Arsene et 2006 Eleuteri et al. \(2003\oa and Wong \(2001\,  Mani et al 1999\ The parameter estimation in survival analysis is particular complex due to the requirements for processing censored observations. Therefore, approach such as ANN and Bayesian statistics may be helpful to deal with the complexity. Indeed, Bayesian survival analysis has been expanded significantly in recent years \(Ibrahim, et al. 2005  We expect that evolutionary computing will be applied to survival analysis, in similar way to ANN and Bayesian approaches  With regard to the application of ANN to survival analysis we suggest three cautions: \(1\he integrated approach should preserve the capability to process censoring otherwise, survival analysis loses its most significant advantage. \(2\ Caution should be taken when the integrated approach changes model struct ure because most survival analysis models, such as Cox's proportional hazards models and accelerated failure time models, are already complex nonlinear models with built-in failure mechanisms.  The model over-fitting may cause model identifiability  problems, which could be very subtle and hard to resolve 3\he integrated approach does not produce significant improvement in terms of model fitting or other measurements, which seemed to be the case in majority of the ANN approach to survival analysis, then the extra complication should certainly be avoided. Even if there is improvement, one should still take caution with the censor handling and model identifiability issues previously mentioned in \(1\ and \(2  3.2. Selected Papers Found in MMR-2004  In the following, we briefly review a few survival analysis related studies presented in a recent International Conference on Mathematical Methods in Reliability, MMR 2004 \(Wilson et al. 2005  Pena and Slate \(2005\ddressed the dynamic reliability Both reliability and survival times are more realistically described with dynamic models. Dynamic models generally refer to the models that incorporate the impact of actions or interventions as well as thei r accumulative history, which 


 17 can be monitored \(Pena and Slate 2005\he complexity is obviously beyond simple regression models, since the dependence can play a crucial ro le. For example, in a loadsharing network system, failure of a node will increase the loads of other nodes and influences their failures  Duchesne \(2005\uggested incorporating usage accumulation information into the regression models in survival analysis.  To simplify the model building Duchesne \(2005\umes that the usage can be represented with a single time-dependent covariate.  Besides reviewing the hazard-based regression models, which are common in survival analysis, Duchesne 2005\viewed three classes of less commonly used regression models: models based on transfer functionals, models based on internal wear and the so-called collapsible models. The significance of these regression models is that they expand reliability modeling to two dimensions. One dimension is the calendar time and the other is the usage accumulation.  Jin \(2005\ reviewed the recent development in statisti cal inference for accelerated failure time \(AFT\odel and the linear transformation models that include Cox proportional hazards model and proportional odds models as special cases. Two approaches rank-based approach and l east-squares approach were reviewed in Jin \(2005\. Osbo rn \(2005\d a case study of utilizing the remote diagnostic data from embedded sensors to predict system aging or degradation.  This example should indicate the potential of survival analysis and competing risks analysis in the prognostic and health management PHM\ since the problem Osborn \(2005 addressed is very similar to PHM. The uses of embedded sensors to monitor the health of complex systems, such as power plants, automobile, medical equipment, and aircraft engine, are common. The main uses for these sensor data are real time assessment of th e system health and detection of the problems that need immediate attention.  Of interest is the utilization of those remote diagnostic data, in combination with historical reliability data, for modeling the system aging or degradation. The biggest challenge with this task is the proper transformation of wear  time The wear is not only influenced by internal \(temperature, oil pressures etc\xternal covariates ambient temperature, air pressure, etc\, but also different each time   4  S UMMARY AND P ERSPECTIVE   4.1. Summary  Despite the common foundation with traditional reliability theory, such as the same probability definitions for survival function  S  t  and reliability  R  t  i.e  S  t  R  t well as the hazards function the exact same term and definition are used in both fields\rvival analysis has not achieved similar success in the field of reliability as in biomedicine The applications of survival analysis seem still largely limited to the domain of biomedicine.  Even in the sister fields of biomedicine such as biology and ecology, few applications have been conducted \(Ma 1997, Ma and Bechinski 2008\ the engin eering fields, the Meeker and Escobar \(1998\ monograph, as well as the Klein and Goel 1992\ to be the most comprehensive treatments  In Section 2, we reviewed the essential concepts and models of survival analysis. In Section 3, we briefly reviewed some application cases of survival analysis in engineering reliability.  In computer science, survival analysis seems to be still largely unknown.  We believe that the potential of survival analysis in computer science is much broader than network and/or software reliability alone. Before suggesting a few research topics, we no te two additional points  First, in this article, we exclusively focused on univariate survival analysis. There are two other related fields: one is competing risks analysis and the other is multivariate survival analysis The relationship between multivariate survival analysis and survival analysis is similar to that between multivariate statistical analysis and mathematical statistics.  The difference is that the extension from univariate to multivariate in survival analysis has been much more difficult than the developm ent of multivariate analysis because of \(1\rvation cens oring, and \(2\pendency which is much more complex when multivariate normal distribution does not hold in survival data.  On the other hand, the two essential differences indeed make multivariate survival analysis unique and ex tremely useful for analyzing and modeling time-to-event data. In particular, the advantages of multivariate survival analysis in addressing the dependency issue are hardly matched by any other statistical method.  We discuss competing risks analysis and multivariate survival analysis in separate articles \(Ma and Krings 2008a, b, & c  The second point we wish to note is: if we are asked to point out the counterpart of survival analysis model in reliability theory, we would suggest the shock or damage model.   The shock model has an even longer history than survival analysis and the simplest shock model is the Poisson process model that leads to exponential failure rate model The basic assumption of the shock model is the notion that engineered systems endure some type of wear, fatigue or damage, which leads to the failure when the strengths exceed the tolerance limits of th e system \(Nakagawa 2006 There are extensive research papers on shock models in the probability theory literature, but relatively few monographs The monograph by Nakagawa \(2006\s to be the latest The shock model is often formulated as a Renewal stochastic process or point process with Martingale theory The rigorous mathematical derivation is very similar to that of survival analysis from counting stochastic processes  which we briefly outlined in section 2.6  It is beyond the scope of the paper to compare the shock model with survival analysis; however, we would like to make the two specific comments. \(1\Shock models are essentially the stochastic process model to capture the failure mechanism based on the damage induced on the system by shocks, and the resulting statistical models are 


 18 often the traditional reliability distribution models such as exponential and Weibull failure distributions.  Survival analysis does not depend on specific shock or damage Instead, it models the failure with abstract time-to-event random variables.  Less restrictive assumptions with survival analysis might be more useful for modeling software reliability where the notions of fatigue, wear and damage apparently do not apply.  \(2\hock models do not deal with information censoring, the trademark of failure time data.  \(3\ Shock models, perhaps due to mathematical complexity, have not been applied widely in engineering reliability yet.  In contrast, the applications of survival analysis in biomedical fields are much extensive.  While there have been about a dozen monographs on survival analysis available, few books on shock models have been published.  We believe that survival analysis and shock models are complementary, a nd both are very needed for reliability analysis, with shock model more focused on failure mechanisms and the survival analysis on data analysis and modeling  4.2. Perspective   Besides traditional industrial and hardware reliability fields we suggest that the following fields may benefit from survival analysis  Software reliability Survival analysis is not based on the assumptions of wear, fatigue, or damage, as in traditional reliability theory.  This seems close to software reliability paradigm. The single biggest challenge in applying above discussed approaches to softwa re systems is the requirement for a new metric that is able to replace the survival time variable in survival analysis. This "time" counterpart needs to be capable of characterizing the "vulnerability" of software components or the system, or the distance to the next failure event. In other words, this software metric should represent the metric-to-event, similar to time-toevent random variable in survival analysis.  We suggest that the Kolmogorov complexity \(Li and Vitanyi 1997\n be a promising candidate. Once the metric-to-event issue is resolved, survival analysis, both univariate and multivariate survival analysis can be applied in a relative straightforward manner. We suggest that the shared frailty models are most promising because we believ e latent behavior can be captured with the shared fra ilty \(Ma and Krings 2008b  There have been a few applications of univariate survival analysis in software reliability modeling, including examples in Andersen et al.'s \(1995\ classical monograph However, our opinion is that without the fundamental shift from time-to-event to new metric-to-event, the success will be very limited. In software engineering, there is an exception to our claim, which is the field of software test modeling, where time variable may be directly used in survival analysis modeling  Modeling Survivability of Computer Networks As described in Subsection 2.3, random censoring may be used to model network survivability.  This modeling scheme is particularly suitable for wireless sensor network, because 1\ of the population nature of wireless nodes and \(2\ the limited lifetime of the wireless nodes.  Survival analysis has been advanced by the needs in biomedical and public health research where population is th e basic unit of observation As stated early, a sensor networ k is analogically similar to a biological population. Furthermore, both organisms and wireless nodes are of limited lifetimes. A very desirable advantage of survival analysis is that one can develop a unified mathematical model for both the reliability and survivability of a wireless sensor network \(Krings and Ma 2006  Prognostic and Health Management in Military Logistics PHM involves extensive modeling analysis related to reliability, life predictions, failure analysis, burnin elimination and testing, quality control modeling, etc Survival analysis may provide very promising new tools Survival analysis should be able to provide alternatives to the currently used mathematical tools such as ANN, genetic algorithms, and Fuzzy logic.  The advantage of survival analysis over the other alternatives lies in its unique capability to handle information censoring.  In PHM and other logistics management modeling, information censoring is a near universal phenomenon. Survival analysis should provide new insights and modeling solutions   5  R EFERENCES   Aalen, O. O. 1975. Statistical inference for a family of counting process. Ph.D. dissertation, University of California, Berkeley  Andersen, P. K., O. Borgan, R D. Gill, N. Keiding. 1993 Statistical Models based on Counting Process. Springer  Arsene, C. T. C., et al. 2006.  A Bayesian Neural Network for Competing Risks Models with Covariates. MEDSIP 2006. Proc. of IET 3rd International Conference On Advances in Medical, Signal and Info. 17-19 July 2006  Aven, T. and U. Jensen. 1999. Stochastic Models in Reliability. Springer, Berlin  Bazovsky, I. 1961. Reliability Theory and Practice Prentice-Hall, Englewood Cliffs, New Jersey  Bakker, B. and T.  Heskes. 1999. A neural-Bayesian approach to survival analysis. IEE Artificial Neural Networks, Sept. 7-10, 1999. Conference Publication No 470. pp832-837  Berzuini, C.  and C. Larizza 1996. A unified approach for modeling longitudinal and failure time data, with application in medical mon itoring. IEEE Transactions on Pattern Analysis and Machine Intelligence. Vol. 18\(2 123 


 19 Bollobás, B. 2001. Random Graphs. Cambridge University Press; 2nd edition. 500pp  Cawley, G. C., B. L. C. Talbot, G. J. Janacek, and M. W Peck. 2006. Sparse Bayesian Ke rnel Survival Analysis for Modeling the Growth Domain of Microbial Pathogens  Chiang C. L. 1960. A stochastic study of life tables and its applications: I. Probability distribution of the biometric functions. Biometrics, 16:618-635  Cox,  D. R. 1972. Regression models and life tables J. R Stat. Soc. Ser. B 34:184-220  Cox, D. R. 1975.   Partial likelihood Biometrika 62:269276  Cox, D. R. & D. Oakes. 1984 Analysis of Survival Data  Chapman & Hall. London  Cressie, N. A. 1993 Statistics for Spatial Data John Wiley Sons. 900pp  Duchesne, T. 2005. Regression models for reliability given the usage accumulation history. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty Y. Armijo. pp.29-40. World Scientific, New Jersey  Eleuteri, A., R. Tagliaferri, L. Milano, G. Sansone, D D'Agostino, S. De Placido,  M. Laurentiis. 2003.  Survival analysis and neural networks. Proceedings of the International Joint Conference on Neural Networks, Vol. 4 20-24 July 2003 Page\(s\:2631 - 2636  Ellison, E., L. Linger, and M Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013, 1997  Fleming, T. R. & D. P. Harrington. 1991. Counting process and survival analysis. John Wiley & Sons. 429pp  Graver, J. and M. Sobel 2005. You may rely on the Reliability Polynomial for much more than you might think Communications in Statistics: Theory and Methods  34\(6\1411-1422  Graves, T. and M. Hamada. 2005. Bayesian methods for assessing system reliability: models and computation. In Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson, et al. pp.41-53  Grimmett, G. 2006 The Random-Cluster Model Springer  Grimmett, G. 1999 Percolation Springer  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis.  Springer. 481pp  Jin Z. 2005. Non-proportional semi-parametric regression models for censored data. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.279-292 World Scientific  Kalbfleisch, J. D. & R. L. Prentice. 1980 The Statistical Analysis of Failure Time Data John Wiley & Sons.  New York. 1980  Kalbfleisch, J. D. &  R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data.  Wiley-InterScience, 2nd ed 462pp  Lisboa, P. J. G. and H. Wong. 2001. Are neural networks best used to help logistic regression? Proceedings of International Joint Conference on Neural Networks, IJCNN 01. Volume 4, 15-19,  July 2001. Page\(s\:2472 - 2477 vol.4  Kauffman, R. J. and B. Wang. 2002. Duration in the Digital Economy. Proceedings of th e 36th Hawaii International Conference on System Sciences \(HICSS’03\ Jan 2003  Kaplan, E. L. & P.  Meier.  1958.  Nonparametric estimation from incomplete observations J. Amer. Statist. Assoc  53:457-481  Klein, J. P. and P. K. Goel 1992. Survival Analysis: State of the Art.  Kluwer Academic Publishes. 450pp  Klein, J. P. and  M. L Moeschberger. 20 03. Survival analysis techniques for ce nsored and truncated data Springer  Krings, A. and Z. S. Ma. 2006.  "Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks MILCOM 2006, Military Communications Conference, 2325 October, 7 pages, 2006  Krings, A. W. 2008.  Survivable Systems.  in Information Assurance: Dependability and Security in Networked Systems Yi Qian, James Joshi, David Tipper, and Prashant Krishnamurthy, Morgan Kaufmann Publishers. \(in press  Lawless, J. F. 1982. Statistical models and methods for lifetime data.  John Wiley & Sons. 579pp  Lawless, J. F. 2003. Statistical models and methods for lifetime data.  John Wiley & Sons. 2nd ed. 630pp  Li, M. and P. Vitanyi. 1997. Introduction to  Kolmogorov Complexity and Its Applications. 2nd ed, Springer  Ma, Z. S. 1997.  Survival analysis and demography of Russian wheat aphid populations.  Ph.D dissertation, 307pp University of Idaho Moscow, Idaho, USA 


 20 Ma, Z. S., and E. J. Bechinski. 2008.  Developmental and Phenological Modeling of Russian Wheat Aphid Annals of Entomological Soc. Am In press  Ma, Z. S. and A. W. Krings. 2008a. The Competing Risks Analysis Approach to Reliability Survivability, and Prognostics and Health Management.  The 2008 IEEEAIAA AeroSpace Conference. BigSky, Montana, March 18, 2008. \(In Press, in the same volume  Ma, Z. S. and A. W. Krings 2008b. Multivariate Survival Analysis \(I\e Shared Frailty Approaches to Reliability and Dependence Modeling. The 2008 IEEE-AIAA AeroSpace Conference. BigSky Montana, March 1-8, 2008 In Press, in the same volume  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(II\ Multi-State Models in Biomedicine and Engineering Reliability. 2008 IEEE International Conference on Biomedical Engineering and Informatics BMEI 2008\27th-30th, 2008 Accepted   Mani, R., J. Drew, A. Betz, P. Datta. 1999. Statistics and Data Mining Techniques for Lifetime Value Modeling ACM Conf. on Knowledge Discovery and Data Mining  Mazzuchi, T. A., R Soyer., and R. V Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Meeker, W. Q. and L. A. Escobar. 1998. Statistical Methods for Reliability Data. Wiley-Interscience  Munson, J. C. 2003. Software Engineering Measurement Auerbach Publications  Nelson, W. 1969. Hazard plotting for incomplete failure data J. Qual. Tech 1:27-52  Nakagawa, T. 2006.  Shock and Damage Models in Reliability Theory. Springer  Osborn, B. 2005. Leveraging remote diagnostics data for predictive maintenance.   In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp. 353-363  Pena, E. A. and E. H. Slate. 2005. Dynamic modeling in reliability and survival analysis. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.55-71  Reineke, D. M., E. A. Pohl, and W. P. Murdock. 1998 Survival analysis and maintenance policies for a series system, with highly censore d data.  1998 Proceedings Annual Reliability and Maintainability Symposium. pp 182-188  Schabenberger, O. and C. A. Gotway. 2005. Statistical Methods for Spatial Data Analysis.  Chapman & Hall/CRC  Severini, T. A. 2000. Likelihood methods in statistics Oxford University Press  Shooman, M. L. 2002. Reliability of Computer Systems and Networks: Fault Tolerance, Analysis and Design. John Wiley and Sons. 551pp  Stillman, R. H. and M. S. Mack isack, B. Sharp, and C. Lee 1995. Case studies in survival analysis of overhead line components. IEE Conferen ce of the Reliability and Distribution Equipment. March 29-31, 1995. Conference Publication No. 406. pp210-215  Therneau, T. and P. Grambsch. 2000 Modeling Survival Data: Extending the Cox Model Springer  Wilson, A.  N. Limnios, S Kelly-McNulty, Y. Armijo 2005. Modern Statistical and Mathematical Methods in Reliability. World Scientific, New Jersey  Xie, M. 1991. Software Reliability Modeling. World Scientific Press    B IOGRAPHY   Zhanshan \(Sam\ Ma holds a Ph.D. in Entomology and is a Ph.D. candidate in Computer Science at the University of Idaho. He has published approximately 30 journal and 30 conference papers, mainly in the former field.  Prior to his recent return to academia, he worked as senior network/software engineers in software industry.  His current research interests include reliability and survivability of wireless sensor networks, fault tolerance survival analysis, evolutionary game theory, evolutionary computation and bioinformatics  Axel W. Krings is a professor of Computer Science at the University of Idaho.  He received his Ph.D. \(1993\ and M.S 1991\ degrees in Computer Science from the University of Nebraska - Lincoln, and his M.S. \(1982\ in Electrical Engineering from the FH-Aachen, Germany.  Dr. Krings has published extensively in the area of Computer Network Survivability, Security, Fault-Tolerance and Realtime Scheduling. In 2004/2005 he was a visiting professor at the Institut d'Informatique et Mathématiques Appliquées de Grenoble, at the Institut National Polytechnique de Grenoble, France.  His work has been funded by DoE/INL DoT/NIATT, DoD/OST and NIST 


