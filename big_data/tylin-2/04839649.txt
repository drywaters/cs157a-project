  1 Development of a Relay Performance Web Tool for the Mars Network  Daniel A. Allard                                                        \             Charles D. Edwards Jet Propulsion Laboratory                                                       Jet Propulsion Laboratory California Institute of Technology                                         California Institute of Technology 4800 Oak Grove Drive, Pasadena, CA 91109                     4800 Oak Grove Drive, Pasadena, CA 91109 818-354-4344                                                            \             818-354-4408 Daniel.allard@jpl.nasa.gov                                                 charles.d.edwards@jpl.nasa  Abstract 227Modern Mars surface mi ssions rely upon orbiting spacecraft to relay communications to and from Earth systems.  An important component of this multi-mission relay process is the collection of relay performance statistics supporting strategic trend analysis and tactical anomaly 
identification and tracking  Through the early Mars Exploration Rover \(MER\ission this data collection was performed via a tedious manual process cumulating in the continuous update of an Excel spreadsheet.  For the Phoenix mission, this process was greatly improved with a new software system called the Relay Data Engineering \(RDE This system provides sharing of performance data via event-driven automated data collection processes, a back-end database and a web user interface  This paper will discuss lessons learned form the development, deployment and operations of the RDE system 1 2  T ABLE OF C ONTENTS 
 1  I NTRODUCTION 1  2  D ATA C OLLECTION P ROCESS B ACKGROUND 2  3  R ELAY D ATA E NGINEERING U PGRADE 2  3  R ELAY D ATA E NGINEERING 
U PGRADE A RCHITECTURE 3  3  A CCOUNTABILITY S ERVICE C ORE ASC 4  4  R ELAY A CCOUNTABILITY A DAPTATION 7  5  R ELAY A CCOUNTABILITY A NALYSIS 13  6  
C ONCLUSIONS 14  A CKNOWLEDGEMENT 14  R EFERENCES 15  B IOGRAPHY 15  1  I NTRODUCTION  The Relay Data Engineering \(RDE\s a part of JPL\222s Multi-Mission Ground Syst em and Services Office MGSS\ulti-project relay operations infrastructure. The objective of this process is to provide access to consolidated relay planning and performance information to support 
flight data delivery planning, data loss tracking analysis and historical performance assessmen t. Over time this process 1  11 978-1-4244-26225/09/$25.00 \2512009 IEEE 2 IEEEAC paper #1289, Version 6 Updated November 4, 2008 will grow to include further related information sources e.g. navigation and tracking and improve the operational relay process through automation and consolidation of tactical overflight changes  From 2004 through 2006, the relay process was coordinated through a series of manual collection and calculation steps culminating in the update of the \223Relay Summary 
Spreadsheet.\224  The spreadsh eet was used to conduct the daily relay tag-up coordination meetings.  It contained the full set of relay passes \(several thousand\ times referred to as a \223database,\224 though of course it did not provide the full capabilities of a database  Since June of 2006 it was determined that the relay process did not provide enough value for its cost as the relay process \223stabilized\224 with only one ground system to consider \(MER  With Phoenix completely dependent upon relay support of Odyssey \(ODY\ars Reconnaissance Orbiter \(MRO possibly Mars Express \(MEX\e necessary to address the complexity of this process once again.  Staffing limits drove the need for a highly automated system, and a 
web interface could provide the demanded level of user access. The eventual developmen t task was called the \223RDE Upgrade.\224  The RDE Upgrade task took advantage of a suite of accountability components called the Accountability Service Core \(ASC\ has been under development since 2004 as part of JPL Deep Space Mission Systems DSMS\itecture prototype and pilot activities.  In early 2006, the JPL Mars Network Office funded the Mission Control, Data Management and Spacecraft Analysis subsystem \(MDAS\ to implement an RDE Upgrade that would provide a Phoenix life-ofmission accountability database using ASC  The ASC was implemented as a set of \223application frameworks\224 include  A Core Information Service Framework 
o  User-defined information tracking 


  2 o  Access to persistent object and tables defined by XML Schema Definition XSD\odels 200  A set of relay data collection applications 200  Collection automation components 200  A responsive web interface to Phoenix life-ofmission relay data they are being collected from the live operational network 200  A framework of auto-test applications  Key challenges included the automation of timely data collection from the legacy Ody ssey mission, as well as the problem of implementing a rich, responsive web interface on top of a rapidly evolving data set  The following section provides a small amount of background on the legacy data collection process that was to be automated with the RDE upgrade 2  D ATA C OLLECTION P ROCESS B ACKGROUND  The original relay process involves the coordination from all relay participants from the earliest stages of planning through post-mission analysis.  Accurate data accountability through this process has been a challenge due to the fact that most key accountability information is found in informal interfaces such as \223b yproduct\224 planning notes, data spreadsheets, and post-pass email reports.  Other information, such as post-pass data volumes, requires calculation directly from telemetry out of relay and user GDS Telemetry Data System \(TDS\atabases For the early MER mission, a full time engineer was staffed to collect data for a \223Relay Summary Report\224 \(RSR\Excel spreadsheet.  Odyssey and Mars Surveyor missions both provided relay support in this time frame.  The manual report update process involved a unreasonable number of individual steps including  Copy-and-paste plans out of a text file  Update individual cells out of orbiter pass latency spreadsheet  Update cells with individual values out of email reports  Execute data volume collection scripts and copy values to spreadsheet As MER progressed, this task was formalized in a set of data collection procedures.  The total process added up to over a thousand individual manual steps per week.  In mid2005, when the MER relay picture \223settled down\224 using Odyssey as primary relay support, the funding for this position was removed and maintenance of the relay report stopped The next section discusses the RDE upgrade task begun in 2006 to tackle this problem with a shared data system and data collection automation mechanisms 3  R ELAY D ATA E NGINEERING U PGRADE  As we began engineering early in the task there were a few candidates to potentially address with an upgrade.   The original, expensive relay data accountability task was a prime candidate for automation. \223Data Accountability\224 is defined as the collection and persistence of data and metadata in a manner that enables end-user access for analysis. \223Relay Data Accountability\224 is defined as the accountability of data relating to the end-to-end relay process o  Transactional \223CRUDS\224 \(create read, update, delete, subscribe interface o  User-configurable deployment of multi-server network  An \223Event-Driven Workflow\224 Framework o  Agent application o  Java application programming interfaces  Supporting common libraries and tools o  Web I/F in Asynchronous Java and XML \(AXAJ Object Notation \(JSON o  Configuration o  Logging o  Model management o  Etc  User-Configurable Persistent Data Layer o  Implement to local standards and best-practices  The RDE was architected as an adaptation of core ASC components. This adaptation effort took advantage of the full gamut of ASC components and identified key ASC extensions, time management in particular  The following additional software was developed to meet target RDE requirements  o  Often performing translation functions to and from legacy/input data formats o  Pass Event Scheduler o  Adaptation of JPL Distributed Object Manager \(DOM\ distributor o  Tie into ASC \223Agent Framework\224 mapping \223mission domain events\224 to user workflow \(i.e. \223event-driven workflow\224 200  A persistent relay data model \(XSD 


  3 One important benefit of choosing Relay Accountability over other tasks is the low to no impact of an accountability system upon end-user machines and networks Another candidate was the tac tical email system used for coordinating overflight pass utilization and operational tactical changes. This \223formalized\224 tactical change management approach adds a number of limitations especially the inability to track relay user transactions for potential audit and analysis. However introducing a new tactical system within the Phoenix timeframe would have had serious impacts upon the tactical network, particularly upon end-users comfortable with an existing process.  Such an upgrade is proposed for the MSL timeframe Extended product tracking functions were also considered in light of a Phoenix end-to-end Product Request Identifier but were cut largely due to scheduling constraints With these and other constraints we chose limit the scope and impact of our RDE task 200  Focus on core Relay Accountability problem over other potential areas of improvement 200  Provide user interface via web server, maximizing access while limiting impact on deployment network, and 200  Not require software delivered to mission workstations, as there will not be schedule margin to deal with integration impacts Requirements for this automated system were gathered as far back as 2003 and updated through the MER mission and again in advance of Phoenix At the beginning of the 2006 fiscal year, the MDAS Data Accountability team proposed to implement a Relay Accountability solution built fro m a common application of software called the Accountability Service Core \(ASC The RDE began integration with Phoenix and relay ground systems prior to the first Phoenix relay operational readiness test \(ORT\ests involved the Phoenix Ground Data System \(GDS\, Odyssey and MRO teams as well as other coordinating management staff.  The RDE was implemented \223in parallel\224 with Phoenix, providing a standalone \223relay accountability service\224 3 tracking data from out of each GDS team. Through each ORT, data collection mechanisms were validated during daily team meetings. The relay upgrade t ook part in four successive ORTs prior to Phoenix EDL and surface operations As mentioned, a primary goal has been to minimize impact on end-user systems.  While there was no need to deliver 3  3 That is, as much of a service as funding and staffing would allow code to user workstations the auto-data collection mechanisms do interface with relay and user GDS systems such as telemetry databases and mission file stores.  At these integration points there is the potential to impact user systems \(e.g. by flooding a telemetry database with query requests\and so an automated RDE has additional quality requirements to minimize impact in these areas A final delivery was completed in October of 2008 with an improved test framework, some automation upgrades, and a number of web user interface improvements based on mission user inputs.  This paper includes findings through this final delivery The following section describes the architecture of this new system as it is built upon the ASC software framework 3  R ELAY D ATA E NGINEERING U PGRADE A RCHITECTURE  As indicated earlier, the RDE Upgrade was implemented as an extension to the ASC system A layered view of the upgrade is illustrated in Figure 1  Figure 1 \226 Layered RDE Architecture with ASC Tables 1 and 2 explai n each upgrade component Table 1 \226 RDE Components  RDE Information Model User persistence model as defined by shared XSD schema.  The current model describes the \223Relay Pass Overflight\224, an Annotation type, and associated table structures RDE Applications Data management tools including most publish functions including planning, latency 


  4 predictions and post-pass relay ACE reports RDE Agent Components Automated relay data collection workflow.  Built with the \223ASC Agent Framework\224 RDE Web UI RDE specific web views Implemented in Javascript HTML, and CSS  Table 2 \226 ASC Components Information Server /Server Framework Serving persistent data defined via user information model and \223CRUD\224 interface.  System provides for user-configurable deployment for a multiserver network.  Solves a problem of \223objectrelational mappi  Agent Application Framework Providing configurable 223event-driven workflow\224 via a framework of application components Web Access Layer Asynchronous ASC service API for web clients via AJAX/JSON and servlets Utilities and Libraries Conf iguration, logging, JMS I/F, etc Persistent Data Store Manage relational data access and database transaction  The next section describes in greater detail ASC components as well as the use of the persistent store 3  A CCOUNTABILITY S ERVICE C ORE ASC   Overview The ASC has been under development since 2004 to address a JPL multi-mission system need for \223common accountability\224 components.  With end-to-end subsystems taking advantage of these common components, the accountability problem becomes gr eatly simplified and thus less costly and risky The ASC was developed in parallel with a military communications network prototype system called SharedNet The SharedNet  sy st em  l ooked at  provi di ng dependable quality-of-service \(QOS\mmunications across \223undependable\224 4 field deployments.  The system needed to be adaptable to broad user information model schemas and provide communications via simple \223CRUD and Subscribe\224 interface.  Ke y concepts and even core interface components were shar ed from the SharedNet work to the ASC.  In particular the hashtable-structured top-level shared object class was ad apted from the SharedNet version ASC adherence to shared technology standards including JPL and others from academia and industry maximizes the adaptability of the system.   These include Extensible Markup Language \(XML\a for persistent model definitions and Java Messaging Service \(JMS\for messaging As a framework, the ASC is specified to provide  200  A unified query and update interface for all accountability data 200  Accountability event gene ration and management 200  Automated reporting 200  Standard Web Service and messaging interfaces 200  An information model defining what is tracked which can be extended for project-specific implementations 200  A framework for building custom components and adding domain-specific logic 200  Administration and configuration utilities for these capabilities  ASC Architecture The Accountability Service Core \(ASC\ is composed of several primary components   An external, underlying persistent data store   A \223shared database service\224 providing a transactional Create, Read, Update Delete CRUD in a manner defined by an explicit XSD information model, and   An \223event-driven workflow\224 framework enabling configuration-based mapping of JMS 4  4 223Undependable\224 including low-bandwidth high levels of signal loss and potentially high latencies 200  A data repository for acc ountability data, including XML to relational data mapping 


  5 event messages to the execution of user adaptation software components  The current ASC implementation uses a relational database as a back-end data store.  It supports standard SQL statements for table and object transactions.  It is limited to tracking hierarchical \(\223is-a\224 relationships across object types, but does not directly support associative \(\223has-a\224 references.  We look to address this limitation in our ongoing work  Other supporting common libraries and tools include   Web I/F in AJAX using JSON  Configuration  Logging  Model management  JMS and Simple Object Access Protocol SOAP  Ease of adaptation has been a driving ASC goal and therefore it offers a mini mal interface set that is configurable enough for end-user domain needs  ASC Server  At the heart of the problem of accountability is a basic information tracking function  The ASC server provides an abstraction layer over the persistent store for local and remote clients.  Access to the ASC is performed via \223service\224 invocations as defined by an XML interface.  This interface was originally implemented via industry standard WSDL/S  However, early pilot efforts determined that the performance using available SOAP implementations failed to keep up to throughput requirements when dealing with large-volume and high-rate transactions.  As JMS was at the time taking hold as a JPL standard, a JMS \223request/reply\224 protocol was implemented in parallel with the SOAP interface.  While this new JMS interface required more code in support of service connection management, it demonstrated acceptable throughput performance with available JMS providers incl uding institutionally supported Fiorano and open source ActiveMQ  Most persistent data management functions are handed off to a data store layer.  In particular, persistence transaction management \(execute/undo\s left to the store layer implementation, typically via a relational database. Ideally this function is identified by an institutional standard and/or supported as an institutional function  The problem mapping end-user information definition models to operational relationa l database has grown in the industry to be called the \223object-relational mapping\224 problem Thi s runt i m e object rel a t i onal m a ppi ng function remains the problem of the ASC server  ASC Service API  As mentioned, ASC provides a transactional \223CRUD\224 API We have found a simple CRUD interface is enough to make the full use of our \223user-domain persistence models\224  Create: Model-defined objects ar e \223Created\224 and inserted as records into tables  Read: The \223Read\224 query capability includes the full expressiveness of standard SQL to all model-defined information types and tables  Update: Modify object values after the object has been created.  While legacy accountab ility systems typically rely on \223creating\224 each data point and not modifying captured data our experience is that a user update capability is required to enable complex obj ect types such as the RDE    Delete: Remove an object from the store  Subscribe: ASC requirements have also carried along the notion of a \223subscribe\224 capability however it is not currently implemented.  To date this has only been implemented via an auto-polli ng use of \223read/query\224.  The aforementioned SharedNet system made great use of a proprietary, priority-based subscribe to meet information delivery requirements in a resource-constrained system particularly in the area of maximizing scant bandwidth resources  Persistent Data Store  At the base of the architecture supporting this set of servers is a central data repository.  This repository is a relational database.  The current implementation requires Oracle however the choice of Oracle is not essential to the architecture.  Presenting the repository is a set of information servers interpreting the relational database by way of an information model \(an XML schema\oth clients and servers utilize the XML schema to interpret data in the database  ASC Adaptation  Development of a new adaptation of the ASC \(i.e. the RDE upgrade\ns with the implementation of an information model describing shared system data types and supporting tables. Much up-front adaptation time is spent engineering and developing this model Provided generic database utilities are used to auto-generate database schemas directly from the model schema. The Core Information Server loads the information model at run-time and manages database transactions as defined by the model.  The server itself is otherwise \223model-independent\224, with no specific domainbased elements 


  6  Specific \223domain data ingestion\224 clients are then developed to transform raw data \(e.g. queried from GDS telemetry databases\nto \223information object updates\224 as allowed by the model.  All client-server transactions are handled through standard APIs and dom ain-independent software infrastructure.  The only softwa re code that needs to be developed provides the specific logic to transform domain data into model-based objects, and this is further simplified by the object interface provide d as part of the core infrastructure.  Standalone, \223daemon\224 client applications are referred to as \223Software Agents\224, though the underlying client software is the same.  Th ese agents are used to ingest data published as messages to the message bus as provided by the JMS message bus  Generally there are two sorts of clients: clients with the responsibility of auto-publicati on of data to the database and clients that present data to the end user.  Specific implemented relay data collection clients are described in a later section  ASC Core Model  A primary goal of ASC is that the user defines all aspects of shared data persistence.   Thus, the minimum of constraints is imposed on end-user persistence models  To support interoperability across multi-mission accountability information models, ASC adaptation model extensions include the following constraint  All highest-level objects extend from an Accountable Item  type with the following parameters   ASC Unique identifier 200  Execution from onMessage 200  Inheritance from abstract AgentComponent o  Typically randomized  Adaptation Unique Identifier  Creation Time  Source  The current ASC supports hierarchical relationships among information-model defined objects and the mapping of userdefined data types to the appropriate tables.  Operational 223information relationship management\224 \(e.g. performing operational modification of persistent objects\ts in the hands of user workflow as it interacts with the core information service  As of the time of this writing, the extensions for the RDE task and other ASC pilots and prototypes have been relatively flat and simple table structures as opposed to rich object types.  Our experiences starting with early prototypes have been that the flat object structure has been easier for users to work with as opposed to deep \223hierarchical\224 information trees, and so the limitations of the current ASC have not, to date, held us back from implementing our key user functionality.  This will be discussed further in Section 4  ASC Agent Framework  The next ASC problem domain to address is automation Funding and staffing profiles of continued JPL operational systems make no space for additi onal daily operations tasks Ideally, the entire GDS will one day evolve to automate all forward and return link data processing functions. Such automation brings with it high expectations of operational system dependability  For example, ongoing funding profiles have precluded 22324/7\224 reliability system with full backup capabilities.  In response, we implemented a dditional data re-collection functions to minimize occasional data collection downtimes providing a level of robustness  During the early era of ASC implementation, the \223agent\224 pattern was growing into use [7 is p a ttern was en ab led  largely by the emergence of messaging as a common and successful system interface pattern  Industry and academic use of agents can be very broad in functional scope \(e.g. inference agents needed to stay focused on providing an automated system from within a \223stove-piped\224 software system alongside of a quickly evolving shared information vocabulary.  This was especially apparent through the first three Phoenix ORTs where the RDE database schema grew from 80 to 94 pass statistics and 3 to 5 tables  The ASC framework attempts to take a \223minimalist\224 approach to adaptation interf aces, however it must provide a broad range of potential user adaptations and deployments The current framework provide s an XML interface to eventtriggered user workflow.  For the RDE task, most adaptation configuration items deal with data collection timing issues and data re-collection functions  The ASC provides automation via event-driven execution of user workflow.  It takes advantage of messaging services to drive event reaction logic  Some components of the user workflow interface include o  Java Logging o  XML configuration o  DSMS message bus I/F  Through ASC prototypes and pilots we have found set of components have been effective at providing for highly adaptive automation mechanisms su ch as seen in the RDE  ASC Subsystem Connectors  


  7 The following lists key ASC subsystem architectural connectors  200  The simple operational interface, supporting remote service calls for database transactions enables complete ignorance on the part of clients as to the nature and implementation of the underlying persistent store.  It enables very low system integration costs  200  The model abstraction also enables tools to autogenerate relational database schemas including data types and mapping to tables.  This makes for low-cost schema updates and thus improves the overall ability of the system to evolve  200  The approach is highly amenable to automated regression testing as each path in the automated collection process can be auto-tested separately  The current ASC implementation has some key limitations from state-of-the-art at the time of original design and development in 2003.  A primary limitation is a locked-in Oracle interface.  This conflicts with a number of MGSS users requesting compliance with MySQL. Ongoing work looks ahead to take advantage of the rapidly advancing state-of-the art addressing basic \223object-relational\224 functionality to address this concern as well as others including support of relational in addition to hierarchical associations  The next section describes in more detail the process of adaptation of this framework for the RDE upgrade     4  R ELAY A CCOUNTABILITY A DAPTATION  Overview The primary goals of the RDE system include 200  Automating collection of key end-to-end relay parameters with a minimum of human intervention and 200  Providing web access to life-of-Phoenix-mission relay database, supporting planning, analysis, and tactical functions The adaptation of ASC for relay accountability faced a number of ongoing challenges toward meeting system goals, including  Maintaining an accurate picture of relay status e.g. provide \223relay situational awareness\224 a highly dynamic operational environment  Dealing with rapidly evolving data interfaces across a broad spectrum of the end-to-end relay process from the start of relay ORTs through surface operations  Automating data collection within the bounds of the secure flight network, while still providing data accessibility outside of flight and  Delivering a dependable \223relay data service\224 with a low level of funding and staffing Adaptation Implementation Approach The RDE tool was developed with an \223adapted\224 Agile methodology As wi t h a com m on Agi l e approach, earl y  development involved simple prototypes with frequent user demonstrations.  However, hard mission deadlines and 200  The ASC CRUD-based service API provides a primary software component connector  200  Messaging provides a primary application connector  200  Events act as \223triggering\224 connectors   Subsystem Constraints  The primary constraint of the ASC system on end-user adaptations is requiring inheritance of the top-level accountable object type  The primary constraint on the ASC is the requirement that user information definition and user workflow be fully configurable and adaptable as part of an adaptation effort  ASC Conclusions  The software architecture and implemented functionality have provided at least the following benefits  200  Decoupled component implementation reduces code-base impact, thereby reducing the impact of implementing to unexpected requirements  200  The model abstraction to persistent data allows end-user applications to deal with objects as objects as opposed to just tables. It supports a widely used table view as well, as most mission data are typically handled as tables rather than objects  


  8 quality requirements meant that the quick-turnaround approach would have to come to an end  We started the RDE adaptation by implementing a simple information model schema desc ribing a set of parameters making up an \223overflight pass\224 object type.  After presenting this model to the multi-mission relay operations team, we implemented \223prototype\224 data collection applications.  These included a \223planning publisher\224 application and a first cut at an automated pass performance data volume collector as an extension of the ASC Agent Framework.  We also implemented a basic web query interface and performed a series of user demonstrations and multi-mission relay planning mee tings, enabling us to adopt user feedback at an early stage As Phoenix Operational Readiness Tests \(ORTs approached, we adopted a more formal development and delivery process.  This included a complete delivery prior to EDL and surface operations, and a final Phoenix delivery in October 2008 was completed incorporating updates resulting from operational issues and user feedback With each ORT, new functionality was added and automation improved. In particular, the \223Overflight Review\224 web view was a focus of daily relay test meetings and at times evolved on a daily basis to keep up with user needs  The following sections describe the various components adapted for RDE The Relay Data Model The first component of the RDE tool to be developed was the information model schema.  The model is defined in the XML Schema Definition Language \(XSD   The m odel  schema defines all the informa tion that is \223tracked\224 by the system. It defines the persistent object types as well as the relational tables that store objects. This model schema is the primary input of the ASC server application  The focus of the RDE model is the relay overflight and associated information. The heart of the RDE model has always been the relay pass, otherwise known as the 223overflight pass\224 type.  During early development of the model, a couple of different overflight representation options were analyzed. In one approach, the overflight was described as a hierarchical set of classes, with \223Odyssey overflights\224 and \223MRO overflights\224 extending an abstract overflight class.  However, the addition of this hierarchical layering complicated the data publishing mechanisms and eventual user interface.  End users expressed greater comfort with seeing all possible available fields in a single view.  The simplest way to achieve this was to consolidate all of the tracked data fields in a single object type, an Overflight object.  This overflight inherits from a simple abstract Accountable Item class  With next-generation missions such as MSL, we expect to further extend the Relay Engineering model and develop new clients to ingest new types of information and from different sources  Pass Identification One key concern of pass data tracking is the identification of the overflight.  Fortunately certain pass parameters can be combined into a natural unique identifier.  Passes are identified by the combination of the following parameters  200  Relay \(or \223hailing\ asset, such as any relay orbiter 200  User \(or \223responding\224\any surface asset 200  Day of year 200  Pass number \(1 to X 200  Year  These parameters combine into a unique natural identifier for each pass that is useful in pass visualization views as well a means for publishing applications to perform live pass object updates.  This same natural identifier is used in each of the input sources.  The rest of this paper will refer to this composite identifier as the \223relay pass identifier\224  Automated Relay Data Collection  A primary driving goal of the RDE is that overflight data be published without end-user intervention  A primary challenge to the development of the RDE has been a lack of established interfaces.  A large amount of required information is pres ent in spreadsheets, email 223reports\224, and other text file byproducts.   Several of these sources underwent nearly continuous format changes from one operational readiness test to the next, up until early Phoenix operations  To provide the data to meet this model, data ingestion clients \(planning and performance were developed.  Note that the total client developm ent time for this initial upgrade took less than one month, once we had a solid model in place  For RDE, there are two sorts of 223reactive\224 clients that run as full-time processes to provide automated data publication  200  A Message Reactor that provides an interface to a legacy file management system called DOM Distributed Object Manager  200  An ASC \223Agent\224 application built that schedules relay events, publishes event messages, and performs automated data volume lookup  


  9 The DOM was utilized for two primary reasons.  First, the DOM is widely used by JPL mission systems and its interface is available to all operational machines with a standard multi-mission software deployment.  Second, and perhaps more importantly, the DOM is capable of publishing messages when files of any type are published to the file store.  These messages may be filtered by file type and other supported metadata.  This allows for the creation of \223message reactor\224 applica tions that can automatically trigger further processing when a file is published simply by reading the correct message off of a shared message bus  In the architecture of the RDE, a message reactor application reacts to one of several possible file types published to DOM by executing a new application, the purpose of which is to extract the newly published file out of the file repository, parse the relevant accountability data out of the file and publish the new data to the RDE database  The Assembled RDE Upgrade System  Figure 2 shows how runtime automation and web servers were assembled operationally     As can be seen, much of the data are being provided through the DOM and message reactor interface   The RDE Overflight Record Lifecycle   The following section discu sses the lifecycle of the 223overflight record\224 and what data sources provide updates at what point in the process  Pass Planning  The RDE \223overflight pass\224 reco rd is first created when a planning file is published with a set of passes not currently present in the database  Pass planning information includes  200  Requested passes 200  Elevation angle 200  Planned data rates  The long-range link planning process includes the creation the APGEN file that contains all pass schedules. The APGEN file is typically published more than once, with an initial version that contains all geometric opportunities, and further updates identifying t hose opportunities for which a Fi g ure 2 \226 Assembled RDE U pg rade S y stem 200  Geometric pass 


  10 relay service is requested. When a planning file is published that contains records already present in the database, each existing record is updated with any changes from the new file.  This is typically to update each geometric pass with the requested times and pass durations  There are a number of concerns when selecting a pass for use.  Limited lander energy resources drive the need for selecting passes with the best telecommunications performance.  The maximum elevation angle of the pass is a primary consideration when choosing a pass for use. When a relay orbiter is in view near the horizon, link performance can be poor due to the large off-boresight angles of the lander and orbiter antennas, the long slant range between orbiter and lander, and potential multipath effects off of the Mars surface and/or lander deck As the orbiter rises, the signal improves until it reaches the maximum elevation, and then degrades the orbiter agai n approaches the horizon. The orbital elevation for a \223quality\224 signal is typically greater than 5-10 degrees. Also, the higher the maximum orbtier elevation angle for a given , the longer the overflight and the greater the possible total volume of data that can be transmitted.  Therefore, overflight passes with a high maximum elevation are typically selected over passes with a low maximum elevation  Planning files and updates are typically published every one or two weeks  Pass Latency Predictions  One parameter the RDE is required to track is the end-ofpass \223latency\224, that is, the time the relay products for a given pass are expected to arri ve at the lander\222s GDS.  On its own, the end-of-pass latency might be considered a secondary parameter in comparison with pass planning and performance.   For relay missions that provide a \223trigger\224 mechanism to indicate product completion such as MRO via the CCSDS File Delivery Protocol \(CFDP\is a parameter is not of high value, as the \223trigger\224 from the publication of the relay file product provides a means to identify that the complete data product has been delivered  For non-CFDP relay there is a significantly greater challenge to timely relay data collection. The primary nonCFDP relay asset in use is Odyssey, which has delivered more data than any other relay orbiter.  Because no external event trigger to drive the data collection, the best that Odyssey can provide is a prediction of the time that all data should be available to the ground system GDS Telemetry Data Service \(TDS\orbital geometry, data rate etc. Nominally, this predict \(plus some margin\d be enough to reliably trigger data collection in a timely manner soon after the data are made available. However, additional latencies are not rare and data may be received at later times, so that partial or no data may be retrieved at the time of the end of pass predict. To mitigate this, it is necessary to re-request data at a later time than the pass  The RDE design utilizes this pass predict to drive an automated lookup process using the \223ASC agent\224 component  The Odyssey latency predictions have been distributed via email as an XML spreadsheet.  To support the RDE tool the Odyssey planning team agreed to publish a CSV comma separated value\ext version of the XML spreadsheet to the DOM file store.  Upon publication, this file is parsed and the latency values are published via the ASC service API \223update\224 function to the RDE database using the natural pass identifier as an index  Pass Volume Predicts  The ability to compare predicted versus actual data volume is an important function of the RDE. Such comparisons require the ingestion of pred icted volumes for each pass However comparisons are not as straightforward as merely picking the right predict for each pass. Each pass has a number of predicted values for the volume. These predicts depend upon a number of concerns such as data rate elevation mask, elevation angl e, and the remote antenna used for transmission \(Helix or Monopole change over the course of the tactical cycle, sometimes more than once, and so the final "correct" predict may not be the same as the original "planned" predict  The source of Phoenix predicts is a file called the 223Integrated Overflight Summary\224, another Microsoft Excel spreadsheet type.  This spr eadsheet contains a table of predicts for each pass with values taking into account forward and return rates, remote antenna used, etc  We devised a means of exporting these predicts to a CSV file and publishing a table of those values to the RDE database, associating the set of predicts for each pass.  To present an accurate predict the web interface presents predicts that align with the latest set of relating pass parameters \(e.g. final forward a nd return rates for the link  The identification of an accu rate predict from post-pass parameters turned out to be one of the bigger challenges of the RDE upgrade task.  Some open issues remain in this area to be addressed in ongoing work, such as the identification of the actual utilized antenna out of lander GDS telemetry  Pass Performance  There are a number of sources for post-past performance The primary tracked value is the transmitted pass volume Both forward and return link vol umes are tracked closely These data are required for all current analysis views  


  11 Other pass-related collected information includes end of pass times, average transmitter power levels, and frame and packet counts  Pass performance data is collected from a variety of sources, including 200  Query of \223raw\224 telemetry database frames 200  Parsed from data product log files 200  Relay ACE text file report \223scorecard\224  We implemented to the broadly used JPL Telemetry Data System to calculate \223raw\224 total telemetry volumes from Relay and User GDS databases.  This was the only mechanism available to calculate data volumes out of the Odyssey data flow  Unlike Odyssey, the MRO mission uses CFDP to distribute relay data.  With MRO/CFDP, a local JPL process provides an XML file containing pass information.  The total pass volume is identified in a field in this file, as well as final Pass Start and End times  The Relay \223Scorecard\224 was provided through the aforementioned DOM interface and included relay calculated data volume, frame and packet counts, and additional average, minimum, and maximum AGC values  Web Views   A major portion of development work involved the construction of a set of web views supporting strategic and tactical use.  To support these views we implemented an extension of the ASC CRUD API in JavaScript Object Notation \(JSON\ of the Asynchronous Java and XML \(AJAX provides an interface between client JavaScript and an ASC servlet running within a Tomcat application.  The AJAX \223asynchronous XML\224 pattern provides for a highly responsive XML interface  Other web components include  200  CSS Style sheets, and 200  JavaScript  The following five views were re leased as part of the final Phoenix-era RDE delivery  200  Export Volumes 200  General Query 200  Drop Dead Uplink Times, and 200  Overflight Report  Overflight Review  The purpose of this view is to present a \223quick summary\224 of pass performance.  This includes an indication of predicted vs. actual values.  This view was used daily during early ORTs to identify and track pass anomalies and reporting discrepancies.  Often enough the view was valuable as a 223double-check\224 of data collected by various operational teams and the RDE software itself It might be considered the most \223mature\224 of the ava ilable views. We expect MSL will benefit from this view      Figure 4 \226 Overflight Review   As shown, this view contains a large number of \223filtering\224 functions as well as other types of modifying functions  Table 3 describes each of the view control options  Table 3 \226 Overflight Review View Control Options  View Control Description Default Days Range Filter passes in view to a time range from < Text Entry Field 1 > days ago to < Text Entry Field 2 >.  This provides a 223moving window\224 of recent passes 3 days prior to 2 days hence SOL/DOY Filter passes by SOL or Day of Year \(DOY None Show Records Limit the query of number of passes displayed 100 UA Filter by User Asset All RA Filter by Relay Asset All Requested Display \223requested\224 passes This includes all passes with a non-null value in the requested field in the APGEN pass plans, as well as any utilized pass \(RA volume 0 True Utilized Displays any pass with an RA True 200  HTML to provide frames, etc 200  Overflight Review 


  12 volume > 0 SOL Time of Day Filter passes by local \(i.e Mars\e of day False Compare Threshold Percentage difference allowed between predicted and actual values to trigger visual discrepancy \(c.f. \223Comparing Predict vs. Actual\224 40 Remove RS Overhead Remove the Reed-Solomon encoding overhead from the displayed RA Volume values Final value = value * 233 255 True Query Re-query data from database N/A Auto-Query Auto-query the data in view according to the time delay selected from the adjacent pull-down menu \(in seconds  Export Export the data in view to csv format \(c.f. \223Export\224 N/A Report Run a JasperSoft report of the selected pass \(c.f. \223Report by Pass\224 N/A   Export Volumes  As mentioned earlier, a primary purpose of this tool is to provide life-of-mission performance data.  Over the course of the early mission, the export function on the \223Overflight Review\224 was used for this purpose. After approximately two months of data had been collected the review page became unresponsively slow to load all the required data This was largely due to the amount of Javascript on the Review page.  So an additional page was added with the purpose of quick export to csv  General Query  Another important use of the RDE tool is ad-hoc analysis To meet this need, we provided a general-purpose query display.  This includes selection of any set of fields as well any number of SQL \223where\224 clauses.  Only the SQL \223add\224 of where clauses are currently supported, \223or\224 is not    Figure 3 \226 General Query View  Drop Dead Uplink Times  Pass plans include a time value called the \223Drop Dead Uplink Time\224.  These times represents the latest possible time that a User GDS team can submit a forward link product to a Relay GDS team and have it delivered via the indicated link    Figure 4 \226 Drop Dead Uplink View  The provided view is not necessarily a great deal better than the current set of pass spread sheets for those who are used to them. However, new functions such as a \223countdown alarm\224 should improve its utility  Overflight Report  Any tabular record in the Review and General Query displays can be selected and vi ewed as a report.  The report view shows all fields for the overflight record in question Underlying this view is the JasperReport software    Figure 5 \226 Example Report  Other Views  Not all of our initial views made the final cut.  Key remaining views include a \223Latest Pass\224 statistics view \(like the Overflight Report auto-update of recent passes utilized 


  13  Use of Web Technologies The web development effort involved a long learning curve for some of the RDE Upgrade development team HTML We used HyperText Markup Language \(HTML\o layout the web framework and page structures AJAX/JSON  The AJAX \(asynchronous JavaScri pt and XML\tern was introduced to support a responsive web GUI.  An implementation of JSON \(JavaScript Object Notation provides the data service inte rface from the web client to a tomcat-maintained servlet. We have run into very few issues with this technology 10 11 12  CSS Cascading Style Sheets \(CSS\provide a straightforward way to manage web page styles. The language is well supported through the open-source FireBug Edito 14  JavaScript We used JavaScript to provide a rich interactive web interface, however our experien ce with JavaScript was not entirely favorable.   The Java Script \(JS approach to APIs including widgets led to our implementing more JS code than we might otherwise have.  We found the 223typeless\224 language occasionally caused confusion.  We did find that it could be \223made to provide\224 a rich web interface though probably a heavier interface than it needs to be.  One real positive for JS is the huge volume of sample code available on the Internet, saving time when dealing with the many implementation problems that spring up JasperReport  We used a toolkit called JasperReport to build a database report we could execute from our web interface.  A JasperReport is edited using the iReport tool. When the report is opened, the JasperSoft library is executed which updates each field from the database and presents the updated report The next section describes some of the \223analysis views\224 that can be generated from data in the RDE store 5  R ELAY A CCOUNTABILITY A NALYSIS  From the start of the Phoenix mission through the current day the RDE tool has been us ed primarily to extract pass statistics for reporting.  The key primary tracked statistic is the total volume of data delivered by each relay spacecraft Typically the \223Export Volumes\224 web view is used for a 223quick\224 export of these data, though the \223General Query\224 view can also be used for this purpose.  Raw tabular data are exported in Comma Separated Value \(CSV\at and then normally imported into an Excel spreadsheet for charting Figure 6 shows the total volume over time from the start of Phoenix mission \(Sol 0\o Sol 120, about a month past the end of prime mission Cumulative Return Link Data Volume 0 5000 10000 15000 20000 25000 30000 35000 0 30 60 90 120 Sol MRO Cumulative Ret Fill DV MRO Cumulative Ret PHX DV ODY Cumulative Ret Fill DV ODY Cumulative Ret PHX DV   Figure 6 \226 Cumulative Return Link Volume Figure 7 shows the same data values, but charted to show the total data volume transmitted per Sol  Return Link Data Volume by Sol PHX Lander Data + Fill Data 0 50 100 150 200 250 300 350 400 0 30 60 90 120 Sol ODY Ret Data Volume \(PHX Data + Fill bits MRO Ret Data Volume \(PHX Data + Fill bits  Figure 7 \226 Data Volume By Sol  On Figure 6 and Figure 7, the Phoenix Data Volume \(DV shown represents actual telemetry data and science product delivered.  The Fill DV on both figures represents \223fill\224 frames that are transmitted when a relay link is open but no actual lander data is available to transmit.  This chart shows that the Odyssey mission has carried the most part of 


  14 Phoenix relay data over MRO.  The low MRO volumes at the very early end of the chart are due to early MRO communications issues with Phoenix that were not resolved until later in the mission.  The volume discrepancies are clearly visible from both charts Figure 8 shows the data volumes for the 128K overflights as a function of Mars LMST \(Local Mean Solar Time 128 kbps Return Link Data Rate 0 10 20 30 40 50 60 70 80 90 100 0:00:00 6:00:00 12:00:00 18:00:00 0:00:00 Local Mean Solar Time ODY MRO PHX Re q uirement:  30 Mb p ass  Figure 8 \226 Data Volume By Sol  Figure 9 shows a comparison of data volume predicts vs pass performance values  Figure 9 \226 Volume Predicts vs. Performance  6  C ONCLUSIONS  For ASC, the next step is to achieve a level of database independence.  We look to take advantage of emerging best practices technologies and patterns \(e.g. Hibernate  t o  upgrade our core server function to database independence and our Agent Framework to take advantage of a workflow execution function or language such as the Business Process execution Language \(BPEL  RDE is planned for update with the Mars Science Laboratory \(MSL\ission.  This will include any required updates to the planning input s \(MSL will utilize a wider range of relay parameters, including higher data rates multiple frequency channels and possibly new modulation and coding schemes and adaptive data rate functionality and a likely major change in the handling of predicts One significant additional RDE improvement proposed is to develop a web-based tactical interface supporting pass utilization requests and tacti cal updates.  Currently, all tactical relay coordination is accomplished via email and phone.  This process could be greatly improved with the upgrade to of a web-based system.  However, if adopted this upgrade would have a large impact on the current operational tactical process and thus must be carefully engineered across a broad user base  Overall we found we were able to accomplish our primary relay accountability goals using our chosen architecture The ASC server held up as a useful way to manage objectrelational transactions over a distributed network, and our event-driven client mechan isms were assembled in a straightforward manner using the existing ASC Agent Framework as well as the DOM message reactor function Our ability to accomplish this broad integration at a consistently low level of funding tells us that these technologies and patterns are worth pursuing as part of further accountability prototype pilot, and deployment activities A CKNOWLEDGEMENT  The work described in this paper was conducted at the Jet Propulsion Laboratory, Californi a Institute of Technology under a contract with the National Aeronautics and Space Administration  The authors wish to further acknowledge other individuals who made key contributions to research and development including Marti DeMore, Lloyd DeForrest, Derek Kiang Ashley Shamilian, Lori Nakamura, Mark Palm, Priscilla Parrish and Mike Tankenson  


  15 R EFERENCES    http://www.w3.org/XML/Schema   eb Orchestration with BPEL\224 http://www.idealliance.org/pa pers/dx_xml03 papers/0406-01/04-06-01.html  Hi bernat e hom e page www.hibernate.org   Al l a rd, Dan and Hut c herson, Joe, \223C om m uni cat i ons Across Complex Space Networks\224, IEEE Aerospace Conference, March 1-8, 2008  W e b Servi ce Defi ni t i on Language http://www.w3.org/TR/wsdl   B a uer, C h ri st i a n and Ki ng Javi n Java Persi s t e nce for Hibernate, New York: Manning Publications, 2007 7] \223Software Agents An Overview\224 http://www.sce.carleton.ca/netm anage/docs/AgentsOverview ao.html  e thodology.org  http://www.riaspot.com artic les/entry/What-is-Ajax  http://www.json.org 11 h ttp to m cat.ap ach e.o r g   12] http://java.sun com/products/servlet  http://www.w3.org/Sty le/CSS    B IOGRAPHY  Dan Allard has worked as a software engineer at the Jet Propulsion Laboratory for the past 17 years.   He currently leads the development of core JPL accountability systems applications and infrastructure Other recent work includes the development of a message-based ground data system for the Mars Science Laboratory as well as research and development of ontologybased distributed communications     Dr. Charles D \(Chad\ards, Jr received his A.B degree in Physics from Princeton University in 1979 and his Ph.D. in Physics from the Calif ornia Institute of Technology in 1984.  Since then he has worked at NASA\222s Jet Propulsion Laboratory, where he currently serves as Manager of the Mars Network Office and as Chief Telecommunications Engineer for the Mars Exploration Program, leading the development of a dedicated orbiting infrastructure at Mars providing essential telecommunications and navi gation capabilities in support of Mars exploration.  Prior to that he managed the Telecommunications and Mission Operations Technology Office, overseeing a broad program of research and technology development in support of NASA\222s unique capabilities in deep space communications and mission operations.  Earlier in his career, Dr. Edwards worked in the Tracking Systems and Applications section at JPL where he carried out research on novel new radio tracking techniques in support of deep space navigation, planetary science, and radio astronomy  


  16  


Thank you Questions 


 18  Astronautical Congress Valencia, 2006 27  Bu reau  In tern atio n a l d e s Po ids et Mesures. \(2 008  August\SI Base Units. [On http://www.bipm.org/en/si/base_units   B IOGRAPHY  Author, Karl Strauss, has been employed by the Jet Propulsion Laboratory for over 22 years.  He has been in the Avionics Section from day One.  He is considered JPL\222s memory technology expert with projects ranging from hand-woven core memory \(for another employer\o high capacity solid state designs.  He managed the development of NASA\222s first Solid State Recorder, a DRAM-based 2 Gb design currently in use by the Cassini mission to Satu rn and the Chandra X-Ray observatory in Earth Orbit.  Karl was the founder, and seven-time chair of the IEEE NonVolatile Memory Technology Symposium, NVMTS, deciding that the various symposia conducted until then were too focused on one technology.  Karl is a Senior IEEE member and is active in the Nuclear and Plasma Scie nce Society, the Electron Device Society and the Aerospace Electronic Systems Society Karl is also an active member of SAE Karl thanks his wonderful wife of 28 years, Janet, for raising a spectacular family: three sons, Justin, Jeremy Jonathan.  Karl\222s passion is trains and is developing a model railroad based upon a four-day rail journey across Australia\222s Northern Outback   


 19 Bollobs, B. 2001. Random Graphs. Cambridge University Press; 2nd edition. 500pp  Cawley, G. C., B. L. C. Talbot, G. J. Janacek, and M. W Peck. 2006. Sparse Bayesian Ke rnel Survival Analysis for Modeling the Growth Domain of Microbial Pathogens  Chiang C. L. 1960. A stochastic study of life tables and its applications: I. Probability distribution of the biometric functions. Biometrics, 16:618-635  Cox,  D. R. 1972. Regression models and life tables J. R Stat. Soc. Ser. B 34:184-220  Cox, D. R. 1975.   Partial likelihood Biometrika 62:269276  Cox, D. R. & D. Oakes. 1984 Analysis of Survival Data  Chapman & Hall. London  Cressie, N. A. 1993 Statistics for Spatial Data John Wiley Sons. 900pp  Duchesne, T. 2005. Regression models for reliability given the usage accumulation history. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty Y. Armijo. pp.29-40. World Scientific, New Jersey  Eleuteri, A., R. Tagliaferri, L. Milano, G. Sansone, D D'Agostino, S. De Placido,  M. Laurentiis. 2003.  Survival analysis and neural networks. Proceedings of the International Joint Conference on Neural Networks, Vol. 4 20-24 July 2003 Page\(s\:2631 - 2636  Ellison, E., L. Linger, and M Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013, 1997  Fleming, T. R. & D. P. Harrington. 1991. Counting process and survival analysis. John Wiley & Sons. 429pp  Graver, J. and M. Sobel 2005. You may rely on the Reliability Polynomial for much more than you might think Communications in Statistics: Theory and Methods  34\(6\1411-1422  Graves, T. and M. Hamada. 2005. Bayesian methods for assessing system reliability: models and computation. In Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson, et al. pp.41-53  Grimmett, G. 2006 The Random-Cluster Model Springer  Grimmett, G. 1999 Percolation Springer  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis.  Springer. 481pp  Jin Z. 2005. Non-proportional semi-parametric regression models for censored data. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.279-292 World Scientific  Kalbfleisch, J. D. & R. L. Prentice. 1980 The Statistical Analysis of Failure Time Data John Wiley & Sons.  New York. 1980  Kalbfleisch, J. D. &  R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data.  Wiley-InterScience, 2nd ed 462pp  Lisboa, P. J. G. and H. Wong. 2001. Are neural networks best used to help logistic regression? Proceedings of International Joint Conference on Neural Networks, IJCNN 01. Volume 4, 15-19,  July 2001. Page\(s\:2472 - 2477 vol.4  Kauffman, R. J. and B. Wang. 2002. Duration in the Digital Economy. Proceedings of th e 36th Hawaii International Conference on System Sciences \(HICSS03\ Jan 2003  Kaplan, E. L. & P.  Meier.  1958.  Nonparametric estimation from incomplete observations J. Amer. Statist. Assoc  53:457-481  Klein, J. P. and P. K. Goel 1992. Survival Analysis: State of the Art.  Kluwer Academic Publishes. 450pp  Klein, J. P. and  M. L Moeschberger. 20 03. Survival analysis techniques for ce nsored and truncated data Springer  Krings, A. and Z. S. Ma. 2006.  "Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks MILCOM 2006, Military Communications Conference, 2325 October, 7 pages, 2006  Krings, A. W. 2008.  Survivable Systems.  in Information Assurance: Dependability and Security in Networked Systems Yi Qian, James Joshi, David Tipper, and Prashant Krishnamurthy, Morgan Kaufmann Publishers. \(in press  Lawless, J. F. 1982. Statistical models and methods for lifetime data.  John Wiley & Sons. 579pp  Lawless, J. F. 2003. Statistical models and methods for lifetime data.  John Wiley & Sons. 2nd ed. 630pp  Li, M. and P. Vitanyi. 1997. Introduction to  Kolmogorov Complexity and Its Applications. 2nd ed, Springer  Ma, Z. S. 1997.  Survival analysis and demography of Russian wheat aphid populations.  Ph.D dissertation, 307pp University of Idaho Moscow, Idaho, USA 


 20 Ma, Z. S., and E. J. Bechinski. 2008.  Developmental and Phenological Modeling of Russian Wheat Aphid Annals of Entomological Soc. Am In press  Ma, Z. S. and A. W. Krings. 2008a. The Competing Risks Analysis Approach to Reliability Survivability, and Prognostics and Health Management.  The 2008 IEEEAIAA AeroSpace Conference. BigSky, Montana, March 18, 2008. \(In Press, in the same volume  Ma, Z. S. and A. W. Krings 2008b. Multivariate Survival Analysis \(I\e Shared Frailty Approaches to Reliability and Dependence Modeling. The 2008 IEEE-AIAA AeroSpace Conference. BigSky Montana, March 1-8, 2008 In Press, in the same volume  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(II\ Multi-State Models in Biomedicine and Engineering Reliability. 2008 IEEE International Conference on Biomedical Engineering and Informatics BMEI 2008\27th-30th, 2008 Accepted   Mani, R., J. Drew, A. Betz, P. Datta. 1999. Statistics and Data Mining Techniques for Lifetime Value Modeling ACM Conf. on Knowledge Discovery and Data Mining  Mazzuchi, T. A., R Soyer., and R. V Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Meeker, W. Q. and L. A. Escobar. 1998. Statistical Methods for Reliability Data. Wiley-Interscience  Munson, J. C. 2003. Software Engineering Measurement Auerbach Publications  Nelson, W. 1969. Hazard plotting for incomplete failure data J. Qual. Tech 1:27-52  Nakagawa, T. 2006.  Shock and Damage Models in Reliability Theory. Springer  Osborn, B. 2005. Leveraging remote diagnostics data for predictive maintenance.   In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp. 353-363  Pena, E. A. and E. H. Slate. 2005. Dynamic modeling in reliability and survival analysis. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.55-71  Reineke, D. M., E. A. Pohl, and W. P. Murdock. 1998 Survival analysis and maintenance policies for a series system, with highly censore d data.  1998 Proceedings Annual Reliability and Maintainability Symposium. pp 182-188  Schabenberger, O. and C. A. Gotway. 2005. Statistical Methods for Spatial Data Analysis.  Chapman & Hall/CRC  Severini, T. A. 2000. Likelihood methods in statistics Oxford University Press  Shooman, M. L. 2002. Reliability of Computer Systems and Networks: Fault Tolerance, Analysis and Design. John Wiley and Sons. 551pp  Stillman, R. H. and M. S. Mack isack, B. Sharp, and C. Lee 1995. Case studies in survival analysis of overhead line components. IEE Conferen ce of the Reliability and Distribution Equipment. March 29-31, 1995. Conference Publication No. 406. pp210-215  Therneau, T. and P. Grambsch. 2000 Modeling Survival Data: Extending the Cox Model Springer  Wilson, A.  N. Limnios, S Kelly-McNulty, Y. Armijo 2005. Modern Statistical and Mathematical Methods in Reliability. World Scientific, New Jersey  Xie, M. 1991. Software Reliability Modeling. World Scientific Press    B IOGRAPHY   Zhanshan \(Sam\ Ma holds a Ph.D. in Entomology and is a Ph.D. candidate in Computer Science at the University of Idaho. He has published approximately 30 journal and 30 conference papers, mainly in the former field.  Prior to his recent return to academia, he worked as senior network/software engineers in software industry.  His current research interests include reliability and survivability of wireless sensor networks, fault tolerance survival analysis, evolutionary game theory, evolutionary computation and bioinformatics  Axel W. Krings is a professor of Computer Science at the University of Idaho.  He received his Ph.D. \(1993\ and M.S 1991\ degrees in Computer Science from the University of Nebraska - Lincoln, and his M.S. \(1982\ in Electrical Engineering from the FH-Aachen, Germany.  Dr. Krings has published extensively in the area of Computer Network Survivability, Security, Fault-Tolerance and Realtime Scheduling. In 2004/2005 he was a visiting professor at the Institut d'Informatique et Mathmatiques Appliques de Grenoble, at the Institut National Polytechnique de Grenoble, France.  His work has been funded by DoE/INL DoT/NIATT, DoD/OST and NIST 


