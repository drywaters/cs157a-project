  1 Time Delay as a Diagnostic Technique for Power Drives  Antonio E. Ginart, Anthony J. Boodhansingh, Kevin McCo rmick, Patrick W. Kalgren, and Michael J. Roemer  Impact Technologies LLC 200 Canal View  Boulevard Rochester, NY 14623 Antonio Ginart@impact-tek.com  Abstract 227A novel technique to evaluate changes over time of trigger circuit parameters, especially the power transistors of standard power electronic converters is proposed. This technique takes advantage of existing instrumentation and provides a key novel feature to assess the aging status of power el ectronic circuits and electric machines as a result of transistor degradation. We propose the use of a low-cost digital circuit, capable of time delay 
tracking.  A simplified platform for the implementation of the time delay tracking and its experimental evaluation are presented. Experimental data is included and the viability of the diagnostic technique as a practical real-time device health-state indicator assessed 1 2   T ABLE OF C ONTENTS  1  I NTRODUCTION 1  2  T IME D ELAY P RINCIPLES 1  3  E VALUATION PLATFORM FOR 
E FFECT OF A GING IN THE T IME D ELAY T ECHNIQUE 2  4  T IME  DELAY TECHNIQUE S IMULATION R ESULTS 4  6  E XPERIMENTAL R ESULTS 5  C ONCLUSIONS 6  R EFERENCES 6 
 B IOGRAPHY 6  1  I NTRODUCTION  Nowadays, power electronic converters, drives, and modern electric machines play an increasingly important role in the performance and overall operation of ships aircraft, space vehicles, gr ound vehicles, and industrial machines. In these applications, maintainability or extended operation is highly desirable during critical operation Standard power drives found in industry and military settings are based on a power electronics controller and an induction or synchronous permanent magnet \(PM\otor Standard inverters contain powerful microcontrollers and high-band signal instrumentation devices with high-voltage isolation. These inverters measure real-time terminal 
voltage and current for each of the mo time measurements of input and output voltage are common when vector control techniques are required. Additionally the inverter and motor include over-heating protection typically located in the power transistors and in the windings of the motor; in some cases, measures to protect 1  1 978-1-4244-26225/09/$25.00 \2512009 IEEE 2 IEEEAC paper #1631, Versi on 3, Updated Oct 31, 2008 the motor bearing against overh eating are also incorporated into the power drive system.  It is highly desirable to utilize this framework and the available onboard resources, such as instrumentation and microcontroller free time  t o  implement early fault detection routines for power 
transistors and motor windings. Such an integrated diagnostic system can increase the overall reliability of power drive systems for military and industrial applications Several studies that include on-line embedded capabilities in power drives have been conducted using features derived from operational conditions and traditional modeling of the system in the low-to-medium frequencies.  [2-8  Historically, the time lapse between the initial control trigger and the actual switching of the transistor, known as time delay, has been one of the accepted methods of characterizing faults found in digital systems semiconductors. From a manufacturing-perspective, the inability of faster switching is considered low performance and is usually associated with defects in fabrication Alternatively, a decrease in a devices switching capability over time is considering aging 
in an electronic circuit Similar techniques are currently used in offline testing of semiconductor devices for micr ocontrollers and other high density devices. However, the approach proposed herein minimizes the introduction of additional hardware or monitoring points by utilizing the on-board current sensors to estimate the time delay. The fundamentals of the proposed technique consist of reconstructing the switching voltage from the current phases by applying a derivative or high frequency passband circuit This allows computation of the rise time of the switching transistor which is then compared to the actual trigger pulse. The time delay is monitored for increases and used as an indicator of aging in the power device 2  T 
IME D ELAY P RINCIPLES  The fundamental On-Line Time Delay \(OLTD\echnique consists of reconstructing the switching voltage from the phase current by deriving  the current in a highly inductive circuit. This allows obtaining the rising time of the switching transistor which is then compared to the actual trigger pulse. The time delay is monitored for increases and used as an indicator of aging in the power device. As these power components deteriorate over time, their internal parasitic elements also age.  Since high frequency responses are dependent on parasitic elements, a change in ringing is expected with component aging. Based on this, it can be 


  2 assumed that ringing characterization could be used to measure the relative aging effects in power components such as switching transistors, diodes, and stator motor windings  The On-Line Time delay Technique is an alternative solution to the ringing characterization presented in  In similar fashion, this system can be integrated with existing hardware resources from the power system, mainly the power inverter, which contains a microprocessor and sensors that already measure currents throughout the system.  Resources from the microprocessor are utilized during free available microcontroller time to perform data acquisition, feature calculation, and fault detection. A diagram illustrating how the On-Line Time Delay technique can be integrated into an existing power system is provided in Figure 1   Power L ine 123 5 46 5 1 46 Dr i v e f Hig h Pa ss Filte r Tr igge r P ulse Tim e Delay Tim e De lay Aging Mode l Di agnostic And Self-healing Strategies Ad di c t i o n Feat ur es 002 o, pea k damp ing Power L ine 123 5 46 5 1 46 Dr i v e f Hig h Pa ss Filte r f Hig h Pa ss Filte r Tr igge r P ulse Tr igge r P ulse Tim e Delay Tim e Delay Tim e De lay Aging Mode l Di agnostic And Self-healing Strategies Ad di c t i o n Feat ur es 002 o, pea k damp ing  Figure 1: On-Line Time Delay 3  E VALUATION PLATFORM FOR E FFECT OF A GING IN THE T IME D ELAY T ECHNIQUE  In order to analyze the effects of aging on IGBT time delay, a testing methodology was developed to induce permanent degradation while preserving device operability A control system was used to induce accelerated transistor degradation by operating the device at high temperature. To regulate the PWM signal applied to the transistor gate, the front and back case temperature, T case\(f and T case\(b gate-toemitter voltage, V GE collector-to-emitter voltage, V CE  collector current, I C and gate current, I G were monitored In addition, a power supply was connected between the collector and emitter of the IGBT  Accelerated Aging Syst em Based on Latching Accelerated aging was performed on IGBTs using the aging system shown in Figure 2. During the aging process the transistor\222s case temper ature was controlled in a feedback loop to provide gradual regulation of the aging process \(damage accumulation accelerated by removing the heat sink from the transistor in order to elevate the junction temperature at a lower operating current, I D The temperature was measured along the front and back surfaces of the power semiconductor to approximate the junction temperature of the device, T j using the thermal model of the power device and thermal resistance values provided by the manufacturer. A data acquisition system was used to measure T case\(f and T case\(b    V GE V CE I C and I G to control the PWM signal applied to the IGBT. The PWM sequence was determined based on estimating the junction temperat ure of the IGBT in realtime. A temperature set point was established at 125% of the maximum operational junction temperature defined by the manufacturer. Then, IGBT s were aged by thermoelectrical stress until latching was observed; experimental data indicated this event o ccurs after observing a gradual increase in I G  Once latching occurred, the tran sistors were turned off for several minutes. After this recovery period, several transistors retained full operational capabilities with nonobservable changes in the nominal operating parameters V DS   I D   V GS   and I G Subsequent latching events, for the same device, typically occurred at 5 o C below the preceding latching event. Also the time between latching events decreased with the number of latching events observed. The occurrence of secondary latching at lower temperature, in a shorter time period, and at the same current is an indication of an incipient fault attributed from device degradation Finally, test boards were fabricated, identical to those shown in Figure 2\(b\ to facilitate rapid aging of multiple transistors and validate the experimental results  V I Temperature Control Drive Control Drive Control Drive Control Drive V I Temperature Control Drive Control Drive Control Drive Control Drive a\iagram b Temp. Sensor  Figure 2: \(a Accelerated Aging System; \(b Aging System Effect of Aging on Time Delay Signature The effects of time delay resu lting from aged, or degraded transistors, were evaluated for a power drive system. A power drive evaluation test-bed was developed to analyze 


  3 the corresponding effects of time delay in a controlled environment.   The correspondence between components of the standard system \(denoted in the dashed frame ringing evaluation test-bed is shown in Figure 3. The simplified diagram showed in Figure 3\(b\ identifies the three main components used to study the effects of the switching form and time delay in power drives, a singlephase of the three-phase motor winding, a free-wheeling diode, and an IGBT transistor. Figure 3\(c\ails the main parametric elements of the power devices involved in the switching  V= 30 Volt V IGBT DIODE Load a c b Motor Drive  MOTOR WINDING IGBT DIODE Trigger L M V R M IGBT L D L Cable MOTOR WINDING  C GC C CE C GE DIODE C AC   L M R M Power Drive Schematic \(a      Simplified  Parametric Model  \(c Circuit Test \(b MOTOR WINDING L E V= 30 Volt V IGBT DIODE Load a c b Motor Drive  MOTOR WINDING IGBT DIODE Trigger L M V R M IGBT L D L Cable MOTOR WINDING  C GC C CE C GE DIODE C AC   L M R M Power Drive Schematic \(a      Simplified  Parametric Model  \(c Circuit Test \(b Circuit Test \(b MOTOR WINDING L E  Figure 3: \(a b Evaluation Test-bed, and \(c  The IGBT is typically modeled with the three terminal capacitances C GE C GC C CE and one parasitic inductancecapacitance, L E From the diode only two parametric components are highlighted, the cathode-anode capacitance C AC  L D otor is represented by its stator inductance and resistance. Note, the winding capacitance and coil ground capacitance are omitted since the main interest is on the power device  Effect of Accelerated Aging in Semiconductors Aging in semiconductors as in other type of materials is regulated by the speed of the decay of the constituent fabrication materials. The key point is how long the structure conserves its original properties without combining with neighbor materials or decomposing into its original components, whichever is the case. For this basic reason, electronic semiconductors present the same type of 223progressive degradation\224 that the materials that constitute them do. The consequence of this assertion is that regardless of the specific aging mechan isms such as hot carrier injection \(HCI\ectromigration \(EM\, time dependent dielectric breakdown \(TDDB\e sinking \(interdiffusion of metal atoms from the gate into semiconductor\ohmic contact degradation \(increase in resistance between metal and semiconductor region\and any other mechanism the aging is regulated by the ra te or speed of the chemical reaction as is presented by some form of the Arrhenius equation  This is not the only equation that can model this phenomenon, but any formula that represents or models movement of particles by diffusion under the presence of any type of field forces can be used for modeling these phenomena In the case of semiconductors, as in most chemical reactions, temperature provides the main degradation catalyst. Additionally, the other powerful force that regulates aging is the electric field. The electric field is typically expressed as a potential or voltage. As such, the manifestation of aging can be studied in two main planes see Figure 4 is the static plane, where the aging process manifests itself by changes in static parameters such as leakage current. The dynamic plane presents, in general, a more clear indication of aging as it relates to transient parameters such as switching time capacitances, inductances, etc. that change with device degradation    T \(I,Tamb V \( E Large Voltages Small Insulation Leakage Current Hot carrier 003 transients recombination regeneration times Accumulation Charges trap Parametric changes Dynamic Plane  Static Plane  Aging Shells   Resonant Point  Figure 4: Two Main Planes \(Static & Dynamic by Aging Dominant Failure Mechanisms In general, power semiconductors share common failure mechanisms. However, the application and design often determines the most probable failure event for a specific device or technology  Thermal Cycling \(Void & Crack Formation Thermal cycling is one of the main phenomena that produces semiconductor aging, particularly in power applications. Device degradation occurs because thermal cycling deteriorates the thermal material that allows the device to release generated heat W h en a com posi t e of multiple materials is exposed to the stress of thermal 


  4 cycling, it deteriorates until a fracture or void space is produced \(Figure 5\iconductor industry uses different materials to produce th e heat transfer path that allows the release of the heat generated. In general, these devices have different coefficien ts of thermal expansion that make the device more susceptible to cracks or fractures due to the internal stresses originated by thermal expansion and contraction. These fractures among different materials typically do not collapse the process but deteriorate the functionality of the device  1 2 3 4  Figure 5: Void Area Creati on Process Due to Thermal Cycling A device containing an IGBT and integrated diode was thermo-electrically stressed.  Figure 5 shows the progression of the void formation and detachment of the dice from metalized structure \(collector \226 emitter\e void creation dominated the IGBT region which was subjected to the thermo-electrical aging. The diode only obtained the heat by thermal conduction and therefore, less void formation and aging is expected about it. The progression of detachment greatly increases the junction-case thermal resistance, R 004 JA The increase of the junction-case thermal resistance has large consequen ces in the changes of the parametric characteristics of the IGBT. Under the same operational conditions the junction temperature increases significantly for the aged devi ce, changing even more the value of the R 004 JA parameter \(positive feedback     IGBT DIODE Healthy Low degraded Medium  degraded Detachment process Detachment process with void formation  Figure 6 - Void Formation and Detachment Progression Due to Thermo-electrical Stress   Hot Carriers Hot carriers play an integral role in aging a wide variety of modern electronic devices Hot carrier, in a general sense, is a term that refers to the high kinetic energy particles \(electron or holes\hat are injected in areas where originally they did not belong, producing device deterioration. Under actual operational conditions, these carriers often cause genera tion and/or accumulation of electrical charge at localized el ectronic states in the device These states result from imperfections in the original SiO 2  for Si-based  devices. Another cause of hot carriers is due to defects induced during the manufacturing process 4  T IME  DELAY TECHNIQUE S IMULATION R ESULTS  In order to demonstrate the viability of this technique a simulation of the simplified test bed and IGBT switching is performed using ICAP softwa re. The IGBT parameters relevant to this work are shown in Table 1  Table 1:  IRG4BC30KD Relevant Parameters IGBT Internal Emitter Inductance \(L E  7.5nH Output Capacitance C oes  110 pF Input Capacitance C ies  920 pF Reverse Transfer Capacitance C res  27 pF C GE 900 pF C CE 83 pF C GC 27pF V GE 0 V CC 30V f=1Mzh See Figure DIODE 42ns 60ns max T J 25 o C Reverse recovery time 80ns 120ns max T J 125 o C I F 12A V R 200V di/dt=200 A IGBT Internal Emitter Inductance \(L E  7.5nH Output Capacitance C oes  110 pF Input Capacitance C ies  920 pF Reverse Transfer Capacitance C res  27 pF C GE 900 pF C CE 83 pF C GC 27pF V GE 0 V CC 30V f=1Mzh See Figure DIODE 42ns 60ns max T J 25 o C IGBT Internal Emitter Inductance \(L E  7.5nH Output Capacitance C oes  110 pF Input Capacitance C ies  920 pF Reverse Transfer Capacitance C res  27 pF C GE 900 pF C CE 83 pF C GC 27pF V GE 0 V CC 30V f=1Mzh See Figure DIODE 42ns 60ns max T J 25 o C Reverse recovery time 80ns 120ns max T J 125 o C I F 12A V R 200V di/dt=200 A  Figure 8 shows the detail of circuit simulated and Figure 8  the simulation results waveform that correspond with the following  1 The current load, I LOAD black 2 The trigger pulse, V IN blue 3 The output voltage, V OUT during switching \(green 4. Estimated V OUT example of OLTD technique proposed\ the derivative of the current  


  5 L1 1mH V= 100 Volt 3 V1 4 R2 2 5 R6 100 ohm 2 IGBT D2 C1 001uf R1 1 VOUT VIN R4 500 ohm Load L1 1mH V= 100 Volt 3 V1 4 R2 2 5 R6 100 ohm 2 IGBT D2 C1 001uf R1 1 VOUT VIN R4 500 ohm Load  Figure 7: ICAP Platform Circuit Simulation \(bottom Simulated Current and Voltages Signal   1 i\(r2 2 vin 3 vout 4 diff 200U 600U 1.00M 1.40M 1.80M ti i 0 0 0K 0 0K 0 0K 0 0K 1 2 3 4  Figure 8: Simulated Current and Voltages Signal The time delay between each waveform is obtained in Figure 9 by magnifying all the waveforms shown in Figure 8. Traditionally the turn-on and turn-off delay times expressed as T on and T off respectively, are estimated by measuring the elapsed time be tween the transition of V in and V out from an off-to-on and on-to off state accordingly However, equivalent time delay estimation can be obtained by measuring the elapsed time between a trigger command and edge trigger of the derivative of the current measurement. According to the results shown in Figure 9 the maximum difference of measured delay time between these two techniques is less than 1 s. The additional delay is caused by in-the-loop monitoring, which includes the transistor drivers, switching transistor, current sensor, and load. However for fixed delay, the offset is approximately constant. A diagram of a circuit that can provide the time delay feature for the diagnostic and prognostic module is presented in Figure 8 1 I load 2 Vin 3 Vout 4 diff \( I load 974u 982u 990us 998us 1.01 ms tim e in seconds 1 2 3 4 t on 1.9us t off 5.04us 4.3us 1 I load 2 Vin 3 Vout 4 diff \( I load 974u 982u 990us 998us 1.01 ms tim e in seconds 1 2 3 4 t on 1.9us t off 5.04us 4.3us  Figure 9 : Detail of the Turn-on and Turn-off Switching Transistor Circuit 6  E XPERIMENTAL R ESULTS  Three transistor devices  were compared in studying their switching characteristics Figure 10 identifies the changes in the dynamic behavior of the IGBT switching properties before and after aging. An appreciable decrease in the overshoot, and attenuation in ringing frequencies were observed for all three transistors after aging an increase in time delay was also observed  IGBT 1 IGBT 2 IGBT 3   Figure 10: Changes in the Ringing Characteristic of New \(top bottom Transistors In order to appreciate the difference among transistors time delay comparisons between a new transistor \(T1\and the three aged transistors \(T 2, T3, and T4\n different stages of degradation are provided in Figure 11  The aged states correspond to the number of latch events each transistor was subjected to before evaluation Transistors T1 through T4 were subjected to 0 through 3 latch events, respectively.  Figure 11 illustrates the increase of time delay compared to the switching trigger command The results are summarized in Table 2 


Time s Volts [V Aging    Gate Voltage   T 1 T 2 T 3 T 4 Trigger Line  Figure 11: Changes in the Ringing Characteristic of T2, T3, and T4   Table 2:  Time Delay and Aging Transistor Time delay s Increase s T1 0.206  T2 0.214 0.07 T3 0.240 0.36 T4 0.262 0.58  Diagnostic Circuit In order to capture the increas e in time delay in the aged IGBT, a real-time diagnostic sensor circuit is proposed as a simple solution to detect IGBT transistor faults using motor phase current measurements. This circuit consists of one operational amplifier in deri vative configuration and one comparator with a reference va lue \(trigger line is used to hold the initial rise front of the derivative of the current and the held value is compared with the actual trigger through a digital exclusive-or \( ex-or\This output is integrated over time to generate the base line that will track the increases in time delay on the systems  Filp-flop Ex-OR Derivative Current Trigger Pulse Time Delay Diagnostic Prognostic MODULE  Hardware  Figure 12 :  Time Delay Circuit Architecture  C ONCLUSIONS  This paper demonstrates the feasibility of a technique to measure the switching time delay of a transistor device. The presented method utilizes the available phase current measurement as a means to derive the time delay in a practical and simple on-line fashion. Furthermore, the minimal and low-cost hardware necessary to implement the proposed solution is introduced. The paper also presents how the time delay metric increases over time with device degradation as shown by subjecting transistors to an accelerated aging process. Th e accelerated aging process utilized a thermo-electrical stress technique where temperature acted as the dom inant degradation catalyst Temperature was generated and controlled through active switching in order to bring the test devices to various levels of degradation The relationship between the increase in delay time and degradation in conjunction with the proposed on-line monitoring methodology serve as a viable solution for the implementation of diagnostic and prognostic capabilities in power drive systems R EFERENCES    AN908 Using the dsPI C30F for Vector Contr o l of an ACI M  M i cr ochip Technology Inc. 2004 2] M. A. Rodr\355guez1, A.Claudio, D Theilliol, and L. G. Vela \223A New Fault Detection Technique for IGBT Based on Gate Voltage Monitoring\224 PESC 2007, pp. 1000-1005  R e liability of Solder Die Attaches for High Power Application,\224 Master\222s Thesis, Dept. Mech. Eng., Univ. Maryland College Park, MD, 2000 4 T herm al Characteri zation of Die-Attach Degradation in the Power MOSFET\224 PhD thesis Virg inia Polytechnic Institute.  2003   W u chen W u   Guo Gao L i m i n Dong Z h engy uan W a ng M  Held P  Jacob, P. Scacco, \223Thermal Reliability of Power Insulated Gate Bipolar Transistor IGBT\ Modules,\224 12 th Annual IEEE Semiconductor Thermal Measurement and Management Symposium, 1996  Katsis D C  v an W y k J D  223Voidinduced T h er m a l I m pedance in Power Semiconductor Modules: Some Transient Temperature Effects\224 IEEE Transactions on I ndustry Applications,Sept.-Oct 2003,Vol.39,  No 5,pp.1239- 1246  Sang Bin L ee Younsi K  Klim an G B  223An Online T echnique for  Monitoring the Insulation Condition of AC Machine Stator Windings\224.IEEE Transaction on Energy Convertion Vol. 20, No 4 Dec 2005, pp737-745  Sang Bin Lee,  Tallam  R.M., Habetler T.G.   \223A Robust, On-line Turnfault Detection Technique fo r Induction Machines Based on Monitoring the Sequence Compone nt Impedance Matrix\224 IEEE Transactions on Power Electronics, May 2003 Vol.18,  No 3,pp 865 872  S Clem ente B R Pelly   A  I sidor i   U nder standing HE XFE T  Switching. Performance," HEXFET Power MOSFETs Designer Manual Application notes and reliab ility Data. International Rectifier 1993 Vol. I Application Note 947   AE  Ginar t   D Br own PW Kalgr e n M J Roem er 223Online Ringing Characterization as a Diagnostic Technique for IGBTs in Power Drives\224 accepted for publication on IEEE TRANS on Instrumentation, & Measurement  A Ginar t  D Br own PW Kalgr e n M J Roem er  223Online Ringing Characterization as a PHM Technique for Power Drives and Electrical Machinery \223.Autotestcon, 2007 IEEE  7-20 Sept. 2007 pp 654 - 659   W u chen W u  Guo Gao L i m i n Dong Z h engy uan W a ng M  Held P  Jacob, P. Scacco, \223Thermal Reliability of Power Insulated Gate Bipolar Transistor \(IGBT odules,\224 Twelfth Annual IEEE Semiconductor Thermal Measurement and Management Symposium 1996 B IOGRAPHY  Antonio E. Ginart S\22289\226M\22201\226SM\22207\  Received the B.Sc and M Sc degrees in Electrical Engineering from Simon Bolivar  University , Caracas, Venezuela  in 1986 and 1990 1.5 20   6                             0 40 60 80 120 1 2 3 x 10 7 0 0.5 2.5 100 


  7 respectively, and the Ph.D. in Electrical Engineering from the Georgia Institute of Tec hnology in 2001. He has over 20 years of experience in motors, electronic drives, and industrial controls. He was an Instructor, Assistant Professor, and later Associate Professor at Simon Bolivar University from 1989 to 2002. He was a consultant for Aureal Semiconductors, Inc in power amplification from 1999 to 2000, where he pioneered the effort to develop Class AD amplifiers. At Impact Technologies, he is responsible of developing inte lligent automated monitoring systems for electrical and electronics equipment for industrial and military applications. His research has led to over 50 publications  Anthony J. Boodhansingh  is a Sr. Project Engineer at Impact Technologies. He received a B.S. in Electrical Engineering from The Pennsylvania State University in 2005 has been a part of the Impact team developing innovative technologies for electronic system health assessment since joining. He has worked intimately over the past two years with the rese arch and development of the power supplies PHM technology. On the technical side he has developed algorithms, test plans and automated test benches in support of software and hardware product development. On the programmatic side he has lead SBIR and commercial efforts for health management in the areas of power electronics, industrial systems, and avionic data systems  Kevin McCormick  is a Project Engineer who has worked in the Electronic Systems PHM group at Impact Technologies since 2007. Kevin\222s significant contributions have spanned multiple domains including work on transistor aging actuator PHM, and digital systems health management Kevin has contributed to numerous journal papers on a range of topics, including various studies on the parametric characteristics of MOSFETs and IGBTs as they degrade Kevin also possesses a detailed knowledge of the PHM Design Tool, a program devel oped by Impact for designing and deploying PHM systems. Recent areas of focus involve implementing cutting-edge FPGA technologies for intelligent processor design and reconfiguration, and wideband gap device reliability research  Patrick W. Kalgren has a B.S. degree in Computer Engineering from Penn State University and manages the Electronic Systems PHM group at Impact Technologies leading the development of improved diagnostics and failure prediction to enable health management for electronic systems. Patrick has a 20+year background in mechanical and electronic system analysis, diagnosis and repair. While previously employed by PSU ARL, Patrick researched automated classifiers and developed performance tests to assess cross data type performance. At Impact, he has developed adv anced signal processing applied AI techniques to fault classification, researched advanced database design and supervised various software projects related to vehicle health management. Patrick is a member of Tau Beta Pi, IEEE, The IEEE Standards Association, and the IEEE Computer Society Michael J. Roemer received his Bachelor of Science degree in Electrical Engineering and his Doctorate in Mechanical Engineering from the State University of New York at Buffalo. He is the co-founder and Director of Engineering at Impact Technologies with over 18 years experience in the development of real-time, monitoring, diagnostic and prognostic systems for a wide range of military and commercial applications.  He has extensive working knowledge in technologies su ch as artificial intelligence methods, vibration analysis, electrical signal analysis, aerothermal performance monitoring, non-destructive structural evaluation and monitoring, and probabilistic risk assessment methods.  Dr. Roemer is a past Chairman of the Machinery Failure Prevention Technology \(MFPT\ Society Prognostic Lead for the SAE E-32 Engine Condition Monitoring Committee, Member of the IGTI Marine committee and ASME Controls and Diagnostics Committee and Adjunct Professor of Mec hanical Engineering at the Rochester Institute of Technology He is the co-author of a recent book published by Wiley titled \223Intelligent Fault Diagnosis and Prognosis for Engineering Systems\224 and has written or co-authored mo re than 100 technical papers related to integrated systems health management 


 Algorithm integrity specifications Identified integrity mechanisms for ADF receipt and retrieval from storage  Storage specifications Identified the verification and validation required when the ADF is pulled from memory in addition to the encryption required during storage Storage specifications also would include any assured hardware backup6 required within the ECU  Margin Identified the memory and processing capability required in excess of supporting the initial set of cryptographic algorithms and security functions for the mission  Process functions Identified the steps required for receipt installation and transition between ADFs in addition to supporting requirements Examples include dynamic transition to the new ADF without a degradation of capabilities ADF installation while not affecting other cryptographic functions operations and use of various algorithm suites  ECU management Identified the management functions that the ECU would support including the logging and auditing of specific events such as the receipt and installation of the new ADF  Delivery Identified the approved process for delivery of the ADFs to the distribution agent To complete the use case development process allocating the requirements to the correct document to be properly verified and tested was essential The two documents which authoritatively support CMI would be a System Technical Requirements Document TRD and a System Security Requirements SSR document An SSR is a document created by the NSA which defines the ECU requirements necessary for the ECU to be certified for use in DoD systems System-level requirements would be allocated to the system TRD and then flowed to the segments These requirements would include detailed exchanges between system segments and external entities to ensure synchronization ECU-specific requirements would be allocated to the SSR as an appendix Inclusion of the CMI ECU requirements into the SSR would ensure that products being reviewed by the NSA Technical Review Board are understood as a whole with all associated requirements The inclusion of the requirements also ensures that no conflicts exist between new requirements supporting CMI and existing SSR requirements Process Summary As identified in this paper many steps need to occur for a program to become CMI compliant in regard to reprogrammability These steps documented above can be leveraged for all future programs Figure 6 shows how the steps all fit together These steps and 8 6 An assured hardware backup is equivalent to an Application Specific Integrated Circuit ASIC or another highly reliable nonprogrammable device Simultaneous use of a reprogrammable technology and an assured hardware backup will ensure continued operation if the reprogrammable component fails supporting details are the beginning of filling the knowledge gap revealed by CMI Use Case Development Review Review Policies System Development IA Functions Requirements Allocation Requirements TRD Derivation Distribution Transition SSR Figure 6 Process Summary 6 PRODUCTS AND INFRASTRUCTURE The IA product trade space for a satellite program includes commercial-off-the-shelf COTS government-off-the-shelf GOTS and program-developed products Identifying early in the program the existing IA products or those that are under-development that meet program requirements will reduce program risk which in turn will translate to cost and schedule savings The added benefit of integrating these products and more specifically the GOTS products into a satellite system will be a reduced burden of CMI on the satellite program GOTS products will be required to be CMI compliant which will reduce the burden on the program to determine how it should implement CMI IA products for a satellite system continue to be among the highest-risk products because of their lengthy certification process in addition to the continuously changing threat environment that the products need to mitigate When considering GOTS or COTS products under development the factors that need to be examined are requirements alignment product delivery dates and finally funding stability An analysis of alternatives should be done to ensure that the product chosen is the one best suited to the program As a risk-mitigation step the program might consider an offramp if there is significant risk of a late or incompatible product Upon selection of a product a best practice is to sign a Memorandum of Agreement MOA between the program and the developer or organization that will be providing the product The MOA ensures that the satellite program will purchase the ECUs and ensures that the requirements and timeline will continue to be aligned After a product selection is made the satellite program should communicate regularly with the supplying organization to avoid surprises due to changed requirements or delivery date slips For DoD systems holding discussions with one of the Service's Cryptographic Modernization Program Offices is 8 


advisable The program office would be aware of currentproduction ECUs in addition to ECUs under development Overall utilizing GOTS and COTS products for any program especially space programs in which research and development are extremely costly is essential for cost containment Additionally the use of these existing products facilitates interoperability with other ECU products which is one of the goals of CMI Relying on existing products is similar to relying on an existing infrastructure Once the requirements are in alignment and the product or infrastructure becomes available integration becomes less burdensome This will eventually become the case once the Key Management Infrastructure KMI becomes available to deliver ADFs An issue in the implementation of reprogrammable ECUs is the difficulty in determining the best method by which the ADFs should be delivered and who should be designated as the distribution agent Although this issue is not space specific due to the long lead-time required for development of space ECUs the issue is an especially time-sensitive one for space This long lead-time was validated by NSA IAD Director Daniel G Wolf who in 2005 said that the NSA established a Space IA Special Program Office to consolidate all of our support to the important and longlead-time programs 5 The end state for the ADF delivery method should be the use of the NSA KMI which was identified as a means to support the delivery of new algorithms in addition to the keys that support those algorithms Unfortunately the KMI program does not plan to support algorithm delivery in the foreseeable future Delaying this delivery support prevents near-term programs from becoming KMI compliant for algorithm delivery which in turn requires alternatives for ADF delivery to be considered After the KMI protocols for algorithm delivery are provided and adopted by the users this problem will become a non-issue How a program should approach this issue and the alternatives for addressing it are identified in Section 9 Phasing in the Transition When considering the choice of ADF distribution agent the primary focus should be on the scope of the ECUs that are affected Are the ECUs within one segment or in all segments or are they part of a larger group in which the satellite system is also just a part An associated issue relates to the ECUs all having the same design and their supporting a standard set of protocols If a limited set of ECUs is within a segment then perhaps that segment could be delegated as the distribution agent A best practice would be to have the MOS be the distribution agent so that the MOS would be aware of all the impacts to the system Instead of the MOS receiving a notice from the Space Segment that a transition is occurring the MOS would be in the driver's seat and would know about a transition before or at the same time that the Space Segment knows about it 7 PHASING IN THE TRANSITION As stated at the beginning of this paper a satellite program's goal should be complete compliance with CMI Because the definition for reprogrammability within CMI is limited and because it is not space-centric there is room for interpretation of the definition Depending on how the definition is interpreted a program may move forward in a way that meets the intent of the CMI and does not compromise the integrity of the satellite system This issue becomes important when discussing various opportunities for phasing in functions and technology to ensure the longterm success of a program The following subsections describe a proposed phased approach for a few areas in which a transition may be of benefit in achieving complete reprogrammability in space Level of Reprogrammability Determining the level of reprogrammability and which components of an ECU can be reprogrammed could follow a phrased approach The three potential levels of programmability could be 1 reprogramming a subset of the ECU's cryptographic algorithms and support software 2 reprogramming all ECU cryptographic algorithms and support software or 3 reprogramming all ECU cryptographic algorithms and support software in addition to ECU management functions Operational algorithms supporting COMSEC and TRANSEC would most likely be the first set of cryptographic functions in the subset that are found to be reprogrammable The second set of functions to consider is the set that supports the ECU itself which would include cryptographic algorithms supporting ADF integrity confidentiality wrapping unwrapping and key storage Applying this phased approach to the level of programmability meets the intent of the CMI without putting excess stress on the first space system to implement CMI As more systems are launched with reprogrammable ECUs the more functions that may be considered for reprogrammability Distribution For ADF distribution a common secure method or common set of methods to reprogram the ECUs needs to be defined to simplify and to standardize infrastructure support for secure rapid and responsive reprogrammability and software ordering distribution and loading which should be the goal of KMI's algorithm delivery capability in the future Meanwhile the near-term solution for ADF delivery will have to draw from the space contractor's proprietary methods Although this solution will meet the intent of the reprogrammability tenet of CMI it may not realize the full potential of connecting to the KMI This potential will be defined by the level of ECU reprogrammability which will determine if the ECU can support changes to the security functions required for KMI interoperability The medium-term solution is for programs to integrate the KMI algorithm delivery protocols once they become available into the ECUs while still utilizing their proprietary methods until KMI becomes available The final 9 


operational capability should be to implement the KMI protocols and utilize them upon product initialization This phased approach is necessary given that the KMI program has yet to deliver the algorithm delivery protocols Technology In Ensuring The Security Of Warfighters Satcom Via Programmable Cryptographic Devices various existing and emerging technologies were identified that could be leveraged for space applications The driving factor behind the use of emerging technology given the cost and inaccessibility of satellites is that absolute reliability and error-free operation are necessities The space community typically insists on the use of proven technologies e.g those at Level Six or Level Seven and above on the Technology Readiness Level scale used by the National Aeronautics and Space Administration NASA and techniques to ensure the reliability of those technologies One path forward in beginning to use emerging technologies without assuming a considerable amount of risk is to utilize an assured hardware backup for reprogrammable components This option provides the ability to reprogram the ECUs if the opportunity presents itself while ensuring that the mission of the system will not be compromised while testing a new technology This ability is especially applicable to critical functions within the ECU After space-qualified Field-Programmable Gate Arrays FPGAs or other reprogrammable technologies have proven their reliability in space environments an assured hardware backup can be minimized to reduce SWAP on the payload.7 Taking steps like these enables reprogrammability to be tested in space without jeopardizing expensive national assets The bottom line is this If the technology is available and proven it should be leveraged in the system Figure 7 shows the previously identified CMI transitions that can occur to support space-borne reprogrammable ECUs Although these transitions are stacked one on top of another in the figure in actuality they are independent of one another therefore one transition may occur prior to another Only time will tell how well space-borne ECUs will converge on complete compliance with CMI requirements Phasing to Space CMI Compliance All Algorithms Level of Operational Aorfithmsg  All Algorithms  Suppo Software  Reprogrammability Supo1rt Softwar Suport Software Managmient Functions Implementation Contractor Specified Transition tece M KMI One-T ime Redundant FPGA wvith Assuedd Space Qua||lified Technology Prlbogmmabile Hardwar Backup FPGA or equivaleint Figure 7 CMI Phased Transition 10 7FPGAs are being considered for use or are currently being used for other 8 THE CALL FOR AN NSA SPACE CMI POLICY Since the initiation of CMI no official NSA guidance related to space has been developed The NSA should create an official guidance document on how to apply the CMI tenets and more specifically how to apply the reprogrammability tenet to support future satellite programs Such guidance will help to alleviate confusion and will create a baseline upon which the CMI compliancy of other satellite programs may be assessed This baseline could include various aspects of reprogrammability but one that would be of great benefit is the margin memory and processing requirement for the ECU This margin requirement could be defined as a percentage over the initial required amount of memory and processing capability However if this margin were not calculated or estimated correctly it could defeat the entire purpose of CMI and negate all the work involved in making the ECU reprogrammable Each program would most likely have a different margin based on the number of IA functions supporting its mission which is why a baseline percentage would be the best way of identifying the margin requirement Although this margin may change over time due to technological or mathematical advances it would be a great starting point for a program The NSA will be the most-qualified organization to perform the margin analysis for the program The suggested process for performing a margin analysis should include a review of historical trends of the algorithms size and complexity discussions with subject matter experts in cryptographic algorithm development and a review of current operational algorithms and existing algorithms for future use This process should also include consideration of reserve capacity for increased code size to support function enhancements Each program and perhaps each of the programs ECUs depending on their function and size may have a different margin requirement 9 CONCLUSIONS This paper and the process it details help to fill a void in the knowledge base for implementation of cryptographic space products that was revealed by the NSA Cryptographic Modernization Initiative which forces space-borne ECUs to become reprogrammable The long lead-time for spaceborne ECUs makes it necessary to reduce the time spent on concept development so that efforts can be focused on ECU development This paper provides a foundation for a satellite program to begin its reprogrammability efforts The first step in this process is understanding reprogrammability and the space IA functions that it affects The next step calls for reviewing existing policies and publicly available best practices Reviewing existing documents will reduce rework especially given that CMI is required for all future parts of communication satellite payloads that are no less critical to mission effectiveness than ECUs 10 


programs and given the probable creation of additional policies and development of new best practices The primary focus of this paper is on three use cases which provide scenarios of the necessary steps to support reprogrammable ECUs Throughout the descriptions of the use cases several issues are identified and their solutions are discussed in depth These issues concern the selection of the ADF distribution agent and the ADF transition which all programs implementing CMI will need to address and can begin to address given the information provided in this paper Although each system may have a slightly different set of requirements the requirements will be similar to those derived from the use cases Products and infrastructure are addressed in this paper because one way of achieving CMI compliance is though the use of existing products Although integrating an existing product into a satellite system might not solve the CMI compliance issue it will certainly reduce the effort required for development The method of distributing the ADFs which should be done by KMI in the future serves to support the products Relying on an existing infrastructure is similar to relying on an existing product Once the requirements are in alignment and the product becomes available integrating the product into the system is easy Problems arise when there is limited understanding of the available functions or when product availability is unknown In the future KMI will be capable of supporting ADF distribution which will reduce a portion of the CMI implementation risk In the near term KMI will be phased in while the contractor-specified distribution method is phased out Technology the enabler of reprogrammable ECUs also needs to be phased in over time As each technology matures to support the space reliability requirement the assured backup can be minimized The remaining feature that should be phased in over time is the level of reprogrammability As satellite programs become more confident of their successful performance and the program's technology becomes more reliable additional portions of the ECU will become reprogrammable Finally the availability of a NSA policy on space-borne CMI compliance would greatly support future compliance efforts and allow for consistency and reuse of potential products In conclusion a CMI-compliant satellite system that supports the warfighter of the future is entirely feasible REFERENCES 1 Leveraging Cybersecurity Interview with Daniel G Wolf Military Information Technology Online Edition Volume 8 Issue 1 February 9 2004 www.militaryinformation-technology.com/article.cfm?DocID=389 Satcom Via Programmable Cryptographic Devices MILCOM 2005 October 19 2005 3 Kurt Bittner and Ian Spence Use Case Modeling 1st Edition Boston Addison-Wesley Professional 2-3 2002 4 CNNS Instruction No 4009 National Information Assurance Glossary June 2006 5 Assurance Provider Designing a Roadmap for Information Security Interview with Daniel G Wolf Military Information Technology Online Edition Volume 10 Issue 1 January 28 2006 http://www.militaryinformation-technology.com/article.cfm?DocID 1294 ACKNOWLEDGMENTS I would like to thank the TSAT Space IA team Maj Ryan Mattson Brian Sessler and Aaron Kuhn and the NSA Space SPO for their continued support to ensure the success of reprogrammable ECUs to support the warfighter BIOGRAPHY Joseph D Bull a Certified Information System Security Professional CISSP has more than five years of experience in system security engineering Currently he is an Associate at Booz Allen Hamilton where he is the Lead Engineer for Information Assurance for the Space Segment of the Transformational Satellite Communications System TSAT program He advises on various aspects of the TSAT architecture including TRANSEC COMSEC and CMI issues impacting the Space Segment and TSAT system He has also participated in the ECU requirements development process for the Air Force Cryptologic Systems Group's GOE Increment One and AVE Increment One TT&C ECU products While employed at Backbone Security he performed program assessments on a government agency enterprise Additionally while at Backbone Joe developed an automated tool to increase the timeliness and reliability of vulnerability assessments While working at the National Security Agency's Information Assurance Directorate he performed research and development on Bluetooth end-toend encryption utilizing Xilinx FPGAs Bull has an MBA from the Smeal College of Business at The Pennsylvania State University a BS in Electrical Engineering from Penn State and a BA in Physics from East Stroudsburg University 2 Joseph D Bull Ensuring the Security of Warfighters 11 


 12 The process  t  Y  t  h  t  is called the intensity process  of the counting process.  It is a stochastic process that is determined by the information contained in the history process F t via Y  t  Y  t  records the number of individuals experiencing the risk at a given time t   As it will become clear that   t is equivalent to the failure rate or hazard function in tr aditional reliability theory, but here it is treated as a stochastic process, the most general form one can assume for it. It is this generalization that encapsulates the power that counting stochastic process approach can offer to survival analysis  Let the cumulative intensity process H  t be defined as   t t ds s t H 0 0      53  It has the property that  E  N  t  F t  E  H  t  F t  H  t   54  This implies that filtration F t, is known, the value of Y  t  fixed and H  t es deterministic H  t is equivalent to the cumulative hazards in tr aditional reliability theory  A stochastic process with the property that its expectation at time t given history at time s < t is equal to its value at s is called a martingale That is M  t martingale if           t s s M F t M E s  55  It can be verified that the stochastic process         t H t N t M 56  is a martingale, and it is called the counting process martingale The increments of the counting process martingale have an expected value of zero, given its filtration F t That is  0      t F t dM E 57 The first part of the counting process martingale [Equation 56  N  t non-decreasing step function.  The second part H  t smooth process which is predictable in that its value at time t is fixed just prior to time t It is known as the compensator  of the counting process and is a random function. Therefore, the martingale can be considered as mean-zero noise and that is obtainable when one subtracts the smoothly varying compensator from the counting process  Another key component in the counting process and martingale theory for survival analysis is the notion of the predictable variation process of M  t oted by    t M It is defined as the compensator of process   2 t M  The term predictable variation process comes from the property that, for a martingale M  t it can be verified that the conditional variance of the increments of M  t  dM  t  h e inc r em ents of   t M That is          t M d F t dM Var t  58  To obtain      t F t dM Var  one needs the random variable N  t  variable with probability   t of having a jump of size 1 at time t The variance of N  t   t  1 t   since it follows b i nom ial distribution    Ignoring the ties in the censored data 2  t  is close to zero and Var  dM  t  F t    t  Y  t  h  t   This implies that the counting process N  t on the filtration F t behaves like a Poisson process with rate  t    Why do we need to convert the previous very intuitive concepts in survival analysis in to more abstract martingales The key is that many of the sta tistics in survival analysis can be derived as the stochastic integrals of the basic martingales described above The stochastic integral equations are mathematically well structured and some standard mathematical techniques for studying them can be adopted  Here, let   t K be a predictable  process An example of a predictable process is the process   t Y Over the interval 0 to t the stochastic integral of su ch a process, with respect to a martingale, is denoted by     0 u dM u K t It turns out that such stochastic integrals them selves are martingales as a function of t and their predictable variation process can be found from the predictable variation process of the original martingale by           0 2 0 u M d u K u dM u K t t 59 The above discussion was drawn from Klein and Moeschberger \(2003 also provide examples of how the above process is applied. In the following, we briefly introduce one of their exampl es  the derivation of the Nelson-Aalen cumulative hazard estimator  First, the model is formulated as           t dM dt t h t Y t dN  60  If   t Y is non-zero, then               t Y t dM t d t h t Y t dN 61  Let   t I be the indicator of whether   t Y  is positive and define 0/0 = 0, then integrating both sides of above \(61 One get 


 13                     0 0 0 u dM u Y u I u d u h u I u dN u Y u I t t t 62  The left side integral is the Nelson-Aalen estimator of H t           0  u dN u Y u I t H t 63  The second integral on the right side           0 u dM u Y u I t W t 64  is the stochastic integral of the predictable process      u Y u I with respect to a martingale, and hence is also a martingale  The first integral on the right side is a random quantity    t H   t du u h u I t H 0        65  For right-censored data it is equal to   t H  in the data range, if the stochastic uncertainty in the   t W is negligible Therefore, the statistic    t H is a nonparametric estimator of the random quantity    t H   We would like to mention one more advantage of the new approach, that is, the martingale central limit theorem.  The central limit theorem of martingales ensures certain convergence property and allows the derivations of confidence intervals for many st atistics.  In summary, most of the statistics developed with asymptotic likelihood theory in survival analysis can be derived as the stochastic integrals of some martingale.  The large sample properties of the statistics can be found by using the predictable variation process and martingale central limit theorem \(Klein and Moeschberger \(2003  2.7. Bayesian Survival Analysis  Like many other fields of statistics, survival analysis has also witnessed the rapid expansion of the Bayesian paradigm.  The introduction of the full-scale Bayesian paradigm is relative recent and occurred in the last decade however, the "invasion" has been thorough.  Until the recent publication of a monograph by Ibrahim, Chen and Sinhaet 2005\val anal ysis has been either missing or occupy at most one chapter in most survival analysis monographs and textbooks.  Ibrahim's et al. \(2005\ changed the landscape, with their comprehensive discussion of nearly every counterp arts of frequentist survival analysis from univariate to multivariate, from nonparametric, semiparametric to parametric models, from proportional to nonproportional hazards models, as well as the joint model of longitudinal and survival data.  It should be pointed out that Bayesian survival analysis has been studied for quite a while and can be traced back at least to the 1970s  A natural but fair question is what advantages the Bayesian approach can offer over the established frequentist survival analysis. Ibrahim et al 2005\entified two key advantages. First, survival models are generally very difficult to fit, due to the complex likelihood functions to accommodate censoring.  A Bayesi an approach may help by using the MCMC techniques and there is available software e.g., BUGS.  Second, the Bayesian paradigm can incorporate prior information in a natural way by using historical information, e.g., from clinical trials. The following discussion in this subsection draws from Ibrahim's et al. \(2005  The Bayesian paradigm is based on specifying a probability model for the observed data, given a vector of unknown parameters This leads to likelihood function L   D   Unlike in traditional statistics is treated as random and has a prior distribution denoted by   Inference concerning is then based on the posterior distribution which can be computed by Bayes theorem d D L D L D              66 where is the parameter space  The term    D is proportional to the likelihood    D L  which is the information from observed data, multiplied by the prior, which is quantified by   i.e          D L D 67 The denominator integral m  D s the normalizing constant of    D and often does not have an analytic closed form Therefore    D often has to be computed numerically The Gibbs sampler or other MCMC algorithms can be used to sample    D without knowing the normalizing constant m  D xist large amount of literature for solving the computational problems of m  D nd    D   Given that the general Bayesian algorithms for computing the posterior distributions should equally apply to Bayesian survival analysis, the specification or elicitation of informative prior needs much of the atte ntion.  In survival analysis with covariates such as Cox's proportional hazards model, the most popular choice of informative prior is the normal prior, and the most popular choice for noninformative is the uniform Non-informative prior is easy to use but they cannot be used in all applications, such as model selection or model comparison. Moreover, noninformative prior does not harness the real prior information. Therefore, res earch for informative prior specification is crucial for Bayesian survival analysis  


 14 Reliability estimation is influenced by the level of information available such as information on components or sub-systems. Bayesians approach is likely to provide such flexibility to accommodate various levels of information Graves and Hamada \(2005\roduced the YADAS a statistical modeling environment that implements the Bayesian hierarchical modeli ng via MCMC computation They showed the applications of YADES in reliability modeling and its flexibility in processing hierarchical information. Although this environment seems not designed with Bayesian surviv al analysis, similar package may be the direction if Bayesian survival analysis is applied to reliability modeling  2.8. Spatial Survival Analysis  To the best of our knowledge spatial survival analysis is an uncharted area, and there has been no spatial survival analysis reported with rigor ous mathematical treatment There are some applications of survival analysis to spatial data; however, they do not address the spatial process which in our opinion should be the essential aspect of any spatial survival analysis. To develop formal spatial survival analysis, one has to define the spatial process first  Recall, for survival analysis in the time domain, there is survival function   R t t T t S  Pr   68  where T is the random variable and S  t e cumulative probability that the lifetime will exceed time t In spatial domain, what is the counterpart of t One may wonder why do not we simply define the survival function in the spatial domain as   S  s  Pr S s  s R d  R > 0  69  where s is some metric for d-dimensional space R d and the space is restricted to the positive region S is the space to event measurement, e.g., the distance from some origin where we detect some point object.  The problem is that the metric itself is an attribute of space, rather than space itself Therefore, it appears to us that the basic entity for studying the space domain has to be broader than in the time domain This is probably why spatial process seems to be a more appropriate entity for studying  The following is a summary of descriptions of spatial processes and patterns, which intends to show the complexity of the issues involved.  It is not an attempt to define the similar survival function in spatial domain because we certainly unders tand the huge complexity involved. There are several monographs discussing the spatial process and patterns \(Schabenberger and Gotway 2005\.  The following discussion heavily draws from Cressie \(1993\ and Schabenberger and Gotway \(2005  It appears that the most widely adopted definition for spatial process is proposed in Cressie \(1993\, which defines a spatial process Z  s in d-dimensions as    Z  s  s D R d  70  Here Z  s denotes the attributes we observe, which are space dependent. When d = 2 the space R 2 is a plane  The problem is how to define randomness in this process According to Schabenberger and Gotway \(2005 Z  s an be thought of as located \(indexed\by spatial coordinates s  s 1  s 2  s n  the counterpart of time series Y  t  t   t 1  t 2  t n   indexed by time.  The spatial process is often called random field   To be more explicit, we denote Z  s  Z  s   to emphasize that Z is the outcome of a random experiment  A particular realization of produces a surface    s Z  Because the surface from whic h the samples are drawn is the result of the random experiment Z  s called a random function  One might ask what is a random experiment like in a spatial domain? Schabenberger and Gotway \(2005\ offered an imaginary example briefly described below.  Imagine pouring sand from a bucket on to a desktop surface, and one is interested in measuring the depth of the poured sand at various locations, denoted as Z  s The sand distributes on the surface according to the laws of physics.  With enough resources and patience, one can develop a deterministic model to predict exactly how the sand grains are distributed on the desktop surface.  This is the same argument used to determine the head-tail coin flipping experiment, which is well accepted in statistical scie nce. The probabilistic coinflip model is more parsimonious than the deterministic model that rests on the exact \(perfect\but hardly feasible representation of a coin's physics. Similarly deterministically modeling the placement of sand grains is equally daunting.  However, th e issue here is not placement of sand as a random event, as Schabenberger and Gotway 2005\phasized.  The issue is that the sand was poured only once regardless how many locations one measures the depth of the sand.  With that setting, the challenge is how do we define and compute the expectation of the random function Z  s Would E  Z  s   s  make sense  Schabenberger and Gotway \(2005\further raised the questions: \(1\to what distribution is the expectation being computed? \(2\ if the random experiment of sand pouring can only be counted once, ho w can the expectation be the long-term average  According to the definition of expectation in traditional statistics, one should repeat the process of sand pouring many times and consider the probability distributions of all surfaces generated from the repetitions to compute the expectation of Z  s is complication is much more serious 


 15 than what we may often reali ze. Especially, in practice many spatial data is obtained from one time space sample only. There is not any independent replication in the sense of observing several independent realizations of the spatial process \(Schabenberger and Gotway 2005  How is the enormous complexity in spatial statistics currently dealt with? The most commonly used simplification, which has also been vigorously criticized, is the stationarity assumption.  Opponents claim that stationarity often leads to erroneous inferences and conclusions.  Proponents counte r-argue that little progress can be made in the study of non-stationary process, without a good understanding of the stationary issues Schabenberger and Gotway 2005  The strict stationarity is a random field whose spatial distribution is invariant under translation of the coordination.  In other word s, the process repeats itself throughout its domain \(Schabenberger and Gotway 2005 There is also a second-order or weak\ of a random field  For random fields in the spatial domain, the model of Equation \(70  Z  s  s D R d  is still too general to allow statistical inference.  It can be decomposed into several sub-processes \(Cressie 1993             s s s W s s Z  D s  71  where  s  E  Z  is a deterministic mean structure called large-scale variation W  s is the zero-mean intrinsically stationary process, \(with second order derivative\nd it is called smooth small-scale variation   s is the zero-mean stationary process independent of W  s and is called microscale variation  s  is zero-mean white noise also called measurement error. This decomposition is not unique and is largely operational in nature \(Cressie 1993\he main task of a spatial algorithm is to determine the allocations of the large, small, and microscale components However, the form of the above equation is fixed Cressie 1993\, implying that it is not appropriate to sub-divide one or more of the items. Therefore, the key issue here is to obtain the deterministic  s  but in practice, especially with limited data, it is usually very difficult to get a unique  s   Alternative to the spatial domain decomposition approach the frequency domain methods or spectral analysis used in time series analysis can also be used in spatial statistics Again, one may refer to Schabenberger and Gotway \(2005  So, what are the imp lications of the general discussion on spatial process above to spatial survival analysis?  One point is clear, Equation \(69 S  s  Pr S s   s R d is simply too naive to be meaningful.  There seem, at least, four fundamental challenges when trying to develop survival analysis in space domain. \(1\ process is often multidimensional, while time pr ocess can always be treated as uni-dimensional in the sense that it can be represented as  Z  t  t R 1  The multidimensionality certainly introduces additional complications, but that is still not the only complication, perhaps not even the most significant 2\One of the fundamental complications is the frequent lack of independent replication in the sense of observing several independent realizations of the spatial process, as pointed out by Cressie \(1993\\(3\e superposition of \(1 and \(2\brings up even more complexity, since coordinates  s of each replication are a set of stochastic spatial process rather than a set of random variables. \(4\n if the modeling of the spatial process is separable from the time process, it is doubtful how useful the resulting model will be.  In time process modeling, if a population lives in a homogenous environment, the space can be condensed as a single point.  However, the freezing of time seems to leave out too much information, at least for survival analysis Since the traditional survival analysis is essentially a time process, therefore, it should be expanded to incorporate spatial coordinates into original survival function.  For example, when integrating space and time, one gets a spacetime process, such as          R t R D s t s Z d   where s is the spatial index \(coordinate t is time.  We may define the spatial-temporal survival function as   S  s  t  Pr T t  s D  72  where D is a subset of the d dimensional Euclidean space That is, the spatial-temporal survival function represents the cumulative probability that an individual will survive up to time T within hyperspace D which is a subset of the d dimensional Euclidian space.  One may define different scales for D or even divide D into a number of unit hyperspaces of measurement 1 unit  2.9. Survival Analysis and Artificial Neural Network   In the discussion on artificial neuron networks \(ANN Robert & Casella \(2004\noted, "Baring the biological vocabulary and the idealistic connection with actual neurons, the theory of neuron networks covers: \(1\odeling nonlinear relations between explanatory and dependent explained\ariables; \(2\mation of the parameters of these models based on a \(training\ sample."  Although Robert & Casella \(2004\ did not mentioned survival analysis, their notion indeed strike us in that the two points are also the essence of Cox s \(1972\roportional Hazards model  We argue that the dissimilarity might be superficial.  One of the most obvious differences is that ANN usually avoids probabilistic modeling, however, the ANN models can be analyzed and estimated from a statistical point of view, as demonstrated by Neal \(1999\, Ripley \(1994\, \(1996 Robert & Casella \(2004\What is more interesting is that the most hailed feature of ANN, i.e., the identifiability of model structure, if one review carefully, is very similar to the work done in survival anal ysis for the structure of the 


 16 Cox proportional hazards model. The multilayer ANN model, also known as back-propagation ANN model, is again very similar to the stratified proportional hazards model  There are at least two advantages of survival analysis over the ANN.  \(1\val analysis has a rigorous mathematical foundation. Counting stochastic processes and the Martingale central limit theory form the survival analysis models as stochastic integral s, which provide insight for analytic solutions. \(2\ ANN, simulation is usually necessary \(Robert & Casella \(2004\which is not the case in survival analysis  Our conjecture may explain a very interesting phenomenon Several studies have tried to integrate ANN with survival analysis.  As reviewed in the next section, few of the integrated survival analysis and ANN made significant difference in terms of model fittings, compared with the native survival analysis alone The indifference shows that both approaches do not complement each other.  If they are essentially different, the inte grated approach should produce some results that are signifi cantly different from the pure survival analysis alone, either positively or negatively Again, we emphasize that our discussion is still a pure conjecture at this stage   3   B RIEF C ASE R EVIEW OF S URVIVAL A NALYSIS A PPLICATIONS     3.1. Applications Found in IEEE Digital Library  In this section, we briefly review the papers found in the IEEE digital library w ith the keyword of survival analysis  search. The online search was conducted in the July of 2007, and we found about 40 papers in total. There were a few biomedical studies among the 40 survival-analysis papers published in IEEE publications. These are largely standard biomedical applications and we do not discuss these papers, for obvious reasons  Mazzuchi et al. \(1989\m ed to be the first to actively  advocate the use of Cox's \(1 972\ proportional hazards model \(PHM\in engineering re liability.  They quoted Cox's 1972\ original words industrial reliability studies and medical studies to show Cox's original motivation Mazzuchi et al \(1989 while this model had a significant impact on the biom edical field, it has received little attention in the reliability literature Nearly two decades after the introducti on paper of Mazzuchi et al 1989\ears that little significant changes have occurred in computer science and IEEE related engineering fields with regard to the proportional hazards models and survival analysis as a whole  Stillman et al. \(1995\used survival analysis to analyze the data for component maintenance and replacement programs Reineke et al. \(1998 ducted a similar study for determining the optimal maintenance by simulating a series system of four components Berzuini and Larizza \(1996 integrated time-series modeling with survival analysis for medical monitoring.  Kauffman and Wang \(2002\analyzed the Internet firm su rvival data from IPO \(initial public offer to business shutdown events, with survival analysis models  Among the 40 survival analysis papers, which we obtained from online search of the IEEE digital library, significant percentage is the integration of survival analysis with artificial neural networks \(ANN In many of these studies the objective was to utilize ANN to modeling fitting or parameter estimation for survival analysis.  The following is an incomplete list of the major ANN survival analysis integration papers found in IEEE digital library, Arsene et 2006 Eleuteri et al. \(2003\oa and Wong \(2001\,  Mani et al 1999\ The parameter estimation in survival analysis is particular complex due to the requirements for processing censored observations. Therefore, approach such as ANN and Bayesian statistics may be helpful to deal with the complexity. Indeed, Bayesian survival analysis has been expanded significantly in recent years \(Ibrahim, et al. 2005  We expect that evolutionary computing will be applied to survival analysis, in similar way to ANN and Bayesian approaches  With regard to the application of ANN to survival analysis we suggest three cautions: \(1\he integrated approach should preserve the capability to process censoring otherwise, survival analysis loses its most significant advantage. \(2\ Caution should be taken when the integrated approach changes model struct ure because most survival analysis models, such as Cox's proportional hazards models and accelerated failure time models, are already complex nonlinear models with built-in failure mechanisms.  The model over-fitting may cause model identifiability  problems, which could be very subtle and hard to resolve 3\he integrated approach does not produce significant improvement in terms of model fitting or other measurements, which seemed to be the case in majority of the ANN approach to survival analysis, then the extra complication should certainly be avoided. Even if there is improvement, one should still take caution with the censor handling and model identifiability issues previously mentioned in \(1\ and \(2  3.2. Selected Papers Found in MMR-2004  In the following, we briefly review a few survival analysis related studies presented in a recent International Conference on Mathematical Methods in Reliability, MMR 2004 \(Wilson et al. 2005  Pena and Slate \(2005\ddressed the dynamic reliability Both reliability and survival times are more realistically described with dynamic models. Dynamic models generally refer to the models that incorporate the impact of actions or interventions as well as thei r accumulative history, which 


 17 can be monitored \(Pena and Slate 2005\he complexity is obviously beyond simple regression models, since the dependence can play a crucial ro le. For example, in a loadsharing network system, failure of a node will increase the loads of other nodes and influences their failures  Duchesne \(2005\uggested incorporating usage accumulation information into the regression models in survival analysis.  To simplify the model building Duchesne \(2005\umes that the usage can be represented with a single time-dependent covariate.  Besides reviewing the hazard-based regression models, which are common in survival analysis, Duchesne 2005\viewed three classes of less commonly used regression models: models based on transfer functionals, models based on internal wear and the so-called collapsible models. The significance of these regression models is that they expand reliability modeling to two dimensions. One dimension is the calendar time and the other is the usage accumulation.  Jin \(2005\ reviewed the recent development in statisti cal inference for accelerated failure time \(AFT\odel and the linear transformation models that include Cox proportional hazards model and proportional odds models as special cases. Two approaches rank-based approach and l east-squares approach were reviewed in Jin \(2005\. Osbo rn \(2005\d a case study of utilizing the remote diagnostic data from embedded sensors to predict system aging or degradation.  This example should indicate the potential of survival analysis and competing risks analysis in the prognostic and health management PHM\ since the problem Osborn \(2005 addressed is very similar to PHM. The uses of embedded sensors to monitor the health of complex systems, such as power plants, automobile, medical equipment, and aircraft engine, are common. The main uses for these sensor data are real time assessment of th e system health and detection of the problems that need immediate attention.  Of interest is the utilization of those remote diagnostic data, in combination with historical reliability data, for modeling the system aging or degradation. The biggest challenge with this task is the proper transformation of wear  time The wear is not only influenced by internal \(temperature, oil pressures etc\xternal covariates ambient temperature, air pressure, etc\, but also different each time   4  S UMMARY AND P ERSPECTIVE   4.1. Summary  Despite the common foundation with traditional reliability theory, such as the same probability definitions for survival function  S  t  and reliability  R  t  i.e  S  t  R  t well as the hazards function the exact same term and definition are used in both fields\rvival analysis has not achieved similar success in the field of reliability as in biomedicine The applications of survival analysis seem still largely limited to the domain of biomedicine.  Even in the sister fields of biomedicine such as biology and ecology, few applications have been conducted \(Ma 1997, Ma and Bechinski 2008\ the engin eering fields, the Meeker and Escobar \(1998\ monograph, as well as the Klein and Goel 1992\ to be the most comprehensive treatments  In Section 2, we reviewed the essential concepts and models of survival analysis. In Section 3, we briefly reviewed some application cases of survival analysis in engineering reliability.  In computer science, survival analysis seems to be still largely unknown.  We believe that the potential of survival analysis in computer science is much broader than network and/or software reliability alone. Before suggesting a few research topics, we no te two additional points  First, in this article, we exclusively focused on univariate survival analysis. There are two other related fields: one is competing risks analysis and the other is multivariate survival analysis The relationship between multivariate survival analysis and survival analysis is similar to that between multivariate statistical analysis and mathematical statistics.  The difference is that the extension from univariate to multivariate in survival analysis has been much more difficult than the developm ent of multivariate analysis because of \(1\rvation cens oring, and \(2\pendency which is much more complex when multivariate normal distribution does not hold in survival data.  On the other hand, the two essential differences indeed make multivariate survival analysis unique and ex tremely useful for analyzing and modeling time-to-event data. In particular, the advantages of multivariate survival analysis in addressing the dependency issue are hardly matched by any other statistical method.  We discuss competing risks analysis and multivariate survival analysis in separate articles \(Ma and Krings 2008a, b, & c  The second point we wish to note is: if we are asked to point out the counterpart of survival analysis model in reliability theory, we would suggest the shock or damage model.   The shock model has an even longer history than survival analysis and the simplest shock model is the Poisson process model that leads to exponential failure rate model The basic assumption of the shock model is the notion that engineered systems endure some type of wear, fatigue or damage, which leads to the failure when the strengths exceed the tolerance limits of th e system \(Nakagawa 2006 There are extensive research papers on shock models in the probability theory literature, but relatively few monographs The monograph by Nakagawa \(2006\s to be the latest The shock model is often formulated as a Renewal stochastic process or point process with Martingale theory The rigorous mathematical derivation is very similar to that of survival analysis from counting stochastic processes  which we briefly outlined in section 2.6  It is beyond the scope of the paper to compare the shock model with survival analysis; however, we would like to make the two specific comments. \(1\Shock models are essentially the stochastic process model to capture the failure mechanism based on the damage induced on the system by shocks, and the resulting statistical models are 


 18 often the traditional reliability distribution models such as exponential and Weibull failure distributions.  Survival analysis does not depend on specific shock or damage Instead, it models the failure with abstract time-to-event random variables.  Less restrictive assumptions with survival analysis might be more useful for modeling software reliability where the notions of fatigue, wear and damage apparently do not apply.  \(2\hock models do not deal with information censoring, the trademark of failure time data.  \(3\ Shock models, perhaps due to mathematical complexity, have not been applied widely in engineering reliability yet.  In contrast, the applications of survival analysis in biomedical fields are much extensive.  While there have been about a dozen monographs on survival analysis available, few books on shock models have been published.  We believe that survival analysis and shock models are complementary, a nd both are very needed for reliability analysis, with shock model more focused on failure mechanisms and the survival analysis on data analysis and modeling  4.2. Perspective   Besides traditional industrial and hardware reliability fields we suggest that the following fields may benefit from survival analysis  Software reliability Survival analysis is not based on the assumptions of wear, fatigue, or damage, as in traditional reliability theory.  This seems close to software reliability paradigm. The single biggest challenge in applying above discussed approaches to softwa re systems is the requirement for a new metric that is able to replace the survival time variable in survival analysis. This "time" counterpart needs to be capable of characterizing the "vulnerability" of software components or the system, or the distance to the next failure event. In other words, this software metric should represent the metric-to-event, similar to time-toevent random variable in survival analysis.  We suggest that the Kolmogorov complexity \(Li and Vitanyi 1997\n be a promising candidate. Once the metric-to-event issue is resolved, survival analysis, both univariate and multivariate survival analysis can be applied in a relative straightforward manner. We suggest that the shared frailty models are most promising because we believ e latent behavior can be captured with the shared fra ilty \(Ma and Krings 2008b  There have been a few applications of univariate survival analysis in software reliability modeling, including examples in Andersen et al.'s \(1995\ classical monograph However, our opinion is that without the fundamental shift from time-to-event to new metric-to-event, the success will be very limited. In software engineering, there is an exception to our claim, which is the field of software test modeling, where time variable may be directly used in survival analysis modeling  Modeling Survivability of Computer Networks As described in Subsection 2.3, random censoring may be used to model network survivability.  This modeling scheme is particularly suitable for wireless sensor network, because 1\ of the population nature of wireless nodes and \(2\ the limited lifetime of the wireless nodes.  Survival analysis has been advanced by the needs in biomedical and public health research where population is th e basic unit of observation As stated early, a sensor networ k is analogically similar to a biological population. Furthermore, both organisms and wireless nodes are of limited lifetimes. A very desirable advantage of survival analysis is that one can develop a unified mathematical model for both the reliability and survivability of a wireless sensor network \(Krings and Ma 2006  Prognostic and Health Management in Military Logistics PHM involves extensive modeling analysis related to reliability, life predictions, failure analysis, burnin elimination and testing, quality control modeling, etc Survival analysis may provide very promising new tools Survival analysis should be able to provide alternatives to the currently used mathematical tools such as ANN, genetic algorithms, and Fuzzy logic.  The advantage of survival analysis over the other alternatives lies in its unique capability to handle information censoring.  In PHM and other logistics management modeling, information censoring is a near universal phenomenon. Survival analysis should provide new insights and modeling solutions   5  R EFERENCES   Aalen, O. O. 1975. Statistical inference for a family of counting process. Ph.D. dissertation, University of California, Berkeley  Andersen, P. K., O. Borgan, R D. Gill, N. Keiding. 1993 Statistical Models based on Counting Process. Springer  Arsene, C. T. C., et al. 2006.  A Bayesian Neural Network for Competing Risks Models with Covariates. MEDSIP 2006. Proc. of IET 3rd International Conference On Advances in Medical, Signal and Info. 17-19 July 2006  Aven, T. and U. Jensen. 1999. Stochastic Models in Reliability. Springer, Berlin  Bazovsky, I. 1961. Reliability Theory and Practice Prentice-Hall, Englewood Cliffs, New Jersey  Bakker, B. and T.  Heskes. 1999. A neural-Bayesian approach to survival analysis. IEE Artificial Neural Networks, Sept. 7-10, 1999. Conference Publication No 470. pp832-837  Berzuini, C.  and C. Larizza 1996. A unified approach for modeling longitudinal and failure time data, with application in medical mon itoring. IEEE Transactions on Pattern Analysis and Machine Intelligence. Vol. 18\(2 123 


 19 Bollobs, B. 2001. Random Graphs. Cambridge University Press; 2nd edition. 500pp  Cawley, G. C., B. L. C. Talbot, G. J. Janacek, and M. W Peck. 2006. Sparse Bayesian Ke rnel Survival Analysis for Modeling the Growth Domain of Microbial Pathogens  Chiang C. L. 1960. A stochastic study of life tables and its applications: I. Probability distribution of the biometric functions. Biometrics, 16:618-635  Cox,  D. R. 1972. Regression models and life tables J. R Stat. Soc. Ser. B 34:184-220  Cox, D. R. 1975.   Partial likelihood Biometrika 62:269276  Cox, D. R. & D. Oakes. 1984 Analysis of Survival Data  Chapman & Hall. London  Cressie, N. A. 1993 Statistics for Spatial Data John Wiley Sons. 900pp  Duchesne, T. 2005. Regression models for reliability given the usage accumulation history. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty Y. Armijo. pp.29-40. World Scientific, New Jersey  Eleuteri, A., R. Tagliaferri, L. Milano, G. Sansone, D D'Agostino, S. De Placido,  M. Laurentiis. 2003.  Survival analysis and neural networks. Proceedings of the International Joint Conference on Neural Networks, Vol. 4 20-24 July 2003 Page\(s\:2631 - 2636  Ellison, E., L. Linger, and M Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013, 1997  Fleming, T. R. & D. P. Harrington. 1991. Counting process and survival analysis. John Wiley & Sons. 429pp  Graver, J. and M. Sobel 2005. You may rely on the Reliability Polynomial for much more than you might think Communications in Statistics: Theory and Methods  34\(6\1411-1422  Graves, T. and M. Hamada. 2005. Bayesian methods for assessing system reliability: models and computation. In Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson, et al. pp.41-53  Grimmett, G. 2006 The Random-Cluster Model Springer  Grimmett, G. 1999 Percolation Springer  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis.  Springer. 481pp  Jin Z. 2005. Non-proportional semi-parametric regression models for censored data. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.279-292 World Scientific  Kalbfleisch, J. D. & R. L. Prentice. 1980 The Statistical Analysis of Failure Time Data John Wiley & Sons.  New York. 1980  Kalbfleisch, J. D. &  R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data.  Wiley-InterScience, 2nd ed 462pp  Lisboa, P. J. G. and H. Wong. 2001. Are neural networks best used to help logistic regression? Proceedings of International Joint Conference on Neural Networks, IJCNN 01. Volume 4, 15-19,  July 2001. Page\(s\:2472 - 2477 vol.4  Kauffman, R. J. and B. Wang. 2002. Duration in the Digital Economy. Proceedings of th e 36th Hawaii International Conference on System Sciences \(HICSS03\ Jan 2003  Kaplan, E. L. & P.  Meier.  1958.  Nonparametric estimation from incomplete observations J. Amer. Statist. Assoc  53:457-481  Klein, J. P. and P. K. Goel 1992. Survival Analysis: State of the Art.  Kluwer Academic Publishes. 450pp  Klein, J. P. and  M. L Moeschberger. 20 03. Survival analysis techniques for ce nsored and truncated data Springer  Krings, A. and Z. S. Ma. 2006.  "Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks MILCOM 2006, Military Communications Conference, 2325 October, 7 pages, 2006  Krings, A. W. 2008.  Survivable Systems.  in Information Assurance: Dependability and Security in Networked Systems Yi Qian, James Joshi, David Tipper, and Prashant Krishnamurthy, Morgan Kaufmann Publishers. \(in press  Lawless, J. F. 1982. Statistical models and methods for lifetime data.  John Wiley & Sons. 579pp  Lawless, J. F. 2003. Statistical models and methods for lifetime data.  John Wiley & Sons. 2nd ed. 630pp  Li, M. and P. Vitanyi. 1997. Introduction to  Kolmogorov Complexity and Its Applications. 2nd ed, Springer  Ma, Z. S. 1997.  Survival analysis and demography of Russian wheat aphid populations.  Ph.D dissertation, 307pp University of Idaho Moscow, Idaho, USA 


 20 Ma, Z. S., and E. J. Bechinski. 2008.  Developmental and Phenological Modeling of Russian Wheat Aphid Annals of Entomological Soc. Am In press  Ma, Z. S. and A. W. Krings. 2008a. The Competing Risks Analysis Approach to Reliability Survivability, and Prognostics and Health Management.  The 2008 IEEEAIAA AeroSpace Conference. BigSky, Montana, March 18, 2008. \(In Press, in the same volume  Ma, Z. S. and A. W. Krings 2008b. Multivariate Survival Analysis \(I\e Shared Frailty Approaches to Reliability and Dependence Modeling. The 2008 IEEE-AIAA AeroSpace Conference. BigSky Montana, March 1-8, 2008 In Press, in the same volume  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(II\ Multi-State Models in Biomedicine and Engineering Reliability. 2008 IEEE International Conference on Biomedical Engineering and Informatics BMEI 2008\27th-30th, 2008 Accepted   Mani, R., J. Drew, A. Betz, P. Datta. 1999. Statistics and Data Mining Techniques for Lifetime Value Modeling ACM Conf. on Knowledge Discovery and Data Mining  Mazzuchi, T. A., R Soyer., and R. V Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Meeker, W. Q. and L. A. Escobar. 1998. Statistical Methods for Reliability Data. Wiley-Interscience  Munson, J. C. 2003. Software Engineering Measurement Auerbach Publications  Nelson, W. 1969. Hazard plotting for incomplete failure data J. Qual. Tech 1:27-52  Nakagawa, T. 2006.  Shock and Damage Models in Reliability Theory. Springer  Osborn, B. 2005. Leveraging remote diagnostics data for predictive maintenance.   In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp. 353-363  Pena, E. A. and E. H. Slate. 2005. Dynamic modeling in reliability and survival analysis. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.55-71  Reineke, D. M., E. A. Pohl, and W. P. Murdock. 1998 Survival analysis and maintenance policies for a series system, with highly censore d data.  1998 Proceedings Annual Reliability and Maintainability Symposium. pp 182-188  Schabenberger, O. and C. A. Gotway. 2005. Statistical Methods for Spatial Data Analysis.  Chapman & Hall/CRC  Severini, T. A. 2000. Likelihood methods in statistics Oxford University Press  Shooman, M. L. 2002. Reliability of Computer Systems and Networks: Fault Tolerance, Analysis and Design. John Wiley and Sons. 551pp  Stillman, R. H. and M. S. Mack isack, B. Sharp, and C. Lee 1995. Case studies in survival analysis of overhead line components. IEE Conferen ce of the Reliability and Distribution Equipment. March 29-31, 1995. Conference Publication No. 406. pp210-215  Therneau, T. and P. Grambsch. 2000 Modeling Survival Data: Extending the Cox Model Springer  Wilson, A.  N. Limnios, S Kelly-McNulty, Y. Armijo 2005. Modern Statistical and Mathematical Methods in Reliability. World Scientific, New Jersey  Xie, M. 1991. Software Reliability Modeling. World Scientific Press    B IOGRAPHY   Zhanshan \(Sam\ Ma holds a Ph.D. in Entomology and is a Ph.D. candidate in Computer Science at the University of Idaho. He has published approximately 30 journal and 30 conference papers, mainly in the former field.  Prior to his recent return to academia, he worked as senior network/software engineers in software industry.  His current research interests include reliability and survivability of wireless sensor networks, fault tolerance survival analysis, evolutionary game theory, evolutionary computation and bioinformatics  Axel W. Krings is a professor of Computer Science at the University of Idaho.  He received his Ph.D. \(1993\ and M.S 1991\ degrees in Computer Science from the University of Nebraska - Lincoln, and his M.S. \(1982\ in Electrical Engineering from the FH-Aachen, Germany.  Dr. Krings has published extensively in the area of Computer Network Survivability, Security, Fault-Tolerance and Realtime Scheduling. In 2004/2005 he was a visiting professor at the Institut d'Informatique et Mathmatiques Appliques de Grenoble, at the Institut National Polytechnique de Grenoble, France.  His work has been funded by DoE/INL DoT/NIATT, DoD/OST and NIST 


