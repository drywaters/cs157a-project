Anomaly Detection in P2P Networks Using Markov Modelling J. DÌaz-Verdejo, G. Maci·-Fern·ndez P. GarcÌa-Teodoro, J. NuÒo-GarcÌa Department of Signal Theory, Telematics and Communications ñ CITIC-UGR Faculty of Computer Science and Teleco mmunications - University of Granada Granada \(Spain e-mail jedv@ugr.es, gmacia@ugr.es, pgteodor@ugr.es, piripo-powa@gmail.com    Abstract The popularity of P2P networks makes them an attractive target for hackers. Potential vulnerabilities in the software used in P2P networking represent a big threat for users and the whole community To prevent and mitigate the risks, intrusion detection techniques have been traditionally 
applied. In this work in progress, a Markov based technique is applied to the detection of anomalies in the usage of P2P protocols. The detector searches for two kinds of anomalies those that appear in the struct ure, grammar and semantics of each of the messages in the prot ocol, and those associated to the sequence of messages \(protocol sessions\. Previous results from other protocols, as HTTP and DNS, confirm the potentialities of the approach Keywords: Network and computer security; Intrusion detection Anomalous behaviour; P2P ne tworks; Markov modelling I   I NTRODUCTION  Societyís current depe ndency on communication networks, especially Internet, together with the increasing 
complexity of networks and the associated protocols, makes it necessary to possess ever more robust and reliable security techniques to protect services and users. In this context, the popularity of P2P networks, with millions of computers connected through networks as eDonkey [1 or B i t T orren t  2 a k e s th em an at tractiv e ta rget for hackers. The ubiquity of associated software allows potential hackers compromising many systems if vulnerabilities in the programs were detected and exploited. For this reason security tools are essential to mitigate the asso ciated risks One of the tools available for this purpose are the Intrusion Detection Systems \(IDS\ [3  4 F i rst pro posed i n  the 1980s, they aim at detecting intrusion events, i.e., actions 
that might put at risk the security of a given target environment. Despite its age, some limitations and challenges still remain, mainly related to the performance achieved by current IDS technologie   I n fact t h i s i s a n  active research area at present IDS systems are usually categorized in two broad sets   depe ndi n g on t h e st r a t e gy used fo r t h e det ect i o n  Signature-based IDSs \(SIDS\ detect intrusions by somehow comparing curr ent events with a set of rules that define known attacks. On the other hand, anomaly-based IDSs \(AIDS\model the normal behavior of a monitored system and its communications, and trigger an alert every time a significant deviation is observed in the analyzed 
behavior. Both kinds of approaches present advantages and drawbacks, the most relevant be ing the greater efficiency of SIDSs when detecting known attacks and also their inability to detect new attacks. A more detailed description of SIDS AIDS and their capabilities can be found in [6  8  In this work in progress, a new technique for anomaly detection previously developed by the authors is applied to P2P systems. The technique named ìService Specification and Stochastic Markovian modelingî \(S3M  1 0   h a s previously been applied to two widely used protocols like HTTP and DNS, with satisfactory results. S3M is a mixed approach that combines the us e of specifications and learning in a stochastic framework, by using probabilistic finite state 
automata \(FSA The proposed technique can model most of the protocols used in network communications and it can be used at two levels. Each individual message of the protocol is modeled at the first level, while the second tries to model the sequences of messages i.e protocol sessions\ It is possible to combine both models to account for potential interactions among anomalies at messages and sessions. The detection at the individual messages level is useful for the detection of attacks targeted at exploiting vulnerabilities in the software by using specially crafted messages or ill-formed messages On the other hand, the detection at the sequence level can detect attacks to the protocol itself \(flaws in the protocol 
design\r attempts to get advantage from it. An example of the latter could be the detection of worms propagating using P2P networks by examining th e traces of the protocol Although it is still at the initial stages, the adaptation of the technique is promising and challenging. In this paper, we highlight the main challenges that we are facing during this research and the strategies follo wed to solve them. In a first phase, only eDonkey and BitTorrent networks are considered The rest of the paper is structured as follows. Some basics of the S3M technique are explained in Section II Section III describes the main challenges found in the adaptation of the technique to P2P protocols, and the strategies followed for obtaining solutions. Finally, Section 
IV summarizes the conclusions of this paper II  T HE M ARKOV BASED S3 M T ECHNIQUE  Most network protocols, especially those in the application layer, present a well defined structure for the messages and the sequences of messages exchanged by the involved service entities. This structure is given through corresponding protocol specifications, and makes it possible to use formal methods to describe both the way in which 
2009 First International Conference on Advances in P2P Systems 978-0-7695-3831-0/09 $26.00 © 2009 IEEE DOI 10.1109/AP2PS.2009.32 156 


each message is generated and the allowed sequences of messages in the protocol [1 I n t h es e case s a F S A ca n b e  used to model the normal behaviour of a given protocol. An example is the well-known FSA describing the behaviour of the TCP protocol [1   When these FSAs are used for intrusion detection, a drawback of this approach is that it only considers the correctness of the message or sequence of messages, in terms of compliance with the specifications. The detection is specification-based This is usually insufficient, as in many cases a single or several \(a sequence\ messages originating an attack still obey the rules specified by the FSA. Besides many protocols are not fully specified, or leave certain details uncovered. This fact w ould allow a hacker to attack a protocol while no violation of its FSA is observed A more advanced approach is to build the FSAs as in the previous method in a first phase, and to evaluate the likelihood of the instances of the protocol by considering probabilities associated to the automata i.e probabilistic finite state automata \(PFSA re, each of the states of the FSA is viewed as a Markov source emitting symbols. The symbols may represent either certain fields of a single protocol message or types of messages e.g ACK, HELLO etc., depending on the level of description considered. This is the approach considered in the S3M modelling, which is briefly described next  A  S3M modelling basics  S3M makes use of the Markov theory [1 t o pr o v i d e a production model consisting of a FSA and the associated probabilities for the observed ev ents or transitions between states. A given sequence of events p can be evaluated by a model to provide a probability P\(p  of the events being generated by the model. From this probability, it is possible to define a normality score  N s p as     p P p N s   Assuming that the model properly represents the normal behavior N s p can be used to classify the events as either normal or anomalous, according to a given threshold   otherwise anomalous p N if normal p class s          B  Estimation of the model  In S3M, the model for a protocol is derived from both the protocol specification and the observation of normal  instances of the protocol in a monitored environment. The model is composed of two main elements a finite state automaton \(FSA\, defined by the states and the links connecting them \(allowed transitions\d b of observable events with some probabilities associated with the different states, events and transitions. First, the topology of the FSA \(states, links or transitions and final/initial states\educed from the specification of the protocol. Second, the transition probabilities, the set of events and their respective probabilities are calculated through a training process in which two complementary information sources are considered i e observed legitimate messages or sequences of messages and ii a priori allowed values of the different message fields. These sources are used for building a stochastic model that allows in a posterior phase, the ca lculation of the probabilities P\(p  For this purpose, it is n ecessary to assign observation probabilities for each event in each state The set of possible events in the FSA may be extracted from a specification or from any other source of information. In this case, it is hard to estimate the information related to the probability P\(p  since usually there is only information about if a given event is allowed or not in each state. For this reason, it is necessary to apply a training procedure, in which traffic traces containing the events \(messages, sequences of messages and field values within messages\alyzed. In this case, the probabilities are estimated by an accounting procedure, considering the frequencies of appearance of each event \(sequences of messages or field values within messages\. Thus, the probability of observing an event e k in a state s i is estimated as   1    ki ki M ji j count e s pe s count e s   where M stands for the total number of events in the state i   It is advisable to combine bo th sources of information i.e specifications and traffic traces as in many real services it is not feasible to acquire th e whole set of events by an inspection of the specifications This makes it essential to perform the training procedure Another significant advantage about the use of the training procedure must be i ndicated. It allows reviewing the topology of the FSA including transition probabilities according to the observed data Hence, the set of accepted transitions could be modified to incorpor ate that knowledge provided by the training data in two aspects: the relative frequency of occurrence of each transition \(event\and the transitions themselves Moreover, many protocols are not completely specified which results in different im plementations, with few minor differences among them. The use of training data to estimate the allowed transitions and to assign probability values to them allows incorporating thes e differences into the model Furthermore, if the set of tr aining data is representative enough, there would be no ne ed to initially establish the FSA manually from specifications, as it could be deduced from the training process. As it will be discussed below, this is an important advantage for complex protocols, as is the case in P2P protocols 
157 


III  C HALLENGES AND S TRATEGIES FOR THE U SE OF S3M  IN P2P  P ROTOCOLS   The aim of this work in progress is to use the S3M technique for the security analysis of P2P protocols. In this direction, we are currently wo rking on the adaptation of the S3M technique in two levels. One model is being developed for the inner structure of each protocol message, and another for the sequences of messages sessions\ exchanged during the different operations of the protocol. In a first phase, we are considering only protocols used by eMule and BitTorrent, as they are widely deployed protocols Although it could seem that, based on the experience on HTTP and DNS, S3M could be applied to P2P protocols in a straightforward way, some new challenges appear when trying to afford this task, mainly due to the special features of P2P protocols. Some of these peculiarities and challenges are described in the following  Protocol complexity P2P protocols are more complex than DNS or HTTP in the sense that they consider many different types of messages, and the sessions in the protocols usually involve the exchange of large sequences of messages. As an example, the specification of the HTTP protocol defines two types of messages i.e request and response, with only eight possible methods i.e GET, POST, HEAD OPTIONS, etc. However, in the eMule implementation of the eDonkey protocol, we have detected more than 12 different kinds of sessions between clients or between a client and an eMule server. Besides, the number of different messages in the protocol is higher than 30 This means that building FSAs for P2P protocols result in really big automata when compared to those obtained for the HTTP or DNS protocols. For this reason, we have designed a methodology that allows the split of the automaton for the whole protocol in different sub-automata representing diverse operations in the protocol. Dealing with these reduced size sub-automata b ecomes now a feasible task especially if this automata should be trained as a previous step to a detection phase. As an example, in Fig. 1 we can see the sub-FSA used for the connection establishment in the eDonkey protocol  Lack of strict implementations As opposed to protocols like HTTP and DNS, which are well specified in publicly available documents, P2P protocols tend to be not well documented, or the specifications lack of information. In many cases, these specifications have been obtained through techniques like reverse engineer ing of the own software that implements the protocol Besides this fact, it is remarkable that even for a same protocol e.g eDonkey, many different implementations appear \(see Table I for implementations of the eDonkey protocol\ Many of them contain certain bugs and others even consider certain extensions for the own application e.g eMule extensions As a consequence of these facts, the process of building automata for the protocol becomes even harder when compared with the HTTP or DN S case. For this reason, in this ongoing work we have designed a methodology for building the automata as a dynamic \(not manual\ process We are currently evaluating an incremental approach that builds the sub-automata in several phases. The final objective is to develop a complete model for the protocol starting from smaller sub-auto mata. In a first phase, a relatively small subset of sub-FSAs is manually generated by using one of the available specifications of the protocol also identifying in this process a subset of the types of messages. Then, some traces are captured in a controlled environment. The available sub-FSAs are verified by using these traces, and the observed deviations are incorporated into the models. In this process, we are somehow training the sub-FSAs, as it is possible to redefine the allowed transitions between states and the available messages when doing these transitions. Next, those messages belonging to TABLE I  P RESENCE OF DIFFERENT IMPLEMENTATIONS FOR THE E D ONKEY PROTOCOL  Implementation Presence of observed clients Percentage of observed clients eDonkey 12940214 14.23 Old mldonkey 4708 0.01 New mldonkey 87941 0.10 Overnet 5844641 6.43 eMule 70302372 77.32 cDonkey 3212 0.004 xMule 126601 0.14 Shareaza 1289576 1.42 aMule 325209 0.36 Data extracted from  Figure 1  Example of sub-FSA for the connection establishment in eDonkey protocol. The initial/final st ates are coloured in yellow 
158 


protocol operations which have been understood by the available sub-FSAs are filte red out of the trace. The resulting dataset is used to infer new sub-automata and types of messages until all the traffic is in accordance with the models. Then, a new phase starts, in which a new dataset is recorded, now in a less restri ctive environment. The same process as in the previous phase is followed until the whole set of sub-automata is esta blished \(or there is some confidence about that\Finally the partial sub-automata are merged according to the observed sequences. The probabilistic nature of the FSA can be introduced at any point just by considering the relative frequencies of appearance  Data representativeness In order to build models that have a good representativeness of the real behavior of the protocols, we need to work with extens ive traces taken from non-controlled environments. This is done in the last phase of our methodology. However, and additional challenge appear in this process. In order to use these models for intrusion detection, a ìcleanî training set [1 sh oul d be used i.e data without attacks to train the models. In other words, as the system is attempting to model the normal behavior of the instances of the protocol, the training set must be representative of this normal operation and should not contain attack instances On the other hand, the traffic should be real and not simulated, as the purpose is to model the normal operation of a real environment with real users 1 As a c o n s eq uence  i f t h er e is no control on the traffic from users, it is very difficult to obtain a trace with no attack instances. Various approaches to this problem have been proposed in the literatu 6  b u t th ey all rely o n th e use of a S-NIDS to filter out the attacks in the captured traffic, which can be inaccurate due to false positives and to detection errors in the process In our approach, for dealing with such issue, during the noncontrolled environment phase, given the fact that preliminary FSAs are built in the first phases, we try to get advantage of this information to filter possible attack instances that appear in the traces. For this task, we are evaluating the use of different non-supervised techniques for pattern recognition e.g clustering techniques. The rationale behind this idea is that protocol procedures or messages that appear in the trace will only be incorporated into the model if they appear a considerab le number of times or they sufficiently resemble a known procedure in the model IV  C ONCLUSIONS  This paper presents the general guidelines of a work in progress aimed at developing an intrusion detection system for detection of anomalies in P2P protocols. It is based on the adaption of a technique based on Markov FSA previously designed by the authors for detection of anomalies in HTTP and DNS protocols The main conclusion from th e preliminary work already done is that it is feasible the adaptation of the technique to the peculiarities that P2P protocols present. The main challenges have been identified and some strategies have been proposed to solve thes e problems. We are currently evaluating the effectiveness of these solutions A CKNOWLEDGMENTS  This work was partially supported by the Spanish National Research Program of the MEC, under project TEC200806663-C03-02 \(70% FEDER funds R EFERENCES  1  Y. Kulback, and D. Bickson \(2004\ule Protocol Specification. Available at http://www.cs.huji.ac il/labs/danss/p2p/resources/emule.pdf 2  B. Cohen \(2008\itTorrent Protocol Specification Available at http://www.bitto rrent.org/beps/bep_0003.html 3  J.P. Anderson \(1980\ Threat Monitoring and Surveillanceî. Technical report, James P Anderson Co Fort Washington, Pennsylvania  4  E.D. Denning \(1987\ion-Detection Modelî. IEEE Transactions on Software Engine ering, Vol. 13, N. 2; pp. 222232  5  T.S. Sobh \(2006\îWired and Wireless Intrusion Detection System: Classifications, Good Ch aracteristics and State-ofthe-artî. Computer Standards & Interfaces, Vol. 28; pp. 670694  6  P.N. Tan, M. Steinbach, and V. Kumar \(2006 to Data Miningî. Addison-Wesley  7  A. Patcha, and J.M. Park 2007\: ìAn Overview of Anomaly Detection Techniques: Exis ting Solutions and Latest Technological Trendsî. Comput er Networks, Vol. 51; pp 3448-3470  8  P. GarcÌa-Teodoro, J.E. DÌaz-V erdejo, G. Maci·-Fern·ndez and E. V·zquez \(2009\-based Network Intrusion Detection: Technique s, Systems and Challengesî. Computers Security, Vol. 28; pp. 18-28  9  J.M.  EstÈvez-Tapiador, P. Ga rcÌa-Teodoro, and J.E. DÌazVerdejo \(2004\: ìMeasuring Norm ality in HTTP Traffic for Anomaly-based Intrusion Dete ctionî. Computer Networks Vol. 45, N  2; pp. 175-193  10  J.M. EstÈvez-Tapiador, P. Ga rcÌa-Teodoro, and J.E. DÌazVerdejo \(2005\ of Web-based Attacks Through Markovian Protocol Parsingî. 10th Symposium on Computers and Communications, pp. 457-462  11  W.A. Shay \(2004\ ìUnders tanding Communications and Networksî. Third ed., Thomson  12  Stevens, W.R., Fenner, B., and Rudoff, A.M. \(2004 Network Programming, the socket s networking API, volume 1. Addison Wesley Profesional, 3 rd Edition  13  W. Feller \(1968\: ìAn Introduction to Probability Theory and its Applications. Vo l. Iî. Third ed John Wiley & Sons  14  M. Berm˙dez-Edo, R. Salazar-Hern·ndez, J.E. DÌaz-Verdejo and P. GarcÌa-Teodoro \(2006 Proposals on Assessment Environments for Anomalybased Network Intrusion Detection Systemsî. LNCS \(Critical Information Infrastructures Security\p. 210-221  15  J. McHugh \(2000\tr usion Detection Systems: a Critique of the 1998 and 1999 DARPA Intrusion Detection System Evaluations as Perform ed by Lincoln Laboratory ACM Transactions on Information and System Security, Vol 3, N. 4; pp. 262-294  16  N. Athanasiades, R Abler, J. Levine, H. Owen, and G. Riley 2003\ Testing and Benchmarking Methodologiesî. Proc. 1st IEEE International Workshop on Information Assurance IWIA; pp. 63-72  17  http://mldonkey.sour ceforge.net/HowManyDonkeys. Last visited May, 2009 
159 


accurate and more complete record of participant activity than is available from other longitudinal study techniques Cueing works well To learn about user motivations, we asked the participants what made them decide to take certain actions \(while reminding them of the context of the action, based on the logs\ch as before looking for additional content to add. We noticed that looking at the screen image while they were answering the question made them much more confident about their answers.  When asked about why they chose to add particular content to the page after browsing or searching for it as opposed to other options they looked at, participants would often comment I remember this while looking at the screen image Participants were also asked if content they added had met their expectations and met the needs or desires they had before looking for new content. If a participant had looked at content, but ultimately did not add any content or did not keep the content they had added, she was asked what was missing from the content options available.  Again, the visual cueing worked very well   Tracking tacit behaviors and the lack thereof One piece of crucial information we were able to obtain from this study was information about user abandonment. Two of the participants in the new user  group ended up abandoning the personalized homepage during the course of the study. We were able to determine this since the log data indicated that they were only on the personalized homepage for a few minutes on the day we sent them an e-mail asking them to send the screenshots of their usage. Participants were reluctant to discuss their abandonment of the personalized homepage and we feel it is unlikely we would have caught this abandonment had we done a traditional longitudinal study where participants report their own usage since we feel they would have been unlikely to report their abandonment of the product. In addition, by discovering this abandonment before the end of the study, we were able to develop questions for the final interviews in order to get a better understanding of their reason for abandoning the product.  This would have been extremely difficult to do if we were looking through diary entries in their presence and the trends may not have been as obvious Furthermore, with most diary studies lasting 2-3 weeks on average due to high drop out rates beyond that period, the abandonment of the personalized homepage would likely have been imperceptible in a traditional diary study. We were only able to begin to observe the abandonment on the third week and if we only had the data collected from the first three weeks, we would have assumed the participant had just been busy during the third week.  This critical information was only attainable because we were able to observe first hand through screenshots the participants activity on the page over an extended period of time Study #1 Summary This study was rewarding.  We found that monthlong longitudinal studies are not only possible, but also fairly straightforward, even when the participants are as in this study\ remote.  It became clear that the visual examination of the screenshots \(particularly when seen in their original event sequence\were acting as powerful retrospective cues for the participants Furthermore, because the data collection process was essentially invisible, we had reason to believe that much of the behavior was genuinely uninfluenced by the presence of our instrumentation But we were left with a question: How accurately were participant able to recall their behaviors?  Were the stories and memories accurate?  Or were they generated in response to our questions, with the potential for falseness  5.  STUDY #2: HOW GOOD ARE THE MEMORIES An important question is whether or not our participants could actually accurately recall the circumstances of their use of the individualized home page To find out how well people could remember their past behaviors from a retrospective-cue such as that provided by IE Capture, we performed a second study Study #2 Design In this study, we recruited 8 participants, \(5 women, 3 men; ages 25 to 46; all with advanced education, but not computer scientists or HCI professionals\. All were located nearby so we could interview them in person at the end of data collection period.  At the beginning of the 7-day experimental run an experimenter installed IE Capture on their personal computer \(no shared computers allowed\nd let it run for 7 days.  At the end of that time, the experimenter would return to hold a structured interview in the location and context in which the web browsing behaviors took place.  The participants were not told the object of the study, but simply that we were studying home computer use All of the participants were screened to be web search users \(of at least 3 searches per week The interview was structured to ask a specific set of questions about the participant s web searching behavior.  In all cases, participants had many more than 3 searches in the week of study \(range: 6 37 searches in the week of study Interview questioning procedure At the beginning of the interview, the experimenter would choose 3 evenly spaced queries in the session log 2 days back before the interview, 4 days back and 6 days Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 5 


 back then visit those pages in the log with IECaptureViewer, our tool to view and scrub through the data logs and screenshots.  \(See Figure 5 for the log viewing tool used.\happened, all participants but one had searches on days 2, 4 and 6.  In the one missing case, the next prior search event was used \(substituting day 3 for day 4 For each of the days in question, the experimenter would jump to the first search query made on that day and show it to the participant.  \(Note that jumping to the screen image in question was important, as to avoid showing the participant later screen images that would have shown them the sequence of events The experimenter then asked the participant to describe what happened next in the search process  The participant was instructed to describe the next event if they felt reasonably confident that they knew what happened While the participant was not prompted for a particular kind of answer, we noted possible variations on their answer. Was the search successful with this query alone?  Did the participant have to continue searching after this point in time? If they continued, did they have to continue refining the current query or do something else entirely?  This free form question made it easy to assess whether or not the participant could recall the situation at all If the participant could not recall, then the experimenter would go forward in time, replaying one event image after another until the participant could recollect what was going on and was able to predict what the next search event would be We were measuring the participant s ability to speak accurately about the next major event in their search process.  That is, having cued their retrospective memory of an event, we measured their ability to recall the next step in the process.  In nearly all cases, the judgment by the research was clear and evident: either the participant could accurately predict what was coming up in the log, or they just couldn t say.  Rarely did a participant guess and feel confident Results For each participant we had two measures the number of correct predictions based on a cued recall, and the number of times they had to go to the next-page before they could recollect what was going on in the search As can be seen in Figure 3, seven of the eight participants could accurately predict the next search event after two days.  That s not terribly surprising given that searches are relatively infrequent \(in this participant pool, the average number of searches / week was 11\ search done two days ago is relatively recent and stands out by its relative rarity among the total number of web events However, as we tested farther into the past \(4 days and 6 days out\t recall was still quite good Even after nearly a week, participants were able to recall the next search event correctly 75% of the time With the additional prompting of advancing to the next page in their cached screenshots using the IE Capture Viewer, participants can recall accurately after seeing only 3 additional screen images taken from the log/screen files   Figure 3. The number of correct next event predictions drops after 2 days, but is still at 75 correct It was clear during the interviews that participants really could recollect not just the next event, but also how this search fit into the larger story of what was going on at the time.  Even after 6 days, participants were able to not just make a prediction about the next event, but also complete the story and say whether or not the entire task \(of which search was just a part\was successful or not 6.  DISCUSSION In Study #1 we discovered that the use of IE Capture enabled our participants to richly and confidently talk about events that happened in the course of their web use.  Even the absence of certain kinds of events, such as the diminution in the use of a particular feature, was available for discussion and introspection Intriguingly, during the interviews, participants seemed able to speak with assurance about what had happened even quite a while ago.  But questions about the accuracy of the recalled memories kept worrying us That concern led to Study #2 and an attempt to measure the accuracy of the recollections  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 


 Figure 4. As the time since last search increases, the average number of next-pages needed to correctly recall the next search event increases Note that even after 6 days, only 3 next-pages are needed to get to completely accurate recall  As became quickly clear, participants were not only able to make accurate predictions about particular events for which they had not been pre-conditioned to attend, but that it was the presence of the cueing screen images that was causing the effect.  More than one participant commented on the inclusiveness of the screen images: because they could often see other windows in the background \(the corner of the Excel spreadsheet, say\ those tiny peripheral context cues would give them a distinct sense of time, activity and place.  [1  A particularly difficult part of the protocol described here is that the recollection process begins on the first query in a search session.  While beginning on the initial query makes the questions unambiguous, it also requires that the participant remember the context that leads up to the query as cued by that single instance.  In our next trials, we plan on backing up two or three pages in order to give our participants a bit more of the temporal context as a way of improving recall even more One possible confound that we noted \(but were luckily able to avoid in this study\ was the potential for confusion over repeated searches.  As others have noted  repeate d s earch es are f a irl y co m m o n W h en  w e  were selecting queries from the participant s log, we took care to avoid queries that had been repeated, as it would be difficult for the participant to reconstruct what happened on that particular instance of the search \(as opposed to other instances within the past few days In the course of our studies, we have actually run many more participants than we tested for this paper.  To date, we now have experience with more than 30 participants, and have been struck both by how well people remember their previous search history when retrospectively cued.  Without cueing or just by cueing with web log history entries, recollection is very low after one day We have also been impressed by the discovery in Study #2 that participants seem to forget about the presence of the IE Capture logger after about four days As we noted in Study #1, people do seem to slightly modify their behavior within the first 3 to 4 days after installation.  There is also compensatory behavior after being reminded of its presence, as when the participant manually causes an upload event to occur Few people disabled IE Capture after it was initially turned on.  This suggests both that participants did not find it to be invasive and that its performance did not affect machine behavior substantially.  In fact, in one memorable instance, we discovered IE Capture was still running in the background of one participant s computer 90 days after the experiment.  Before turning it off, we took advantage of the situation to do a study of 30, 60 and 90 day post recollections and then going through the same protocol as before.  We repeated the experimental design from above, locating searches done 30, 60 and 90 days in the past.  Surprisingly, we found that our participant could recall many of the search events from as long as 90 days ago, albeit with many more next-page looks at the surrounding context.  In each case, after looking at a few surrounding pages, the participant would exclaim oh yes that search and then proceed to describe the rest of the session fairly accurately This unexpected anecdote gives us confidence that memories for the relatively recent past, say within the past week or two as measured in our studies can be relied upon as a source of correct behavioral information 7.  CONCLUSION Using a lightweight, passive longitudinal study method for capturing screenshots with log data, then conducting later interviews using the screenshots as retrospective cues, we were able to collect accurate contextual information from participants with little effort on their part. Participant dropout rates also seem to be lower than traditional longitudinal studies. From a researcher perspective, the data analysis can be completed more quickly and the recollections of the participants are fairly certain to be accurate While the results of this logging method are promising, privacy concerns for participants make it a bit more difficult to recruit for studies. While we have had reasonably good experience in recruiting participants, we have had to be very clear in explaining what is happening and the data retention policies.  These privacy concerns must be kept in consideration when recruiting participants to ensure proper lead team for recruitment and that any potential participants are properly briefed on the data being collected Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 


 Once explained clearly, the use of the screenshot logger and the retrospective cued recall method seems promising as a way of quietly collecting longitudinal behavioral data accurately 8.  ACKNOWLEDGMENTS We would like to thank Cindy Yepez for her help and efforts in recruiting participants for the study. In addition we would like to thank the participants of this study 9.  REFERENCES  l ackw ell A  J o n e s, R Milic-Fra y li ng N. a n d K Rodden, Combining Logging with Interviews to Investigate Web Browser Usage in the Workplace Position paper for workshop Usage analysis: Combining Logging and Qualitative Methods, ACM Conference on Human Factors in Computing Systems \(ACM CHI 2005 April 2005  i, E. H., P i rolli, P P itk o w J   The scent of a site A system for analyzing and predicting information scent usage, and usability of a web site in Proc. CHI 2000 ACM Press, pp 161-167.  \(2000  d t, J Weis s  H an d Kle m m e r, S txt 4l8r Lowering the Burden for Diary Studies Under Mobile Conditions. CHI 07, April 23-May 3, 2007, San Jose California  er, W  F.: W h at i s a u tobiog raph ical m e m o r y  In D. Rubin \(Ed.\tobiographical Memory \(pp. 25-49 Cambridge: Cambridge University Press, 1986  r ew er, W  F Qu a litati ve an al y s i s o f t h e recall s of randomly sampled autobiographical events. In M. M Gruneberg, P. E. Morris, & R. N. Sykes \(Eds.\ractical Aspects of Memory: Current Research and Issues \(Vol. 1 pp. 263-268\. Chichester: Wiley, 1988  Gh as s a n A l Q ai m a ri an d Darren  McR o s tie. K A L D I: A computer-aided usability engineering tool for supporting testing and analysis of humancomputer interaction. In J Vanderdonckt and A. Puerta, editors, Proceedings of the 3rd International Conference on Computer-Aided Design of User Interfaces \(CADUI'99\recht, October 1999. Louvain-la-Neuve, Kluwer  o J  Con t ex t T r ig g e red Visu al Episodic Memory Prosthesis, Proceedings of the International Symposium on Wearable Computing, Washington, DC October 2000, pp. 185  ory M. Y., M  A  Hear s t  T h e State of t h e A r t in Automated Usability Evaluation of User Interfaces ACM Computing Surveys, v 33, n 4, pp. 470-516 \(2001  n i avsky  M Observing the User Experience:  A practioner s guide to user research Morgan Kaufman 2003  Jon e s  R Milic-Fra y li ng  N., R odden  K., Blackw e l l  A.: Contextual Method for the Redesign of Existing Software Products , International Journal of HumanComputer Interaction, Vol. 22, No. 1-2, Pages 81-101 2007  L a m m i n g  M., Brow n  P C a rter K., El dridg e, M  Flynn, M., Louie, G., Robinson, P., & Sellen, AJ. \(1994 The design of a human memory prosthesis. Computer Journal, 37\(3\, 153-163  i e m a n J  T h e D i ar y  St u d y   A W o rk place Oriented Research Tool to Guide Laboratory Efforts. In Proceedings of CHI: ACM Conference on Human Factors in Computing Systems pp. 321-26, 1993 13 Russell D M  Gr i m e s  C  Assigned and selfchosen tasks are not the same in web search Proceedings of the 40th Annual International Conference on Systems Software, HICSS 2007, Kona, Hawai i, \(2007  Roedig er, H.L   McDer m ott, K.B Creatin g f a l s e  memories: Remembering words that were not presented in lists. Journal of Experimental Psychology: Learning Memory and Cognition. 21, 803-814 \(1995   T eev an J   T h e re:s ear ch eng i n e Helpi n g people  return to information on the Web. Paper presented at the Proceedings of the ACM Symposium on User Interface Software and Technology \(UIST 05\A \(2005  on B.A Ev a n s  J.J., E m s l ie, H  Mal i n e k  V  Evaluation of NeuroPage: a new memory aid. Journal of Neurology, Neurosurgery and Psychiatry, 63, 113-115 1997  C   Sh ep h e rd, M. A Goalbased Classification of Web Information Tasks.Proceedings of the Annual Meeting of the American Society for Information Science and Technology, , Austin, TX. \(ASIS&T\ \(2006  i och i A   C Hid, D A stu d y o f co m p u t er s u pported  user interface evaluation using maximal repeating pattern analysis.  Proceedings of ACM CHI 91, pp 301-305 1991  Hou s e N. Interv ie w Viz: V i s u alizatio n Ass i sted  Photo Elicitation. Ext. Abstracts CHI 2006, ACM Press pp.1463-8.  \(2006  C o llier, J Vis u al An t h r opolog y  P h otog raphy  As a Research Method. Holt, Rinehart and Winston, New York 1967  tille, S. S. , C. K u kla, an d X. Ma E liciting us er  preferences using image-based experience sampling and reflection," in Proc. CHI '02 Extended Abstracts on Human Factors in Computing Systems. New York, NY ACM Press, pp. 738-739. \(2002  a n Gog et. al U n c ov ering t h e Problem S ol v i ng Process: Cued Retrospective Reporting Versus Concurrent and Retrospective Reporting, Journal of Experimental Psychology: Applied, v11 n4 p237-244 Dec 2005  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


    Figure 5. IE Capture ViewerÑa tool for reviewing the participantês log and screen images for discussion and retrospective cuing.  The participantês screen image is visible in the center of the display, with the stack of windows present at the time of screen capture, an essential part of cueing for long-term recall.  The lists on the right hand side are for quickly moving among the log events and captured images for discussion purposes with the participant  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


 10 RTAX2000 S 14 88% 168 0 0 2352 125  RTAX4000 S 15 100% 1000 0 0 15000 125  RTAX2000 S 8 100% 2000 0 0 16000 125  RTAX1000 S 8 89% 1000 0 0 8000 125  RTAX1000 S 11 92% 2000 0 0 22000 125  RTAX2000 S 24 92% 1000 0 0 24000 125  RTAX2000 S 14 100% 168 0 0 2352 125  RTAX2000 S 14 88% 168 0 0 2352 125  RTAX4000 S 8 100% 168 0 0 1344 125  RTAX2000 S 82 95% 1500 0 0 123000 125  RTAX2000 S 82 95% 1500 0 0 123000 55  RTAX250S 82 98% 1500 0 0 123000 125  RTAX250S 82 98% 1500 0 0 123000 55           TOTAL 2842  TOTAL 3,057,244  LTOL 936  LTOL 507,604  HTOL 1906  HTOL 2,549,640    


 11 R EFERENCES     Reddy, M.K. and S.M. Reddy 223Detecting FET Stuck-Open Faults in CMOS Latc hes and Flip-Flops,\224 IEEE Design and Test of Computers Vol. 3 , No. 5 , pp. 17-26, October 1986   2 R Mad g e , M. Vilg is, a nd V. Bhide, "Achieving Ultra High Quality and Reliability in Deep Sub-Micron Technologies using Metal Layer Configurable Platform ASICs", MAPLD 2005    Kewal Sal u ja, \223Di g i t a l Sy st em Fundam e nt al s, Lect ure 11\224, Department of Electrical Engineering, University of Wisconsin Madison    Yu W e i  Papos ng  M oo Ki t Lee Peng W e ng Ng C h i n  Hu Ong, \223IDDQ Test Challenges in Nanotechnologies: A Manufacturing Test Strategy\224, Asian Test Symposium 2007. ATS apos;07. 16 th Volume , Issue , 8-11 Oct. 2007 Page\(s\211 \226 211  NASA GSFC Advi sory NA-GSFC 2004-06   Dan El ft m a nn, Sol o m on W o l d ay and M i nal  Sawant  New Burn In \(BI\ethodology for Testing of Blank and Programmed Actel 0.15 \265m RTAX-S FPGAs MAPLD 2005   M i nal Sawant Dan El ft m a nn,  W e rner van den Abeel an John McCollum, Solomon Wolday and Jonathan Alexander 223Post Programming Burn-in of Actel O.25um FPGA\222s\224 MAPLD 2002  B IOGRAPHY   John worked 2 years at Faichild R&D on bipolar switching performance specifically platinum dopedlife time control and the development of Ion Implantation.  He worked 15 years at Intel developing Intel's first bipolar PROM, Ion Implantation, the world's first 16K DRAM, as well as 64K and 256K DRAMs.  Mr. McCollum developed Intel's first dual layer metal CMOS technology for the 386 microprocessor.  He co-founded Actel and worked the last 20 years on process, antifuse and flash cell development and FPGA Architecture at Actel.  He holds over 50 patents   covering Process Technology, Antifuse and NVM technology, FPGA Architecture, Analog Processing and Radiation Hardening.  He has presented numerous papers at IEDM, MAPLD, CSME, SPWG, and the FPGA Symposium. He is currently a Fellow in the Technology Development Department 


Time Time 50 350   10 0                   10 1                   10 2 12.5 50 350   10 0                   10 1  12 expected from Figure 7, the width of the uncertainty region is compressed by the curvature of the monopulse response resulting in a detection-primitive with greater uncertainty than the variance admits.  A filter lag or so-called cluster tracking can easily result in a 5% or greater offset and degraded consistency.  After 300 seconds the curves peak up because the target is appr oaching a low-elevation beampoint limit.  This occurs anytime a target is tracked into the edge of the radar\222s field of re gard and can lead to radar-toradar handover difficulty         300 300 80  s D 2 k,1 k y    s D 2 k,1 k y   100 150 200 250 10 1                    10 5 1 0  Figure 8 - Consistency versus distance from beam center Monopulse Mismatch The next set of curves plotted in Figure 9 show the sensitivity of detection-primitive consistency to a mismatch in the monopulse slope.  All of these curves were generated using a linear monopulse response derived from the slope of the true monopulse response at beam center.  The slope of the 80% curve is 0.8 times th e beam-center slope; the 90 curve is 0.9 times the beam-center slope; and so on for 100%, 110% and 120%.  Again, the order of curves in the graph is the same as the legend order A steep slope tends to expand y I 222s uncertainty while a gentle slope tends to compress it.  An expanded uncertainty leads to a smaller consistency while a compressed uncertainty leads to a larger consistency.  This behavior can be observed in the family of curves in Figure 9.  Curves for the steeper slopes are on the botto m while curves for more gentle slopes are on top.  The notable feature of this set of curves is that the sensitivity to a mismatch in the monopulse slope is not very significant       100 150 200 250 10 1                    90 100 110 120  Figure 9 - Consistency versus monopulse mismatch Range-Bias Error The complex nature of the monopulse radar models presents ample opportunity to introduce errors in the software implementation.  One such e rror introduced in a \275 rangecell-width bias in the detection-primitive range which in turn resulted in a significant degradation to 2 9 k D The fact that 2 9 k D is measured in different coordinates compared to the bias made it difficult to determine which value or algorithm was to blame.  Examining the intermediate consistency values led directly to the error source A comparison between biased 2 1 k D  2 2 k D and 2 3 k D values and unbiased 2 2 k D values is shown in Figure 10.  The unbiased 2 2 k D is the bottom-most curve and the biased 2 3 k D is the top-most curve with a value around 80.  This large value for 2 3 k D indicates that there is a lot more uncertainty in the range measur ements compared to what is predicted by the range varian ce.  Since the range-variance calculation is easy to confirm, the problem must be in the algorithms that model or manipulate range A notable feature of Figure 10 is the sensitivity of the centroiding algorithm to range bias in the detection primitives.  The range bias is ba rely noticeable in the biased 2 1 k D and 2 2 k D curves.  Of course, if the unbiased 2 2 k D  curve existed as a baseline it would be relatively easy to spot the error 


Time Time Time 50 350   10 0                   10 1                   10 2 50 350   10 1                   10 0                   10 1 50 350   10 0                   10 1  13         Isolated No SNR Adjust  Figure 11 - Centroiding for isolated range cells Filter Tuning Now that the centroided m easurements are reasonably consistent, the parameters that govern track filtering can be examined.  As previously promised, the effects and corrections for atmospheric refr action and sensor bias have been disabled so that 2 8 k D can be analyzed using a sliding window.  Of course the full analysis would include these effects and 2 8 k D at each time step would be collected and averaged over many trials Plots of the effect of changing process noise in a nearlyconstant-velocity filter are shown in Figure 12 and Figure 13 for Cartesian position and velocity respectively.  In both figures, the plotted values have been divided by 3 so that the desired value is always 1.  Increasing the process noise up to a point should increase the updated uncertainty and reduce 2 8 k D values.  Except near th e end of the trajectory when the measurements are off of beam center, the curves in Figure 12 and Figure 13 appear inconclusive for this expected trend If 2 8 k D values are way out of range there are additional intermediate filter values that can be examined.  For example, the state extrapolati on algorithms can be examined by comparing the consistency of 1 210 Isolated With SNR Adjust 300 300 300 0.005 212 212 212 212 212 kkkk T kkkk D xhzSxhz 35        s D 2 k Range   D 2 k,2 biased D 2 k,1 biased D 2 k,2  Figure 10 - Range bias error in detection primitive Centroiding Algorithm From Section 3, assuming that the centroided-range uncertainty for an isolated range cell is the same as its detection-primitive uncertainty may be incorrect Collecting and plotting 2 3 k D values only from isolated range-cell measurements can be used to analyze such assumptions.  The plots in Figure 11 compare differences between the isolated-cell algorithm defined in Section 3, an algorithm that modifies the uncertainty based on the SNR in the isolated cell, and the 2 3 k D values from all measurements 34\was used to modify the range uncertainty for the upper line labeled Isolated with SNR Adjust    4 22  2 2  resRi o R R Rn bdp bm  s D 2 k,3 Range    s D 2 k,8 Position     212 1 can also be examined using \(35 The residual is also commonly used to determine the assignment cost  212 kk z  P  k  k1 with z k The consistency of the innovation covariance k T kkkkk RHPHS 100 150 200 250 10 1                    D 2 k,3 biased 100 150 200 250 10 2                    100 150 200 250 10 1                    0.5 50  Figure 12 \226 Position consistency, filter tuning example  r  t t 34 If the All Centroided curve \(middle\as the baseline doing nothing \(lower\imates the uncertainty and 33\imates the uncertainty.  Dividing by the square root of the observed SNR leads to a more consistent covariance; however, there is currently no statistical evaluation to justify it             210 210 1 1 1 2 All Centroided 


Time 50 350   10 1                   10 0                   10 1  14         300 0.005  s D 2 k,8 Velocity   100 150 200 250 10 2                    0.5 50  Figure 13 \226 Position consistency, filter tuning example 5  C ONCLUSION  Calculating and observing the behavior of covariance consistency at different levels  in the radar signal processing chain represents a very powerfu l tool that can be used to assess the accuracy and softwa re implementation of radar signal-processing algorithms.  Analyzing covariance consistency is applicable to radar systems both in the field and in simulations.  The primary challenge in both arenas comes down to properly accounting for the true target states that contribute to detections, detection primitives measurements, and state estimates For a fielded radar syst em, achieving covariance consistency is usually a s econdary consideration behind achieving and maintaining track s.  Indeed, until recently radar specifications did not even include requirements for covariance consistency.  Recent covariance consistency requirements stem from the fact that the use of radar systems in sensor netting applications is on the rise Currently the combined e ffects of off-beam-center measurements, atmospheric correction, bias correction clustering and centrioding, data association, and filtering on state covariance consistency throughout a target\222s trajectory are not well known.  This is particularly true for radars using wideband waveforms and multiple hypotheses or multiple frame trackers.  Numerical results presented here indicate that algorithms early in the radar signal processing chain can significantly degrad e covariance consistency and that some errors are better tolerated than others For a simulated target in a modeled system, truth relative to some global reference is known. However, transforming truth through different refere nce frames and accounting for changes that occur during various radar processing algorithms is not as simple as it appears.  The techniques in this paper help expose this hidden complexity and provide a framework for discussing and expanding the future development of covariance consistency techniques.  Such future developments include issues related to mapping truth through the convolution operation typically used to simulate wideband signal processing and the fast Fourier transforms typically used in pulse-Doppler processing.  Sophisticated tracking algorithms that carry multiple hypotheses, associate across multiple frames, or weight the association of multiple targets within a single frame pose significant challenges in properly associating truth with state estimates.  Additional work, including an investigation of track-to-truth assignment, is needed before covariance consistency techniques can be applied to these algorithms Another area that needs furthe r analysis is the use of a sliding window to approximate the covariance behavior expected during a set of Monte-Carlo trials.  Various timedependent variables such as the target\222s range and orientation, the transmit waveform, the radar\222s antenna patterns toward the target, missed detections, and false alarms could easily viol ate the assumption that measurement conditions are nearly stationary over the time of the window.  It is importa nt to understand the conditions when this assumption is violated Finally, the examples presented here included a relatively benign arrangement of targets.  Further analysis in dense target environments with the related increase in merged detections, merged measurements, and impure tracks is needed.  Further analysis for targets traveling over different trajectories is also needed Even so, the techniques presented here can be extended to many of these analyses R EFERENCES  1  S. Blackman and R. Popoli Design and Analysis of Modern Tracking Systems Artech House, 1999 2  Y. Bar-Shalom and X. R. Li Multitarget-Multisensor Tracking: Principles and  Techniques YBS Publishing, Storrs, CT, 1995 3  Y. Bar-Shalom, Editor Multi-target-Multi-sensor Tracking: Advanced Applications and  Vol. I Artech House, Norwood, MA, 1990 4  D. B. Reid, \223An Algorithm for Tracking Multiple Targets,\224 IEEE Trans. on Automatic Control Vol. 24 pp. 843-854, December 1979 5  T. Kurien, \223Issues in the Design of Practical Multitarget Tracking Algorithms,\224 in Multitarget-Multisensor Tracking Y. Bar-Shalom \(ed.\43-83, Artech House, 1990 6  R.P.S. Mahler, Statistical Multisource-Multitarget Information Fusion, Artech House, 2007 


 15 7  B.-N. Vo and W.-K. Ma, \223The Gaussian Mixture Probability Hypothesis Density Filter,\224 IEEE Trans Signal Processing Vol. 54, pp. 4091-4104, November 2006 8  B. Ristic, S. Arulampalam, and N. Gordon Beyond the Kalman Filter Artech House, 2004 9  Y. Bar-Shalom, X. Rong Li, and T. Kirubarajan Estimation with Applications to Tracking and Navigation, New York: John Wiley & Sons, pg. 166 2001 10  X. R. Li, Z. Zhao, and V. P. Jilkov, \223Estimator\222s Credibility and Its Measures,\224 Proc. IFAC 15th World Congress Barcelona, Spain, July 2002 11  M. Mallick and S. Arulampalam, \223Comparison of Nonlinear Filtering Algorithms in Ground Moving Target Indicator \(GMTI Proc Signal and Data Processing of Small Targets San Diego, CA, August 4-7, 2003 12  M. Skolnik, Radar Handbook, New York: McGrawHill, 1990 13  A. Gelb, Editor Applied Optimal Estimation The MIT Press, 1974 14  B. D. O. Anderson and J. B. Moore Optimal Filtering  Prentice Hall, 1979 15  A. B. Poore, \223Multidimensional assignment formulation of data ass ociation problems arising from multitarget and multisensor tracking,\224 Computational Optimization and Applications Vol. 3, pp. 27\22657 1994 16  A. B. Poore and R. Robertson, \223A New multidimensional data association algorithm for multisensor-multitarget tracking,\224 Proc. SPIE, Signal and Data Processing of Small Targets Vol. 2561,  p 448-459, Oliver E. Drummond; Ed., Sep. 1995 17  K. R. Pattipati, T. Kirubarajan, and R. L. Popp, \223Survey of assignment techniques for multitarget tracking,\224 Proc  on Workshop on Estimation  Tracking, and Fusion: A Tribute to Yaakov Bar-Shalom Monterey CA, May 17, 2001 18  P. Burns, W.D. Blair, \223Multiple Hypothesis Tracker in the BMD Benchmark Simulation,\224 Proceedings of the 2004 Multitarget Tracking ONR Workshop, June 2004 19  H. Hotelling, \223The generalization of Student's ratio,\224 Ann. Math. Statist., Vol. 2, pp 360\226378, 1931 20  Blair, W. D., and Brandt-Pearce, M., \223Monopulse DOA Estimation for Two Unresolved Rayleigh Targets,\224 IEEE Transactions Aerospace Electronic Systems  Vol. AES-37, No. 2, April 2001, pp. 452-469 21  H. A. P.  Blom, and Y. Bar-Shalom, The Interacting Multiple Model algorithm for systems with Markovian switching coefficients IEEE Transactions on Au tomatic Control 33\(8  780-783, August, 1988 22  M. Kendall, A. Stuart, and J. K. Ord, The Advanced Theory of Statistics, Vol. 3, 4th Edition, New York Macmillan Publishing, pg. 290, 1983 23  T.M. Cover and P.E. Hart, Nearest Neighbor Pattern Classification, IEEE Trans. on Inf. Theory, Volume IT-13\(1 24  C.D. Papanicolopoulos, W.D. Blair, D.L. Sherman, M Brandt-Pearce, Use of a Rician Distribution for Modeling Aspect-Dependent RCS Amplitude and Scintillation Proc. IEEE Radar Conf 2007 25  W.D. Blair and M. Brandt-Pearce, Detection of multiple unresolved Rayleigh targets using quadrature monopulse measurements, Proc. 28th IEEE SSST March 1996, pp. 285-289 26  W.D. Blair and M. Brandt-Pearce, Monopulse Processing For Tracking Unresolved Targets NSWCDD/TR-97/167, Sept., 1997 27  W.D. Blair and M. Brandt-Pearce, Statistical Description of Monopulse Parameters for Tracking Rayleigh Targets  IEEE AES Transactions, Vol. 34 Issue 2,  April 1998, pp. 597-611 28  Jonker and Volgenant, A Shortest Augmenting Path Algorithm for Dense and Sparse Linear Assignment Problems, Computing, Vol. 38, 1987, pp. 325-340 29  V. Jain, L.M. Ehrman, and W.D. Blair, Estimating the DOA mean and variance of o ff-boresight targets using monopulse radar, IEEE Thirty-Eighth SSST Proceedings, 5-7 March 2006, pp. 85-88 30  Y. Bar-Shalom, T. Kirubarajan, and C. Gokberk 223Tracking with Classification-Aided Multiframe Data Association,\224 IEEE Trans. on Aerospace and Electronics Systems Vol. 41, pp. 868-878, July, 2005   


 16 B IOGRAPHY  Andy Register earned BS, MS, and Ph  D. degrees in Electrical Engineering from the Georgia Institute of Technology.  His doctoral research emphasized the simulation and realtime control of nonminimum phase mechanical systems.  Dr. Register has approximately 20 years of experience in R&D with his current employer, Georgia Tech, and product development at two early-phase startups. Dr. Register\222s work has been published in journals and conf erence proceedings relative to mechanical vibration, robotics, computer architecture programming techniques, and radar tracking.  More recently Dr. Register has b een developing advanced radar tracking algorithms and a software architecture for the MATLAB target-tracking benchmark.  This work led to the 2007 publication of his first book, \223A Guide to MATLAB Object Oriented Programming.\224  Mahendra Mallick is a Principal Research Scientist at the Georgia Tech Research Institute \(GTRI\. He has over 27 years of professional experience with employments at GTRI \(2008present\, Science Applications International Corporation \(SAIC Chief Scientist \(2007-2008\, Toyon Research Corporation, Chief Scientist 2005-2007\, Lockheed Martin ORINCON, Chief Scientist 2003-2005\, ALPHATECH Inc., Senior Research Scientist 1996-2002\, TASC, Principal MTS \(1985-96\, and Computer Sciences Corporation, MTS \(1981-85 Currently, he is working on multi-sensor and multi-target tracking and classification bas ed on multiple-hypothesis tracking, track-to-track association and fusion, distributed filtering and tracking, advanced nonlinear filtering algorithms, and track-before-detect \(TBD\ algorithms He received a Ph.D. degree in  Quantum Solid State Theory from the State University of New York at Albany in 1981 His graduate research was also based on Quantum Chemistry and Quantum Biophysics of large biological molecules. In 1987, he received an MS degree in Computer Science from the John Hopkins University He is a senior member of the IEEE and Associate Editor-inchief  of the Journal of Advances in Information Fusion of the International Society of Information Fusion \(ISIF\. He has organized and chaired special and regular sessions on target tracking and classific ation at the 2002, 2003, 2004 2006, 2007, and 2008 ISIF conferences. He was the chair of the International Program Committee and an invited speaker at the International Colloquium on Information Fusion \(ICIF '2007\, Xi\222an, China. He is a reviewer for the IEEE Transactions on Aerospa ce and Electronics Systems IEEE Transactions on Signal Pr ocessing, International Society of Information Fusion, IEEE Conference on Decision and Control, IEEE Radar Conference, IEEE Transactions on Systems, Man and Cybernetics, American Control Conference, European Signal Processing Journal and International Colloquium on Information Fusion ICIF '2007   William Dale Blair is a Principal Research Engineer at the Georgia Tech Research Institute in Atlanta, GA. He received the BS and MS degrees in electrical engineering from Tennessee Technological University in 1985 and 1987, and the Ph.D. degree in electrical engineering from the University of Virginia in 1998. From 1987 to 1990, he was with the Naval System Division of FMC Corporation in Dahlgren, Virginia. From 1990 to 1997, Dr Blair was with the Naval Surface Warfare Center, Dahlgren Division NSWCDD\ in Dahlgren, Virg inia. At NSWCDD, Dr Blair directed a real-time experiment that demonstrated that modern tracking algorithms can be used to improve the efficiency of phased array radars. Dr Blair is internationally recognized for conceptualizing and developing benchmarks for co mparison and evaluation of target tracking algorithms Dr Blair developed NSWC Tracking Benchmarks I and II and originated ONR/NSWC Tracking Benchmarks III and IV NSWC Tracking Benchmark II has been used in the United Kingdom France, Italy, and throughout the United States, and the results of the benchmark have been presented in numerous conference and journal articles. He joined the Georgia Institute of Technology as a Se nior Research Engineer in 1997 and was promoted to Principal Research Engineer in 2000. Dr Blair is co-editor of the Multitarg et-Multisensor Tracking: Applications and Advances III. He has coauthored 22 refereed journal articles, 16 refereed conference papers, 67 papers and reports, and two book chapters. Dr Blair's research interest include radar signal processing and control, resource allocation for multifunction radars, multisen sor resource allocation tracking maneuvering targets and multisensor integration and data fusion. His research at the University of Virginia involved monopulse tracking of unresolved targets. Dr Blair is the developer and coordinator of the short course Target Tracking in Sensor Systems for the Distance Learning and Professional Education Departmen t at the Georgia Institute of Technology. Recognition of Dr Blair as a technical expert has lead to his election to Fellow of the IEEE, his selection as the 2001 IEEE Y oung Radar Engineer of the Year, appointments of Editor for Radar Systems, Editor-InChief of the IEEE Transactions on Aerospace and Electronic Systems \(AES\, and Editor-in- Chief of the Journal for Advances in Information Fusion, and election to the Board of Governors of the IEEE AES Society,19982003, 2005-2007, and Board of Directors of the International Society of Information Fusion   


 17 Chris Burton received an Associate degree in electronic systems technology from the Community College of the Air force in 1984 and a BS in Electrical Engineering Technology from Northeastern University in 1983.  Prior to coming to the Georgia Institute of Technology \(GTRI\ in 2003, Chris was a BMEWS Radar hardware manager for the US Air Force and at MITRE and Xontech he was responsible for radar performance analysis of PAVE PAWS, BMEWS and PARCS UHF radar systems Chris is an accomplished radar-systems analyst familiar with all hardware and software aspects of missile-tracking radar systems with special expertise related to radar cueing/acquisition/tracking for ballistic missile defense ionospheric effects on UHF radar calibration and track accuracy, radar-to-radar handover, and the effects of enhanced PRF on radar tracking accuracy.  At GTRI, Chris is responsible for detailed analysis of ground-test and flight-test data and can be credited with improving radar calibration, energy management, track management, and atmospheric-effects compensation of Ballistic Missile Defense System radars   Paul D. Burns received his Bachelor of Science and Masters of Science in Electrical Engineering at Auburn University in 1992 and 1995 respectively. His Master\222s thesis research explored the utilization of cyclostationary statistics for performing phased array blind adaptive beamforming From 1995 to 2000 he was employed at Dynetics, Inc where he performed research and analysis in a wide variety of military radar applications, from air-to-air and air-toground pulse Doppler radar to large-scale, high power aperture ground based phased array radar, including in electronic attack and protection measures. Subsequently, he spent 3 years at MagnaCom, Inc, where he engaged in ballistic missile defense system simulation development and system-level studies for the Ground-based Midcourse defense \(GMD\ system. He joined GTRI in 2003, where he has performed target tracking algorithm research for BMD radar and supplied expertise in radar signal and data processing for the Missile Defense Agency and the Navy Integrated Warfare Systems 2.0 office.  Mr. Burns has written a number of papers in spatio-temporal signal processing, sensor registration and target tracking, and is currently pursuing a Ph.D. at the Georgia Institute of Technology  


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


