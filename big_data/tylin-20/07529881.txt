Hardware Architectures for Frequent Itemset Mining Based on Equivalence Classes Partitioning Martin Letras Computer Science Department National Institute of Astrophysics Optics and Electronics Puebla Mexico Email mletras@ccc.inaoep.mx Raudel Hern  andez-Le  on Data Mining Research Team Advanced Technologies Application Center La Habana Cuba Email rhernandez@cenatav.co.cu Rene Cumplido Computer Science Department National Institute of Astrophysics Optics and Electronics 
Puebla Mexico Email rcumplido@inaoep.mx 
Abstract 
Frequent itemset mining algorithms have proved their effectiveness to extract all the frequent itemsets in datasets however in some cases they do not produce the expected results in an acceptable time according to the application requirements For this reason FPGA-based hardware architectures for frequent itemset mining have been proposed in the literature to accelerate this task Most of the reported architectures are limited by the number of distinct items that could be processed and the available resources in the employed FPGA device This study proposes a compact hardware architecture for frequent itemset mining 
capable of minimg all the frequent itemsets regardless of the number of distinct items and transactions in the dataset The proposed architectural design implements a partition strategy based on equivalence classes The partition on equivalence classes allows to divide the search space into disjoint sets that can be processed in parallel Accordingly a parallel architecture is proposed to exploit the beneìts of the proposed search strategy 
Index Terms 
Frequen Itemset Mining Hardware Architecture FPGA 
I I NTRODUCTION Nowadays information technology is present in every activity that we perform in smartphones personal computers 
and even household appliances connected to the Internet that interchange and generate big amounts of data This amount of data and the diversity of the information exceeds the human capacity to process it and obtain rules that describe the relationship among the data Data mining has emerged to solve this problem using automatic or semi-automatic processes to analyze datasets to nd patterns and then perform classiìcation or prediction tasks  One of the most spread technique in Data Mining is the Association Rule Mining technique which computes rules in form of implications among a set of items A crucial step in the association rules generation is to count the frequency of items and itemsets to know their relevance this process is 
known as frequent itemset mining Nevertheless looking for frequent itemsets may become an expensive task due to large amount of data sparse datasets and a low minimum support value henceforth 
 For these reasons sometimes the implementations of these algorithms cannot return a solution in an acceptable time One way to deal with this problem is to improve existing algorithms in order to reducing execution time and proposing new heuristics to explore the search space or using different data representations In recent years there is a trend to develop specialized hardware architectures to reduce the execution time of algo 
002 
002\003\004 
rithms In literature there are several hardware architectures based on FPGA Field Programmable Gate Arrays and GPUs Graphic Processor Units that perform a full implementation of frequent itemset mining algorithms This paper describes a FPGA-based hardware architecture for frequent itemsets mining that takes advantage of inner parallelism in the algorithms The main goal is to produce a compact hardware architecture in area resources that is able to nd all the frequent itemsets regardless of the number of items and transactions in the datasets The architecture also must be able to achieve a better speed up compared to optimized software implementations of frequent itemset mining algorithms 
Most of the reported work in literature has been designed for a xed problem size in others words they have a limit on the number of different items that are processed restricted by the resources of the device employed or by memory constraints Our proposed architectures address the previous limitations by partitioning the problem then generating partial solutions for each partition and nally combining all the partial solutions to construct a global solution The partition scheme is mainly based on equivalence classes This approach divides the entire search space into disjoint sets of the original search space in consequence all the equivalence classes may be processed in an independent way Accordingly a parallel architecture also 
is proposed to exploit the beneìts of the independent partitions into equivalence classes This paper is organized as follows in section 2 frequent itemset mining and the most important theoretical concepts are exposed In section 3 related work and previously proposed hardware architectures for frequent itemset mining are disclosed Section 4 introduces the proposed search strategy based on equivalence classes continuing in section 5 with the implementation of the proposed architectures section 6 describes the experimental setup and the execution time comparison Finally in section 7 conclusions and possible future research lines are drawn 
2016 IEEE International Parallel and Distributed Processing Symposium Workshops 16 $31.00 © 2016 IEEE DOI 10.1109/IPDPSW.2016.98 289 
2016 IEEE International Parallel and Distributed Processing Symposium Workshops 978-1-5090-3682-0/16 $31.00 © 2016 IEEE DOI 10.1109/IPDPSW.2016.98 289 


002 003 002\004 002\005 002\006 002\004 
be a set of items Let be a set of transactions where each transaction is a set of items such as  And let be an itemset such as  without loss of generality we will assume that all items in each transaction are sorted in lexicographic order The support value of the itemset is the number of transactions over containing  An itemset is called frequent if its support is greater than or equal to a given threshold value  A brute force approach that traverses all the possible itemsets calculating their support and removing infrequent itemsets is inefìcient in the worst case a total of itemsets could be generated for distinct items For example in gure 1 we have four items the search space contains 16 itemsets but only 9 are frequent the gray ones for  The number of itemsets and operations grows exponentially according to the number of different items transactions and the value Several algorithms have been proposed to efìciently nd all frequent itemsets There are those based on candidate generation like Apriori 1 that e xplores the search space using breathìrst search and other ones based on pattern growth like FPGrowth that creates a tree structure called FP-tree and Eclat that uses a depth-ìrst search These algorithms nd all the frequent itemsets but in some cases they do not return a response in an acceptable time according to the application requirements III R ELATED W ORK In recent years hardware architectures have been explored to offer a solution to the acceleration of frequent itemset mining algorithms The leading technologies are GPU and FPGA and the most employed algorithms are Apriori FP-Gro wth 8 and recently Eclat In 3 4 16 the proposed architectures are an Apriori implementation that use systolic arrays The reason to implement a systolic array is to decrease the number of the connections among processors elements and to ease the control The disadvantage of these approaches is that they are limited by the resources of the FPGA device employed and the number of processor elements implemented on the chip In 14 a systolic tree structure is proposed to implement a hardware version of FP-Growth algorithm A systolic tree is an array of pipelined processing elements in a multi-dimensional tree pattern In another hardw are architecture to e x ecute FP-Growth algorithm is proposed Authors proposed an equivalence class segmentation based on Eclat algorithm but once the segmentation is done FP-growth algorithm takes the control to generate frequent itemsets The results reported show that the proposed architecture achieves better performance than the hardware implementation reported in speciìcally for the Chess dataset the architecture achieves a speedup of one order of magnitude Recently hardware architectures based on the Eclat algorithm have been developed 13 In 20 the architecture per forms a depth rst search strategy Authors proposed a binary representation for datasets and itemsets The architecture is formed by an external memory FIFOs an intersection module and a support counting module They obtain good results for sparse datasets and a maximum acceleration of 48x is achieved All the previous works in literature have proved their efìciency to accelerate frequent itemset mining algorithms But all of them are limited by the hardware resources available according to the FPGA device employed For this reason our goal is to implement a hardware architecture that can obtain all the frequent itemset regardless of the number of different items transactions and hardware resources IV D ATA REPRESENTATION AND SEARCH STRATEGY PROPOSED Our proposal consists in a variant of the search strategy proposed for Eclat algorithm and its primary objective is to perform independient partitions of the search space The data representation employed is the vertical binary vector because the intersection and support counting operation can be implemented as a combinatorial system The transactions are coded in 32 bits integers using the compressed array representation reported in The w ord size is 32 bits because the external memory is 32 bits wide For example in gure 2 a binary vector dataset of ve items is shown For items and  their support values are calculated counting the set bits in the correspondent vectors being and  The intersection operation is performed using boolean operations For example to get the itemset an operation between the binary vector of item and is performed The result is the binary vector shown in gure 2 and  Our search strategy is a combination of breadth and depth 
002 
Fig 1 Transaction dataset and search space II F REQUENT I TEMSET M INING Frequent itemset mining is a method for market basket analysis and was introduced in by Agra w al Finding frequent itemsets and associations rules is essential for marketing applications improving the arrangement of products on shelves and suggesting other products The rst algorithm for frequent itemset mining was formalized by Agrawal in the 90ês 1 and it is used to nd patterns in datasets These datasets are represented by transactions each transaction is labeled with a unique identiìer Frequent itemset mining can be deìned as follows Formally let 
002 003 004 004 
null a b c d ab ac bc bd cd ad abc abd bcd acd abcd ID Items 
004 002\003\004 004 002\003\004 002\003\004 005 006 005\006 
1 a d 2 b, c, d 3 a c 4 a, c d 5 a b 6 a, c, d 7 b, c 8 a, c d 9 b c 10 a, d 
003 004 005\006\006\006\005\004 007 010 010 003 011 011 003 011 007 011 012 013 012 012 014 015 012 012 016\017 007 014\015 016\017 007 014 015 014\015 012 
290 
290 


A Compact Hardware Architecture 
Memory Subsystem 
007 010 003 004 
a Dataset using vertical binary vectors Fig 3 Search strategy proposed for four items rst search This strategy has the advantage that the search space can be partitioned and each partition of the search space can be processed in parallel For example gure 3 describes the behaviour of the proposed strategy The rst step consists in taking item Fig 4 Hardware architecture that performs the proposed search strategy Fig 5 Low level design of the proposed architectural design Figure 4 shows a high-level diagram of the proposed architecture This architecture is composed of a general purpose processor an UART module an off-chip memory a memory subsystem and the hardware accelerator Figure 5 describes a block diagram of the hardware accelerator It consists of two dual block RAM memories called 
Block RAM Block RAM Preìx Su\002x Coun\003ng Set Bits Adder Support Register S_min Register Comparator new_itemset Load Preìx Load Su\002x AXI MASTER AXI MASTER Address Address 
and generate all the 2-itemsets being  and frequent itemsets The next step is to generate all the 3-itemsets is generated intersecting and  is generated intersecting and  is generated intersecting and and so on until no more itemsets could be generated In this way this search strategy can generate the equivalence classes in an independent way V H ARDWARE A RCHITECTURES P ROPOSED In order to exploit the beneìts of the proposed search strategy two hardware architectures have been proposed The rst one consists in a compact architecture that mines each equivalence class in a sequential manner The second one is a parallel implementation that distributes the workload between two processor elements Our rst proposal is the implementation of a full hardware implementation of the proposed search strategy The behaviour of this architecture is divided into two parts the rst one consists in the generation of the frequent items and the second one consists in the frequent itemset mining using the proposed search strategy and  The BRAMs have a storage capacity of 122 KiB in consequence they can store one million of transactions but the architecture is not only limited to process one million of transactions because the Load Sufìx and Load Preìx modules can iterate to cover more than one million of transactions The outputs of each memory are connected to gates that perform the intersection using 32-bits words The counting support module receives as inputs two 32-bit words that are the result of the AND gates The output of the counting support module is accumulated in the support register until all the transactions have been covered And nally a comparator veriìes the support register value with the register value If the current itemset is a frequent itemset the preìx label is concatenated with the sufìx label and then the concatenated label is stored in the off-chip memory with its corresponding binary vector Figure 6 shows two nite states machines that describe the behavior of the proposed architecture The rst state machine corresponds to the frequent items generation task In state  the architecture receives the initial direction where the binary vectors are stored the number of transactions the number of items the label of the actual item the direction where the frequent item labels will be stored and the value In state  the architecture reads the binary vector of the current item and stores it in the load preìx BRAM and then the counting set bits module computes the support value In state ifthe item is frequent its label is stored in the off-chip memory as a frequent itemset State veriìes that all the items have 
002\003\004 002\003\004 
123 45 6 
Standalone OS Interrupt Handler Software Program Hardware Accelerator Programmable Logic Module MMU Arbiter 
a 1 1 2 0 3 1 4 1 5 1 6 1 7 1 8 1 9 0 10 1 b 1 1 2 1 3 0 4 0 5 0 6 0 7 1 8 0 9 1 10 1 ab 1 1 2 0 3 0 4 0 5 0 6 0 7 1 8 0 9 1 10 0 b Intersection and support counting operations in binary vectors Fig 2 Data Representation and operations used by our proposal            
Processing System 
System Bus Memory UART 
abcde 110011 2 0 1 110 3 1 0 1 0 1 4 1 0 1 1 1 5 1 0 0 0 1 6 1 0 1 1 0 7 0 1 1 0 0 8 10111 901 1 01 1010011 abcde 111011 2 0 1 1 1 0 3 1 0 1 0 1 4 1 0 1 1 1 5 1 0 0 0 1 6 1 0 1 1 0 7 1 1 1 0 0 8 1 0 1 1 1 901101 1011011 
014 014\015 014\020 014\021 014\015\020 014\015 014\020 014\015\021 014\015 014\021 014\020\021 014\020 014\021 022\023\024\025 004\026 002\027\025\025\004\026 016\017 007 012 012 012 012 012 012 
291 
291 


S0 S1 S2 S3 S4 S0: Receive Ini\002al Parameters S1: Read Item S2: Compare Support Value and write in FI memory sec\002on S3: Stop condi\002on S4: Indi cate comple\002on 
002 003 005 005 005 005 002 003 002 003 002 003 
012 012 030 014\005 015\005 020\005 021 014 022\023\024\025 004\026\024\021 014 022\023\024\025 004\026 002\027\025\025\004\026 004\031\024\032\002\024\031\002 012 033 004\031\024\032\002\024\031\002 004\031\024\032\002\024\031\002 014\015\005 014\020\005 014\021 012 014\015 012 014\020 012 014\015\020 014\015\020 014\015 014\015 014\021 014\015 014\015\020 014\015\020 014\015 014\020\021 014\020 014\015\020 014\015\021 014\015 014\015\020\021 030 014\005 015\005 020\005 021 014 030 015\005 020\005 021 015 020 021 
002\003 002 003 002 003 002 003 002 003 002 003 002 003 002 003 002 003 
Frequent itemsets in memory Preìx BRAM Sufìx BRAM Frequent Frequent itemsets in memory Preìx BRAM Sufìx BRAM Result in Sufìx BRAM Frequent Output Flush preìx BRAM 
a Items mining state machine b Itemset mining state machine Fig 6 Finite state machines of the proposed search strategy Fig 7 Search space for item a been processed The second state machine describes the behaviour of the frequent itemset mining stage In state TABLE I 2ITEMSETS GENERATION  abYes ab acYes ab ac adYes ab ac ad aYes TABLE II O PERATIONS PERFORMED BY THE ARCHITECTURE ab ac ad ab ac abc Yes Yes No ab ac ad abc ab ad abd Yes Yes Yes ab ac ad abc abd ac ad acd No Yes Yes ab ac ad abc abd ad No No Yes ab ac ad abc abd abc abd abcd No No Yes and the preìx of itemset 
 the 2-itemsets are mined Table I describes the operations involved in state  The rst step consists in receiving a set of initial items for this example the initial items are being the itemsets the equivalence class to process The rst item in the initial items list determines the equivalence class to process The second step consists in performing the intersection and support counting of the 2-itemsets Preìx and Sufìx BRAMs are used in this task The item is stored in BRAM and the next items will be stored in BRAM to perform the intersection and support counting operation All the frequent 2-itemsets will be stored in the off-chip memory Once that the have been calculated the next step corresponds to state and it consists in the mining Table II describes the operations employed in this stage using the search space of gure 7 The soinstate the binary vector of is stored in Preìx BRAM and in state  then itemset is stored in Sufìx BRAM In state  the intersection operation and the support counting operation indicate that is a frequent itemset and in consequence is written in the off-chip memory The itemset is not ushed from the Preìx BRAM because there is an itemset that shares the same preìx So and are intersected to generate a new itemset In this case is ushed from memory because the next itemset in memory is and it is a 3-itemset and they do not share the same preìx The intersection of two itemsets can only be performed if both of them have the same cardinality and share the same preìx For example the preìx of itemset is is  although they have the same cardinality they do not share the same preìx and they cannot be intercepted to generate a new itemset In contrast for itemsets and  they share the preìx and the same cardinality in consequence they can generate the itemset  The previous steps are executed until no more itemsets can be generated With the intention to get a speed up a dual-core architecture is proposed Figure 8 shows a high-level representation In the logical programmable area two hardware accelerators are implemented with the intention of distributing the workload between the two of them Previously it has mentioned that the proposed search strategy has the advantage of splitting the search space into disjoint sets or classes in consequence each core can process an equivalence class independently Figure 8 describes the partition of the search space for four items The rst processor element receives the set of items and it processes the equivalence class  Meanwhile the second processor element receives the sets of items and it process the equivalence classes  and  The dual core architecture obtains a parallelism to process independent equivalence classes and this impacts directly on the performance of the proposed search strategy VI E XPERIMENTAL RESULTS AND PERFORMANCE EVALUATION The hardware architectures have been evaluated using area and execution time metrics The area evaluation is performed using the hardware report usage that provides Vivado HLS synthesizer The execution of the architecture has been compared to the execution time of Apriori Eclat and FP-Growth software implementations 5 These implementations can be found on the personal website of Christian Borgelt 
B Dual Core Hardware Architecture A Experimental setup 
007 007 002 003 010 003 002 010 003 004 002 002 
S0: Generate 2-itemsets S1: Load Preìx S2: Load Su\003x S3: Compare support value and write in FI memory sec\002on S4: Stop condi\002on S5: Indi cate comple\002on S1 S2 S3 S4 S5 S0 
292 
292 


In the literature diverse datasets have been used to test the functionality of the software algorithms and hardware architectures In an algorithm is proposed to generate synthetic datasets that imitates the characteristics of transactions in the retailing environment TABLE III D ATASETS USED TO VALIDATE THE H ARDWARE A RCHITECTURE   average size of transactions  average size of the maximal potentially large itemsets  number of potentially large itemsets and number of items  All these characteristics are used to generate synthetic datasets For this work three values for  40 60 and 90 have been chosen The values for are 3 5 and 10 Table III summarizes the dataset parameter settings and also an estimated of the size in MB of the datasets In this section the performance results for the compact architecture and the dual core architecture are presented From gure 9 to 12 the compact hardware architecture and the dual core hardware architecture are compared to Apriori FPGrowth and Eclat the axis represents the execution time of each experiment and the axis represents the minimum support value Figure 9 shows the performance of the three software implementations and the hardware architectures for the chess dataset For chess dataset FP-growth obtained the value and the control signals this is the most compact design obtained For the dual core architecture although there has been an increment in the resources employed the architecture is still a compact one Intuitively an advantage of the compact design is that the number of cores that can be attached to the architecture can grow and the workload can be divided among other processor elements to speed up the execution time 
Hardware Accelerator Programmable Logic Module Standalone OS Interrupt Handler Software Program 
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 10 2 10 1 10 0 10 1 10 2 10 3    Support Runtime \(seconds   Apriori Eclat  FpäGrowth  FIM Hardware  Dual Core FIM Hardware 0.045 0.05 0.055 0.06 0.065 0.07 0.075 0.08 0.085 0.09 0.095 10 1 10 2    Support Runtime \(seconds   Apriori Eclat  FpäGrowth  FIM Hardware  Dual Core FIM Hardware 
Fig 8 Partition of the search space using 2 processor elements The software implementations have been tested on a PC with an Intel i3-3217U processor at 1.8 GHz and 8 GB DDR2 RAM memory with Windows 7 ultimate The execution time considers the input and output operations and the CPU time for all the algorithms and the hardware architectures The FPGA device employed is a Zynq 7020 of Xilinx Chess 0.013 37 3196 75 T40I3N500k 11.9 40 500k 299 T40I3N1000k 24.1 40 1000k 300 T60I5N500k 18.9 60 500k 500 The characteristics employed to generate the datasets are number of transactions Fig 9 Execution time comparison Chess Fig 10 Execution time comparison T40I3N500k best results among the four software implementations The maximum speedup obtained by the compact architecture is 2.9x and 5.8x for the dual core architecture compared to the Fp-growth algorithm The better performance reported for the hardware architectures is when they have to deal with sparse datasets Figures 10 11 12 The experiments show that the proposed architectures have good performance and it obtains a speedup of 4x for the compact architecture and 12.7x for the dual core architecture compare with the best software implementations it depends on the dataset and the support value Table IV shows the area reports for both architectures The operation frequency reported for both is 114 Mhz The elements reported are DSP48E Flip Flops and LUTs For the compact architecture the usage of Flip-Flops 3  and LUTs 9  is minimum because the architecture only needs a few registers to store 
006 006 006 006 006 006 006 006 006 006 006 006 006 006 
002\003\004 
Dataset Size MB Average Length Transaction Number of Transactions Number of Items 
System Bus Memory UART 
MMU Arbiter Hardware Accelerator         
Memory Subsystem 
B Validation datasets C Performance evaluation 
007 010 003 034 017 010 003 035 026 012 
293 
293 


013 
 volume 22 pages 207Ö216 ACM 1993  Rak esh Agra w al Ramakrishnan Srikant et al F ast algorithms for mining association rules In  volume 1215 pages 487Ö499 1994  Zachary K B ak er and V iktor K P rasanna Ef cient hardw are data mining with the apriori algorithm on fpgas In  pages 3Ö12 IEEE 2005  Zachary K Bak er and V iktor K Prasanna An architecture for ef cient hardw are data mining using reconìgurable computing systems In  pages 67Ö75 IEEE 2006  Christian Bor gelt Ef cient implementations of apriori and eclat In  2003  Christian Bor gelt An implementation of the fp-gro wth algorithm In  pages 1Ö5 ACM 2005  Christian Bor gelt Christian bor gelt s web pages urlhttp://www.borgelt.net/ìmgui.html 2015  Jia wei Han Jian Pei and Y iwen Y in Mining frequent patterns without candidate generation In  volume 29 pages 1Ö12 ACM 2000  Raudel Hern  andez-Le  on J Hern  andez-Palancar Jes  us A Carrasco-Ochoa and Jos  e Fco Mart  nez-Trinidad Algorithms for mining frequent itemsets in static and dynamic datasets  14\(3 2010  Alejandro Mesa Claudia Fere grino-Uribe Ren  e Cumplido and Jos  e Hern  andezPalancar A highly parallel algorithm for frequent itemset mining In  pages 291Ö300 Springer 2010  Philip E R oss T op 11 technologies of the decade  48\(1 2011  Philip Russom et al Big data analytics  2011  Shaobo Shi Y ue Qi and Qin W ang Accelerating intersection computation in frequent itemset mining with fpga In  pages 659Ö665 IEEE 2013  Song Sun Michael Stef fen and Joseph Zambreno A reconìgurable platform for frequent pattern mining In  pages 55Ö60 IEEE 2008  Song Sun and Joseph Zambreno Design and analysis of a reconìgurable platform for frequent pattern mining  22\(9 2011  D W Thoni and Alfred Stre y  No v el strate gies for hardw are acceleration of frequent itemset mining with the apriori algorithm In  pages 489Ö492 IEEE 2009  Y ing-Hsiang W en Jen-W ei Huang and Ming-Syan Chen Hardw are-enhanced association rule mining with hashing and pipelining  20\(6 2008  Ian H W itten and Eibe Frank  Morgan Kaufmann 2005  Mohammed Ja v eed Zaki Scalable algorithms for association mining  12\(3 2000  Y an Zhang F an Zhang Zheming Jin and Jason D B ak os An fpga-based accelerator for frequent itemset mining  6\(1 2013 
0.05 0.055 0.06 0.065 0.07 0.075 0.08 0.085 0.09 0.095 10 1 10 2 10 3      Apriori Eclat  FpäGrowth  FIM Hardwar e  Dual Core FIM Hardwar e Support Runtime \(seconds 
Fig 11 Execution time comparison T40I3N1000k Fig 12 Execution time comparison T60I5N500k TABLE IV H ARDWARE RESOURCES USED BY PROPOSED HARDWARE ARCHITECTURE  Expression 0 2618 0 3806 Multiplexer 1943 2891 Registers 3475 5234 Shift Memory 0 164 0 279 20 3475 4725 32 5234 6976 
Compact Architecture Dual Core Architecture Name DSP48E FF LUT DSP48E FF LUT Total Utilization  9 3 9 14 4 13 
ACM SIGMOD Record Proc 20th int conf very large data bases VLDB Field-Programmable Custom Computing Machines 2005 FCCM 2005 13th Annual IEEE Symposium on Field-Programmable Custom Computing Machines 2006 FCCMê06 14th Annual IEEE Symposium on FIMIê03 Proceedings of the IEEE ICDM workshop on frequent itemset mining implementations Proceedings of the 1st international workshop on open source data mining frequent pattern mining implementations ACM SIGMOD Record Intelligent Data Analysis Advances in Pattern Recognition IEEE Spectrum TDWI Best Practices Report Fourth Quarter High Performance Computing and Communications  2013 IEEE International Conference on Embedded and Ubiquitous Computing HPCC EUC 2013 IEEE 10th International Conference on Reconìgurable Computing and FPGAs 2008 ReConFigê08 International Conference on Parallel and Distributed Systems IEEE Transactions on Field Programmable Logic and Applications 2009 FPL 2009 International Conference on Knowledge and Data Engineering IEEE Transactions on Data Mining Practical machine learning tools and techniques Knowledge and Data Engineering IEEE Transactions on ACM Transactions on Reconìgurable Technology and Systems TRETS 
Compactness is the main advantage of the proposed hardware architecture Although it is a compact design both versions accelerate the frequent itemset mining problem and speedup can be achieved VII C ONCLUSIONS In this paper a search strategy for frequent itemset mining that nds all the frequent itemsets regardless of the number of different items The proposed search strategy ts well for hardware implementations because it splits the search space into separate equivalence classes making disjoint sets of the original dataset In consequence the amount of itemsets stored in the memory is reduced this is an advantage for memory constrained scenarios like in the hardware architecture development Another advantage of the partition into separate equivalence classes is that the equivalence classes can be distributed among a set of processor elements to parallelize and distribute the workload The most remarkable feature of this architecture is that gets a 4x to 12.7x speedup despite its compactness Based on the results obtained in this research it is possible to implement an array of processor elements in other words scale up the proposed dual-core architecture from 2 to processor elements to get a better speedup A CKNOWLEDGMENT Martin Letras is supported by the Mexican National Council for Science and Technology CONACyT scholarship number 298024 R EFERENCES  Rak esh Agra w al T omasz Imieli  nski and Arun Swami Mining association rules between sets of items in large databases In 
0.04 0.05 0.06 0.07 0.08 0.09 0.1 10 1 10 2 10 3    Support Runtime \(seconds   Apriori Eclat  FpäGrowth  FIM Hardware  Dual Core FIM Hardware 
294 
294 


VLDB 94  San Francisco  CA  USA pp  487-499  Morgan Kaufinann Publishers Inc  1994 4 Han  J  J Pei  and Y Yin   Mining frequent patterns without candidate generation  In ACM International Conference on Software Engineering  Artificial Intelligence  Networking and Parallel 7 Agrawal R Srikant R   Fast algorithm for mining association rules   Proceedings of 20th International Conference on Very Large Data Bases VLDB  Morgan Kaufman Press  I 994  487-499 8 J S Park  M S Chen  P S Yu   Efficient parallel data mining of association rules  4th International Conference on Information and Knowledge Management  1995  I I 233-235P 9 S Brin et ai   Dynamic itemset counting and implication rules for market basket data  Proceedings of the ACM SIGMOD International Conference on Management of Data  1997  123I 40 10 Jeffrey Dean  Sanjay   Map  Reduce Simplified  Data Processing on Large Clusters  OSDI  04 Sixth Symposium on Operating System Design and Implementation 2004  I th DBA  06  Anaheim  CA USA,2006  pp  75 82  ACT A Press  6 Yildiz  B and SIGMOD 93  New York  NY  USA  1993  pp  207 216  2 Han  J  and M Kamber  Data 13   Distributed Computing SNPD  12  Kyoto  20 I 2  IEEE 236 24 I  13 S Hammoud   MapReduce Network Enabled Algorithms for Classification Based on Association Rules  Thesis  20 I I  14 Yanjie Gao   Data Processing with Spark Technology  Application and Performance Optimization M  China machine Press  201411  1-2  15 Qiu H Gu R Yuan C Distributed Processing  Symposium Workshops IPDPSW  2014 IEEE International IEEE  2014 1664-1671 16 Gunduz  G Y AFIM A Parallel Frequent Itemset Mining Algorithm with Spark[C  Parallel Agrawal  R  T Imielinski  and Lin M  Lee P I Ergenc   Comparison of two association rule mining algorithms without candidate generation  In 2nd ed  ed  Morgan Kaufinann  2006 3 Agrawal  R and R Srikant   Fast algorithms for mining association rules in large databases  In  Fokoue  E   UCI Machine Learning Repository  Irvine  CA University of California  School of Information and Computer Science  20 I 2  I Proceedings of the Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data  Proceedings of the 24th lASTED International Conference on Database and Applications  Proceedings of the 70th lASTED International Conference on Artificial Intelligence and Applications  Shi Z   Parallel Implementation of Apriori Algorithm Based on MapReduce  In Proceedings of the 13 Swami   Mining association rules between sets of items in large databases  In Mining Concepts and Techniques N ACKNOWLEDGMENT Any comments and suggestions are welcome from the reviewers The authors will be thankful to them The authors would also be thankful to all those people who involved in carrying out this research work The authors are also thankful to the Department of Computer Science of Dr Babasaheb Ambedkar Marathwada University Aurangabaad for providing the infrastructure to carry out the research REFERENCES Proceedings of the 20th International Conference on Very Large Data Bases  SMART 2016 ISBN 978-1-5090-3543-4 Adaptive Apriori Algorithm for Frequent Itemset Mining  251 ACM SIGMOD International Conference on Management of Data  et al  A 7993 B  SIGMOD 00  New York  NY  USA 2000  pp  112  5 Pavon  J  S Vi ana  and S Gomez   Matrix Apriori Speeding up the search for frequent patterns  In SIGMOD  93  2010  pp  450-457  ACM  Copyright Hsueh S   Apriori-based Frequent Itemset Mining Algorithms on MapReduce  Proc  of the 16th International Conference on Ubiquitous Information Management and Communication ICUlMC 12  New York  NY  USA  ACM  Article No  76  20 I 2  12 Li N  Zeng L  He Q 


    Fig. 8. Comparative Analysis Classification Algorithms V  CONCLUSION Associative Classification techniques are used to make better decision in critical situations. The proposed associative classification called as Classification of microarray gene expression data using associative classification and gene expression intervals used to clas sify the gene expression with gene intervals in affected gene expression. The experimental results are carried out by using the gene expression of breast cancer. The associative classification on gene expression data obtained the best prediction and accuracy of the classification result. The proposed algorithm was tested with two class and multi class data sets. The classification algorithm was compared with the classical classification algorithms such as Linear Discriminant Analysis, SVM, and Decision Tree. After the comparison of traditional classification algorithms, as per the view of possible error rates the Associative Classification algorithm is best for biological data. The results of this work are used to drug designer for cancer diseases. The proposed algorithm works on gene expression data. In future, it will be implemented on hadoop and big data mining for biological data VI  R EFERENCES  1   Morgan Kaufmann Publishers Elsevier 2002 2   Second Edition PicasetOy Helsinki, 2005 3  Nagata, K., Washio, T., Kawahara, Y. and Unami, A  prediction from toxicogenomic data based on class association rule  ELSEVIER journal Toxicology Reports, vol.41, no.10 pp. 1133-1142, 2014 4  Garcia, S., Luengo, J., S·ez, J. A., LÛpez, V. and Herrera, F survey of discretization techniques: Taxonomy and empirical  Knowledge and Data Engineering, IEEE Transactions vol. 25, no.4, pp.734-750, 2013 5  Alves,R., Rodriguez.B.D.S and Aguilar. R.J.S  analysis: a survey of frequent pattern mining from gene expression  Briefings in Bioinformatics 2009, vol.2, no.2, pp.210-224 6   Miner: Maximal Confident Association Rules Miner Algorithm for Up/Down Applied Mathematics and Information Sciences vol.8 no.2, pp.799-809, 2014 7    BMC Bioinformatics vol.19, no.1, pp.7986, 2003 8  Snousy, A. M. B., El-Deeb, H. M., Badran, K. and Al Khlil, I. A  based classification algorithms on cancer  Egyptian Informatics Journal vol.12 no.2 pp.73-82. 2011 9  Refaeilzadeh, P., Tang, L. and Liu, H   Encyclopedia of database systems, Springer US pp. 532-538 2009   R. Agrawal and R. Srikant, Fast Algorithms for Mining Association Rules Proceedings of the 20th Int. Conf. on Very Large Data Bases VLDB94\,475486, Santiago de Chile, Chile 1994   Alagukumar, S., and Lawrance R., "A Selective Analysis of Microarray Data Using Association Rule Mining Procedia Computer Science Vol.47, pp.3-12, 2015 doi:10.1016/j.procs.2015.03.177   Alagukumar  Cancer Data Analysis Using Frequent Pattern Mining and Gene  International Journal of Computer Applications ISSN 0975 8887, no.1, pp.9-14, June 2015   Pasquier, N., Bastide, Y., Taouil, R., & Lakhal, L Pruning closed itemset lattices for association rules  In BDA'1998 international conference on Advanced Databases pp. 177-196. 1998   Giugno R, Pulvirenti A, Cascione L, Pigola G, Ferro A MIDClass: Microarray Data Classification by Association Rules and Gene Expression Intervals. Tang H, ed. PLoS ONE 2013;8\(8\:e69873. doi:10.1371/journal.pone.0069873   http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE1379   Wang, Zuncai, et al. "The prognostic biomarkers HOXB13 IL17BR, and CHDH are regulated by estrogen in breast cancer Clinical Cancer Research 13.21 pp. 6327-6334, 2005    cancer progression and host polymorphisms in the chemokine system: role of the macrophage chemoattractant protein-1 \(mcp 2518 g allele Clinical Chemistry 51: 452 5.2005   Dash, Rajashree, Rajib Lochan Paramguru, and Rasmita Dash Comparative analysis of supervised and unsupervised discretization techniques." International Journal of Advances in Science and Technology 2.3 \(2011\: 29-37   0 10 20 30 40 50 60 70 80 90 100 LDA SVM Decision Tree CACGE Accuracy and Error rate Classification Algorithms Accuracy Error Rate 


on items contained in each item group When the number of pivots increases the entire database is split into a ner granularity and the number of partitions increase correspondingly Such a ne granularity leads to a reduction in distance computation among transactions On the other hand when the pivot number k continues growing the number of transactions mapped into one hash bucket signiﬁcantly increases thereby leading to a large candidate-object set and high shufﬂing cost see Figs 3b and 3c Consequently the overall execution time is optimized when k is 60 for both algorithms see Fig 3a 6.2 Minimum Support Recall that minimum support plays an important role in mining frequent itemsets We increase minimum support thresholds from 0.0005 to 0.0025 percent with an increment of 0.0005 percent to evaluate the impact of minimum support on FiDoop-DP The other parameters are the same as those for the previous experiments Fig 4a shows that the execution times of FiDoop-DP and Pfp decrease when the minimum support is increasing Intuitively a small minimum support leads to an increasing number of frequent 1-itemsets and transactions which have to be scanned and transmitted Table 2 illustrates the size of frequent 1-itemsets stored in FList and the number of nal output records of the two parallel solutions under various minimum-support values Fig 4a reveals that regardless of the minimum-support value FiDoop-DP is superior to Pfp in terms of running time Two reasons make this performance trend expected First FiDoop-DP optimizes the partitioning process by placing transactions with a high similarity into one group rather than randomly and evenly grouping the transaction Fig 4b conﬁrms that FiDoop-DP’s shufﬂing cost is signiﬁcantly lower than that of Pfp thanks to optimal data partitions offered by FiDoop-DP Second this grouping strategy in FiDoop-DP minimizes the number of transactions for each GList under the premise of data completeness which leads to reducing mining load for each Reducer The grouping strategy of FiDoop-DP introduces computing overhead including signature-matrix calculation and hashing each band into a bucket Nevertheless such small overhead is offset by the performance gains in the shufﬂing and reduce phases Fig 4a also shows that the performance improvement of FiDoop-DP over Pfp is widened when the minimum support increases This performance gap between FiDoop-DP and Pfp is reasonable because pushing minimum support up in FiDoop-DP lters out an increased number of frequent 1-itemsets which in turn shortens the transaction partitioning cost Small transactions simplify the correlation analysis among the transactions thus small transactions are less likely to have a large number of duplications in their partitions As a result the number of duplicated transactions to be transmitted among the partitions is signiﬁcantly reduced which allows FiDoop-DP to deliver better performance than Pfp 6.3 Data Characteristic In this group of experiments we respectively evaluate the impact of dimensionality and data correlation on the performance of FiDoop-DP and Pfp by changing the parameters in the process of generating the datasets using the IBM Quest Market-Basket Synthetic Data Generator 6.3.1 Dimensionality The average transaction length directly determines the dimensions of a test data We conﬁgure the average transaction length to 10 40 60 and 85 to generate T10I4D 130 blocks T40I10D 128 blocks T60I10D 135 blocks T85I10D 133 blocks datasets respectively In this experiment we measure the impacts of dimensions on the performance of FiDoop-DP and Pfp on the 8-node Hadoop cluster The experimental results plotted in Fig 5a clearly indicate that an increasing number of dimensions signiﬁcantly raises the running times of FiDoop-DP and Pfp This is because increasing the number of dimensions increases the number of groups thus the amount of data transmission sharply goes up as seen in Fig 5b The performance improvements of FiDoop-DP over Pfp is diminishing when the dimensionality increases from 10 to 85 For example FiDoop-DP offers an improvement of 29.4 percent when the dimensionality is set to 10 the improvement drops to 5.2 percent when the number of dimensions becomes 85 In what follows we argue that FiDoop-DP is inherently losing the power of reducing the number of redundant transactions in high-dimensional data When a dataset has a low dimensionality FiDoop-DP tends to build partitions Fig 4 Impact of minimum support on FiDoop-DP and Pfp TABLE 2 The Size of FList and the Number of Final Output Records Under Various Minimum-Support Values minsupport 0.0005 0.001 0.0015 0.002 0.0025 FList 14.69k 11.6k 9.71k 6.89k 5.51k OutRecords 745 588 465 348 278 XUN ET AL FIDOOP-DP DATA PARTITIONING IN FREQUENT ITEMSET MINING ON HADOOP CLUSTERS 109 


each of which has distinct characteristics compared with the other partitions Such distinct features among the partitions allow FiDoop-DP to efﬁciently reduce the number of redundant transactions In contrast a dataset with high dimensionality has a long average transaction length therefore data partitions produced by FiDoop-DP have no distinct discrepancy Redundant transactions are likely to be formed for partitions that lack distinct characteristics Consequently the beneﬁt offered by FiDoop-DP for highdimensional datasets becomes insigniﬁcant 6.3.2 Data Correlation We set the correlation among transactions i.e corr to 0.15 0.25 0.35 0.45 0.55 0.65 and 0.75 to measure the impacts of data correlation on the performance of the two algorithms on the 8-node Hadoop cluster The Number of Pivots is set to 60 see also Section 6.1 The experimental results plotted in Fig 5c clearly indicate that FiDoop-DP is more sensitive to data correlation than Pfp This performance trend motivates us to investigate the correlation-related data partition strategy Pfp conducts default data partition based on equal-size item group without taking into account the characteristics of the datasets However FiDoop-DP judiciously groups items with high correlation into one group and clustering similar transactions together In this way the number of redundant transactions kept on multiple nodes is substantially reduced Consequently FiDoop-DP is conducive to cutting back both data transmission trafﬁc and computing load As can be seen from Fig 5c there is an optimum balance point for data correlation degree to tune FiDoop-DP performance e.g 0.35 in Fig 5c If data correlation is too small Fidoop-DP will degenerate into random partition schema On the contrary it is difﬁcult to divide items into relatively independent groups when data correlation is high meaning that an excessive number of duplicated transactions have to be transferred to multiple nodes Thus a high data correlation leads to redundant transactions formed for partitions thereby increasing network and computing loads 6.4 Speedup Now we are positioned to evaluate the speedup performance of FiDoop-DP and Pfp by increasing the number of data nodes in our Hadoop cluster from 4 to 24 The T40I10D 128 blocks dataset is applied to drive the speedup analysis of the these algorithms Fig 6 reveals the speedups of FiDoop-DP a nd Pfp as a function of the number of data nodes The experimental results illustrated in Fig 6a show that the speedups of FiDoop-DP and Pfp linearly scale up with the increasing number of data nodes Such a speedup trend can be attributed to the fact that increasing the number of data nodes under a xed input data size inevitably 1 reduces the amount of itemsets being handled by each node and 2 increases communication overhead among mappers and reducers Fig 6a shows that FiDoop-DP is better than Pfp in terms of the speedup efﬁciency For instance the FiDoop-DP improves the speedup efﬁciency of Pfp by up to 11.2 percent with an average of 6.1 percent This trend suggests FiDoopDP improves the speedup efﬁciency of Pfp in large-scale The speedup efﬁciencies drop when the Hadoop cluster scales up For example the speedup efﬁciencies of FiDoopDP and Pfp on the 4-node cluster are 0.970 and 0.995 respectively These two speedup efﬁciencies become 0.746 and 0.800 on the 24-node cluster Such a speedup-efﬁciency trend is driven by the cost of shufﬂing intermediate results which sharply goes up when the number of data nodes scales up Although the overall computing capacity is improved by increasing the number of nodes the cost of synchronization and communication among data nodes tends to offset the gain in computing capacity For example the results plotted in Fig 6b conﬁrm that the shufﬂing cost Fig 5 Impacts of data characteristics on FiDoop-DP and Pfp Fig 6 The speedup performance and shufﬂing cost of FiDoop-DP and Pfp 110 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS VOL 28 NO 1 JANUARY 2017 


is linearly increasing when computing nodes are scaled from 4 to 24 Furthermore the shufﬂing cost of Pfp is larger than that of FiDoop-DP 6.5 Scalability In this group of experiments we evaluate the scalability of FiDoop-DP and Pfp when the size of input dataset dramatically grows Fig 7 shows the running times of the algorithms when we scale up the size of the T40I10D data series Figs 7a and 7b demonstrate the performance of FiDoop-DP processing various datasets on 8-node and 24-node clusters respectively Fig 7 clearly reveals that the overall execution times of FiDoop-DP and Pfp go up when the input data size is sharply enlarged The parallel mining process is slowed down by the excessive data amount that has to be scanned twice The increased dataset size leads to long scanning time Interestingly FiDoop-DP exhibits a better scalability than Pfp Recall that see also from Algorithm 1 the second MapReduce job compresses an initial transaction database into a signature matrix which is dealt by the subsequent process The compress ratio is high when the input data size is large thereby shortening the subsequent processing time Furthermore Fidoop-DP lowers the network trafﬁc induced by the random grouping strategy in Pfp In summary the scalability of FiDoop-DP is higher than that of Pfp when it comes to parallel mining of an enormous amount of data 7R ELATED W ORK 7.1 Data Partitioning in MapReduce Partitioning in databases has been widely studied for both single system servers e.g and distributed storage systems e.g BigTable PNUTS[31 The existing approaches typically produce possible ranges or hash partitions which are then evaluated using heuristics and cost models These schemes offer limited support for OLTP workloads or query analysis in the context of the popular MapReduce programming model In this study we focus on the data partitioning issue in MapReduce High scalability is one of the most important design goals for MapReduce applications Unfortunately the partitioning techniques in existing MapReduce platforms e.g Hadoop are in their infancy leading to serious performance problems Recently a handful of data partitioning schemes have been proposed in the MapReduce platforms Xie et al  developed a data placement management mechanism for heterogeneous Hadoop clusters Their mechanism partitions data fragments to nodes in accordance to the nodes processing speed measured by computing ratios In addition Xie et al  designed a data redistribution algorithm in HDFS to address the data-skew issue imposed by dynamic data insertions and deletions CoHadoop is a H a d oop s lightweight extension which is designed to identify relateddataﬁlesfollowedbyamodiﬁeddataplacement policy to co-locate copies of those related les in the same server CoHadoop considers the relevance among les that is CoHadoop is an optimization of HaDoop for multiple les A key assumption of the MapReduce programming model is that mappers are completely independent of one another Vernica et al  broke such an assumption by introducing an asynchronous communication channel among mappers T his c hannel e nables the m appers to see global states managed in metadata Such situationaware mappers SAMs can enable MapReduce to exibly partition the inputs Apart from this adaptive sampling and partitioning were proposed to produce balanced partitions for the reducers by sampling mapper outputs and making use of obtained statistics Graph and hypergraph partitioning have been used to guide data partitioning in parallel computing Graph-based partitioning schemes capture data relationships For example Ke et al applied a graphic-execution-plan graph EPG to perform cost estimation and optimization by analyzing various properties of both data and computation Their estimation module coupled with the cost model estimate the runtime cost of each vertex in an EPG which represents the overall runtime cost a data partitioning plan is determined by a cost optimization module Liroz-Gistau et al proposed the MR-Part technique which partitions all input tuples producing the same intermediate key co-located in the same chunk Such a partitioning approach minimizes data transmission among mappers and reducers in the shufﬂe phase The approach captures the relationships between input tuples and intermediate keys by monitoring the execution of representative workload Then based on these relationships their approach applies a min-cut k-way graph partitioning algorithm thereby partitioning and assigning the tuples to appropriate fragments by modeling the workload with a hyper graph In doing so subsequent MapReduce jobs take full advantage of data locality in the reduce phase Their partitioning strategy suffers from adverse initialization overhead Fig 7 The scalability of FiDoop-DP and Pfp when the size of input dataset increases XUN ET AL FIDOOP-DP DATA PARTITIONING IN FREQUENT ITEMSET MINING ON HADOOP CLUSTERS 111 


7.2 Application-Aware Data Partitioning Various efﬁcient data partitioning strategies have been proposed to improve the performance of parallel computing systems For example Kirsten et al  developed two general partitioning strategies for generating entity match tasks to avoid memory bottlenecks and load imbalances Taking into account the characteristics of input data Aridhi et al proposed a novel density-based data partitioning technique for approximate large-scale frequent subgraph mining to balance computational load among a collection of machines Kotoulas et al built a data distribution mechanism based on clustering in elastic regions Traditional term-based partitioning has limited scalability due to the existence of very skewed frequency distributions among terms Load-balanced distributed clustering across networks and local clustering are introduced to improve the chance that triples with a same key are collocated These selforganizing approaches need no data analysis or upfront parameter adjustments in a priori Lu et al studied k nearest neighbor join using MapReduce in which a data partitioning approach was designed to reduce both shufﬂing and computational costs In Lu’s study objects are divided into partitions using a Voronoi diagram with carefully selected pivots Then data partitions i.e Voronoi cells are clustered into groups only if distances between them are restricted by a speciﬁc bound In this way their approach can answer the k-nearest-neighbour join queries by simply checking object pairs within each group FIM for data-intensive applications over computing clusters has received a growing attention efﬁcient data partitioning strategies have been proposed to improve the performance of parallel FIM algorithms A MapReducebased Apriori algorithm is designed to incorporate a new dynamic partitioning and distributing data method to improve mining performance This method divides input data into relatively small splits to provide exibility for improved load-balance performance Moreover the master node doesn’t distribute all the data once rather the rest data are distributed based on dynamically changing workload and computing capability weight of each node Similarly Jumbo adopted a dynamic partition assignment technology enabling each task to process more than one partition Thus these partitions can be dynamically reassigned to different tasks to improve the load balancing performance of Pfp Uthayopas et al  investigated I/O and execution scheduling strategies to balance data processing load thereby enhancing the utilization of a multi-core cluster system supporting association-rule mining In order to pick a winning strategy in terms of data-blocks assignment Uthayopas et al incorporated three basic placement policies namely the round robin range and random placement Their approach ignores data characteristics during the course of mining association rules 8F URTHER D ISCUSSIONS In this study we investigated the data partitioning issues in parallel FIM We focused on MapReduce-based parallel FPtree algorithms in particular we studied how to partition and distribute a large dataset across data nodes of a Hadoop cluster to reduce network and computing loads We argue that the general idea of FiDoop-DP proposed in this study can be extended to other FIM algorithms like Apriori running on Hadoop clusters Apriori-based parallel FIM algorithms can be classiﬁed into two camps namely count distribution and data distribution  For the count distribution camp each node in a cluster calculates local support counts of all candidate itemsets Then the global support counts of the candidates are computed by exchanging the local support counts For the data distribution camp each node only keeps the support counts of a subset of all candidates Each node is responsible for delivering its local database partition to all the other processors to compute support counts In general the data distribution schemes have higher communication overhead than the count distribution ones whereas the data distribution schemes have lower synchronization overhead than its competitor Regardless of the count distribution or data distribution approaches the communication and synchronization cost induce adverse impacts on the performance of parallel mining algorithms The basic idea of Fidoop-DP—grouping highly relevant transactions into a partition allows the parallel algorithms to exploit correlations among transactions in database to cut communication and synchronization overhead among Hadoop nodes 9C ONCLUSIONS A ND F UTURE W ORK To mitigate high communication and reduce computing cost in MapReduce-based FIM algorithms we developed FiDoop-DP which exploits correlation among transactions to partition a large dataset across data nodes in a Hadoop cluster FiDoop-DP is able to 1 partition transactions with high similarity together and 2 group highly correlated frequent items into a list One of the salient features of FiDoopDP lies in its capability of lowering network trafﬁc and computing load through reducing the number of redundant transactions which are transmitted among Hadoop nodes FiDoop-DP applies the Voronoi diagram-based data partitioning technique to accomplish data partition in which LSH is incorporated to offer an analysis of correlation among transactions At the heart of FiDoop-DP is the second MapReduce job which 1 partitions a large database to form a complete dataset for item groups and 2 conducts FP-Growth processing in parallel on local partitions to generate all frequent patterns Our experimental results reveal that FiDoop-DP signiﬁcantly improves the FIM performance of the existing Pfp solution by up to 31 percent with an average of 18 percent We introduced in this study a similarity metric to facilitate data-aware partitioning As a future research direction we will apply this metric to investigate advanced loadbalancing strategies on a heterogeneous Hadoop cluster In one of our earlier studies see for details we addressed the data-placement issue in heterogeneous Hadoop clusters where data are placed across nodes in a way that each node has a balanced data processing load Our data placement scheme can balance the amount of data stored in heterogeneous nodes to achieve improved data-processing performance Such a scheme implemented at the level of Hadoop distributed le system HDFS is unaware of correlations among application data To further improve load balancing 112 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS VOL 28 NO 1 JANUARY 2017 


mechanisms implemented in HDFS we plan to integrate FiDoop-DP with a data-placement mechanism in HDFS on heterogeneous clusters In addition to performance issues energy efﬁciency of parallel FIM systems will be an intriguing research direction A CKNOWLEDGMENTS The work in this paper was in part supported by the National Natural Science Foundation of P.R China No.61272263 No.61572343 Xiao Qin’s work was supported by the U.S National Science Foundation under Grants CCF-0845257 CAREER The authors would also like to thank Mojen Lau for proof-reading R EFERENCES  M J Zaki Parallel and distribu ted associat ion mining A survey IEEE Concurrency  vol 7 no 4 pp 14–25 Oct 1999  I Pramudiono and M Kitsuregawa  Fp-tax Tree structure based generalized association rule mining in Proc 9th ACM SIGMOD Workshop Res Issues Data Mining Knowl Discovery  2004 pp 60–63  J De an a n d S Gh e ma wa t M ap re du ce  S i mp l i e d da ta pr o ce s si n g on large clusters ACM Commun  vol 51 no 1 pp 107–113 2008  S Sakr A Liu and A G Fayoumi The family of mapred uce and large-scale data processing systems ACM Comput Surveys  vol 46 no 1 p 11 2013  M.-Y Lin P.-Y Lee and S.-C Hsueh Apriori-based frequent itemset mining algorithms on mapreduce in Proc 6th Int Conf Ubiquitous Inform Manag Commun  2012 pp 76:1–76:8  X Li n  Mr a pr io ri  As so ci a ti o n ru le s a lg o ri th m ba se d on mapreduce in Proc IEEE 5th Int Conf Softw Eng Serv Sci  2014 pp 141–144  L Zhou Z Zhong J Chang J Li J Huang and S Feng Balanced parallel FP-growth with mapreduce in Proc IEEE Youth Conf Inform Comput Telecommun  2010 pp 243–246  S Hong Z Huaxuan C Shiping and H Chunyan The study of improved FP-growth algorithm in mapreduce in Proc 1st Int Workshop Cloud Comput Inform Security  2013 pp 250–253  M Riondato  J A DeBrabant R Fonseca and E Upfal Parma A parallel randomized algorithm for approximate association rules mining in mapreduce in Proc 21st ACM Int Conf Informa Knowl Manag  2012 pp 85–94  C Lam Hadoop in Action  Greenwich USA Manning Publications Co 2010  H Li Y Wang D Zhang M Zhang and E Y Chang PFP Parallel FP-growth for query recommendation in Proc ACM Conf Recommender Syst  2008 pp 107–114  C Curino E Jones Y Zhang and S Madden Schism A workload-driven approach to database replication and partitioning Proc VLDB Endowment  vol 3 no 1-2 pp 48–57 2010  P Uthayop as and N Benjamas Impact of i/o and execution scheduling strategies on large scale parallel data mining J Next Generation Inform Technol  vol 5 no 1 p 78 2014  I  P r a m u d i o n o a n d M  K i t s u r e g a w a  P a r a l l e l F P g r o w t h o n P C cluster in Proc.Adv.Knowl.DiscoveryDataMining  2003 pp 467–473  Y Xun J Zhang and X Qin Fidoop Parallel mining of frequent itemsets using mapreduce IEEE Trans Syst Man Cybern Syst  vol 46 no 3 pp 313–325 Mar 2016 doi 10.1109 TSMC.2015.2437327  S Owen R Anil T Dunning and E Friedman Mahout Action  Greenwich USA Manning 2011  D Borthakur  Hdfs architecture guide HADOOP APACHE PROJECT Available  http://hadoop.apache.org/common/docs current/hdfs design.pdf 2008  M Zaharia M Chowdhury M J Franklin  S Shenker and I Stoica Spark Cluster computing with working sets in Proc 2nd USENIX Conf Hot Topics Cloud Comput  2010 p 10  W Lu Y Shen S Chen and B C Ooi Efﬁcient proces sing of k nearest neighbor joins using mapreduce Proc VLDB Endowment  vol 5 no 10 pp 1016–1027 2012  T Kanung o D M Mount N S Netanya hu C D Piatko R Silverman and A Y Wu An efﬁcient k-means clustering algorithm Analysis and implementation IEEE Trans Pattern Anal Mach Intell  vol 24 no 7 pp 881–892 Jul 2002  A K Jain Data clustering 50 years beyond k-means Pattern Recog Lett  vol 31 no 8 pp 651–666 2010  D Arthur and S Vassilvitskii  k-means  The advantages of careful seeding in Proc 18th Annu ACM-SIAM Symp Discr Algorithms  2007 pp 1027–1035  J Leskovec A Rajaraman and J D Ullman Mining Massive Datasets  Cambridge U.K Cambridge Univ Press 2014  A Stupar  S Mich el and R Schen kel Rankred uce–pr ocessin g k-nearest neighbor queries on top of mapreduce in Proc 8th Workshop Large-Scale Distrib Syst Informa Retrieval  2010 pp 13–18  B Bahmani A Goel and R Shinde Efﬁcient distributed locality sensitive hashing in Proc 21st ACM Int Conf Inform Knowl Manag  2012 pp 2174–2178  R Panigrahy Entropy based nearest neighbor search in high dimensions in Proc 17th Annu ACM-SIAM Symp Discr Algorithm  2006 pp 1186–1195  A Z Broder M Charikar  A M Frieze and M Mitzenma cher Min-wise independent permutations J Comput Syst Sci  vol 60 no 3 pp 630–659 2000  L Cristofor ARtool Association rule mining algorit hms and tools 2006  S Agrawal V Narasayya  and B Yang Integrating vertical and horizontal partitioning into automated physical database design in Proc ACM SIGMOD Int Conf Manag Data  2004 pp 359–370  F Chang J Dean S Ghema wat W Hsieh D Wallach  M  Burrows T Chandra A Fikes and R Gruber Bigtable A distributed structured data storage system in Proc 7th Symp Operating Syst Des Implementation  2006 pp 305–314  B F Cooper R Ramakrishn an U Srivastava A Silberstein P Bohannon H.-A Jacobsen N Puz D Weaver and R Yerneni Pnuts Yahoo!’s hosted data serving platform Proc VLDB Endowment  vol 1 no 2 pp 1277–1288 2008  J Xie and X Qin The 19th heterogenei ty in computing workshop HCW 2010 in Proc IEEE Int Symp Parallel Distrib Process Workshops Phd Forum  Apr 2010 pp 1–5  M Y Eltabakh Y Tian F  Ozcan R Gemulla A Krettek and J McPherson Cohadoop Flexible data placement and its exploitation in hadoop Proc VLDB Endowment  vol 4 no 9 pp 575 585 2011  R Vernica A Balmin K S Beyer and V Ercegovac Adaptive mapreduce using situation-aware mappers in Proc 15th Int Conf Extending Database Technol  2012 pp 420–431  Q Ke V Prabhakar an Y Xie Y Yu J Wu and J Yang Optimizing data partitioning for data-parallel computing uS Patent App 13/325,049 Dec 13 2011  M Liroz-Gis tau R Akbarinia D Agrawal E Pacitti  and P Valduriez Data partitioning for minimizing transferred data in mapreduce in Proc 6th Int Conf Data Manag Cloud Grid P2P Syst  2013 pp 1–12  T Kirsten L Kolb M Hartung A Gro H K  opcke and E Rahm Data partitioning for parallel entity matching Proc VLDB Endowment  vol 3 no 2 pp 1–8 2010  S Kotoulas E Oren and F Van Harmelen Mind the data skew Distributed inferencing by speeddating in elastic regions in Proc 19th Int Conf World Wide Web  2010 pp 531–540  L Li and M Zhang The strategy of mining associat ion rule based on cloud computing in Proc Int Conf Bus Comput Global Inform  2011 pp 475–478  S Groot K Goda and M Kitsuregawa  Towards improv ed load balancing for data intensive distributed computing in Proc ACM Symp Appl Comput  2011 pp 139–146  M Z Ashra D Taniar and K Smith ODAM An optimiz ed distributed association rule mining algorithm IEEE Distrib Syst Online  vol 5 no 3 p 1 Mar 2004 Yaling Xun is currently a doctoral student at Taiyuan University of Science and Technology She is currently a lecturer in the School of Computer Science and Technology Taiyuan University of Science and Technology Her research interests include data mining and parallel computing XUN ET AL FIDOOP-DP DATA PARTITIONING IN FREQUENT ITEMSET MINING ON HADOOP CLUSTERS 113 


Jifu Zhang received the BS and MS degrees in computer science and technology from the Hefei University of Tchnology China and the PhD degree in pattern recognition and intelligence systems from the Beijing Institute of Technology in 1983 1989 and 2005 respectively He is currently a professor in the School of Computer Science and Technology TYUST His research interests include data mining parallel and distributed computing and artiﬁcial intelligence Xiao Qin received the PhD degree in computer science from the University of Nebraska-Lincoln in 2004 He is currently a professor in the Department of Computer Science and Software Engineering Auburn University His research interests include parallel and distributed systems storage systems fault tolerance real-time systems and performance evaluation He received the U.S NSF Computing Processes and Artifacts Award and the NSF Computer System Research Award in 2007 and the NSF CAREER Award in 2009 He is a senior member of the IEEE Xujun Zhao received the MS degree in computer science and technology in 2005 from the Taiyuan University of Technology China He is currently working toward the PhD degree at Taiyuan University of Science and Technology His research interests include data mining and parallel computing  For more information on this or any other computing topic please visit our Digital Library at www.computer.org/publications/dlib 114 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS VOL 28 NO 1 JANUARY 2017 


