Accelerating Support Count for Association Rule Mining on GPUs Vasileios Zois Department of Computer Science University of Southern California Email vzois@usc.edu Anand Panangadan Department of Computer Science California State University Fullerton Email apanangadan@fullerton.edu Viktor Prasanna Department of Electrical Engineering University of Southern California Email prasanna@usc.edu 
Abstract 
In this work we present a highly parallel workef田ient algorithm for performing support count on a GPU We develop a compressed data layout scheme that enables high off 
chip memory bandwidth utilization Our data layout results in low overhead parallel coordination while reducing the memory requirements of support count We evaluate our algorithm through extensive experimentation both on synthetically generated and real data We achieve maximum throughput of 50 billion evaluations per second for our parallel two phase algorithm while outperforming that of non work-ef田ient implementations on a multi-core CPU and a GPU by almost 40 
 Resolving bank con琶cts results in reduction of the execution time per iteration of our algorithm up to 6 Employing additional optimizations such as 
 
loop unrolling leads to improvement in execution time up to 18 
I I NTRODUCTION Support count is a core operation used by a variety of Association Rule Mining\(ARM algorithms 11 13 In ARM we are concerned with discovering combinations of items that frequently appear together in transaction databases These combinations are called itemsets or rules and are connected to a quantity known as support value Support values indicate the proportion of transactions that contain the corresponding rule They are used for describing the level of interestingness that a rule exhibits with respect to 
Keywords work-ef田iency shared memory bank con琶cts frequent itemset mining accelerators gpu search strategy 
existing transactions Interesting rules are often selected by applying various constraints and metrics that require for the corresponding support value to be calculated A variety of ARM algorithms have been developed over the years Some of the most well known are 
 FP-Growth H-mine 20 Eclat 26 and OP 16 All of these algorithms make use of support count to discover interesting rules There is also a variety of applications for which ARM is useful including but not limited to mining consumer patterns analyzing traf田 accident patterns bioinformatics applications intrusion detection in critical systems and web usage mining 18 
Apriori 
In general there are two distinct phases associated with discovering frequently appearing rules in transaction data A candidate generation phase where a search strategy is intelligently applied to generate potential candidates and a candidate evaluation phase where the support value of the corresponding rules is computed Regardless of the search strategy the support value is always calculated and used either directly or as part of a more intricate interestingness measure to generate new candidate rules The search space for new candidates is often massive and in principle can be exponential in the number of distinct items appearing in the transaction dataset Mitigating the effects of generating too 
many candidates is commonly achieved through the use of search heuristic search which is able to prune the search space allowing the discovery only of important rules Therefore it is important for any proposed solution to be widely applicable regardless of what search strategy is used Parallel solutions to data mining problems including ARM algorithms 24 ha v e been well studied in the past Algorithms based on shared memory multi-core architectures  were the rst to g ather attention in ARM man y of which were designed as a derivation of existing sequential solutions Accelerators ha v e been considered as a viable alternative due to the computationally intensive properties of 
ARM problems An e xtensi v e literature e xists co v ering various GPU adaptations of sequential ARM algorithms  Ho we v er  man y of these e xtensions f ail to pro vide a generic work-ef田ient algorithm that is widely applicable to a variety of candidate generation strategies In this work we study support count independently of any search strategy that may be used to generate the rule candidates Our goal is to provide a formal analysis of support count computation and exploit its speci田 characteristics to design a parallel work-ef田ient algorithm that is widely applicable to massively parallel architectures Accelerating support count is bene田ial as it constitutes a core operation for a variety of 
ARM algorithms We summarize the contributions of this work 1 We describe a work-ef田ient parallel algorithm that is suitable for massively parallel architectures 2 We design a compressed layout scheme that enables ne grain coordination among the participating processing elements and reduces the memory requirements of support count 3 We describe a mechanism to resolve bank con琶cts through replication of partial results across distinct memory locations 
2016 IEEE International Parallel and Distributed Processing Symposium Workshops 16 $31.00 ｩ 2016 IEEE DOI 10.1109/IPDPSW.2016.60 1423 
2016 IEEE International Parallel and Distributed Processing Symposium Workshops 978-1-5090-3682-0/16 $31.00 ｩ 2016 IEEE DOI 10.1109/IPDPSW.2016.60 1423 


        1     1    1 
N 
1 2 3 
Problem De渡ition 
   002  003  
Apriori Apriori Apriori Apriori Apriori Apriori Support count transaction dataset supported Given a transaction dataset containing transactions each of which contain a subset of distinct items and a set of rules that are supported with respect to a given minimum support threshold  generate the set of the most frequently occurring rules Apriori anti-monotone principle 
4 We extensively evaluate the proposed algorithm using real and synthetic data Compared to the non-workef田ient solution implemented both on a CPU and a GPU we increase the throughput by 40  Resolving bank con琶cts results in reduction of the execution time per iteration of our algorithm up to 6 Additional loop unrolling optimizations improves execution time up to 18 II R ELATED W ORK Agrawal and Srikant rst described the widely used algorithm to nd all frequently occurring rules in transaction databases using a breadth-途st search approach A signi田ant portion of succeeding work in this eld aimed at resolving performance issues connected to the high computational complexity of  These methods include techniques for candidate generation intelligent search strategies and optimized data structures 10 and these have yielded moderate performance improvement over the original solution Our work addresses the performance issue by optimizing candidate generation the most computation intensive phase of the search algorithm Speci田ally we describe a work-ef田ient parallel algorithm that is oblivious of the speci田 parameters of a given search strategy The algorithm only considers the properties of the candidates that are being generated Leveraging the capabilities of parallel architectures for computation intensive ARM is an active area of research Zaki et al de v eloped a parallel v ersion of using a multi-core shared memory architecture Experiments using synthetic data resulting in 8X speed-up over a singlethreaded implementation A parallel implementation of FPgrowth another ARM algorithm for a multi-core platform was described by Liu et al Algorithms applicable to share-nothing parallel architectures were developed in  These solutions mak e use of data structures replicated across multiple machines to store the information necessary for calculating support count In our work we target a shared memory platform and develop data layout techniques aimed at storing rule candidates on off-chip memory in a manner that improves overall bandwidth utilization Accelerators have been a prominent candidate for ARM given their highly parallel nature In a GPU-accelerated version of is presented with speed-ups ranging between 2X 10X compared to a serial implementation Zhang et al introduced a GPU-accelerated implementation of Eclat Their work describes a hybrid solution based on breadth and depth-途st search to expose the parallelism intrinsic to the problem That approach achieves speed-ups ranging from 6X to 30X when compared with state of the art serial implementations of Eclat and FP-Growth More recently FPGAs were used in to accelerate the Eclat algorithm The authors attain a speed-up of 68X as compared to the serial implementation on a board consisting of four FPGAs Wang et al mak e use of an Automata Processor that pro vides a hardware implementation of non-deterministic nite automata to accelerate  They achieved speed-ups of up to 94X when compared to the state of the art serial implementation of  Compared to these works we establish a baseline based on a highly optimized multi-core implementation and present signi田ant speed-up gains using our parallel workef田ient algorithm III P ROBLEM F ORMULATION W ORK E FFICIENCY is calculated for a set of rule candidates with respect to a given collection of transactions We refer to this collection as the and denote it with  The dataset can be represented as a 2-D array of rows and columns Each row constitutes a bit vector representing a single transaction that consists of any combination of items also called attributes Let denote the set of all distinct attributes available in the transaction A rule is a non-empty subset of  it is referred to as an rule if and only if it consists of exactly attributes Note that we do not distinguish between two rules that constitute distinct permutations of the same items Calculating the support count of a rule with respect to is a fundamental step in ARM algorithms The support count of rule  denoted as  is the proportion of transactions in which contain rule  Note that a transaction needs only to be a superset of to support it It is often required to retrieve the set of most frequent rules given a minimum support threshold  A rule of length is given a minimum support threshold if and only if  If a rule is supported then it is called a frequently occurring frequent or a prominent rule The problem addressed in this work can be described as follows The problem can be generalized with two parameters and  to represent the maximum rule length i.e search depth to be discovered and the maximum number of rules to be considered when generating new candidates i.e search breadth respectively Complete search is then a special case where and  Rule candidates can be generated ef田iently based on the which states that if any rule of length is not frequent in the transaction dataset then its length super rule is also not frequent This property is used to iteratively generate the the most frequent rules by relying on the most frequent rules discovered at a previous phase of the algorithm Any rule generated with this process still needs to be evaluated against the transaction dataset in order for its support value to be computed Only rules that satisfy the minimum support threshold are retained and used as the basis for new candidates at the next iteration This process is repeated multiple times until the given maximum rule length 
D M N N I i i i  i X I L L L X D X sup X D X T X minS X L minS sup X minS X D M N L minS L C K C N K L L L L 
1424 
1424 


D R minS C R i C R generate candidates R D i C support count R D I measure C R D R select R minS R L L N P N L N N L  L N L X D j j D i j X j sup X D i X j sup X 
M i 1 N j 1 M i 1 L j 
2 3 4 5 6 7 
1      1                  1 0             
interestingness measure pre度 suf度 tail tail AND 
Figure 1 Break down of the execution time for each individual phase We observe that even across different iterations support count remains the most expensive operation the other stages of the algorithm This is primarily related to the number of candidates which increase exponentially per iteration Although the number of transactions is also important for determining the overall complexity of the algorithm the generated candidates are typically many orders of magnitude larger accounting for large part of the computational cost It is possible to calculate the number of candidates generated at iteration 
002 002 
Algorithm 1 describes the typical iterative ARM algorithm which includes candidate generation using the anti-monotone principle Initially the algorithm creates rules of size using the available items from the transaction data In Line 4 an is computed to rank the rule candidates in relation to their importance One may use support count computed in Line 3 as the interestingness measure or other metrics such as con電ence lift or conviction of a rule In Line 5 we use the corresponding interestingness measure to select a subset of the most prominent rules This subset is then used as the basis for generating new candidates in the next iteration New candidates are generated through the addition of items that are yet to appear in the corresponding rule Line 2 This process can potentially create different permutations of the same items We assume the existence of a simple duplicate elimination mechanism as this is not the focus of this work Frequent Rule Set Generation transaction dataset candidate array minimum threshold maximum rule length Most frequent rule array 1 In Algorithm 1 we can potentially execute all of the distinct phases in parallel although each phase has to be completed in sequence However we note that there is little gain in parallelizing every phase because not all of these phases are as computationally intensive as support count We performed detailed experiments using real and synthetic data on a sequential implementation of Algorithm 1 In Fig 1 we present a quantitative comparison of the individual steps that are required for generating the most prominent rules Even across multiple iterations considering increasing rule length support count remains compute intensive when compared to in Algorithm 1 by computing the number of combinations without repetitions of items selected from available items 1 The rule candidates consist of two parts For a single rule we refer to the rst items as the and the last item as being added as the  A frequent suf度 can be combined with suf度es thus creating the  The size of the dictates the number of rule candidates that require evaluation Support count can be viewed as a modi兎d vector-matrix multiplication that performs logical operations instead of addition and multiplication In this case the vector is the rule candidate and the matrix is the transaction dataset  The format of the computation is independent of the data representation As shown in Eq 2 the rule candidate and the corresponding transaction can be represented as bit vectors of equal length indicating with   in the th position an existing non-existing item  An alternative representation shown in Eq 3 represents rule candidates as sets of indices that correspond to items contained in a transaction 2 3 Modeling support count as a matrix-vector operation is important for exploiting the extensive literature on parallel matrix operations However the matrix formulation leads to a sub-optimal implementation of support count computation This is because frequent rules contain items that often appear as a subset of many candidate rules Subsequently given the generation process that was described above a large number of rule candidates will appear to have a common pre度 We can therefore avoid a large number of redundant operations through data reuse and computation sharing when evaluating common pre度es Although this seems straightforward it can be challenging to achieve In principle a work-ef田ient GPU algorithm for support count should ensure low overhead coordination between threads Additionally it should also be mindful of the global memory bandwidth and the shared memory size limitations Finally it should scale easily to increasing number of resources Due to the size of the candidate rule set and the differences in the computation format between the pre度 and the suf度 these requirements can be dif田ult to satisfy 
002 003 004\002 004\003 005\002 005\003 004\005\006\007\003 010\011\012\013\014\015\011 016\011\017\011\020\021\022\023 024\011\025\011\015\012\021\011\022\026\012\025\027\030\027\012\021\011\013 016\014\031\031\032\015\021\022\026\032\014\025\021 
Algorithm 1 Input Output for to do end for return 
002\003\004\005\006\007\010\011\012\013\014\010\015\004\013\016\017\011\020\021\013 022\007\004\023\024\007\010\011\012\013\025\013 
004   005 005 
1425 
1425 


002\022 006 022 033 022 034 022 004\004\022 004\006\022 004\003\022 004\033\022 004\034\022 005\004\022 005\006\022 005\003\022 005\033\022 006\002\022 006\004\022 006\006\022 004\022 002 022 002 022 004 022 004\022 004 022 004 022 004 022 002 022 004 022 002 022 002 022 004 022 004 022 002 022 004 022 004 022 004 022 004 022 004 022 002\022 
Algorithm 2 Input Output for to do for to do for to do if then end if end for if then end if end for end for return 
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 
P N L L M N L M M P N L L N L P D R SC rulesP erT hread ceil rr/tt start tid rulesP erT hread end start rulesP erT hread t tnum i start end evaluate j rlen item R i r j evaluate evaluate D row s item evaluate break evaluate SC i SC i evaluate SC L L N L N L P N L 
 1  1  1   1 1 1 0          0 1 0           0  1       1 1 1  1 
Figure 2 Multi-phase parallel pre度 evaluation required for rule lengths larger than the available processing elements high data parallelism and low overhead coordination between PEs 
A sequential execution of support count requires at most operations where is the total number of transactions This is because there are frequent rules of length the support value of which can be shared across candidate suf度es Sharing is achieved per transaction and can be exploited to implement a look-ahead mechanism used for early termination when the pre度 evaluates to  A parallel version of support count when using processing elements PEs is considered work-ef田ient if it requires asymptotically the number of operations to complete Support Count Multi-core Algorithm Transaction data Candidate rules array Rule frequency array 1 We design such an algorithm that is applicable to massively parallel architectures exploiting the maximum available parallelism while avoiding any coordination overhead between PEs In the next section we rst present a naive parallel implementation of support count followed by our parallel work-ef田ient solution which we call  IV W ORK E FFICIENT S UPPORT C OUNT C OMPUTATION Developing a parallel solution suitable for massively parallel architectures that is both work-ef田ient and highly parallel requires coordination of the work assigned to participating PEs while at the same time enabling high data parallelism Additionally support count requires ef田ient sharing of computed results between the PEs that evaluate the individual pre度es and those that process the corresponding suf度es In this section we rst describe a naive parallel algorithm that exhibits high parallelism while not being work-ef田ient and then compare it against our proposed parallel workef田ient algorithm Our work-ef田ient solution is coupled with a sophisticated data layout scheme described in V to enable A naive parallel algorithm for computing support count works by partitioning the rule candidates across different PEs Each rule candidate is evaluated independently enabling high parallelism although avoiding partial result sharing when a common pre度 is encountered Algorithm 2 shows the pseudocode for the naive algorithm assuming a shared memory multicore architecture The corresponding algorithm can be adapted for use with a GPU Support count is a structured problem so we do not need to worry about load balancing The naive algorithm has the property of being embarrassingly parallel since the evaluation of distinct candidate rule sets is completely independent However it is not work-ef田ient as it performs many redundant operations in lack of a sharing the common pre度 results among multiple processing elements A more suitable solution that avoids redundant operations is described in the following subsection We developed a parallel work-ef田ient solution for support count that operates in two phases In the rst phase we use a parallel reduction operation to evaluate the common of the candidate rules The resulting value is stored in onchip memory which is used from the PEs of the second phase to evaluate the suf度es in the  Both phases are executed in sequence for each transaction in the input data A synchronization barrier is used between the two phases to ensure read after write consistency We expect the added cost from the synchronization to be minimal compared to the actual work saved Computing the support value for a rule candidate set requires PEs for the pre度 and PEs for the individual suf度es However the peak resource requirement will be  In case the pre度 is larger than the available resources we can execute the reduction in multiple steps by partitioning the items into groups and merging the intermediate results as shown in Fig 2 The maximum achieved parallelism is dictated by the number of distinct pre度es  which constitute a single candidate set and the maximum 
         006 004 004 004 006 006     
two-phase support count A Naive Algorithm B Two-Phase Support Count TPSC pre度 tail 
1426 
1426 


                    002 004 006 006 004 004 006 
002\022 007\022 003\022 034\022 004\022 002\022 007\022 003\022 034\022 005\022 002\022 007\022 003\022 034\022 006\022 002\022 007\022 003\022 034\022 033\022 002\022 007\022 003\022 034\022 035\022 002\022 007\022 003\022 034\022 004\022 002\022 007\022 003\022 034\022 005\022 002\022 007\022 003\022 034\022 006\022 002\022 007\022 003\022 034\022 033\022 002\022 007\022 003\022 034\022 035\022 002\022 002\022 002\022 002\022 002\022 007\022 007\022 007\022 007\022 007\022 003\022 003\022 003\022 003\022 003\022 034\022 034\022 034\022 034\022 034\022 004\022 005\022 006\022 033\022 035\022 002\022 007\022 003\022 034\022 004\022 005\022 006\022 033\022 035\022 026\012\025\027\030\027\012\021\011\022\003\036\015\014\017\011\013\022\012\015\015\012\037\022 032!\022"\012#\032\015\022\032\015\027\011\015\022 026\032\017\014"\025\022"\012#\032\015\022\032\015\027\011\015\022 026\032"\031\015\011\013\013\011\027\022\015\014\017\011\022\020\012\025\027\030\027\012\021\011\013\022 
Algorithm 3 Input Output for to do end for for to do for to do end for end for return 
operations to complete Each group of PEs will be responsible for few rules within which each processing element will evaluate a single rule candidate against all transactions In comparison two phase support count requires the same number of operations as the sequential approach operations for the rst phase and for the second phase The total amount of work equals to  4 We can determine the ratio of work saved by computing the fraction of the work performed for the naive algorithm over that of two-phase support count The resulting value is shown in Fig 4 It turns out to be proportional to the rule length It is also important to note that the resource requirement for two-phase support count is many times lower than that of the naive approach This makes it suitable for accelerators where resource ef田iency and utilization is pivotal V D ATA L AYOUT Implementation of two phase support count on massively parallel architectures is non-trivial since each phase needs to be executed in sequence with the other This creates limitations in relation to the maximum achievable parallelism We also require a data layout scheme that enables high data parallelism but with low overhead coordination of the participating PEs In this section we consider three distinct data layout schemes for organizing data that are relevant to support count They are used to enable high data parallelism high off-chip memory utilization but incur little overhead for coordination of the participating PEs during execution An overview of the three schemes is presented in Fig 3 Candidate rules and transactions are represented in two possible ways either by a enumerating the available items and storing their indices or b storing bit vectors in which items are represented as bits using or  to indicate their appearance or not in the corresponding rule or transaction Regardless of the representation that is being used we can visualize the candidate rules and the transaction dataset as 2D-arrays of size and respectively Any data layout scheme derived from this assumption can use either a or b as the underlying representation assuming a suf田iently large list of items i.e  The only difference appears during data processing where unpacking of the bit vectors is required The algorithms presented in Section IV-A and IV-B evaluate the candidate rules on a per transaction basis Therefore the row-major layout is more suited for the transaction data as it enables easier sequential access On a GPU this scheme also enables memory coalescing which effectively hides the high access latency of global memory However the access pattern at a single transaction depends on the candidate rule that is being evaluated Candidate rule items may appear out of order or have gaps making memory coalescing inef田ient We use shared memory to store the corresponding transactions along with a set of rules This reduces the cost of not-coalesced memory accesses for a single transaction to only one per rule set and exploits the low latency of shared memory However after loading data into shared memory synchronization is required before starting computation Therefore caching transactions in shared memory to avoid not-coalesced memory accesses has to be traded-off with the cost of synchronization Naive Support Count transaction data candidate rules array Rule Support Count Array 1 The naive algorithm requires the candidate rules to be stored using column-major order because individual threads are responsible for processing only a single rule Memory coalescing is ensured as long as consecutive threads are able to access adjacent memory locations This can be achieved through utilization of column-major ordering as it results in a strided access pattern Assuming a block of threads items can be loaded into shared memory using lock-step coalesced access requests 
P N L N L L M L M P N L L M P N L N L M P N L M N N L L N L K M N N K K D rules sc j rlen srules tid j bDim rules i j bDim syncthreads t tnum evaluate j rlen item srules tid j bDim evaluate evaluate sd item count evaluate sc i count sc L 
 1      1  1  1  1  1  1 1 0 32    0        0 1 0            
Figure 3 Possible data layout schemes for the candidate rules array number of items in the transaction data It is possible to assign multiple candidate sets to a single group of PEs thus minimizing the global resource requirement with minimal effect on the achievable parallelism Executing the aforementioned process is important for making parallel support count work-ef田ient Assuming multiple groups of PEs that are responsible for individual partitions of the candidate rules the naive algorithm requires in total 
2 3 4 5 6 7 8 9 10 11 12 13 14 
1427 
1427 


    32           0           1              
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 
Compressed Rule Candidate CRC A Multi-core Implementation B GPU implementation all 
N L L L P N L N L L N N  D CRC SC tid<tlen sCRC tid CRC bid tlen tid syncthreads t tnum tid  plen vote wid all D sCRC tid j tnum syncthreads vote vid count vote vid D sCRC tid j tlen syncthreads tid  psize SC bid tnum plen tid plen count N L L  
On the other hand two phase support count makes use of a specialized data layout scheme which we refer to as the scheme We exploit the concepts of pre度 and tail de渡ed in Section III Items appearing in a rule from the previous iteration constitute the pre度 being stored which is followed by the tail of items that are used to create the new rule candidates Each combination of a pre度 and a tail is represented by a single row that describes a set of rule candidates each having pre度 of length  We refer to such a combination as candidate rule collection At iteration  there exist at most such rows having length of  Apart from reducing the space complexity associated with storing the rule candidates the CRC scheme enables low-level coordination between threads with no additional overhead Individual PEs are assigned to evaluate speci田 items based on their Id Furthermore we can ne tune the block size to adapt to the varying number of items i.e  in order to effectively utilize the available resources VI I MPLEMENTATION In this section we discuss the low level details of our implementation for the algorithms described in IV We developed both a multi-core implementation and a GPU kernel for the naive algorithm The multi-core implementation is based on Algorithm 2 and is implemented using pthreads Each thread is responsible for consuming approximately the same number of rules and since they have the same length we can achieve a balanced workload distribution during execution It uses a transaction dataset consisting of a collection of bit vectors organized in row-major order Similarly the candidate rules array is stored in row-major format This layout enables for fast sequential read accesses from the participating threads The cache is also utilized as rules are evaluated against a single transaction before proceeding to the next one We also reduce the number of operations by stopping execution when a rule partially evaluates to zero In the naive GPU kernel of Algorithm 3 a block of threads is assigned to a collection of rules The transaction dataset is represented as a collection of bit vectors organized in rowmajor order However in contrast to the CPU implementation the rules array is organized in column-major order to enable memory coalescing A block of threads is responsible for evaluating a xed number of rules Each thread evaluates the items of a single rule that is stored in shared memory Transactions are loaded individually into shared memory through cooperation of the threads in a block Thus un-coalesced accesses resulting from out of order rule indices are served by shared memory which has lower latency Two-phase support count relies on a row-major ordering of the transaction dataset The candidate rules array is constructed using the compressed format described in Section V A single Two-Phase Support Count transaction data compressed rule candidates Rule Support Count Array 1 SC compressed row corresponds to a collection of candidate rules All the possible collections for a given iteration are stored together in array organized using row major ordering Different thread blocks are responsible for processing a single or multiple candidate rule collections We make use of loop unrolling to increase performance when multiple rule collections are being processed by a block A single transaction can be accessed directly from global memory or cached in shared memory before accessing The latter method involves an extra synchronization step which we found through experimentation to be costly Alternatively the former method incurs uncoalesced memory accesses from evaluating the pre度 The tail will consist of indices that appear in sequence given a large enough i.e 256 list of available items enabling coalescing We developed two kernels one that uses shared memory to cache a single transaction and another that directly accesses the corresponding transaction Both of these versions are evaluated in our experiments At the end of the pre度 evaluation phase the resulting value is propagated through shared memory to the next phase Because we use the warp voting function to evaluate the pre度 the result is available to all threads participating in the evaluation We consider two options after this step a store the result in a single dedicated memory location or b replicate the result in many distinct memory locations The rst method reduces the number of write requests and increases bank con琶cts as distinct threads will have to access the same location to learn the result of the rst phase The second method exhibits higher parallelism and reduces shared memory bank con琶cts as read requests can be spread out from individual threads to distinct memory locations We developed a third kernel that attempts to minimize shared memory bank 
  007 006 004 006 006 006    
Algorithm 4 Input Output if then end if for to do if then end if if then end if end for if then end if return 
1428 
1428 


most prominent rules We considered rules to be prominent if their support value was among the highest It is important to note that our algorithm can be tuned easily to perform a complete search Our goal was to show that our solution is widely applicable for applications that require the support count operation even though they may use alternate candidate generation algorithms or heuristics Therefore two-phase support count can be used with a vast number of ARM algorithms for accelerating support counting In our experiments we measured the throughput in terms of evaluations per second for varying candidate rule lengths An evaluation refers to computing if a given rule candidate is included in a single transaction The reason for using this metric was twofold Implicitly it indicates that support count is a compute intensive operation even for moderate sized transaction data Explicitly it presents the immense gains in performance of the work-ef田ient solution over a non workef田ient approach Additional experiments aim at measuring the execution time and showing the scalability of two-phase support count Varying the rule length was the most appropriate choice for a fair comparison of the presented algorithms  Regardless of using a heuristic or doing a complete search adding a single item will increase the complexity of support count proportionally to unless being workef田ient In contrast including an extra transaction will add at most two extra evaluation steps i.e pre度 evaluation tail evaluation which are executed in parallel having no signi田ant effect on the total execution time In our experiments apart from the multi-core implementation and the naive GPU kernel we consider three tpsc variations default-tpsc that assigns a single candidate rule collection to unique block of threads nbc-tpsc which employs our strategy to resolve bank con琶cts mrs-tpsc which assigns multiple collections of candidates rules to a single block using shared memory to avoid uncoalesced memory access through caching of transactions and mr-tpsc which similar to mrs-tpsc without using shared memory Type Abbreviation   Items  Trans  Synth Data 1 446,138990 Synth Data 2 55,297476 Synth Data 3 458,512410 Synth Data 4 68,400827 Synth Data 5 443,306735 Synth Data 6 305,287653 Real Data Accidents 572,340183 Real Data Connection 129,67557 Real Data Retail 1024,88162 Real Data T40I10D100K 999,100000 Table I Experimental Data In this section we present the results of our experiments on synthetic and real data We measure the achieved throughput for naive-sc default-tpsc and our multi-core implementation by increasing the candidate rule length The results of our experiments on synthetic data are summarized in Fig 5 In all 
  
a Accidents b Retail c T40I10D100K d Synthetic 1 Figure 4 A snapshot of the sparsity pattern for a subset of the transaction data that were used in our experiments The axis indicates different transactions and the axis corresponds to bit digit signifying an item尽 appearance or not in the given transaction con琶cts using the aforementioned technique We describe in Algorithm 4 the kernel to resolve bank con琶cts when processing a single collection of rule candidates VII E XPERIMENTAL R ESULTS We evaluated the performance of our design on a platform consisting of a 32-core Intel\(R Xeon\(R CPU E5-2650 clocked at 2.60 GHz and a Tesla K40c GPU W e used CUDA and C to implement the two GPU kernels mentioned in Sect.III We experimented with synthetically generated and real data that are commonly available to the data mining community  Their properties are summarized in T ableI W e also present in Fig 4 a snapshot describing the sparsity pattern for a subset of the experimental data Our experimental evaluation shows that our solution is adaptable to both dense or sparse data Additionally we were able to show increased performance for varying number of items in the transaction data a property that affects the achievable parallelism Throughout our experiments we used a simplistic heuristic known as beam search to prune the rule candidate search space by expanding only the 
002 003\002\002 004\002\002 005\002\002 006\002\002 007\002\002 002 003\002\002 004\002\002 005\002\002 006\002\002 007\002\002 010\011 013 003\014\003\006\015 002 003\002\002 004\002\002 005\002\002 006\002\002 007\002\002 002 003\002\002 004\002\002 005\002\002 006\002\002 007\002\002 010\011 013 004\002\015\016 002 003\002\002 004\002\002 005\002\002 006\002\002 007\002\002 002 003\002\002 004\002\002 005\002\002 006\002\002 007\002\002 010\011\012\013\012\004\006\017\007 002 003\002\002 004\002\002 005\002\002 006\002\002 007\002\002 002 003\002\002 004\002\002 005\002\002 006\002\002 007\002\002 010\011\012\013\012\007\002\016\005\004 
Y X 
A Achieved Throughput 
 1000  1  
K K O N L L 
1429 
1429 


002 003 004\002 004\003 005\002 005\003 006\002 006\003 007\002 002\004 002\005 002\006 002 002 003 004\002 004\003 005\002 005\003 006\002 006\003 002\004 002\005 002\006 002 002 003 004\002 004\003 005\002 005\003 006\002 006\003 007\002 002\004 002\005 002\006 002 002 003 004\002 004\003 005\002 005\003 006\002 006\003 002\004 002\005 002\006 002 002 003 004\002 004\003 005\002 005\003 006\002 006\003 007\002 002\004 002\005 002\006 002 002 003 004\002 004\003 005\002 005\003 006\002 006\003 007\002 002\004\002\005\002\006\002 
3 6 
 
026\027\012\007\030\004\007\010\005\013\032\013    
Figure 5 Achieved throughput for increasing length rule candidates The Y-axis is measured in billion evaluations per second The X-axis shows the corresponding rule length The gures also depict the theoretical trend-line that is computed as an ampli田ation of the naive-sc on the GPU using Eq 4 cases default-tpsc outperforms naive-sc on the gpu as well as the alternative implementation on the cpu The performance of naive-sc is higher than its cpu counterpart due to the higher resource availability of the GPU However its maximum throughput is only 5 billion evaluations per second and drops very quickly for all the different types of dataset In contrast default-tpsc has a throughput ranging from 2x to 30x times higher for larger rule lengths presenting overall a more gradual drop TPSC relies heavily on a large list of items to achieve increased parallelism during execution and improve performance For this reason we observe a steeper drop in throughput and a lower throughput maximum for synthetic data 2 and 4 The available items in these cases are less than the minimum block size i.e 128 threads that is required to achieve full multiprocessor warp occupancy Therefore resource underutilization is the main reason for the observed performance drop When the number of items in the transaction data is large enough the observed throughput follows the theoretical trendline We determine the theoretic throughput using Eq 4 to amplify the the naive-sc throughput of the GPU The depicted gures show the trend-line and not the actual ampli兎d throughput as naive-sc saturates the GPU resources for rule length greater than 18 resulting in signi田ant performance drop So the actual bene鍍s from tspc are more signi田ant for large rule candidate lengths due to the algorithm being resource ef田ient In fact for synthetic data 1,3 5 and 6 the theoretical throughput is around 35 billion evaluations per second Although it is greater than the observed throughput\(i.e 15 to 20 billion compared to naive-sc on the GPU tspc exhibits almost 40x improvement As with the synthetic dataset we performed similar experiments this time on real data We present the observed throughput in Fig 6 Overall the results follow similar pattern with those of the experiments on synthetic data For accidents dataset we observe the highest throughput of 50 billion evaluations per second For all dataset default-tpsc exhibits a gradual decrease in throughput because the number of items are enough to fully utilize the GPU尽 resources Through this round of experiments we observe that the throughput is also affected by the number of transactions although at a lesser extend Additionally due to the different sparsity patterns variations in the throughput are also noticeable Retail and TK100 although having similar size exhibit different characteristics in relation to their sparsity pattern with the former consisting of many dense regions and the latter having only few This is the reason behind the 10x difference in their observed throughput 
In this section we study in detail the effect of shared memory bank con琶cts and uncoalesced off-chip memory accesses Since it was established that default-tpsc has a better performance than naive-sc and its multi-core counterpart we focus on comparing default-tpsc with its optimized variants nbc-tpsc mrs-tpsc and mr-tpsc In Fig 7 and Fig 8 we present the maximum percentage improvement in execution time per iteration which we obtained from using the aforementioned kernel variations on synthetic and real data respectively Resolving bank con琶cts produced a stable to improvement in execution time across all dataset Such a performance improvement is evident at the worst case where 32-way bank con琶ct occurs Bank con琶cts are caused during partial result sharing creating additional overhead that is upper bounded by the warp size across multiple warps For this 
027\011$\012\014\017\021\036\021\031\013\020  025\012%&\011\036\013\020\022\036\022'\031\014  025\012%&\011\036\013\020\022\036\022\020\031\014 021\(\011\032\015\011\021\030\020 
026\027\012\007\030\004\007\010\005\013\031\013    026\027\012\007\030\004\007\010\005\013\033\013    026\027\012\007\030\004\007\010\005\013\034\013    026\027\012\007\030\004\007\010\005\013\035\013    026\027\012\007\030\004\007\010\005\013\036\013  
B Shared Memory Utilization  Bank Con琶cts 
1430 
1430 


002 004\002 005\002 006\002 007\002 003\002 033\002 034\002 035\002 002\004\002\005\002 006\002 
002 005 007 033 035 004\002 004\005 004\007 004\005\006\007\003\033 
C Increased Rule Length 
002 005 007 033 035 004\002 004\005 004\007 004\033 004\035 005\002 020\020\030\027\011\025\021\013 026\032\025\025\011\020\021\030\032 025 011\021\012\030\017 023\004\002\002 023\004\002\004 
 
002\003\004\005\006\007\010\011\012\013\014\010\015\004\013'\013\022\015\(\023\011\\004\015\004\012\007\013 024\007\024!\004\007\013 002\003\004\005\006\007\010\011\012\013\014\010\015\004\013'\013\022\015\(\023\011\\004\015\004\012\007\013 024\007\024!\004\007\013 
Figure 6 Throughput for increasing length rule candidates The Y-axis is measured in billion evaluations per second The X-axis shows the corresponding rule length As before the gures include the theoretical trend-line computed from amplifying naive-sc on the GPU using Eq 4 reason the observed improvement is stable depending mostly on the item number Assigning multiple candidate rule collections to a single block resulted in Figure 7 Percentage improvement in execution time as compared to the default-tpsc kernel for the individually optimized kernels when using synthetic data   Figure 8 Percentage improvement in execution time as compared to the default-tpsc kernel for the individually optimized kernels when using real data For both kernels we observe a similar behaviour when we increase rule candidate length to a number larger than 32 After that point we require evaluating the pre度 in two phases following a technique similar to parallel reduction although using warp vote functions This extra phase requires an additional synchronization step which increases the total execution time per iteration Additionally when we have prominent rules with item indices in sequence i.e accidents dataset as indicated by its sparsity pattern caching transactions does not provide any improvement However when there are many rules with out of sequence pre度es the cost of uncoalesced memory accesses matches the synchronization cost as indicated by experiments on dataset 1 VIII C ONCLUSION In this paper we studied the support count operation commonly used in association rule mining problems We proposed a work-ef田ient parallel algorithm that is suitable for massively parallel architectures Furthermore we presented a data layout scheme used to enable low overhead coordination of the processing elements reduce the memory requirements and achieve high off-chip memory bandwidth utilization Furthermore we discussed in detail low level optimization strategies related to effective use of shared memory while presenting a simple strategy for resolving shared memory bank con琶cts incurring minimal additional work However there is still some additional issues that we need to address Firstly we already considering resolving the issue of 
025 015\013\036\021\031\013\020 015\036\021\031\013\020 
improvement over the default-tpsc execution time A combination of loop unrolling and increase shared memory utilization from storing more candidate rules in the same block was the reason for the observed improvement In contrast enabling caching of transactions in shared memory with kernel mrs-tpsc presented less improvement in the relative execution time compared to mr-tpsc The culprit is this case is the additional synchronization step which is required after loading the data in shared memory Finally experiments performed on dataset 2 and 4 indicate similar behavior to our previous experiments where multiprocessor underutilization was limiting the maximum possible performance increase Even in the case where we increase the workload of participating blocks interleaved execution of warps is limited since as the block size is small In this section we discuss the effects of discovering rules with length larger than 32 Due to lack of space we present the results from the execution on synthetic data 1 and accidents which are good representatives of the observed behaviour We focus on the mrs-tpsc and mr-tpsc variations which we established to be highly optimized throughout our experiments   
025 015\013\036\021\031\013\020 015\036\021\031\013\020 
037\005\005\010 \004\012\007!\013   002 003 004\002 004\003 005\002 005\003 006\002 006\003 007\002 002\004\002\005\002 006\002 011\012\012\004\005\007\010\011\012\013   002 003 004\002 004\003 005\002 005\003 006\002 006\003 007\002 002\004\002\005\002\006\002 004\007\024\010$\013  014%\031&&\013 
18 
027\011$\012\014\017\021\036\021\031\013\020  025\012%&\011\036\013\020\022\036\022'\031\014  025\012%&\011\036\013\020\022\036\022\020\031\014 021\(\011\032\015\011\021\030\020    002 004 005 006 007 002\004\002\005\002 006\002 
1431 
1431 


 volume 22 pages 207216 ACM 1993  R Agra w al R Srikant et al F ast algorithms for mining association rules In  volume 1215 pages 487499 1994  E Ansari G Dastghaibif ard M K eshtkaran and H Kaabi Distrib uted frequent itemset mining using trie data structure  35\(3 2008  M Atzmueller and F  Puppe Sd-mapa f ast algorithm for e xhausti v e subgroup discovery In  pages 617 Springer 2006  C Creighton and S Hanash Mining gene e xpression databases for association rules  19\(1 2003  W  F ang M Lu X Xiao B He and Q Luo Frequent itemset mining on graphics processors In  pages 3442 ACM 2009  K Geurts G W ets T  Brijs and K V anhoof Pro斗ing of high-frequenc y accident locations by use of association rules  1840 2003  A Ghoting G Buehrer  S P arthasarathy  D Kim A Nguyen Y K Chen and P Dubey Cache-conscious frequent pattern mining on modern and emerging processors  16\(1 2007  G Grahne and J Zhu Ef ciently using pre度-trees in mining frequent itemsets In  volume 90 2003  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation In  volume 29 pages 112 ACM 2000  J Hipp U G  untzer and G Nakhaeizadeh Algorithms for association rule mininga general survey and comparison  2\(1 2000  R Jin and G Agra w al An algorithm for in-core frequent itemset mining on streaming data In  pages 8pp IEEE 2005  R Jin and G Agra w al Systematic approach for optimizing comple x mining tasks on multiple databases In  pages 1717 IEEE 2006  E Lindholm J Nick olls S Oberman and J Montrym Nvidia tesla A uni兎d graphics and computing architecture  2 2008  J Liu Y  P an K W ang and J Han Mining frequent item sets by opportunistic projection In  pages 229238 ACM 2002  L Liu E Li Y  Zhang and Z T ang Optimization of frequent itemset mining on multiple-core processor In  pages 12751285 VLDB Endowment 2007  B Mobasher  R Coole y  and J Sri v asta v a Automatic personalization based on web usage mining  43\(8 151 2000  E  Ozkural B Ucar and C Aykanat Parallel frequent item set mining with selective item replication  22\(10 2011  J Pei J Han H Lu S Nishio S T ang and D Y ang H-mine Hyper structure mining of frequent patterns in large databases In  pages 441448 IEEE 2001  I Pramudiono and M Kitsure ga w a P arallel fp-gro wth on pc cluster  In  pages 467473 Springer 2003  C Silv estri and S Orlando gpudci Exploiting gpus in frequent itemset mining In  pages 416425 IEEE 2012  A T ajbakhsh M Rahmati and A Mirzaei Intrusion detection using fuzzy association rules  9\(2 2009  T  T assa Secure mining of association rules in horizontally distrib uted databases  26\(4 2014  K W ang M Stan and K Skadron Association rule mining with the micron automata processor In  2015  M J Zaki Scalable algorithms for association mining  12\(3 2000  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors In  pages 4343 IEEE 1996  F  Zhang Y  Zhang and J D Bak os Accelerating frequent itemset mining on graphics processing units  66\(1 2013  Y  Zhang F  Zhang Z Jin and J D Bak os An fpga-based accelerator for frequent itemset mining  6\(1 2013 
002 004 005 006 007 002 004\005\035 005\003\033 006\035\007 003\004\005 002 033 004\005 004\035 005\007 002 004\005\035 005\003\033 006\035\007 003\004\005 
ACM SIGMOD Record Proc 20th int conf very large data bases VLDB IAENG International Journal of Computer Science Knowledge Discovery in Databases PKDD 2006 Bioinformatics Proceedings of the fth international workshop on data management on new hardware Transportation Research Record Journal of the Transportation Research Board The VLDB Journal FIMI ACM SIGMOD Record ACM sigkdd explorations newsletter Data Mining Fifth IEEE International Conference on Data Engineering 2006 ICDE06 Proceedings of the 22nd International Conference on IEEE micro Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining Proceedings of the 33rd international conference on Very large data bases Communications of the ACM Parallel and Distributed Systems IEEE Transactions on Data Mining 2001 ICDM 2001 Proceedings IEEE International Conference on Advances in Knowledge Discovery and Data Mining Parallel Distributed and Network-Based Processing PDP 2012 20th Euromicro International Conference on Applied Soft Computing Knowledge and Data Engineering IEEE Transactions on Proceedings of the 2015 IEEE 29th International Parallel and Distributed Processing Symposium Knowledge and Data Engineering IEEE Transactions on Supercomputing 1996 Proceedings of the 1996 ACM/IEEE Conference on The Journal of Supercomputing ACM Transactions on Recon堵urable Technology and Systems TRETS 
Figure 9 Execution time measured for increasing rule size  the Xaxis indicates the rule length multiprocessor under-utilization For dataset with low number of items we can assign individual groups of threads in the same block to different rule collections effectively increasing the block size as well as utilization Secondly we would like to adapt our solution to an architecture consisting of multiple GPUs and address challenges related to partial result sharing A CKNOWLEDGMENT This work was supported by the U.S National Science Foundation under grant ACI-1339756 R EFERENCES  Frequent itemset mining dataset repository  2015 URL http://杜i.ua.ac.be/data  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases In 
014\010\015\004\013\016!\004\005!\021\013 026\027\012\007\030\004\007\010\005\013\031\013 014\010\015\004\016!\004\005!\021\013 037\005\005\010 \004\012\007!\013 
015\036\021\031\013\020 015\013\036\021\031\013\020 
1432 
1432 


 Frequent pattern mining Current status and future directions  2007  R Agra w al and R Srikant F ast algorithms for mining association rules in  1994  M J Zaki Scalable algorithms for association mining  vol 12 no 3 pp 372390 2000  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation in  2000  Y  Zhang F  Zhang and J Bak os Frequent itemset mining on large-scale shared memory machines in  2011  F  Zhang Y  Zhang and J D Bak os  Accelerating frequent itemset mining on graphics processing units  vol 66 no 1 pp 94117 2013  Y  Zhang  An fpga-based accelerator for frequent itemset mining  vol 6 no 1 pp 2:12:17 May 2013  P  Dlugosch  An ef田ient and scalable semiconductor architecture for parallel automata processing  vol 25 no 12 2014  I Ro y and S Aluru Finding motifs in biological sequences using the micron automata processor in  2014  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases in  1993  H No yes Micron尽 automata processor architecture Recon堵urable and massively parallel automata processing in  June 2014 keynote presentation  M J Zaki  Parallel data mining for association rules on shared-memory multi-processors in  1996  L Liu  Optimization of frequent itemset mining on multiple-core processor in  2007  I Pramudiono and M Kitsure ga w a P arallel fp-gro wth on pc cluster in  2003  E Ansari  Distributed frequent itemset mining using trie data structure  vol 35 no 3 p 377 2008  W  F ang  Frequent itemset mining on graphics processors in  2009  B Goethals Surv e y on frequent pattern mining  Univ of Helsinki Tech Rep 2003  C Bor gelt Ef cient implementations of apriori and eclat in  2003 p 90  Frequent itemset mining dataset repository   http mi.ua.ac.be/data  J Rabae y  A Chandrakasan and B Nik oli  c  2nd ed Pearson Education 2003 
100 1000 10000 5 50 500 5000 re_sup = 0.12 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails for re_sup = 0.08 re_sup = 0.08 
GPU fails re_sup = 0.42 re_sup = 0.42 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm  Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails re_sup = 0.3 re_sup = 0.3 b T100 Figure 11 Performance prediction with technology normalization as a function of input size implementation In contrast the capability of our AP ARM solution scales nicely with the data size since the AP was designed for processing streaming data With the challenge of the big data era a number of other complex pattern mining tasks such as frequent sequential pattern mining and frequent episode mining have attracted great interests in both academia and industry We plan to extend the proposed CPU-AP infrastructure and automaton designs to address more complex pattern-mining problems A CKNOWLEDGMENT This work was supported in part by the Virginia CIT CRCF program under grant no MF14S-021-IT by C-FAR one of the six SRC STARnet Centers sponsored by MARCO and DARPA NSF grant EF-1124931 and a grant from Micron Technology R EFERENCES  J Han 
et al Data Min Knowl Discov Proc of VLDB 94 IEEE Trans on Knowl and Data Eng Proc of SIGMOD 00 Proc of CLUSTER 11 J Supercomput et al ACM Trans Recon堵urable Technol Syst et al IEEE TPDS Proc of IPDPS14 Proc of SIGMOD 93 Proc of Fifth International Symposium on Highly-Ef田ient Accelerators and Recon堵urable Technologies et al Proc of Supercomputing 96 et al Proc of VLDB 07 Proc of PAKDD 03 et al IAENG Intl J Comp Sci et al Proc of DaMoN 09 Proc FIMI 03 Digital Integrated Circuits 
1 10 100 1000 10000 0.1 1 10 100 1000 
a Webdocs 
699 


