 2016 World Conference on Futuristic Trends in Research and Innovation for Social Welfare \(WCFTRí16  Frequent Itemset Mining Techniques - A Technical Review   Tushar M. Chaure Department of Computer Technology YCCE Nagpur, India tusharchaure55@gmail.com  Kavita R. Singh Department of Computer Technology YCCE Nagpur, India singhkavita19@yahoo.co.in    Abstract Frequent Itemset Mining is one of the most popular techniques to extract knowledge from data. However, these mining methods become more prob lematic when they are applied to Big Data. Fortunately, recent improvements in the field of parallel programming provide many tools to tackle this problem However, these tools come with their own technical challenges such as balanced data distribution and inter-communication costs. In this paper, we are presenting a detailed survey of Hadoop, which helps in storing data and parallel processing in distributed environment. Here we have explored various Frequent Itemset Mining techni ques on parallel and distributed environment. The aim of this pap er is to present a comparison of different frequent itemset mining techniques and help to develop efficient and scalable freque nt itemset mining techniques  Index Terms  Frequent itemsets, association rule mining mining algorithms  I  I NTRODUCTION  Data mining is the process of extraction of information from large databases and it is a powerful new technology having a great potential to help researchers as well as companies on the most importan t information in their data  Data m i ning tools are used to predict the future trends and behaviors thus allowing businesses to make knowledge-driven decisions Frequent itemset mining in distributed environment is a problem and must be performed using a distributed algorithm that does not require exchange of raw data between the participating sites. Distributed data mining is the process of mining data in distributed data sets. According to Zaki in [2 two dominant architectures ex ist in the distributed environments i.e distributed memory architecture \(DMA shared memory architecture \(SMA In DMA, each processor has its own database or memory and has access to it. DMA systems access to other local databases is possible only via message exchange. DMA offers a simple programming method, but limited bandwidth may reduce the scalability. On the other hand, in SMA each processor has direct and equal access to the database in the system. Thus, parallel programs on such systems can be implemented easily A set of items in a database is known as itemset. If the occurance of items in a particular transaction is frequent, it is called as frequent itemset and th e support \(or c ount\frequent itemset is greater than some us er-specified minimum support Frequent Pattern Growth \(FP-Growth\ algorithm is one of the most popularly used data mining approach for finding frequent itemset But the ma in challenge faced  by various frequent itemset mi ning algorithm is its execution time in distributed environment Distributed sources of voluminous data have created the need for distributed data mini ng. The conventional data mining algorithms/techniques which work efficiently on centralized databases have some limitations of its own when applied on distributed databases In distributed data mining, data is located at distributed locations and mi ning is performed on every local database to find globally mined data. Figure 1 depicts the architecture for dist ributed data mining This paper is organized as follows: Section 2 discuss about the various techniques/al gorithms proposed by authors for frequent itemset mining over th e years, Section 3 presents a brief overview of Hadoop and its architecture, and at last Section 4 gives a conclusion                   Figure 1: Architecture for distributed data mining    Local DM  Local DM  Local DM  Local DB 2   Local DB 1   Local DB N   Global Data Mining  


 2016 World Conference on Futuristic Trends in Research and Innovation for Social Welfare \(WCFTRí16   II  L ITERATURE REVIEW  In the following section we will discuss various algorithms/techniques that are pro posed by authors for frequent itemset mining in the last few years In 1994 authors prese nted the Count Distribution algorithm  C k i.e Candidate itemsets in its own local database. Each Site exchanges its local support with other sites to obtain entire support for all candidate itemsets. Thus, each site obtains entire support for all the candidate item sets and its local support with other sites to obtain the co mmon support for all candidate itemsets. Each site obtains L k the frequent itemset along with the candidate itemset with length of k+1 obtained from each site by execution of Apriori gen\(\function on L k  The Count Distribution algorithmís main advantage is that it doesnít exchange data t uples between processors instead it exchanges the counts. CD algorithmís communication overhead is O\(|C|*n at each phase, where  is the size of candidate itemsets C and n is the number of datasets. The limitation of the CD algorith m is that the complexity O\(C k n  becomes high if there are more number of candidate itemsets C k generated at site n  To overcome the limitation of the CD algorithm, in 1996 authors proposed the fast dist ribution mining algorithm \(FDM   each s ite pla y different roles i.e., in the beginning a site is considered as ìhome siteî for a specific produced set of candidate sets a nd then it subsequently changes to a polling site to get response time from other sites and at last it becomes remote site. The remote site returns support count of the items and the polling site generate the frequent itemsets for large databases FDMís advantage over Count Distribution algorithm  is that the com m unication over h ead is reduced to O Cp|*n\is the po tentially large candidate itemsets and n is the number of sites, respectively. FDM generates fewer number of candidate itemsets as compared to CD, when the number of disjoint candidate itemsets is large among various sites. FDM reduces the communication overhead to some extent, but it can further improve its efficiency if the candidate itemsets are reduced and global pruning of itemsets is done In the same direction authors proposed an Optimized Distributed Association Mi ning \(ODAM algorithm in  2003. ODAM calculates 1-itemset from each site and then broadcasts those itemsets and generates global frequent 1itemsets. In the next step, each site generates candidate 2itemsets, computes its support count and at the same time eliminates infrequent itemsets. ODAM algorithm generates frequent 2-itemsets globally a nd iterates through the main memory transaction. At last, it generates the support count and the final frequent itemset is generated ODAM exchanges fewer messages than FDM which improves its efficiency to a large extent. But the message exchange size increases linearly as we increase the number of sites. Thus, the message passing overhead becomes high as compared to FDM. Another limitation of ODAM is that it doesnít show the local frequent itemsets that are generated at each site To overcome the above mentioned limitations, authors proposed a Distributed Trie Fr equent Itemset Mining \(DTFIM    In DTFIM algorith m, each site scans its local database and determines the lo cal count \(1-itemsets vector is kept to make support count of every item. At the end each site synchronizes their data structure and the trie copies are alike. In the second pass, the candidate 2-itemsets are calculated and a 2-d array is used for this purpose. At the end counts are synchronized and th e global support count for candidate 2-itemset are calculated and the trie copies are updated. In the next step, for each pass k \(k>=3 itemsets are calculated. The process is repeated and pruning is performed simultaneously at each stage. The final output is frequent itemset The complexity of DTFIM is O\(n 2  than ODAM. DTFIMís efficiency also increases as the algorithm uses trie structure whic h is useful for pruning at local site.  The limitation of this algorithm is high message passing overhead A slight variation in DTFIM algorithm can improve the time required for mining frequent itemsets. In 2012, authors proposed a technique which mine frequent itemset using a gossip base i.e  A communication between sites which is purely based on random communication\ trie based apriori structure is used to improve the performance First the local frequent itemsets are computed and the support count is checked and the gossi p-based global aggregation is performed. This algorithm is more efficient as it is employing trie data structure and grouping of nodes The complexity of this algorithm is O\(nlogn\which is far better than DTFIM algorithmís complexity. Also, the Gossip based communication helps in reducing the computation overhead of finding frequent itemset. But, the scalability is high as it requires minimum communication cost and comparison costs The gossip based algorithm proved  by grouping the node s and arranging them into a hierarchical structure at each site in the distributed environment. It can be achieved by using Hadoop whic h allows distributed processing of large databases across the nodes in a distributed environment. By using Hadoop, we can reduce the message passing overhead as the Hadoop Distributed File System HDFS t ore the data in trie s t ructure at each site   Another advantage of using Hadoop is that it helps to reduce the fault tolerance. Since Hadoop uses replication between the sites in the distributed environment. Thus, every node has the copy of data if any node fails to communicate In the next section, we discuss about Hadoop technology its architecture and its working in a distributed environment  III  H ADOOP  Hadoop is a free open source platform, which helps in storing data and parallel pro cessing in a distributed 


 2016 World Conference on Futuristic Trends in Research and Innovation for Social Welfare \(WCFTRí16  environment. Hadoop splits the lar ge database into blocks of data and distributes over the clusters in the distributed environment. To process the data, MapReduce is used for parallel processing on the clusters, thus reducing the execution time The Hadoop Distributed File System \(HDFS a distributed file system wh ich is designed to run on commodity hardware. It is many similar to the existing distributed file systems. But, there are some differences between HDFS and other distribut ed file systems which makes it significant. HDFS is highly fault-tolerant and is designed in such a way that it can be deployed on low-cost hardware HDFS also provides high throughput access to application data and is very suitable for applications that have large data sets   Figure 2: HDFS Ar  Figure 2 shows the HDFS master/slave  An HDFS cluster consists of two parts viz., a single NameNode and more than one DataNode. NameNode is a master server that regulates access to files by clients and manages the file system namespace. There are a number of DataNodes in HDFS, usually one per node in the cluster. The DataNode manages the storage which is attached to the nodes that they are running on. HDFS exposes a file system namespace and allows the user data to be stored in files Internally in an HDFS, a file is split into one or more blocks and these blocks are then stored in a set of DataNodes The NameNode is also used to execute file system namespace operations which include opening a file, closing a file and renaming files and directories in the HDFS. It also performs the mapping of blocks of data to the DataNodes. On the clientís side, the DataNodes are responsible for serving the read and write requests from the HDFS. The DataNodes also perform operations such as block creation, deletion, and replication upon the instruction provided from the NameNode  IV  C ONCLUSION  As we have reviewed different techniques of frequent itemset mining in parallel and distributed environments, most of the techniques/algorithms ha ve shortcomings of their own Although, hadoop technology can provides a better platform to overcome the shortcomings of the above mentioned mining techniques A CKNOWLEDGMENT  The author is highly gratified to her respected guide Prof K. R. Singh for admirable guida nce and support to complete this paper. Author is also tha nkful to Project Development Lab Department of Computer Technology, Yeshwantrao Chavan College of Engineering, Nagpur \(I ndia facilities to complete this ma nuscript in present nature. The author would like to thank Prof. K. R. Singh \(HOD, Dept. of CT\ily members for financial and moral supports throughout their technical education  R EFERENCES  1  Olusegn Folorunso and Adewale O. Ogunde, ìData Mining as a Technique for Knowledge Management in Business Process Redesign,î In Electronic Journal of Knowledge Management Volume 2, Issue 1, pp. 43-54, 2004 2  Mohammed J. Zaki, ìParallel and Distributed Association Mining: A Survey,î In Proceedings of Concurrency, IEEE Volume 7, Issue 4, pp. 14-25, 1999 3   Christian B orgelt, ìAn Implementation of the FP-Growth  Algorithm,î In Proceedings of the 1 st International Workshop on Open Source Data Mining: Frequent Pattern Mining Implementations, pp. 1-5, 2005 4    M. H. Dunh am, ìData Mining: Introductor y and Advanced Topics,î Prentice Hall, 2002 5   Tirum ala Prasad and MHM Krishna Prasad D istributed Count Association Rule Mining Algorithm,î In International Journal of Computer Trends and Technology, Volume 1, Issue 3, pp. 370-374, 2011 6   David W. C heung, Jiawei Ha n, Vin c en t T Ng, Ada W. Fu and Yongjian Fu, ìA Fast Distribution Algorithm for Mining Association Rule,î In Proceedings of Parallel and Distributed Information Systems, pp. 31-42, 1996 7   M. Z. Ashrafi, D. Taniar and K. Smit h, ìOptimized  Distributed Association Rule Mining,î In Proceedings of IEEE distributed system online, Volume 5, No. 3, pp. 1-18, 2004 8   ard and M. keshatkar an  Distributed Trie Frequent Itemset Mining,î In Proceedings of International Multi Conference of Engineers and Computer Scientists, Volume 1, pp. 978-988, 2008 9    M. Bagheri S. Mirian Hosse inabad i, H. Mashay ekhi and J Habibi, ìMining Distributed Frequent Itemset using Gossip Based Protocol,î In Proceedings of Ubiquitous Intelligence and Computing and 9th International Conference on Autonomic and Trusted Computing \(UIC/ATC\pp. 780-785, 4-7 Sep 2012 10   Chanchal Yadav, Shul ian g Wang and Manoj Kumar Algorithm and approaches to handle large Data-A Survey,î In International Journal of Computer Science and Network Volume 2, Issue 3, 2013 11   L. Brankovic and V. Esti vill-Castro Privacy issues in knowledge discovery and data mining,î In Proceedings of Australian Institute of Computer Ethics Conference, pp. 89-99 1999 


 2016 World Conference on Futuristic Trends in Research and Innovation for Social Welfare \(WCFTRí16  12   Wei Fan and Albert Bif e t, ìBig Data: Cur rent Status and  Forecast to the Future,î In Proceedings of Special Interest Group on Knowledge Discovery and Data Mining Explorations Volume 14, Issue 2, pp. 1-5, 2012 13   R. Agrawal T. Im ie lins k i, and A  Swam i  M ining association rules between sets of items in large databases,î In Proceedings of Special Interest Group on Management Of Data Volume 22, Issue 2, pp. 207-216, 1993 14   SungHwan Kim, Jung-Ho Eom and Tai M y oung Chung Data Security Hardening Methodology Using Attributes Relationship,î In Proceedings of International Conference on Information Science and Applications, pp. 1-2, 2013 15    Ferenc bodon, ìA Trie-based Apriori Implementation for Mining Frequent Item sequences,î In Journal of Association for Computing Machinery, pp. 56-65, 2005 16   Gang Wu Huxing Zhang  Meikang Qui, Zhong Ming Jiayin Li and Xiao kin, ìA Decentralized Approach for Mining Event Correlations in Distributed System Monitoring,î In Journal of Parallel and Distributed Computing, Volume 73 Issue 3, pp. 330-340, 2013 17    M. A Mottalib Kazi Sham sul Arefin, Moham m a d Majharul Islam, Md. Arif Rahman, and Sabbeer Ahmed Abeer Performance Analysis of Distributed Association Rule Mining with Apriori Algorithm,î In International Journal of Computer Theory and Engineering, Volume 3, No. 4, pp. 484-488, 2011 18   Assaf Schuster and R a n Wolff, ìCommuni cation Efficient Distributed Mining of Association Rules,î In Journal of Association for Computing Machinery Special Interest Group on Management of Data, Volume 8, Issue 2, pp. 171-196, 2001 19   Lai Yang Zhongzhi Shi, X u L.D., Fan Liang and I. Kirsh   DH-TRIE Frequent Pattern Mining on Hadoop using JPA,î In Proceedings of IEEE International Conference on Granular Computing, pp. 875-878, 2011 20   Tao Xiao Chunfeng Yuan and Yihua Hu ang, ìPSON A Parallelized SON Algorithm with MapReduce for Mining Frequent Sets,î In Proceedings of Fourth International Symposium on Parallel Architectures, Algorithms and Programming, pp. 252-257, 2011 21   Suhasini A. Itkar and U d ay V. Kulk ar ni, ìDistributed Algorithm for Frequent Pattern Mining using Hadoop Map Reduce Framework,î In Journal of Association of Computer Electronics and Electrical Engineers, pp. 15-24, 2013 22   C. R. Valencio, F. T. O y ama, P. S. Neto and R. C. G. de Souza, ìComparative Study of Algorithms for Mining Association Rules,î In Proceedings of Parallel and Distributed Computing, Applications and Technologies \(PDCAT 280, 2011 2   M. A. Khan, Z. A  Mem o m  and S. Khan, ìHighly Available Hadoop NameNode Architecture,î In Proceedings of Advanced Computer Science Applications and Technologies \(ACSAT pp. 167-172, 2012                                                  


         


that case our proposed mining algorithm outperforms the conventional Apriori algorithm Fig. 3. TicTacToe data   Fig. 4. T8I5D100K   Fig. 5. T6I4D100K  V  CONCLUSIONS  AND  SUMMARY In this paper, we have developed a genetic based approach and compared the results with the results given by the Apriori algorithm for mining maximal frequent item sets. We have obtained the results through experimental analysis on real data sets using both of these algorithms. Several advantages have been demonstrated by the experimental analysis of this algorithm in comparison with Apriori algorithm, which are as follows 200  It gives better results than the Apriori algorithm by accessing large data sets for less numbers of nodes especially when the support value is set low by the users 200  For large data sets and low support value, both of these algorithms give the same solution by giving the same number of maximal frequent item sets. To get this solution Apriori considers a large number of candidate item sets with respect to a genetic based approach 200  For large data sets and high support values, Apriori performs better than a genetic based approach, since the genetic algorithm uses global search mechanism Apriori uses a level by level search procedure and it gets the solution by accessing less numbers of nodes because solution is near the root node. The nodes close to the root of lexicographic tree have higher support values 200  Low support value generates a long size frequent pattern which provides information like frequency of an exponential number of smaller sub patterns. In that case a genetic based approach performs better than other existing algorithms 200  The experimental results of a genetic based approach demonstrate the effect of generations of individuals, and prune all the subsets and supersets in a lexicographic tree, which is cost effective in the case of counting the support value and reducing the search space dramatically The other areas which should be focused for further research include developing genetic operators in such a way that it will help to reduce generating the same individuals and it can increase valid individuals for further generation Considering repetitive individuals sometimes takes a longer time to get the solution A CKNOWLEDGMENT  This research work was funded by School of Engineering and ICT, University of Tasmania, Australia, and website http://www.utas.edu.au/cricos, under CRICOS Provider Code 00586B  R EFERENCES  1  M M. J  K a bir  S  X u  B. H  K a ng a nd Z  Z h ao 223 A N o ve l Approach to Mining Maximal Frequent Itemsets Based on Genetic Algorithm,\224 in International Conference on Information Technology and Applications \(ICITA 2014 2  R C. A g ar w al C. C A g g ar w al and V V  V P r asa d  223 A Tr ee  Projection Algorithm For Generation of Frequent Itemsets,\224 Parallel Distrib. Comput. Spec. Issue High Perform. Data Min vol 61, no. 3, pp. 350\226371, 2001 3  A  S a l leb  Z M a a z ou zi  and C V r a in  223Mi n i n g M a xi m a l F r eq u e nt Itemsets by a Boolean Based Approach,\224 in European Conference on Artificial intelligence 2002, pp. 285\226289 4  J  H a n, J  P e i a n d Y  Y i n 223 M i n i n g F r e que nt P a tte r n s w i tho u t  Candidate Generation,\224 ACM SIGMOD vol. 29, no. 2, pp. 1\22612 2000 5  J  H i p p  U  G 374ntz e r a nd G  N a kh ae iz ade h 223 A l g or ithm s f o r  association rule mining\227a general survey and comparison,\224 ACM sigkdd Explor. \205 vol. 2, no. 1, pp. 58\22664, 2000 6  R. J  K u o an d C  W  S h i h 223 A s s o ciati o n r u l e m i ni ng t h r o ug h the a n t  colony system for National Health Insurance Research Database in 44 


Taiwan,\224 Comput. Math. with Appl vol. 54, no. 11\22612, pp. 1303\226 1318, Dec. 2007 7 J  H Holla n d   Adaptation in Natural and Artificial Systems Ann Arbor: University of Michigan Press, 1975 8  K  F M a n  K S  T a n g   a n d S  Kwo n g  223 G e n e t i c Al g o r i t h m s 037   Concepts and Applications,\224 IEEE Trans. Ind. Electron vol. 43 no. 5, 1996 9  D  Be as l e y  D  R. B u l l  an d R R Ma r tin 223 A n O v e r v iew of  G e ne tic  Algorithms\037: Part 1 , Fundamentals,\224 Univ. Comput vol. 15, no. 2 pp. 58\22669, 1993   M  S r i n i v a s an d L  M  P a tn ai k 223G e n et i c A lgo r i t h m s A S u r v ey  224  Computer \(Long. Beach. Calif vol. 27, no. 6, pp. 17\22626, 1994  D E  G o ld b e rg  Genetic Algorithms in Search, Optimization and Machine Learning Addison-Wesley Longman Publishing Co., Inc Boston, MA, USA, 1989  Z  M i ch a l ew i c z  Genetic algorithms + data structures = evolution programs Berlin: Springer, 1992  R  A gra w a l a n d R  Sri kan t  223F a s t A l gori t h m s for M i n i n g  Association Rules,\224 in 20th International Conference on Very Large Data Bases 1994, pp. 487\226499 14  D  I  L i n and Z  M. K e de m  223 P ince r S e a r c h A  N e w  A l go r ithm f o r  Discovering the Maximal Frequent Set,\224 in 6th International Conference on Extending Database Technology   R  J   B a y a rd o 223E ffic i e nt ly M i ni n g L on g Pa tt ern s from D a t a b a s e s  224 ACM SIGMOD pp. 85\22693, 1998 16  R  C  A g a r w a l  C  C  A g g a r w a l  a n d V  V  V  P r a s a d  223 D e p t h f i r s t  generation of long patterns,\224 Proc. sixth ACM SIGKDD Int. Conf Knowl. Discov. data Min. - KDD \22200 vol. 2, pp. 108\226118, 2000  D Bu rdi c k   M  Ca li m l i m   an d J   Geh r k e  223M AFI A a m a xi m a l  frequent itemset algorithm for transactional databases,\224 Proc. 17th Int. Conf. Data Eng no. X, pp. 443\226452 18  K   G o uda an d M. J  Z a ki, \223 G e n Max 037   A n E f f icie n t A l go r ithm f o r  Mining,\224 Data Min. Knowl. Discov vol. 11, no. 3, pp. 223\226242 2005    A l a t a and E. Akin, \223An efficient genetic algorithm for automated mining of both positive and negative quantitative association rules,\224 Soft Comput vol. 10, no. 3, pp. 230\226237, Apr 2005 20  W D o u  J  Hu  K Hi r a s a wa   a n d  G Wu  223 Q u i c k r e s p on s e d a t a  mining model using genetic algorithm,\224 2008 SICE Annu. Conf pp 1214\2261219, Aug. 2008  A  Sa ll e b a ou i ssi C Vra i n   C  Norte t  X K o n g  a n d D  C a ss a r d  223QuantMiner for Mining Quantitative Association Rules,\224 Mach Learn. Res vol. 14, no. 1, pp. 3153\2263157, 2013 22  A  S a l le b a o u is s i C  V r ai n an d C. N o r te t, \223 Q ua n tM ine r 037  A  Genetic Algorithm for Mining Quantitative Association Rules,\224 in 20th International Joint Conference on Artificial Intelligence 2007 pp. 1035\2261040  J  Hua n g Y  Ch e-t s un g  a nd  C  F u 223 A Gen e t i c A l g ori t h m  B a s e d Searching of Maximal Frequent Itemsets,\224 in International conference on artificial intelligence 2004    45 


0.80 0.76 0.72 0.68 0.64 0.60 0.56 0.52 0.48 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0   Apriori-AP counting speedup Apriori-AP overall speedup  Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time 
0.1 1 10 100 3000 2000 1000 0 1000 2000 3000 4000 5000 
0.6 0.5 0.4 0.3 0.2 0.1 0.0 10 100 
1 10 100 6000 4000 2000 0 2000 4000 6000 8000 10000 
0.20 0.19 0.18 0.17 0.16 0.15 0.14 0.13 0.12 0.11 0.10 0.09 0.08 0.07 0.06 0.05 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0 Apriori-AP counting speedup Apriori-AP overall speedup  Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time  0 10000 5000 15000 c Webdocs Figure 5 The performance results of Apriori-AP on three real-world benchmarks DP time SR time and CPU time represent the data process time on AP symbol replacement time on AP and CPU time respectively Webdocs switches to 16-bit encoding when relative minimum support is less then 0.1 8-bit encoding is applied in other cases 
E vs Eclat Eclat et al Eclat Eclat 
0.80 0.75 0.70 0.65 0.60 0.55 0 100 200 300 400 Computation Time \(s Relative minimum support 1.0X Symbol replacement time 0.5X Symbol replacement time 0.1X Symbol replacement time Figure 7 The impact of symbol replacement time on Apriori-AP performance for Pumsb how symbol replacement time affects the total AprioriAP computation time A reduction of 90 in the symbol replacement time leads to 2.3X-3.4X speedups of the total computation time The reduction of symbol replacement latency will not affect the performance behavior of AprioriAP for large datasets since data processing dominates the total computation time 
Apriori Eclat Equivalent Class Clustering   is another algorithm based on Downward-closure uses a vertical representation of transactions and depth-ìrst-search strategy to minimize memory usage Zhang  proposed a h ybrid depth-ìrst/breadth-ìrst search scheme to expose more parallelism for both multi-thread and GPU versions of  However the trade-off between parallelism and memory usage still exists For large datasets the nite memory main or GPU global memory will become a limiting factor for performance and for very large datasets the algorithm fails While there is a parameter which can tune the trade-off between parallelism and memory occupancy we simply use the default setting of this parameter for better performance Figure 8 shows the speedups that the sequential 
0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0 AP counting speedup Apriori-AP overall speedup    Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time  b Accidents 
T40D500K Counting T40D500K Overall T100D20M Counting T100D20M Overall Webdocs5X Counting Webdocs5X Overall Speedup Relative Minimum Support Figure 6 The speedup of Apriori-AP over Apriori-CPU on three synthetic benchmarks 
1 10 100 
a Pumsb 
696 


0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.1 1 10 Pumsb Accidents T40D500K Webdocs Webdocs5X T100D20M ENWiki Speedup Relative Minimum Support 
Figure 8 The performance comparison of CPU sequential and algorithm achieved with respect to sequential Apriori-CPU Though has 8X performance advantage in average cases the vertical bitset representation become less efìcient for sparse and large dataset high trans and freq item ratio This situation becomes worse as the support number decreases The Apriori-CPU implementation usually achieves worse performance than  though the performance boost of counting operation makes Apriori-AP a competitive solution to parallelized  Three factors make a poor t for the AP though it has better performance on CPU 1 requires bit-level operations but the AP works on byte-level symbols 2 generates new vertical representations of transactions for each new itemset candidate while dynamically changing the values in the input stream is not efìcient using the AP 3 Even the hybrid search strategy cannot expose enough parallelism to make full use of the AP chips Figure 9 and 10 show the performance comparison between Apriori-AP 45nm for current generation of AP and sequential multi-core and GPU versions of  Generally Apriori-AP shows better performance than sequential and multi-core versions of  The GPU version of shows better performance in Pumsb Accidents and Webdocs when the minimum support number is small However because of the constraint of GPU global memory Eclat-1G fails at small support numbers for three large datasets ENWiki T100D20M and Webdocs5X ENWiki as a typical sparse dataset causes inefìcient storage of bitset representation in Eclat and leads to early failure of EclatGPU and up to 49X speedup of Apriori-AP over Eclat-6C In other benchmarks Apriori-AP shows up to 7.5X speedup over Eclat-6C and 3.6X speedup over Eclat-1G This gure also indicates that the performance advantage of AprioriAP over GPU/multi-core increases as the size of the dataset grows The AP D480 chip is based on 45nm technology while the Intel CPU Xeon E5-1650 and Nvidia Kepler K20C on which we test are based on 32nm and 28nm technologies respectively To compare the different architectures in the same semiconductor technology mode we show the performance of technology projections on 32nm and 28nm technologies in Figure 9 and 10 assuming linear scaling for clock frequency and square scaling for capacity The technology normalized performance of Apriori-AP shows better performance than multi-core and GPU versions of Eclat in almost all of the ranges of support that we investigated for all datasets with the exception of small support for Pumsb and T100D20M Apriori-AP achieves up to 112X speedup over Eclat-6C and 6.3X speedup over Eclat-1G The above results indicate that the size of the dataset could be a limiting factor for the parallel algorithms By varying the number of transactions but keeping other parameters xed we studied the behavior of Apriori-AP and Eclat as the size of the dataset increases Figure 11 For T100 the datasets with different sizes are obtained by the IBM synthetic data generator For Webdocs the different data sizes are obtained by randomly sampling the transactions or by concatenating duplicates of the whole dataset In the tested cases the GPU version of fails in the range from 2GB to 4GB because of the nite GPU global memory Comparing the results using different support numbers on the same dataset it is apparent that the smaller support number causes Eclat-1G to fail at a smaller dataset This failure is caused by the fact that the ARM with a smaller support will keep more items and transactions in the data preprocessing stage While not shown in this gure it is reasonable to predict that the multicore implementation would fail when the available physical memory is exhausted However Apriori-AP will still work well on much larger datasets assuming the data is streamed in from the hard drive assuming the hard drive bandwidth is not a bottleneck VII C ONCLUSIONS AND THE F UTURE W ORK We present a hardware-accelerated ARM solution using Micronês new AP architecture Our proposed solution includes a novel automaton design for matching and counting frequent itemsets for ARM The multiple-entry NFA based design was proposed to handle variable-size itemsets MENFA-VSI and avoid routing reconìguration The whole design makes full usage of the massive parallelism of the AP and can match and count up to itemsets in parallel on an AP D480 48-core board When compared with the based single-core CPU implementation the proposed solution shows up to 129X speedup in our experimental 
Apriori Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat F Normalizing for technology Eclat G Data size Eclat Eclat Eclat Apriori 
18 432 
 
697 


0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 1 10 100 1000 10000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm a Webdocs\(1.4GB 0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 10 100 1000 10000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails d Webdocs5X 7.1GB Figure 10 Performance comparison among Apriori-AP Eclat-1C Eclat-6C and Eclat-1G with technology normalization on four large datasets 
0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(second Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm b Accidents 34MB 0.40 0.36 0.32 0.28 0.24 0.20 0.16 0.12 0.08 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm c T40D500K\(49MB Figure 9 Performance comparison among Apriori-AP Eclat1C Eclat-6C and Eclat-1G with technology normalization on three small datasets results on seven real-world and synthetic datasets This APaccelerated solution also outperforms the multicore-based and GPU-based implementations of 0.60 0.55 0.50 0.45 0.40 0.35 0.30 0.25 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails c T100D20M 6.3GB 
ARM a more efìcient algorithm with up to 49X speedups especially on large datasets When performing technology projections on future generations of the AP our results suggest even better speedups relative to the equivalent-generation of CPUs and GPUs Furthermore by varying the size of the datasets from small to very large our results demonstrate the memory constraint of parallel ARM particularly for GPU 
Eclat Eclat 
0.80 0.76 0.72 0.68 0.64 0.60 0.56 0.52 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm 
0.20 0.15 0.10 0.05 0.00 1 10 100 1000 10000 100000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(second Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails b ENWiki\(3.0GB 
a Pumsb 16MB 
698 


 Frequent pattern mining Current status and future directions  2007  R Agra w al and R Srikant F ast algorithms for mining association rules in  1994  M J Zaki Scalable algorithms for association mining  vol 12 no 3 pp 372Ö390 2000  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation in  2000  Y  Zhang F  Zhang and J Bak os Frequent itemset mining on large-scale shared memory machines in  2011  F  Zhang Y  Zhang and J D Bak os  Accelerating frequent itemset mining on graphics processing units  vol 66 no 1 pp 94Ö117 2013  Y  Zhang  An fpga-based accelerator for frequent itemset mining  vol 6 no 1 pp 2:1Ö2:17 May 2013  P  Dlugosch  An efìcient and scalable semiconductor architecture for parallel automata processing  vol 25 no 12 2014  I Ro y and S Aluru Finding motifs in biological sequences using the micron automata processor in  2014  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases in  1993  H No yes Micronês automata processor architecture Reconìgurable and massively parallel automata processing in  June 2014 keynote presentation  M J Zaki  Parallel data mining for association rules on shared-memory multi-processors in  1996  L Liu  Optimization of frequent itemset mining on multiple-core processor in  2007  I Pramudiono and M Kitsure ga w a P arallel fp-gro wth on pc cluster in  2003  E Ansari  Distributed frequent itemset mining using trie data structure  vol 35 no 3 p 377 2008  W  F ang  Frequent itemset mining on graphics processors in  2009  B Goethals Surv e y on frequent pattern mining  Univ of Helsinki Tech Rep 2003  C Bor gelt Ef cient implementations of apriori and eclat in  2003 p 90  Frequent itemset mining dataset repository   http mi.ua.ac.be/data  J Rabae y  A Chandrakasan and B Nik oli  c  2nd ed Pearson Education 2003 
100 1000 10000 5 50 500 5000 re_sup = 0.12 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails for re_sup = 0.08 re_sup = 0.08 
GPU fails re_sup = 0.42 re_sup = 0.42 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm  Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails re_sup = 0.3 re_sup = 0.3 b T100 Figure 11 Performance prediction with technology normalization as a function of input size implementation In contrast the capability of our AP ARM solution scales nicely with the data size since the AP was designed for processing streaming data With the challenge of the big data era a number of other complex pattern mining tasks such as frequent sequential pattern mining and frequent episode mining have attracted great interests in both academia and industry We plan to extend the proposed CPU-AP infrastructure and automaton designs to address more complex pattern-mining problems A CKNOWLEDGMENT This work was supported in part by the Virginia CIT CRCF program under grant no MF14S-021-IT by C-FAR one of the six SRC STARnet Centers sponsored by MARCO and DARPA NSF grant EF-1124931 and a grant from Micron Technology R EFERENCES  J Han 
et al Data Min Knowl Discov Proc of VLDB 94 IEEE Trans on Knowl and Data Eng Proc of SIGMOD 00 Proc of CLUSTER 11 J Supercomput et al ACM Trans Reconìgurable Technol Syst et al IEEE TPDS Proc of IPDPSê14 Proc of SIGMOD 93 Proc of Fifth International Symposium on Highly-Efìcient Accelerators and Reconìgurable Technologies et al Proc of Supercomputing 96 et al Proc of VLDB 07 Proc of PAKDD 03 et al IAENG Intl J Comp Sci et al Proc of DaMoN 09 Proc FIMI 03 Digital Integrated Circuits 
1 10 100 1000 10000 0.1 1 10 100 1000 
a Webdocs 
699 


