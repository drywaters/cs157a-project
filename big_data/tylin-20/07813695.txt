 International Conference on Computing, Communication and Automation \(ICCCA2016  ISBN  978-1-5090-1666-2/16/$31.00 \2512016 IEEE 87  Observations on Factors Affecting Performance of MapReduce based Apriori on Hadoop Cluster   Sudhakar Singh Department of Computer Science Institute of Science  BHU Varanasi  India sudhakarcsbhu@gmail.com Rakhi Garg Department of Computer Science Mahila Mahavidyalaya, BHU Varanasi, India rgarg@bhu.ac.in P. K. Mishra Department of Computer Science Institute of Science, BHU Varanasi  India mishra@bhu.ac.in  Abstract Designing fast and scalable algorithm for mining frequent itemsets is always being a most eminent and promising problem of data mining. Apriori is one of the most broadly used and popular algorithm of frequent itemset mining Designing efficient algorithms on MapReduce framework to process and analyze big datasets is contemporary research nowadays. In this paper, we have focused on the performance of MapReduce based Apriori on homogeneous as well as on heterogeneous Hadoop cluster We have investigated a number of factors that significantly affects the execution time of MapReduce based Apriori running on homogeneous and heterogeneous Hadoop Cluster Factors are specific to both algorithmic and nonalgorithmic improvements Considered factors specific to algorithmic improvements are filtered transactions and data structures Experimental results show that how an appropriate data structure and filtered transactions technique drastically reduce the execution time The non-algorithmic factors include speculative execution, nodes with poor performance, data locality distribution of data blocks, and parallelism control with input split size We have applied strategies against these factors and fine tuned the relevant parameters in our particular application Experimental results show that if cluster specific parameters are taken care of then there is a significant reduction in execution time Also we have discussed the issues regarding MapReduce implementation of Apriori which may significantly influence the performance Keywords Frequent Itemset Mining Apriori Heterogeneous Hadoop Cluster; MapReduce; Big Data I  I NTRODUCTION  x-none Frequent itemset mining on big data sets is one of the most contemporary research in Data Mining and  Bi g Data 2  In  orde r to m ine intel lig ence from  big  data set s  da ta  mining algorithms are being re-designed on MapReduce framework to be executed on Hadoop cluster. Hadoop  extremely large scale and fault tolerant parallel and distributed system for managing and processing of big data. MapReduce  have algorithm t  transforms the data into valuable and precise information Frequent itemset mining is one of the most important technique of data mining Apriori 5  is  the m os t fam ou s simple and well known algorithm for minin g frequent itemsets using candidate itemsets generation Many parallel and distributed version of Apriori algorithm have been designed to enhance the speed and to mine large scale datasets 6   These algorithms are efficient in analyzing data but not in managing large scale data Hadoop is an excellent infrastructure that provides an integrated service of managing and processing excessive volumes of datasets The core constituent s  of Hadoop are Hadoop Distributed File System HDFS and MapReduce 8  H D FS provides scalable and fast access to its unlimited storage of data MapReduce is a parallel programming model that provides an efficient and scalable processing of large volumes of data stored in HDFS  x-none An application executes as a MapReduce job on Hadoo p cluster MapReduce provides high scalability   as a job is partitioned into a number of smaller tasks to run in parallel on multiple nodes in cluster. MapReduce programming model is so simplified that programmers only need to focus on processing data rath er than on parallelism related details e.g data & task partition, load balancing etc. The performance of a MapReduce job running on Hadoop cluster can be optimized in two ways The first one is the algorithm specific where algorithmic optimization can be incorporated directly The s econd one is the cluster specific where one can adjust some parameters of cluster configurations and input size for datasets. Many techniques have been proposed to optimize the performance of Apriori algorithm on MapReduce frame work Hadoop is designed on the implicit assumption that nodes in  possible to have homogeneous nodes. Most of the laboratories and institutions are used to have heterogeneous machines. So it beco mes essential to adopt proper strategies when running MapReduce job on heterogeneous Hadoop cluster The performance of a MapReduce job running on Hadoop cluster is greatly affected by tuning of various parameters specific to cluster configuration For Apr iori like CPU intensive algorithms the granularity of input split may lead to a major difference in execution times In this paper we have incorporated two algorithm specific techniques data structures and filtered transactions in Apriori algorithm which  greatly reduce the execution time on both homogeneous and heterogeneous cluster We have investigated some factors specific to cluster configuration to make the execution faster Factors central to our discussion are speculative execution performance of physical node versus virtual node, distribution of data blocks   and parallelism control using input split size 


 International Conference on Computing, Communication and Automation \(ICCCA2016   88   x-none Moreover we have also discussed the issue regarding MapReduce implementation of Apriori that quite possibly influence the execution time We have executed the different variation of MapReduce based Apriori on our local heterogeneous Hadoop cluster and found that we can achieve fast er  execution by tuning the cluster specific parameters without making algorithmic improvements  The rest of the paper is organized as follows Section 2 introduces some fundamental concepts regarding Apriori algorithm Hadoop cluster and MapReduce programming paradigm. Section 3 summarizes works related to optimization of Apriori on MapReduce framework and performance imp rovement of MapReduce job on heterogeneous clusters Experimental platform is described in section 4 Factors affecting the performance of MapReduce based Apriori and strategies adopted to improve the performance along with the experimental results are dis cussed in section 5 Finally section 6 concl udes the paper   II  B ASIC C ONCEPTS  x-none A  Apriori Algorithm Apriori is an iterative algorithm proposed by R Agrawal and R. Srikant wh ic h fi nds f re quent it em set s b y ge nerat in g candidate itemsets. Apriori name of the algorithm is based on the apriori property which states that all the subset k1 itemsets of a frequent kite mset must also be frequent  Apriori first scans the database and count the support of each item, and then checks against minimum support threshold to generate set of frequent 1-itemset L 1  In k th iteration    candidate k-itemsets C k  are generated from frequent k1 itemsets L k-1   Again entire database is scanned to count the support of candidates C k  and tested against minimum support to generate frequent k-itemsets L k  In each iteration generate and test steps are being carried out until there is no possibility to generate more new candidates.  Candidates C k are generated from frequent itemsets L k-1 using joining and pruning actions Frequent itemsets L k-1  are conditionally joined with itself such that two itemsets of L k-1 are joined if and only if their first k 2  items are equal and k 1 th item of first itemset is lexicographically smaller than respective item of the other itemset. Pruning based on Apriori property reduces the size of candidates C k  by removing infrequent itemsets   x-none B  Hadoop and MapReduce Hadoop is designed on the fundamental principle of distributing computing power to where the data is rather than movement of data as in traditional parallel and distributed computing system using MPI Message Passing Interface Hadoop is an extremely scalable and highly fault tolerant distributed infrastructure that automatically handles parallelization, load balancing and data distribution [10  The  core components of Hadoop are HDFS and MapReduce whi ch are inspired by Google's File System GFS   and Google's MapReduce model H DF S is capab le  of  st ori ng excessive volumes of data and fast accessing to the stored data It provides high fault tolerance and high availability of data. Files are stored in HDFS after breaking into smaller data blocks default block size is 64 MB Blocks are replicated across multiple nodes in the cluster \(default replication factor is 3\A Hadoop cluster works on master-slave architecture in which one node is a master node and remaining nodes are slave nodes Master node known as NameNode controls the slave nodes known as DataNodes Slave nodes hold all the data blocks and perform map and reduce tasks A computational application runs as a MapReduce job on input datasets residing in HDFS of Hadoop cluster A MapReduce job consists of map and reduce tasks and both work on data in the form of key, value pairs. Map and reduce tasks are being executed by Mapper and Reducer class respectively of MapReduce framework An additional combiner class may also be used executing reduce task and known as mini reducer There are a number of instances of Mapper and Reducer running in parallel but a Reducer starts only when all the Mapper has been completed. Mapper takes input as assigned datasets, process it and produces a number of  key value  pairs as output These key value  pairs are assigned to Reducers after sorting and shuffling by  assigned key and list of values associated with this key to a particular Reducer. Reducer takes input as key, list of values  pairs and produce new key, value pairs. Combiner works on the output of Mappers of one node to reduce the data transfer load from Mappers to R ed ucers In MapReduce framework only a single time communication occurs when output of Mappers are being transferred to Reducers [10   Apriori is an iterative algorithm which generates frequent k-itemsets in k th  iteration Corresponding to an iteration of Apriori one has to trigger a new MapReduce job/phase In each M ap Reduce phase input data is read from HDFS and frequent itemsets of previous phase from DistributedCache 9  to generate frequent itemsets of current phase  III  R ELATED W ORKS  Apriori is re-designed on MapReduce framework by many authors 13 bu t m os t of  them  are strai gh t f orward  implementations. FPC \(Fixed Passes Combined-counting\ and DPC Dynamic Passes Combined-counting algorithms combine the multiple consecutive passes of SPC \(Single Pass Counting\ to enhance the performance  SPC is a straight forward implementation of Apriori on MapReduce framework Algorithm proposed by F Kovacs and J Illes 19  i nvok e s candidate generation inside Reducer as it is used to be inside Mapper Authors also proposed a triangular matrix data structure for separate counting of 1 and 2-itemsets in a single iteration L Li and M Zhang   pr op os ed a da ta set  distribution strategy for heterogeneous Hadoop cluster and used it on a single MapReduce phase implementation of Apriori Honglie Yu et al  pro posed  an al gorithm  on Hadoop that uses Boolean Matrix and AND  operator on this matrix A parallel randomized algorithm PARMA proposed by Matteo Riondato et al 22  discov e rs ap pro xim at e  fr eq ue nt  itemsets from a sample of dataset s  Many works have been done for improving MapReduce performance in heterogeneous Hadoop clusters J Xie et al   de velo ped a data placem ent m anag em e nt m echan is m  an d 


 International Conference on Computing, Communication and Automation \(ICCCA2016   89   incorporated two algorithms \(named as Initial Data Placement and Data Distribution\into HDFS. The first algorithm divides a large input file into even-sized fragments and then assigns fragments to heterogeneous nodes in a cluster according to data processing speed of nodes.  Processing speed of nodes is  quantified by calculating computing ratio of nodes Second algorithm overcomes the dynamic data load balancing problem. The default Hadoop job scheduler FIFO degrades the performance on heterogeneous cluster. A scheduling algorithm LATE \(Longest Approximate Time to End\is proposed by M Zah aria et al 24  that can i m pro ve the Hadoop res pons e ti m e by a factor of 2 in a cluster of 200 virtual machines on   This algorithm provides the solution to how to robustly perform speculative execution for maximizing performance. LATE scheduler does not focus on the problem resulting from the phenomenon of dynamic loading that is addressed by LA Load-Aware scheduler proposed by Hsin-Han You et al  raz Ahm ad et al   ns to improve MapReduce performance in heterogeneous clusters Tarazu consists of a set of three schemes which are Communication-Aware Load Balancing of Map computation CALB\across the nodes, Communication-Aware Scheduling of Map computation CAS to avoid network traffic and Predictive Load Balancing of Reduce computation PLB across the nodes A white paper on Hadoop performance tuning 27  explains the tuning of various configuration parameters of Hadoop cluster which directly affects the performance of a MapReduce job Some major parameters described in that  speculative execution, maximum map/reduce tasks, buffer size for sorting, temporary space and JVM tuning  IV  E XPERIMENTAL P LATFORM  A local Apache Hadoop-2.6.0 heterogeneous cluster consisting of five nodes is installed and configured. One node is fully devoted to NameNode \(NN\ and remaining four nodes serve as DataNodes DNs Cluster has both type of nodes physical and virtual as well as with different number of cores and RAM but all are running Ubuntu 14.04. Five nodes cluster has been installed using four physical machines VMware is used to create virtual machine environment. Table I shows the architecture of physical machines used in cluster while Table II  shows the configuration of the heterogeneous cluster with description of each node in cluster TABLE I  A RCHITECTURE OF M ACHINES U SED IN C LU S TER  Machine  CPU Type  Cores  RAM  Operating System  Machine A  Intel Xenon E5 2620 @ 2.10 GHz  2 \327 6 12  16 GB  Win dow 7  Machine B  Intel Xenon E5504 2.00 GHz  1 \327 4 = 4  2 GB  Ubuntu 14.04  Machine C  Intel Xenon E5504 2.00 GHz  1 \327 4 = 4  2 GB  Ubuntu 14.04  Machine D  Intel Xenon E5 2630 @ 2.30 GHz  2 \327 6 12  32 GB  Window Server  TABLE II  C ONFIGURATION OF H ETEROGENEOUS H ADOOP C LUSTER  Node  Node Type  Hosted on  Cores  RAM  NameNode \(NN  Virtual  Machine A  4  4 GB  DataNode1 \(DN1  Physical  Machine B  4  2 GB  DataNode2 \(DN2  Physical  Machine C  4  2 GB  DataNode3 \(DN3  Virtual  Machine D  4  4 GB  DataNode4 \(DN4  Virtual  Machine D  4  4 GB  Al l the algorithms are implemented using JAVA and MapReduce 2.0 APIs Hadoop-2.x version has introduced an improved and optimized framework MapReduce 2.0 \(MRv2 MRv2 also known as NextGen MapReduce or YARN Yet Another Resource Negotiator  2 wh i ch co ntr o ls job scheduling and manages cluster resources Experiments were carried out on two click-steam datasets BMS_WebView_1 and BMS_WebView_2 from a web store 29  Def a ult set ti ng of number of Mappers and Reducers are 12 and 4 respectively; it is explicitly stated whenever it is changed V  F ACTORS A FFECTING THE P ERFORMANCE  As we have mentioned in earlier section that the performance of a MapReduce job can be enhanced either by improving the algorithm running as job or by fine tuning of various parameters specific to cluster An algorithm showing good performance on homogeneous cluster drastically becomes poor on heterogeneous cluster  In this section we have discussed various factors that influence the performance of MapReduce based Apriori on homogeneous and he terogeneous Hadoop cluster We have also applied techniques against these factors to improve the performance   x-none A  Data Structures and Filtered Transactions These two techniques are incorporated in the Apriori algorithm. Hash tree and trie \(prefix tree  ar e the  cent ral data structures in Apriori algorithm F Bodon pro pos ed the hash table trie data structure by applying hashing techniques on trie. Hash table trie was promising theoretically but failed to perform experimentally Trie is the best performing data structure for the sequential implementation of Apriori significant influence of the data structures can be found in 30 I n our  earli er study  3 2 we hav e  investigated the influence of the three data structures on MapReduce based Apriori when executed on our local Hadoop cluster Experimental results showed that hash table trie drastically outperforms trie and hash tree with respect to execution time Here we will represent only a part of its experimental results and compare with the filtered transactions method Transaction filtering was first used by C Borgelt in  efficient sequential implementation of Apriori If t  is a transaction of database then a filtered transaction of t  is the itemset obtained by removing infrequent items from tr ansaction t  Filtered transactions are sufficient to determine all the frequent itemsets 34-35  Her e we on ly  m ention the  pseudo codes for Apriori with filtered transactions Algorithms from 1 to 7 depict the pseudo code of the driver 


 International Conference on Computing, Communication and Automation \(ICCCA2016   90   class that contains three jobs and pseudo codes of Mapper Reducer and Combiner classes of the three jobs. Algorithm 8 depicts the pseudo code of filtered transaction method Algorithm 1. DriverApriori  Find frequent 1 itemset L 1   Job1  submitted single time   O neItemsetMapper   ItemsetCombiner   ItemsetReducer   end Job1  Find filtered transactions   JobFT   submitted single time   FT ItemsetMapper   ItemsetCombiner   FT ItemsetReducer   end JobFT  Find frequent k items et L k   for \(k = 2; L k 1  k   Job2  submitted multiple times   K ItemsetMapper   ItemsetCombiner   ItemsetReducer   end Job  end for   Algorithm 2. OneItemsetMapper, k = 1  Input  a block b i  of dat abase  key: byte offset of the line  value: a transaction t i  for each t i  b i  do   for each item i  t i  do   write \(i, 1   end for  end for   Algorithm 3.  ItemsetCombiner  key: itemset  value: key's value list  for each key k do   fo r each value v of k's value list   sum += v   end for   write\(k, sum  end for   Algorithm 4.   ItemsetReducer  key: itemset  value: key's value list  for each key k do   for each value v of k's value list   sum += v   end for   if sum >= min_supp_count   write\(k, sum   end if  end for   Algorithm 5. FT ItemsetMapper  Input a block b i  of database and L k 1   key: byte offset of the line  value: a transaction t i  read frequent items from cache file in L 1  L 1  may be a trie or hash table trie  for each t i   block b i  do   Ft = filterTransaction\(L 1  t i  Ft is the filtered transaction   write \(Ft , 1  end for   Algorithm 6.   FT ItemsetReducer  key: itemset  value: key's value list  for each key k do   for each value v of k's value list   sum += v   end for   write\(k, sum  end for   Algorithm 7. K   Input  a block b i  of set of filtered transactions and L k 1   key: byte offset of the line  value: a filtere d transaction t i  read frequent \(k 1 itemsets from cache file in L k 1  L k 1  may be a trie or hash table trie  C k  apriori gen\(L k 1  C k  may be a trie or hash table trie  for each t i   block b i  do   occurrences = removeAtEnd\(t i    C t  subset\(C k  t i  C t  may be a List   for each candidate c  C t  do   write \(c, occurrences   end for  end for   Algorithm 8. Filtered Transaction  filterTransaction\(L 1 t i   parameters: trie L 1  with frequent items and a transaction t i  retur n value: filteredItems   filteredItems empty string   for each item i of transaction t i   for each child node c of root of  L 1   if\(i < c    break    else if\(i > c    continue    el se append item to filteredItems    filteredItems = filteredItems + " " + i    break   end for   end for   return filteredItems  end filterTransaction   We have introduced a job for transaction filtering named as JobFT in between the Job1 and Job2 Job1 generates frequent 1-itemsets L 1 and Job2 generates frequent k-itemsets L k in k th   ItemsetMapper of JobFT reads L 1  from distributed cache and input file from HDFS. For each transaction t i it checks against L 1  to filter infrequent items from t i  and produces F t  1  as key-value pairs where F t  is the filtered transaction FT-ItemsetReducer sums up the values associated with same filtered transaction and produces filtered transaction with its occurrence frequency JobFT produces transactions with its occurring frequency in HDFS KItemsetMapper of Job2 reads frequent itemsets L k-1  from distributed cache and filtered transactions from HDFS The method removeAtEnd  modifies the transaction by removing the occurrence frequency as well as returns this occurrence frequency Candidates with this occurrence frequency are produced as key-value pairs We have examined the algorithms for both data structures i.e. trie and hash table trie ItemsetCombiner is same for all the three jobs since it makes the local sum on one node. ItemsetReducer makes the sum of 


 International Conference on Computing, Communication and Automation \(ICCCA2016   91      local counts of the candidates received from all the nodes checks count against minimum support threshold and produces candidates and its count as key-value pairs We have executed the MapReduce based Apriori using data structure trie and hash table trie \(HTtrie\without filleted transactions and then with filtered transactions. Figures 1 and 2 show the execution times corresponding to trie hash table trie \(HTtrie\ trie on filtered transactions \(TrieOnFT\ and hash table trie on filtered transactions HTtrieOnFT for datasets BMS_WebView_1 and BMS_WebView_2 respectively Fig. 1  Execution time of four variations of Apriori on BMS_WebView_1 From both the Fig 1 and Fig  2 we can see that the two techniques hash table trie and filtered transactions drastically reduce the execution time when applied independently and adds more improvement when applied jointly Fig. 2  Execution time of four variations of Apriori on BMS_WebView_2 x-none B  Speculative Execution Speculative execution is a strategy of Hadoop that provides fault tolerance and reduces job execution time 1 When a TaskTracker a daemon process running on slave nodes that executes map and reduce tasks\performs poorly or crashes the JobTracker a demon process running on master node that accepts job and submit tasks to TaskTrackers launches another backup task on another nodes to accomplish the task faster and successfully This process of redundant execution is called speculative execution and the backup copy of task is called speculative task 25  Am on g the or i gin al  and speculative tasks which one completes first is kept while other  good at most of the time but it affects cluster efficiency by duplicating tasks So it should be disabled it is enabled by default when a cluster has limited resources Speculative execution is best suited in homogeneous environments but degrades the job performance in heterogeneous environments  One can dis able sp ec ul at ive execu ti on for M ap per s  and Reducers by setting the value of "mapreduce.map.speculative and "mapreduce.reduce.speculative" to false either in mapredsite.xml file of cluster configuration or in job configuration of MapReduce code   We have executed the MapReduce based Apriori with trie data structure when speculation is on and off. Fig   3 shows the observed execution time on dataset BMS_WebView_1 for v arying value of minimum support   Fig. 3  Execution time of trie based Apriori when speculative execution is enabled and disabled In Fig 3 it can be seen that speculative execution comes in action only for a job taking longer time to complete Speculative task is not launched for short jobs. It is launched when all the tasks of a job is assigned resources and then for a task running for a longer time or failed to complete   x-none C  Detection and Elimination of Slower Nodes Nodes in a heterogeneous cluster are of different capability Heterogeneity may arise due to differences i n hardware as well as using virtualization technology Virtualization facilitates efficient utilization of resources and environments for different operating systems. There are many benefits of VM-hosted \(virtual machine hosted\Hadoop such as lower cost of installation, on demand cluster setup, reuse of remaining physical infrastructure on demand expansion and contraction of cluster size   I n our lo ca l cl us te r Ta ble 1 and 2 NN NameNode DN3 DataNode3 and DN4 DataNode4 are virtual machines while DN1 DataNode1 and DN2 DataNode2 are physical machines However virtualization provides efficient re-use and management of resources but on the cost of performance. Virtual machines are slower in comparison to physical machines We have observed that eliminating slower node from cluster improves the performance. Detecting a slower node is not obvious always We have used the idea of measuring heterogeneity in terms of data processing speed proposed in    wh e re sa m e Ma pRedu c e job with  sam e am ou nt of da ta  is  separately executed on each node and running time of each node is recorded. We have not executed the job separately on each node but on whole cluster We have executed the job with 12 Mappers corresponding to frequent 2-itemsets generation for higher and lower value of minimum support on 


 International Conference on Computing, Communication and Automation \(ICCCA2016   92      BMS_WebView_1 dataset Fig 4 and Fig  5 show the sn apshot of execution time of 12 Mappers on 4 DNs of the cluster for two values of minimum support Fig. 4  Sn apshot of execution time of 12 Mappers on 4 DNs for higher value of minimum support Fig. 5  Sn apshot of execution time of 12 Mappers on 4 DNs for lower value of minimum support Fig. 6  Execution time of trie based Apriori on two different clusters In Fig 4 and 5 the DN on which the particular task is executed can be found by exploring  4 tasks showing elapsed time 28 29 and 30 sec are on VMs DataNodes DN3 and DN4 whereas tasks showing elapsed time 19 and 20 sec are on physical DataNodes DN1 and DN2 Similar difference can be seen in Fig 5 where task s showing around 15 minutes are on VMs DataNodes and around 8 minutes are on Physical DataNodes. Removing both DN3 and DN4 greatly slowdown the performance but removing only one of them significantly enhances the performance. Fig. 6 shows the relative influence A natural question may arise here that is it a better idea to remove a slower DN? Here we would like to mention the two  perspective As a Hadoop developer one can design and incorporate an efficient data distribution scheme that assigns data blocks to DNs as per ratio of their processing speed Similarly an efficient load balancing scheme can be incorporated to migrate tasks from busy or slower DNs to idle  perspective where we only focus of MapReduce based applications not on underlying  eliminate slower DN if that reduces the execution time x-none D  Data Locality and Data Block Distribution HDFS breaks a large file into smaller data blocks of 64 MB and stores as 3 replicated copies on DNs of cluster. Data blocks are the physical division of data Data file can be logically divided using input split. The number of Mappers to be run for a job is equal to the number of logical splits. If input split is not defined then the default block size is the split size In all the earlier cases discussed above, we have specified the split size which resulted into 12 Mappers and input file was not divided physically. In this case we have not used split size instead divide the input file into smaller blocks of 200 KB. So for the dataset BMS_WebView_1 there are 5 data blocks block0 block1 block2 block3 block4 This requires 5 Mappers to being executed corresponding to these 5 data blocks When an input file is put into HDFS it is automatically splitted into blocks and distributed to different  control Each time when putting the input file into HDFS  results a different distribution We set the replication factor RF\ to 1 so that a block resides on only one DN. We put the same input file into HDFS twice and get two different block distributions \(BD\ BD1 and BD2. Now we set the replication factor to 4 so that all blocks would be available on each DN Table III describes the three block distributions BD1, BD2 and BD3 The influence of three block distributions on the execution time of trie based Apriori for different value of minimum support is shown in Fig 7  TABLE III  T ABLE S TYLES  BD  RF  Blocks on DN1  Blocks on DN2  Blocks on DN3  Blocks on DN4  BD1  1  block1 block2  block0 block3  No Block  block4  BD2  1  block0 block3  block2  block4  block1  BD3  4  All 5 blocks  All 5 blocks  All 5 blocks  All 5 blocks   


 International Conference on Computing, Communication and Automation \(ICCCA2016   93     Fig. 7  Execution time of trie based Apriori on three different block distributions x-none In Fig   7, BD1 exhibits minimum execution time compared to BD2 and BD3 In BD1 blocks are located only on three DNs i.e two physical a nd one VMs DN So in this case Mappers are not running on both slower DNs tha t make the execution faster. Execution time for BD2 is poor than that o f BD1 due to using both VMs DNs  BD3 exhibits the worst performance since all the blocks are available on each DN MapReduce processes a data block locally on the DN where the block is present. F or block distribution BD3, all Mappers are running on the same DN since all blocks are locally available to each node In different attempt of running a job DN may be d ifferent each time but all the Mappers are being run on a same DN. All Mapper s running on the same DN does not make use of available resources which leads to increased execution time Here it can be seen that due to higher replication factor data locality may be a hurdle that slow down the execution  x-none E  Controlling Parallelism with Split Size x-none Hadoop is designed to process big datasets that does not mean one cannot be benefited for small datasets. Apriori is a CPU-intensive algorithm and consumes a significant time for smaller datasets. To reduce the execution time we need more than on e task running in parallel Split is used to control the number of map tasks for a MapReduce job A split may consist of multiple blocks and there may be multiple splits for a single block So without changing the block size user can control the number of Mappers to be run for a particular job We have used the method setNumLinesPerSplit\(Job job int numLines  of class NLineInputFormat  from MapReduce library to set the number of lines per split. In our earlier cases we were running multiple Mappers against different parts of the same block Here we set the split size 5K lines on block distribution BD3 which contains 5 data blocks This creates 12 splits i.e 12 Mappers  size for BD3 the n  blocks are considered as input s plits. Fig   8 shows the difference in execution time for these two cases  Here it can be seen that how the split size controls the parallelism Smaller split size launches more number of Mappers which consequently increase the parallelism. It does not mean that more number of Mappers always results into better performance. Increasing the number of Mappers beyond a particular point starts to degrade the performance due to unnecessary overheads and shortage of resources   To achieve the right level of parallelism it must be taken care that the map task is CPU-intensive or CPU-light as well as the size of dataset to be processed Fig. 8  Execution time of trie based Apriori with Input Split and without Input Split on BD3 x-none F  Issues Regarding MapReduce Implementation The efficiency of an algorithm running as a MapReduce job is extensively influenced by data structure used and algorithm itself A third factor that cannot be ignored is the implementation technique.  Implementation technique may be regarding to implementation of various modules of Apriori e.g candidate generation support counting of candidates against each transaction pruning of infrequent itemsets or regarding to MapReduce implementation of Apriori MapReduce implementation of Apriori is central to discussion here. A major issue in MapReduce based Apriori is to invoke candidate generation i.e apriori-gen  Algorithm 7 at appropriate place inside Mapper class. In our implementations we have invoked apriori-gen  inside customized method map of Mapper class. In Mapper class, two methods setup  and map  are customized and one method apriori-gen  is defined Method setup  is called once at the beginning of a task It is customized to read frequent itemsets of previous iteration from distributed cache and to initialize prefix tree Method apriori-gen  generates candidates using prefix tree containing frequent itemsets The map  is invoked for each line of input split of dataset If there are 100 lines of input assigned to a Mapper then map method will be invoked for 100 times Subsequently it invokes apriori-gen  repeatedly each time Since apriori-gen  method produces candidates which is independent of input instance, so need not to invoke repeatedly inside map  method The apriori-gen  metho d is  computation intensive and increases the execution time when invoked repeatedly. This repeated computation can be fixed if we invoke apriori-gen  outside of map  Theoretically it sounds good but did not work when invoked inside setup  method We have also tried another way in which apriorigen  is invoked inside overrided method run  of Mapper class but again could not achieve expected reduction in execution time VI  C ONCLUSIONS  In this paper we have investigated a number of factors affecting the performance of MapReduce based Apriori algorithm on homogeneous and heterogeneous Hadoop 


 International Conference on Computing, Communication and Automation \(ICCCA2016   94   cluster and presented strategies to improve the performance It has been shown that how hash table trie data structure and transaction filtering technique can significantly enhance the performance Factors like speculative execution physical  VMs DataNodes, data locality block distribution and split size are such that their proper tuning can directly enhance the performance of a MapReduce job even without making algorithmic optimization Approaches of MapReduce implementation of Apriori is another important factor that also influence the performance R EFERENCES  1  J. Han and M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann Publishers 2006  2  J S Ward a  b y d ata a survey of big d ata d  http://arxiv.org/abs/1309.5821v1 3  Apache Hadoop, http://hadoop.apache.org 4  Big data is useless without algorithms Gartner says http://www.zdnet.com/article/big-datais useless-withoutal gorithmsgartner-says/, Retrieved Nov. 2015 5   algorithms for mining association rules  Proceedings Twentieth International Conference on Very Large Databases, Santiago 1994 pp. 487 499 6   and distributed association mining a survey Concurrency, IEEE, vol 7, no. 4,pp. 14 25, 1999 7  K. Bhaduri, K. Das, K. Liu, H. Kargupta and J. Ryan, Distributed Data Mining Bibliography 2008  8  HDFS  Architecture Guide https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html Retrieved Sept 2015  9  MapReduce Tutorial http://hadoop.apache.org/docs/current/hadoopmapreduce-client/hadoop-mapreduce-clientcore/MapReduceTutorial.html, Retrieved Sept. 2015 10  Yahoo Hadoop Tutorial http://developer.yahoo.com/hadoop/tutorial/index.html 11   ACM SIGOPS Operating Systems Review vol 37 no 5 pp 29 43 2003  12    Commun., vol. 51, pp 107 113, 2008 13   mining using clouds an experimental implementation of apriori over mapreduce    14   Apriori: association rules algorithm based on mapreduce  IEEE 2014  15   as a programming model for association rules algorithm on hadoop  nternational Conference on Information Sciences and Interaction Sciences ICIS 2010 vol. 99, no. 102, pp. 23 25  16   implementation of apriori algorithm based on mapreduce  h ACIS International Conference on  Software Engineering Artificial Intelligence Networking and Parallel  Distributed Computing IEEE 2012 pp 236 241  17   Hadoop as a platform for distributed association rule mining   COMPUTING 2013 the Fifth International Conference on Future Computational Technologies and Applications, pp. 62 67  18  M-Y Lin P-Y Lee and S based frequent itemset mining algorithms on mapreduce in  Proceedings 6th International Conference on  Ubiquitous Information Management and  2012, Article 76 19   itemset mining on Hadoop  Proceedings IEEE 9th International Conference on Computational Cybernetics \(ICCC\Hungry, 2013, pp. 241 245  20   strategy of mining association rule based on cloud computing   Proceedings IEEE International Conference on Business Computing and Global Informatization BCGIN 2011 pp 29 31 21  Honglie Yu, Jun Wen, Hongmei Wang and Li Jun, "An improved apriori algorithm based on the boolean matrix and Hadoop Procedia Engineering 15 \(2011\1827-1831, Elsevier 22  Matteo Riondato, Justin A. DeBrabant, Rodrigo Fonseca and Eli Upfal PARMA: a parallel randomized algorithm for approximate association rules mining in mapreduce in Proceedings 21st ACM international conference on information and knowledge management 2012 pp 8594  23  Jiong Xie et al Improving mapreduce performance through data placement in heterogeneous hadoop clusters in IEEE International Symposium on Parallel & Distributed Processing,  Workshops and Phd Forum \(IPDPSW\ 2010, pp. 19  24  Matei Zaharia Andy Konwinski Anthony D Joseph Randy Katz and Ion Stoica Improving mapreduce performance in heterogeneous environments in 8th USENIX Symposium on Operating Systems Design and Implementation \(OSDI\ 2008, vol. 8, no. 4, pp. 2942  25  Hsin-Han You, Chun-Chung Yang and Jiun-Long Huang, "A load-aware scheduler for MapReduce framework in heterogeneous cloud environments in Proceedings of the ACM Symposium on Applied Computing, 2011, pp. 127-132 26  Faraz Ahmad Srimat Chakradhar Anand Raghunathan and T N Vijaykumar, "Tarazu optimizing MapReduce on heterogeneous clusters," ACM SIGARCH Computer Architecture News, vol. 40, no. 1 pp. 61-74, 2012 27  HADOOP PERFORMANCE TUNING white paper Impetus Technologies Inc October 2009 https://hadooptoolkit.googlecode.com/files/White%20paperHadoopPerformanceTuning.pdf 28  Apache Hadoop NextGen MapReduce YARN http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarnsite/YARN.html, Retrieved Sept. 2015 29  SPMF Datasets http://www.philippe-fournierviger.com/spmf/index.php?link=datasets.php 30   ata structure for data  in Mathematical and Computer Modelling, vol 38  no. 7, pp. 739-751, 2003 31  Ferenc Bodon A fast apriori implementation in Proceedings IEEE ICDM workshop on frequent itemset mining implementations  90, 2010 32  Sudhakar Singh, Rakhi Garg and P. K. Mishra  analysis of apriori algorithm with different data structures on hadoop cluster  International Journal of Computer Applications, vol. 128, no. 9, pp. 4551  2015  33  Christian Borgelt Efficient implementations of apriori and 351clat in Proceedings IEEE ICDM workshop on frequent itemset mining   34  Ferenc Bodon, "Surprising results of Trie-based fim algorithms," FIMI 2004  35  Ferenc Bodon A trie-based APRIORI implementation for mining frequent item sequences," in Proceedings 1st international workshop on open source data mining: frequent pattern mining  implementations ACM, 2005 36  Hadoop Wiki Virtual Hadoop https://wiki.apache.org/hadoop/Virtual%20Hadoop  


002 004\002 005\002 006\002 007\002 003\002 033\002 034\002 035\002 002\004\002\005\002 006\002 
002 005 007 033 035 004\002 004\005 004\007 004\005\006\007\003\033 
C Increased Rule Length 
002 005 007 033 035 004\002 004\005 004\007 004\033 004\035 005\002 020\020\030\027\011\025\021\013 026\032\025\025\011\020\021\030\032 025 011\021\012\030\017 023\004\002\002 023\004\002\004 
 
002\003\004\005\006\007\010\011\012\013\014\010\015\004\013'\013\022\015\(\023\011\\004\015\004\012\007\013 024\007\024!\004\007\013 002\003\004\005\006\007\010\011\012\013\014\010\015\004\013'\013\022\015\(\023\011\\004\015\004\012\007\013 024\007\024!\004\007\013 
Figure 6 Throughput for increasing length rule candidates The Y-axis is measured in billion evaluations per second The X-axis shows the corresponding rule length As before the gures include the theoretical trend-line computed from amplifying naive-sc on the GPU using Eq 4 reason the observed improvement is stable depending mostly on the item number Assigning multiple candidate rule collections to a single block resulted in Figure 7 Percentage improvement in execution time as compared to the default-tpsc kernel for the individually optimized kernels when using synthetic data   Figure 8 Percentage improvement in execution time as compared to the default-tpsc kernel for the individually optimized kernels when using real data For both kernels we observe a similar behaviour when we increase rule candidate length to a number larger than 32 After that point we require evaluating the preìx in two phases following a technique similar to parallel reduction although using warp vote functions This extra phase requires an additional synchronization step which increases the total execution time per iteration Additionally when we have prominent rules with item indices in sequence i.e accidents dataset as indicated by its sparsity pattern caching transactions does not provide any improvement However when there are many rules with out of sequence preìxes the cost of uncoalesced memory accesses matches the synchronization cost as indicated by experiments on dataset 1 VIII C ONCLUSION In this paper we studied the support count operation commonly used in association rule mining problems We proposed a work-efìcient parallel algorithm that is suitable for massively parallel architectures Furthermore we presented a data layout scheme used to enable low overhead coordination of the processing elements reduce the memory requirements and achieve high off-chip memory bandwidth utilization Furthermore we discussed in detail low level optimization strategies related to effective use of shared memory while presenting a simple strategy for resolving shared memory bank conîicts incurring minimal additional work However there is still some additional issues that we need to address Firstly we already considering resolving the issue of 
025 015\013\036\021\031\013\020 015\036\021\031\013\020 
improvement over the default-tpsc execution time A combination of loop unrolling and increase shared memory utilization from storing more candidate rules in the same block was the reason for the observed improvement In contrast enabling caching of transactions in shared memory with kernel mrs-tpsc presented less improvement in the relative execution time compared to mr-tpsc The culprit is this case is the additional synchronization step which is required after loading the data in shared memory Finally experiments performed on dataset 2 and 4 indicate similar behavior to our previous experiments where multiprocessor underutilization was limiting the maximum possible performance increase Even in the case where we increase the workload of participating blocks interleaved execution of warps is limited since as the block size is small In this section we discuss the effects of discovering rules with length larger than 32 Due to lack of space we present the results from the execution on synthetic data 1 and accidents which are good representatives of the observed behaviour We focus on the mrs-tpsc and mr-tpsc variations which we established to be highly optimized throughout our experiments   
025 015\013\036\021\031\013\020 015\036\021\031\013\020 
037\005\005\010 \004\012\007!\013   002 003 004\002 004\003 005\002 005\003 006\002 006\003 007\002 002\004\002\005\002 006\002 011\012\012\004\005\007\010\011\012\013   002 003 004\002 004\003 005\002 005\003 006\002 006\003 007\002 002\004\002\005\002\006\002 004\007\024\010$\013  014%\031&&\013 
18 
027\011$\012\014\017\021\036\021\031\013\020  025\012%&\011\036\013\020\022\036\022'\031\014  025\012%&\011\036\013\020\022\036\022\020\031\014 021\(\011\032\015\011\021\030\020    002 004 005 006 007 002\004\002\005\002 006\002 
1431 
1431 


 volume 22 pages 207Ö216 ACM 1993  R Agra w al R Srikant et al F ast algorithms for mining association rules In  volume 1215 pages 487Ö499 1994  E Ansari G Dastghaibif ard M K eshtkaran and H Kaabi Distrib uted frequent itemset mining using trie data structure  35\(3 2008  M Atzmueller and F  Puppe Sd-mapÖa f ast algorithm for e xhausti v e subgroup discovery In  pages 6Ö17 Springer 2006  C Creighton and S Hanash Mining gene e xpression databases for association rules  19\(1 2003  W  F ang M Lu X Xiao B He and Q Luo Frequent itemset mining on graphics processors In  pages 34Ö42 ACM 2009  K Geurts G W ets T  Brijs and K V anhoof Proìling of high-frequenc y accident locations by use of association rules  1840 2003  A Ghoting G Buehrer  S P arthasarathy  D Kim A Nguyen Y K Chen and P Dubey Cache-conscious frequent pattern mining on modern and emerging processors  16\(1 2007  G Grahne and J Zhu Ef ciently using preìx-trees in mining frequent itemsets In  volume 90 2003  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation In  volume 29 pages 1Ö12 ACM 2000  J Hipp U G  untzer and G Nakhaeizadeh Algorithms for association rule miningÑa general survey and comparison  2\(1 2000  R Jin and G Agra w al An algorithm for in-core frequent itemset mining on streaming data In  pages 8Öpp IEEE 2005  R Jin and G Agra w al Systematic approach for optimizing comple x mining tasks on multiple databases In  pages 17Ö17 IEEE 2006  E Lindholm J Nick olls S Oberman and J Montrym Nvidia tesla A uniìed graphics and computing architecture  2 2008  J Liu Y  P an K W ang and J Han Mining frequent item sets by opportunistic projection In  pages 229Ö238 ACM 2002  L Liu E Li Y  Zhang and Z T ang Optimization of frequent itemset mining on multiple-core processor In  pages 1275Ö1285 VLDB Endowment 2007  B Mobasher  R Coole y  and J Sri v asta v a Automatic personalization based on web usage mining  43\(8 151 2000  E  Ozkural B Ucar and C Aykanat Parallel frequent item set mining with selective item replication  22\(10 2011  J Pei J Han H Lu S Nishio S T ang and D Y ang H-mine Hyper structure mining of frequent patterns in large databases In  pages 441Ö448 IEEE 2001  I Pramudiono and M Kitsure ga w a P arallel fp-gro wth on pc cluster  In  pages 467Ö473 Springer 2003  C Silv estri and S Orlando gpudci Exploiting gpus in frequent itemset mining In  pages 416Ö425 IEEE 2012  A T ajbakhsh M Rahmati and A Mirzaei Intrusion detection using fuzzy association rules  9\(2 2009  T  T assa Secure mining of association rules in horizontally distrib uted databases  26\(4 2014  K W ang M Stan and K Skadron Association rule mining with the micron automata processor In  2015  M J Zaki Scalable algorithms for association mining  12\(3 2000  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors In  pages 43Ö43 IEEE 1996  F  Zhang Y  Zhang and J D Bak os Accelerating frequent itemset mining on graphics processing units  66\(1 2013  Y  Zhang F  Zhang Z Jin and J D Bak os An fpga-based accelerator for frequent itemset mining  6\(1 2013 
002 004 005 006 007 002 004\005\035 005\003\033 006\035\007 003\004\005 002 033 004\005 004\035 005\007 002 004\005\035 005\003\033 006\035\007 003\004\005 
ACM SIGMOD Record Proc 20th int conf very large data bases VLDB IAENG International Journal of Computer Science Knowledge Discovery in Databases PKDD 2006 Bioinformatics Proceedings of the fth international workshop on data management on new hardware Transportation Research Record Journal of the Transportation Research Board The VLDB Journal FIMI ACM SIGMOD Record ACM sigkdd explorations newsletter Data Mining Fifth IEEE International Conference on Data Engineering 2006 ICDEê06 Proceedings of the 22nd International Conference on IEEE micro Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining Proceedings of the 33rd international conference on Very large data bases Communications of the ACM Parallel and Distributed Systems IEEE Transactions on Data Mining 2001 ICDM 2001 Proceedings IEEE International Conference on Advances in Knowledge Discovery and Data Mining Parallel Distributed and Network-Based Processing PDP 2012 20th Euromicro International Conference on Applied Soft Computing Knowledge and Data Engineering IEEE Transactions on Proceedings of the 2015 IEEE 29th International Parallel and Distributed Processing Symposium Knowledge and Data Engineering IEEE Transactions on Supercomputing 1996 Proceedings of the 1996 ACM/IEEE Conference on The Journal of Supercomputing ACM Transactions on Reconìgurable Technology and Systems TRETS 
Figure 9 Execution time measured for increasing rule size  the Xaxis indicates the rule length multiprocessor under-utilization For dataset with low number of items we can assign individual groups of threads in the same block to different rule collections effectively increasing the block size as well as utilization Secondly we would like to adapt our solution to an architecture consisting of multiple GPUs and address challenges related to partial result sharing A CKNOWLEDGMENT This work was supported by the U.S National Science Foundation under grant ACI-1339756 R EFERENCES  Frequent itemset mining dataset repository  2015 URL http://ìmi.ua.ac.be/data  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases In 
014\010\015\004\013\016!\004\005!\021\013 026\027\012\007\030\004\007\010\005\013\031\013 014\010\015\004\016!\004\005!\021\013 037\005\005\010 \004\012\007!\013 
015\036\021\031\013\020 015\013\036\021\031\013\020 
1432 
1432 


 Frequent pattern mining Current status and future directions  2007  R Agra w al and R Srikant F ast algorithms for mining association rules in  1994  M J Zaki Scalable algorithms for association mining  vol 12 no 3 pp 372Ö390 2000  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation in  2000  Y  Zhang F  Zhang and J Bak os Frequent itemset mining on large-scale shared memory machines in  2011  F  Zhang Y  Zhang and J D Bak os  Accelerating frequent itemset mining on graphics processing units  vol 66 no 1 pp 94Ö117 2013  Y  Zhang  An fpga-based accelerator for frequent itemset mining  vol 6 no 1 pp 2:1Ö2:17 May 2013  P  Dlugosch  An efìcient and scalable semiconductor architecture for parallel automata processing  vol 25 no 12 2014  I Ro y and S Aluru Finding motifs in biological sequences using the micron automata processor in  2014  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases in  1993  H No yes Micronês automata processor architecture Reconìgurable and massively parallel automata processing in  June 2014 keynote presentation  M J Zaki  Parallel data mining for association rules on shared-memory multi-processors in  1996  L Liu  Optimization of frequent itemset mining on multiple-core processor in  2007  I Pramudiono and M Kitsure ga w a P arallel fp-gro wth on pc cluster in  2003  E Ansari  Distributed frequent itemset mining using trie data structure  vol 35 no 3 p 377 2008  W  F ang  Frequent itemset mining on graphics processors in  2009  B Goethals Surv e y on frequent pattern mining  Univ of Helsinki Tech Rep 2003  C Bor gelt Ef cient implementations of apriori and eclat in  2003 p 90  Frequent itemset mining dataset repository   http mi.ua.ac.be/data  J Rabae y  A Chandrakasan and B Nik oli  c  2nd ed Pearson Education 2003 
100 1000 10000 5 50 500 5000 re_sup = 0.12 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails for re_sup = 0.08 re_sup = 0.08 
GPU fails re_sup = 0.42 re_sup = 0.42 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm  Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails re_sup = 0.3 re_sup = 0.3 b T100 Figure 11 Performance prediction with technology normalization as a function of input size implementation In contrast the capability of our AP ARM solution scales nicely with the data size since the AP was designed for processing streaming data With the challenge of the big data era a number of other complex pattern mining tasks such as frequent sequential pattern mining and frequent episode mining have attracted great interests in both academia and industry We plan to extend the proposed CPU-AP infrastructure and automaton designs to address more complex pattern-mining problems A CKNOWLEDGMENT This work was supported in part by the Virginia CIT CRCF program under grant no MF14S-021-IT by C-FAR one of the six SRC STARnet Centers sponsored by MARCO and DARPA NSF grant EF-1124931 and a grant from Micron Technology R EFERENCES  J Han 
et al Data Min Knowl Discov Proc of VLDB 94 IEEE Trans on Knowl and Data Eng Proc of SIGMOD 00 Proc of CLUSTER 11 J Supercomput et al ACM Trans Reconìgurable Technol Syst et al IEEE TPDS Proc of IPDPSê14 Proc of SIGMOD 93 Proc of Fifth International Symposium on Highly-Efìcient Accelerators and Reconìgurable Technologies et al Proc of Supercomputing 96 et al Proc of VLDB 07 Proc of PAKDD 03 et al IAENG Intl J Comp Sci et al Proc of DaMoN 09 Proc FIMI 03 Digital Integrated Circuits 
1 10 100 1000 10000 0.1 1 10 100 1000 
a Webdocs 
699 


