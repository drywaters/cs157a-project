TSINGHUA SCIENCE AND TECHNOLOGY ISSN ll 1007-0214 ll 03/09 ll pp149\226159 Volume 22 Number 2 April 2017 Load Feedback-Based Resource Scheduling and Dynamic Migration-Based Data Locality for Virtual Hadoop Clusters in OpenStack-Based Clouds Dan Tao 003  Zhaowen Lin and Bingxu Wang Abstract With cloud computing technology becoming more mature it is essential to combine the big data processing tool Hadoop with the Infrastructure as a Service 050IaaS\051 cloud platform In this study we 002rst propose a new Dynamic Hadoop Cluster on IaaS 050DHCI\051 architecture which includes four key modules monitoring scheduling Virtual Machine 050VM\051 management and VM migration modules The load of both physical hosts and VMs is collected by the monitoring module and can be used to design resource scheduling and data locality solutions Second we present a simple load feedback-based resource scheduling scheme The resource allocation can be avoided on overburdened physical hosts or the strong scalability of virtual cluster can be achieved by 003uctuating the number of VMs To improve the 003exibility we adopt the separated deployment of the computation and storage VMs in the DHCI architecture which negatively impacts the data locality Third we reuse the method of VM migration and propose a dynamic migration-based data locality scheme using parallel computing entropy We migrate the computation nodes to different host\050s\051 or rack\050s\051 where the corresponding storage nodes are deployed to satisfy the requirement of data locality We evaluate our solutions in a realistic scenario based on OpenStack Substantial experimental results demonstrate the effectiveness of our solutions that contribute to balance the workload and performance improvement even under heavy-loaded cloud system conditions Key words Hadoop resource scheduling data locality Infrastructure as a Service 050Iaas\051 OpenStack 1 Introduction Cloud computing is one of the hottest areas of  017 Dan Tao and Bingxu Wang are with School of Electronic and Information Engineering Beijing Jiaotong University Beijing 100044 and Jiangsu High Technology Research Key Laboratory for Wireless Sensor Networks Nanjing 210003 China E-mail dtao@bjtu.edu.cn 017 Zhaowen Lin is with Network and Information Center Institute of Network Technology Science and Technology on Information Transmission and Dissemination in Communication Networks Laboratory National Engineering Laboratory for Mobile Network Security Beijing University of Posts and Telecommunications Beijing 100876 China E-mail linzw@bupt.edu.cn 003 To whom correspondence should be addressed Manuscript received 2016-10-01 revised 2016-12-26 accepted 2016-12-27 research at home and abroad which integrates largescale computing storage and network resource via a network and provides these resources for different users on demand   As an open-source framework for distributed system architecture Hadoop can achieve large-scale data computing and storage and is usually deployed on physical cluster There are some drawbacks in traditional Hadoop clusters First its deployment and con\002guration are tedious tasks When Hadoop starts running the realtime monitoring on Hadoop consumes plenty of manpower and 002nancial resources Second the 003uctuation of tasks causes imbalance of resource utilization With the appearance of peaks in the tasks resource bottlenecks may be encountered In contrast the troughs in the tasks will bring idle resource Hadoop cannot realize dynamic resource allocation Third the utilization of high-performance computers in physical 


150 Tsinghua Science and Technology April 2017 22\0502\051 149\226159 clusters is insuf\002cient particularly for computation and storage resource resulting in severe resource wastage To solve the abovementioned problems it is essential to deploy a Hadoop cluster on an OpenStack-based cloud as its service   This study adopts OpenStack which can provide an Infrastructure as a Service 050IaaS\051 solution in the form of Virtual Machines 050VMs\051 Sahara an open-source project is developed to rapidly deploy a Hadoop cluster in an OpenStackbased cloud environment A virtual cluster which can simplify cluster management enables autonomic management of the underlying hardware facilitating cost-effective workload consolidation and dynamic resource allocations for better throughput and energy ef\002ciency However virtualization in such cloud platforms is known to cause performance overheads   Understanding how to optimize the performance of a Hadoop cluster has attracted considerable attention Researchers have accumulated a series of research achievements on resource scheduling and data locality in the related context Scheduling techniques for dynamic resource adjustment have been recently addressed Sandholm and Lai  presented a dynamic priority parallel task scheduler for Hadoop It allowed users to control their allocated capacity by adjusting their spending time Sharma et al  proposed a MapReduce resource Orchestrator 050MROrchestrator\051 framework which dynamically identi\002ed resource bottlenecks and resolved them through 002ne-grained coordinated and on-demand resource allocations However the abovementioned studies focused on a resource scheduling-based traditional Hadoop cluster Lama and Zhou  studied automated resource allocation and con\002guration of the MapReduce environment in the cloud without considering the load of physical hosts Zuo et al  proposed a resource evaluation model based on entropy optimization and dynamic weighting The entropy optimization 002ltered the resources that satis\002ed user QoS and system maximization by goal function constraints of maximum entropy and the entropy increase principle which achieved optimal scheduling and satis\002ed user QoS Liu et al  presented an adaptive method aiming at spatio-temporal ef\002ciency in a heterogeneous cloud environment A prediction model based on an optimized kernel-based extreme learning machine algorithm was proposed for a quick forecast of job execution duration and space occupation which consequently facilitates the process of task scheduling For data locality to address the con\003ict between locality and fairness Zaharia et al  proposed a simple delay scheduling algorithm wherein a job waited for a limited amount of time for a scheduling opportunity on a node that has data on it Experimental results showed that waiting can achieve both high fairness and high data locality Jin et al  proposed an availabilityaware data placement strategy and its basic idea was to dispatch data based on the availability of each node for reducing network traf\002c and improve data locality Both works were studied on a traditional Hadoop cluster Thaha et al  presented a data locationaware virtual cluster provisioning strategy to identify the data location and provision the cluster near the storage However multiple tasks might be executed on a same physical host which negatively impacted system performance Motivated by this we propose load feedback-based resource scheduling and dynamic migration-based data locality solutions based on a novel Dynamic Hadoop Cluster on IaaS 050DHCI\051 architecture The resource utilization can be improved by the load balance of physical hosts and the 003exible scalability of VMs Moreover based on the separated deployment of the computation and storage VMs computation VMs can be quickly migrated to match their corresponding storage VMs in order to effectively guarantee data locality The remainder of this study is organized as follows In Section 2 we introduce a DHCI architecture Based on this architecture load feedback-based resource scheduling and dynamic migration-based data locality solutions are explored in Section 3 In Section 4 we perform a comprehensive evaluation to validate our solutions Finally we conclude this study in Section 5 2 DHCI Architecture There exists a huge difference on Hadoop's running environment between a physical cluster and an IaaS cloud platform   In the IaaS cloud environment Hadoop is deployed on VMs provided by the cloud platform In this case the Hadoop cluster cannot suf\002ciently understand the resource usage of the underlying physical hosts which will result in load imbalance and performance degradation In addition the scalability of the Hadoop cluster is not satisfactory In contrast the virtual Hadoop cluster on the cloud 


Dan Tao et al Load Feedback-Based Resource Scheduling and Dynamic Migration-Based Data Locality for    151 platform is more convenient for the 003exible adjustment of cluster scaling Motivated by this we integrate Hadoop onto an IaaS cloud platform and propose a new DHCI architecture In the DHCI architecture illustrated in Fig 1 we introduce four kernel modules besides the original packages of private cloud and Hadoop 017 Monitoring Module  Considering that different clusters in a virtual environment are isolated Hadoop cannot obtain the load of physical hosts at all A monitoring module is introduced to periodically monitor the load on the physical hosts as well as the VMs The load information collected can be used to provide the basis for resource scheduling 017 Scheduling Module  It is responsible for two aspects 1 periodically pushing the load information of the physical hosts to the scheduling node e.g ResourceManager in Hadoop and 2 issuing the corresponding scalability strategy to the VM management module according to the load of the VM clusters 017 VM Management Module  It achieves dynamic scaling of the VMs by adding or deleting operations This is an execution module which takes instructions from the scheduling module and interacts with the VMs on the IaaS platform 017 VM Migration Module  It is used to detect a task's data locality and execution process Once this module 002nds 1 the execution progress of a task is slower than a given threshold and 2 its CN and SN do not meet the data locality it will migrate this task to a suitable physical host by the storage of data duplication In summary the DHCI architecture has two features 1 joint load monitoring and 2 003exible resource scheduling The monitoring module monitors the physical and virtual resources with full awareness of the current system load conditions This necessary data can be utilized to optimize subsequent resource scheduling Through the scheduling and VM management modules the resource utilization can be optimized according to the load balancing of the physical hosts and the 003exible scalability of the VMs Based on the idea of mobile computing the reuse of VMs migration achieved by the VM migration module in the DHCI architecture can also reduce bandwidth consumption and improve system performance 3 Resource Scheduling and Data Locality 3.1 Load feedback based resource scheduling The resource scheduling selects appropriate resources assigned to different tasks for execution   Currently most evaluation indicators are static or predictive physical performance items such as the computing power of CPU storage capacity and network bandwidth   However in the dynamic environment of cloud computing it is dif\002cult for these indicators to re\003ect the actual service ability of the physical resource In our solution the load of physical hosts can be described from two aspects CPU utility rate and load average Load average is a kind of performance Fig 1 DHCI architecture 


152 Tsinghua Science and Technology April 2017 22\(2 149–159 parameter e.g memory disk and network This parameter denotes the average utilization rate of run queues The higher the values of CPU utility rate and load average the heavier the workload of a physical host The VMs mentioned here run Linux OS therefore the performance of the system can be monitored every minute using the Top command in Linux For the whole VM cluster we adopt a uni\002ed script to collect the status of resource consumption In the DHCI architecture for a physical host its load information will be uploaded and fed back to the scheduling module periodically via the monitoring module We adopt a single-level threshold method to compare the load information and its work\003ow can be illustrated in Fig 2 Once the load exceeds a preset threshold the physical host is considered as stressed out and the resource application using it will be canceled Otherwise the resource application will be supported One of the most signi\002cant advantages of integrating Hadoop onto the IaaS cloud platform is 003exibility In other words the scale of the virtual cluster can dynamically adjust according to its real-time workload Similarly for the virtual Hadoop cluster comprising multiple VMs the monitoring module in the DHCI architecture collects its load information and feeds it back to the scheduling module A double-level threshold method is used to distinguish between the lowest load VM and the highest load VM as shown in Fig 3 If the load exceeds a ceiling value the VM addition operation will be issued and a new VM will be created on the lowest load physical host However if it Fig 2 Single-level threshold resource scheduling algorithm Fig 3 Double-level threshold resource scheduling algorithm is below a 003oor value the VM deletion operation will be issued and excessive VM\(s will be deleted on the highest load physical host 3.2 Dynamic migration based data locality Large-scale distributed systems aim at processing data as close as possible to the storage location to reduce data movement between the computer and storage facilities which is typically known as data locality   Data locality is a crucial factor impacting the performance of a virtual Hadoop system In a traditional Hadoop cluster comprising physical hosts computation VMs used for task computation denoted by CNs and storage VMs used for data storage denoted by SNs are combined into a single VM The advantage is that CNs can directly obtain data from SNs while avoiding data transmission across a network However this deployment is no longer an effective method for a virtual Hadoop system The scalability of a virtual Hadoop cluster can be achieved by dynamically adding or deleting VMs The combination of CNs and SNs results in poor scalability This means that once CNs are added or deleted their corresponding SNs should be applied with the same operation Moreover in the process of VM migration VMs functioning as CNs and SNs will cause massive data movement thereby reducing the ef\002ciency of VM migration Therefore in the DHCI architecture the separation of CNs and SNs is adopted to improve 003exibility In particular they are deployed as respective VMs In this manner CNs can be migrated to a suitable place based on the idea of mobile computing It is obvious 


Dan Tao et al Load Feedback-Based Resource Scheduling and Dynamic Migration-Based Data Locality for    153 that this deployment form offers several advantages over a centralized method   1 strong scalability which allows for respective 003uctuating numbers of CNs or SNs and 2 003exible migration i.e CNs can be migrated without considering any other SNs Compared to that of the traditional Hadoop cluster data locality in the DHCI architecture can be classi\002ed into three categories   as illustrated in Fig 4 017 Host data locality CNs and SNs are deployed on the same host e.g VM1 and VM2 are on Host1 017 Rack data locality CNs and SNs are deployed on the same rack but different hosts e.g VM1 and VM4 are on Rack1 017 Across-rack data locality CNs and SNs are deployed on different racks e.g VM1 and VM10 are on Rack1 and Rack2 respectively Experimental results have shown that the speeds of task execution for meeting different types of data locality are signi\002cantly different under the same conditions   In particular the task completion time for meeting rack data locality and acrossrack data locality approaches three and four times as long as that for meeting host data locality respectively Data transmission between co-located VMs is often as ef\002cient as local data access mainly because inter-VM communication within a single host is optimized by the hypervisor   Hence we consider to improve host data locality in order to optimize the performance of the DHCI architecture Considering that the separation of CNs and SNs in\003uences data locality we dynamically migrate the CNs to any host\(s or rack\(s where the corresponding SNs are deployed to guarantee data locality During the migration process a VM remains on and the program executed in this VM Fig 4 Three types of data locality in a virtual Hadoop cluster keeps track of the running state Even if this VM is connected to a network the network connection will not be affected In fact the cost of migrating the VM is considerably less than that of reading/writing operations among different VMs   First the initial resource allocation should keep data locality In the Hadoop YARN adopted ContainerAllocator is responsible for communicating with Resource Manager and applying resources for tasks Usually there exist three backups for each task in HDFS Considering the level difference of data locality there will be multiple resource requests VMs can be allocated resource prioritized by host data locality rack data locality and across-rack data locality A resource request for a task can be described as a tuple  Priority Hostname Capability Containers   where Hostname can represent the ID of the host or the rack Consider the case in Fig 4 as an example wherein a task applies a resource in order from the host to the rack If this task can acquire a resource from a certain host the request will stop otherwise it will apply it one by one in the following manner  20,“Host1\(VM1,VM2   20,“Host2\(VM3,VM4,VM9   20,“Host3\(VM5,VM6,VM10,VM11 1   20 Rack1”,“memory:2G”,“1   20 Rack2”,“memory:2G”,“1   Second data locality should be optimized in the process of task execution Hadoop monitors task execution and judges whether data locality is satis\002ed or not If not Hadoop continues to search whether there exist one or more hosts which can meet the requirement of data locality Then CN will migrate to the correct one The real-time dynamics and uncertainty of cloud resources make resource management and task scheduling challenging   Parallel computing entropy is developed from Shannon information entropy which has the characteristics of symmetry nonnegativity and scalability Sun et al  have proved that parallel computing entropy can be maximized if and only if the load is completely balanced in the homogeneous cluster or the grid environment In this study we extend the method of parallel computing entropy into the heterogeneous cluster or cloud computing environment To accurately grasp the dynamic load and available information of resources we propose 


154 Tsinghua Science and Technology April 2017 22\0502\051 149\226159 a parallel computing entropy based VM migration solution In particular the load of the i th physical host can be represented as L i  L i D n X j D 1  ij s j 0501\051 where n denotes the total number of VMs on the physical host i  For the physical host i   ij denotes the ratio of the number of VMs with the task type j to the total number of VMs and s j denotes the computation of VM with the task type j  The relative load K i of physical host i  which is the ratio of the individual load to the total load of m physical hosts can be represented as follows K i D L i  m X i D 1 L i 0502\051 Considering the difference among performance parameters 050e.g CPU memory and bandwidth\051 of each physical host we express the processing capacity of the physical host as follows P i D f P cpu  i  P loadaverage  i g 0503\051 where P cpu  i and P loadaverage  i respectively denotes the CPU utility rate and the load average of the available physical host i  We quantify the above equation as follows P total D 013P cpu  i C 014P loadaverage  i 0504\051 where 013 and 014 are constant coef\002cients and satis\002es 013 C 014 D 1  Hence the relative load K i of the physical host i can be expressed as follows K i D L i P total i  m X i D 1 L i P total  i 0505\051 Assume that there are m physical hosts in a cloud computing environment At time t  the relative load of the physical host i is denoted as K i 050 t 051 Parallel computing entropy of the whole physical cluster at time t can be de\002ned as follows H.t D m X i D 1 K i t ln 1  K i t 0506\051 During the migration process the migration can be selected by the maximum entropy increment at each step and thus make the execution time of all the tasks the shortest Therefore the increase of parallel computing entropy causes 0501\051 the decrease in calculation amount of physical host\050s\051 with the biggest load and 0502\051 a more balanced load of other physical host and thus a decrease in the total execution time of all the tasks The goal of load balancing is to increase the parallel computing entropy as far as possible to reduce the total execution time of all the tasks The amount of computation for all the VMs on each physical host can be calculated using the sampling interval T  and the parallel computing entropy H 050 t 051 can be calculated by Eq 0506\051 Once the value of H.t is less than the threshold of 015  the load balancing of the system is unsatisfactory to some extent the VM migration process is needed To achieve a completed CN migration three major issues 050\2232W1H\224\051 should be solved 3.2.1 Which CN needs to be migrated The physical host needs to be migrated as it is the highest-loaded device Similarly the CN that needs to be migrated is the CN experiencing the heaviestcomputation load For a CN needs to satisfy the need to be migrated it must satisfy two conditions 0501\051 it is the highest-computation CN i.e CN has the greatest amount of calculations and 0502\051 CN and its corresponding SN are on a separate host or rack as shown in Algorithm 1 3.2.2 Where should a CN be migrated The destination host to which a CN is migrated should include its corresponding SNs store data replication In a virtual Hadoop cluster each task can be partitioned into several Map and Reduce tasks Each Map task runs map functions processing one data block 050128 MB by default in YARN\051 The data replication of each data block is three by default and can be stored in different  Algorithm 1 Determine CN to be migrated  Input the load L i of m physical hosts Output CN begin 1 Calculate the average load of the system L avg D P m i D 1 L i m  2 Calculate the load difference 4 L i D L i 000 L avg  3 Sort 4 L i from the largest to the smallest and store q 1  q 2   q m into the queue Q ph  4 Select the 002rst element q 1 in the queue Q ph  q 1 denotes the ID of physical host with the highest load 5 Calculate the corresponding computation amount c j of CN j on the physical host q 1  where 1 6 j 6 s  6 Sort c j from largest to smallest and store CN 1  CN 2   CN s into queue Q CN  7 Select the 002rst element CN 1 in queue Q CN  CN 1 denotes the ID of CN with the heaviest-computation amount 8 return CN 1 end  


Dan Tao et al Load Feedback-Based Resource Scheduling and Dynamic Migration-Based Data Locality for    155 hosts even racks Here we choose the least-loaded host which satis\002es the requirement of data locality as the destination host with the lowest cost 3.2.3 How should one migrate a CN The selected OpenStack cloud platform can support VM migration very quickly The whole migration process involves three kinds of physical hosts the source destination and control hosts We mainly utilize the Python interface function in the Libvirt tool to migrate the VM and data transmission can be realized in a tunneled way As space is limited the further description will not be given 4 Emulation and Analysis In this emulation we choose OpenStack as the cloud platform and Hibench as the Hadoop performance testing tool Hibench can provide a series of typical Hadoop benchmark test cases which can be directly used to conduct a performance test Here three classic benchmark test cases WordCount counts the words in the input 002les TeraSort sorts the data generated by TeraGen and Sort sorts the data written by the random writer are adopted to evaluate the performance of the proposed DHCI architecture The hardware con\002guration for the testing environment is listed in Table 1 4.1 Comparison of running time under the same load First we compare the running times using three classic schedulers FIFO Scheduler Fair Scheduler and Capacity Scheduler for the traditional Hadoop cluster and DHCI architecture It should be noted that FIFODHCI Fair-DHCI and Capacity-DHCI are de\002ned as the three schedulers used in the DHCI architecture The emulation results in Fig 5 show that the running time in the DHCI architecture is less than that in the traditional Hadoop cluster with the same workload data volume is 2 GB Using Fair Scheduler as an example for the WordCount TeraSort and Sort cases the running Table 1 Hardware con\002guration for the testing environment Parameter Con\002guration CPU type 4-core 2.4 GHz Intel\(R Xeon R Memory 32 GB Network card Three 2 Gbps LANs OS Linux 14.04 VM images Virtual CPU 2 GB RAM and 30 GB HDD Fig 5 Comparison of running time for the three schedulers under the two architectures times in the DHCI architecture are decreased by 14 9 and 8 respectively The proposed scheduling approach can outperform other traditional approaches mainly because in the DHCI architecture the resource allocation can be avoided on overburdened physical hosts or the strong scalability of the virtual cluster can be achieved by 003uctuating the number of VMs Second we evaluate the running time under two architectures with different data volumes from 256 MB to 2048 MB as illustrated in Fig 6 As far as WordCount is concerned the emulation results show that the performance of the DHCI architecture is superior to the traditional Hadoop cluster Obviously the more the data volume the greater the bene\002ts of the DHCI architecture be apparent 4.2 Comparison on running time under the load pressure Once the workload approaches the peak value or valley value the DHCI architecture will process tasks by dynamically increasing or decreasing the number of virtual machines In this way compared to the Fig 6 Comparison of running time under two architectures with different data volumes 


156 Tsinghua Science and Technology April 2017 22\(2 149–159 traditional Hadoop cluster the DHCI architecture can process the same workload with less running time by optimizing resource utilization To test the operational condition of the Hadoop cluster under certain load pressure in a private cloud environment when the workload on virtual cluster 1 reaches the maximum a new virtual cluster 2 will be added as shown in Fig 7 Supposing that the original workload of the task WordCount TeraSort and Sort run on virtual cluster 1 is 2 GB The effect on the operational ef\002ciency of the task on the two architectures with different scales of virtual clusters can be illustrated in Fig 8 There is no doubt that the dynamic increase of virtual clusters allows for resource utilization as well as load balancing and thus results in less running time Under a speci\002c workload compared to the running time for the traditional Hadoop cluster that for the traditional cluster under load pressure decreases to 67  71  and 55  for WordCount TeraSort and Sort respectively This depicts that Fig 7 Adding new virtual cluster\(s onto IaaS cloud platform for relieving load pressure Fig 8 Comparison of running time under two architectures with different scales of virtual clusters multiple tasks can be simultaneously operated on a virtual Hadoop cluster with relatively low cost In the case of load pressure compared with the running time for the traditional Hadoop cluster that for the DHCI architecture decreases to 56  59  and 52  respectively Under a speci\002c workload we compare the running time for the traditional Hadoop cluster with that of the DHCI architecture under load pressure they are close to 2:1 e.g 913 vs 507 633 vs 371 and 2450 vs 1266 respectively The results clearly show that with the same available computation resource of the cloud platform the DHCI architecture can signi\002cantly reduce the running time and thus improve the operational ef\002ciency of tasks 4.3 Comparison of CPU utility rate and load average between the two architectures In this section we evaluate the performance parameters of each physical host when the WordCount task is executed in the DHCI architecture Figure 9 shows the CPU utility rates of four physical hosts Ph1 Ph2 Ph3 and Ph4 from the 1st minute to the 14th minute In this experiment we set 50  as a threshold Once the CPU utility rate is greater than this threshold we regard the workload of the physical host as excessive In this case the resource on this physical host will not be allocated for task\(s any more From the curves in Fig 9 we 002nd that in the 3rd 6th 7th 8th and 10th minutes the CPU utility rates of part of the physical hosts exceeded the preset threshold of 50  For example in the 3rd minute both the CPU utility rates of Ph1 and Ph2 are 53  which is greater than 50  After 1 min the CPU utility rates of Ph3 and Ph4 increase gradually to accomplish workload sharing Correspondingly their values of Ph1 and Ph2 decrease to 43  and 49  respectively Figure 10 shows the load average of four physical hosts during a period of 14 min This experiment sets Fig 9 CPU utility rates of multiple physical hosts in the DHCI architecture 


Dan Tao et al Load Feedback-Based Resource Scheduling and Dynamic Migration-Based Data Locality for    157 Fig 10 The load averages of multiple physical hosts in the DHCI architecture 70  as a threshold Similarly when the load average exceeds this threshold the workload of the physical host is regarded as severe This striking trend in Fig 7 shows that when a physical host's workload rises and exceeds the preset threshold other physical hosts in the same virtual cluster will take part in workload balancing and thus improve the ef\002ciency Figures 9 and 10 give the performance parameters CPU utility rate CPU for short as well as the load average LA for short of 4 physical hosts during a period of 14 minutes for two different architectures We take their averages for each physical host respectively as illustrated in Figs 11 and 12 Then we calculate their variances to re\003ect the 003uctuations in workload of multiple physical hosts The variance of CPU utility rate in the traditional Hadoop cluster and the DHCI architecture are 0.196 and 0.1 respectively The ef\002ciency of the cluster load balance in the DHCI architecture is superior to that in the traditional Hadoop cluster From the perspective of load average the similar conclusion can be drawn 4.4 Test on data locality optimization To verify the effectiveness of the data locality optimization strategy we also use the benchmark Fig 11 Average CPU utility rate of each physical host in the two architectures Fig 12 Average load average of each physical host in the two architectures test cases namely WordCount TeraSort and Sort with 2 GB data volume The data in Fig 13 shows the difference in testing data from the DHCI architecture with and without data locality optimization respectively We can conclude that the time taken to execute these tasks with data locality optimization is less than that without data locality optimization while under the same data volume condition 5 Conclusion In this study we designed a novel dynamic Hadoop cluster IaaS architecture by introducing the following four modules monitoring scheduling VM management and VM migration modules In particular we proposed resource scheduling and data locality solutions We assessed the ef\002ciency of our solutions on the aforementioned virtual Hadoop cluster Convincing experimental results show that our solutions can effectively balance the load and improve system performance Acknowledgment This work was supported by the Open Project Program Fig 13 Comparison of running time in the DHCI architecture with and without data locality optimization 


158 Tsinghua Science and Technology April 2017 22\0502\051 149\226159 of Jiangsu High Technology Research Key Laboratory for Wireless Sensor Networks 050No WSNLBKF201503\051 the Fundamental Research Funds for the Central Universities 050No 2016JBM011\051 Fundamental Research Funds for the Central Universities 050No 2014ZD03-03\051 the Priority Academic Program Development of Jiangsu Higher Education Institutions and Jiangsu Collaborative Innovation Center on Atmospheric Environment and Equipment Technology References   L Z Wang J Tao H Marten A Streit S U Khan J Kolodziej and D Chen MapReduce across distributed clusters for data-intensive applications in IEEE 26th International Parallel and Distributed Processing Symposium Workshops and PhD Forum  Shanghai China 2012 pp 2004\2262011   M Armbrust A Fox R Grif\002th and M Zaharia A view of cloud computing Communications of the ACM  vol 53 no 4 pp 50\22658 2010   H Kang Y Chen J L Wong R Sion and J Wu Enhancement of Xen's scheduler for MapReduce workloads in Proceedings of the 20th International ACM Conference on High Performance Distributed Computing  New York NY USA 2011 pp 251\226262   T Sandholm and K Lai Dynamic proportional share scheduling in Hadoop in Proceedings of the 15th International Conference on Job Scheduling Strategies for Parallel Processing  Springer-Verlag 2010 pp 110\226131   B Sharma R Prabhakar S H Lim M T Kandemir and C R Das MROrchestrator A 002ne-grained resource orchestration framework for MapReduce clusters in IEEE 5th International Conference on Cloud Computing  Hawaii HI USA 2012 pp 1\2268   P Lama and X Zhou AROMA Automated resource allocation and con\002guration of MapReduce environment in the cloud in Proceedings of the 9th International Conference on Autonomic Computing  2012   L Y Zuo Z B Cao and S B Dong Virtual resource evaluation model based on entropy optimized and dynamic weighted in cloud computing 050in Chinese\051 Journal of Software  vol 24 no 8 pp 1937\2261946 2013   Q Liu W D Cai J Shen Z J Fu X D Liu and N Linge A speculative approach to spatialtemporal ef\002ciency with multi-objective optimization in a heterogeneous cloud environment Security and Communication Networks  vol 9 no 17 pp 4002\2264012 2016   M Zaharia D Borthakur J Sarma K Elmeleegy S Shenkeret and I Stoica Delay scheduling A simple technique for achieving locality and fairness in cluster scheduling in Proceedings of the 5th European Conference on Computer Systems  ACM 2010 pp 265\226278   H Jin X Yang X H Sun and I Raicu ADAPT Availability-aware MapReduce data placement for nondedicated distributed computing in IEEE International Conference on Distributed Computing Systems  Macau China 2012 pp 516\226525   A F Thaha M Singh A H M Amin N M Ahmad and S Kannan Hadoop in OpenStack Data-location-aware cluster provisioning in IEEE the 4th World Congress on Information and Communication Technologies  2014 pp 296\226301   Z Fadika and M Govindaraju DELMA Dynamically elastic MapReduce framework for CPU-intensive applications in The 11th IEEE/ACM International Symposium on Cluster Cloud and Grid Computing  2011 pp 454\226463   Y Kong M J Zhang and D Y Ye A belief propagationbased method for task allocation in open and dynamic cloud environments Knowledge-based Systems  vol 115 pp 123\226132 2016   C L Cheng J Li and Y Wang An energy-saving task scheduling strategy based on vacation queuing theory in cloud computing Tsinghua Science and Technology  vol 20 no 1 pp 28\22639 2015   R Q Sun J Y Yang Z Gao and Z Q He A virtual machine based task scheduling approach to improve data locality for virtualized Hadoop in IEEE/ACIS 13th International Conference on Computer and Information Science  2014 pp 297\226302   R Q Sun J Yang A Gao and Z Q He A resource scheduling approach to improving data locality for virtualized Hadoop cluster 050in Chinese\051 Journal of Computer Research and Development  vol 51 no Suppl pp 189\226198 2014   X P Bu J Rao and C Z Xu Interference and localityaware task scheduling for MapReduce applications in virtual clusters in Proceedings of the 22nd International Symposium on High-Performance Parallel and Distributed Computing  New York NY USA 2013 pp 227\226238   Q Zhang L Liu Y Ren K Lee Y Z Tang X Zhao and Y Zhou Residency aware inter-VM communication in virtualized cloud Performance measurement and analysis in Proc of the 6th IEEE International Conference on Cloud Computing  2013 pp 204\226211   Z J Fu X M Sun Q Liu L Zhou and J G Shu Achieving ef\002cient cloud search services multikeyword eanked aearch over encrypted cloud data supporting parallel computing IEICE Transactions on Communications  vol E98-B no 1 pp 190\226200 2015   H Y Sun W X Xie X Yang and K Lu A load balancing algorithm based OH parallel computing entropy in HPC 050in Chinese\051 Journal of Shenzhen University Science and Engineering  vol 24 no 1 pp 64\22668 2007 


Dan Tao et al Load Feedback-Based Resource Scheduling and Dynamic Migration-Based Data Locality for    159 Dan Tao is currently a professor with School of Electronic and Information Engineering Beijing Jiaotong University China She received the PhD degree from Beijing University of Posts and Telecommunications China in 2007 She was a visiting scholar in the Department of Computer Science Illinois Institute of Technology USA during 2010-2011 She has published more than 50 papers in the academic journals and conferences Her research interests include wireless networks and cloud computing Zhaowen Lin is currently an associate professor with Institute of Network Technology Beijing University of Posts and Telecommunications China He received the PhD degree from Beijing University of Posts and Telecommunications China in 2008 His research interests include future Internet key technology and network security Bingxu Wang is currently a postgraduate student of School of Electronic and Information Engineering Beijing Jiaotong University China He received the bachelor degree from Shandong University of Science and Technology China in 2013 His research interest is cloud computing 


rees China from all in pioneer net lica proceeda China China nals IEEE Who and CM 2377-3782 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/TSUSC.2017.2690301, IE\EE Transactions on Sustainable Computing 12 W Jia 


Experiment Setup Proposed attack scenario 
SECTION V EXPERIMENTS 
We allude to the lumps being Flush Reloaded by the advertisement versary as being observed since Flush-Reload basically screens access to information in the lump  Flushing a piece by means of clflush thus observing that piece should be possible without knowing the physical address of the lump since clflush takes the lump's virtual address for this situation in the adversary's address space as its operand  We call a quicker reload amid the Reload stage a watched occasion or perception  We additionally embrace ideas from measurable order and utilize the term false negative to allude to missed perceptions of the casualty's entrance to the checked piece and false positives to allude to watched occasions that are brought about by reasons other than the casualty's entrance to the observed piece We characterize a Flush-Reload convention in which the foe procedure screens a rundown of lumps at the same time and over and again until taught generally  It will first attempt to Reload the principal lump record the reload time and Flush it promptly thereafter At that point it will rehash these means on the second lump the third and so on until the last lump in the rundown  At that point the foe will sit tight for a precisely computed day and age before beginning once again from the primary piece so that the interim between the Flush and Reload of a similar lump is of an objective level 3 from the Reload of the main lump to the end of the holding up period is called one Flush-Reload cycle  An outline of the Flush-Reload convention is appeared in following Fig  
 
 chi k 2 R eI oa din g  chi nk 1  Rel o a di ng I dl e l ooping l 
Here we execute experiments on two systems  One system is a laptop and the second system is a dedicated server facilitating different services  We have installed windows server 2012 on both machines  First of all we used NMap to find OS finger printing the 
FI ng 
chi Id 
AUJS  ng 
 ch k 2 225 us bR eload Cycl e 
 


as 
using the Nassus we found the vulnerabilities available in VMs  Then generated side channel attack on both machines using flush  reload attack to get cryptographic keys and peeks inside the VMs whereas docker provides much more isolation and it was quite difficult to get all keys and peeking mechanism in docker Algorithm Flush Reload Hardware Specification System 1 System2 Use Laptop Dedicate Server 
Windows Server 2012 CPU Intel\256 Core\231 i3-4010U CPU  Intel Xeon 2.S Ghz 1.70Ghz Cores 02 Physical 04 Physical 04 Logical OS Logical RAM 4GB SGB 
 


x86-64 
L1 128 KB 256KB L2 512 KB 01 MB L3 03 MB 08MB Supporting Software Name Function Ubuntu and Windows Ubuntu and Windows server 2008 are being used to Server 2008 host multiple servers on Virtual machines Web Server Docker Container and Docker Microsoft Hyper-V NMap Nessus Docker Container cape a bit of Software in an entire file system that contains everything expected to run code runtime framework instruments framework libraries anything that can be introduced on a server This ensures the product will dependably run the same paying little respect to its surroundings  MS Hyper-V Microsoft Hyper-V codenamed Viridian and some time ago known as Windows Server Virtualization is a local hypervisor it can make virtual machines on 
frameworks running Windows Nmap is a security app used to find has and benefits on a PC organize hence making a IIguide ll of the system Nessus is a remote security checking device which filters a PC and raises a caution in the event that it 
 


225 225\225 225 225 1 111 225 225 
i 
GnuPG and Open SSL Results finds any vulnerabilities that noxious programmers could use to access any PC you have associated with a system GnuPG is an entire and free execution of the OpenPGP standard as characterized by RFC4880 otherwise called PGP  GnuPG permits to encode and sign your information and correspondence highlights a flexible key administration framework and additionally get to modules for a wide range of open key indexes  
 
26 
tt 26 
c e 
140 120 g 0 25 
System A 180    O penSSLD 9 7LR 
E Ubgcryp t 1 6LR             29 
Ubgcrypt 1 6FR 
 
 
_ OpenSS LD 9 7FR _ OpenSSLD  9.7FR 180    O penSSLD 9 7LR O pen SSL1 0.1 FR   III   Op enSSL1 0.1 LR 160 PolarSS L 1.3  3FR   0   Po l arSS L1 3 3LR 
B 
27 29 Encr ypt ions   Ill   O penS SL1 0.1 LR 160 PolarSSL1  3.3FR   0   PolarSS L1  3.3LR Ubgcrypt1 6FR Ubg c rypt1  6LR 27 Encr y pt i ons  2 System 
  
28 28 
140 25 
 


Conclusion 
1 
References 
1 
machine 8 https www.toptal.com/linux/separation-anxiety-isolating-your-system-with-linu\x-namespaces 9 Isolation in Cloud Computing and Privacy-Enhancing Technologies Suitability of PrivacyEnhancing Technologies for Separating Data Usage in Business Processes Prof Dr Noboru Sonehara Prof Dr Isao Echizen Dr SvenWohlgemuth National Institute of Informatics 2-1-2 Hitotsubashi Chiyoda-ku Tokyo sonehara@nii.ac  jp  10  Performance Isolation and Fairness for Multi-Tenant Cloud Storage David Shue Michael J Freedman and Anees Shaikh Princeton University ylBM TJ Watson Research Center 
An Updated Performance Comparison of Virtual Machines and Linux Containers Wes Felter Alexandre Ferreira Ram Rajamony Juan Rubio IBM Research Austin TX fwmf apferrei rajamony rubiojg@us.ibm.com 2 A Unified Operating System for Clouds and Manycore fos David Wentzlaff Charles Gruenwald III Nathan Beckmann Kevin Modzelewski Adam Belay Lamia Youseff Jason Miller and Anant Agarwal 3 Containers and Cloud From LXC to Docker to Kubernetes DAVID BERNSTEIN 4 Containers and Clusters for Edge Cloud Architectures a Technology Review Claus Pahl Irish Centre for Cloud Computing and Commerce IC4  Lero the Irish Software Research Centre Dublin City UniversityDublin 9 Ireland 5 Containerisation and the PaaS Cloud Claus Pahl 6 http://www.slideshare.net/BodenRussell/kvm-and-docker-lxc-benchmarking-w\ith-openstack 7 http://stackoverflow com q uestio ns/1604 7306/how-is-docker d ifferent from-a-no rma I-vi rtua 
It has been observed from experiments that container provides much more isolation among multiple users multi-tenants in cloud virtualization as compared to virtual machines Taking the example of Docker container which is light weight more secure and fast processing virtualization technique and getting much more familiarity due to its characteristics Also Container provides isolation at every instance of virtualization like at process level at file system level network level and at inter process communication lPe level 
 


