Load Balancing on PC Clusters with the Super-Programming Model Dejiang Jin  and  Sotirios G. Ziavras Department of Electrical and Computer Engineering New Jersey Institute of Technology Newark, NJ 07102  ABSTRACT Recent work in high-performance computing has shifted attention to PC cluster  For PC-clusters, member nodes are independent comput ers connected by generalpurpose networks. The latency of data communications is long and load balancing among the nodes becomes a 
critical issue. We introduce a new model for program development on PC clusters, namely the SuperProgramming Model \(SPM to address this issue. In SPM PC clusters are modeled as a single virtual machine with PC as their processing units. The workload is modeled as a collection of Super-Instructions SIs achieve a limited workload Application programs are coded using SIs. SIs are dynamically assigned to available PC at run time. For limited workload, no SIs overloads any PC.  Therefore, dynamic load balancing becomes an easier task.  We apply SPM to mining association rules. Our 
experiments show that under normal conditions the workload is balanced very well. A performance model is also developed to describe the scalable behavior of SPM 1. Introduction High-performance computing has recently shifted its attention to PC clusters containing commercial off-theshelf \(COTS\nodes, for cost-effective parallel computing  This trend m a kes high-perform ance com puting much less expensive and more accessible. These systems are suitable for large-scale problems, such as data mining of very large databases [1 Each node of a PC cluster is an independent computer running a general-purpose 
operating system. A general-purpose interconnection network, that most oft en is an Ethernet-based, connects these nodes together. Data comm unication among the PCs is controlled by application layer software rather than by lower-level system software or hardware. The latency of  This work was supported in part by the U.S. Department of Energy under grant ER63384 data communications on a PC cl uster is usually longer than on parallel processing systems that contain specialized hardware for communication networks. Therefore 
programming models developed for the latter are not suitable for programming PC clust ers. For PC clusters, it is more difficult to exploit lo w-level or fine-grain parallelism existing in programs.  It is more appropriate to adopt coarse- or medium-grain programming. This reduces the adverse effect of long communica tion delays.  For PC clusters, load balancing among member computers becomes a critical issue for high performance. It tries to 215appropriately\216 assign tasks among processing nodes so that minimize the idle time of the processing nodes while other nodes are busy. Only if the workload on these 
member computers is well balanced, we will be able to achieve high performance. Ot herwise, some computer nodes will be idle for significant periods of time during the computation and the overall efficiency of the system will diminish In existing programming models, the way to decompose applications is norm ally function-oriented Applications are decomposed into function units. To reuse code in a given application domain area, these units are implemented as library functions. For example, BLAS Basic Linear Algebra Subprogram s\has "building block routines for performing basic vector and matrix operations 
 m only used in the development of high quality linear algebra software. Each subprogram completes a single operation, such as matrix-matrix multiplication, no matter how large the matrices are. In this fashion, the workload of each \215block\216 is various, it is very difficult to balance For this reason we introduce the Super-Programming Model \(SPM for cluster computing. The workload is modeled as a collection of Super-Instructions SIs have limited atomic workload. Application programs are 
modeled as Super-Programs \(SPs coded with SIs. At run time, SIs are dynamically assigned to available PCs. The maximum execution time for each SI is well estimated and adjusted with parameters. SPM makes the workload easier to balance among the PC nodes.  If the degree of Proceedings of the 2003 International Conference on Parallel Processing Workshops \(ICPPW\22203 1530-2016/03 $ 17.00 \251 2003 IEEE 


parallelism in the super-progr am is much larger than the number of nodes in the cluster, then nodes have little chance to be idle. The wo rkload will be balanced very well SPM can be adopted for any paral lel application To effectively support application portability among different computing platforms and to also balance the workload in parallel systems, we suggest that an effective instruction set architecture \(ISA for each application domain.  For a particular application domain, it is not difficult to determine a set of frequently used operations These operations then become part of the chosen ISA SPs utilize these SIs in the coding process.  Then, as long as an efficient implementation exists for each of these SIs on given computing platforms, code portability is guaranteed and good load balancing becomes more feasible by focusing on scheduling SIs.  Ideally, the chosen ISA should be orthogonal, containi ng as few SIs as possible that are also adequate to develop any program in the corresponding application domai n with the smallest possible number of general-purpose instructions In this paper, we apply the SPM model to a data mining problem in order to prove that it can address the load balancing problem very well 2 Relevant Research 2.1 The Mining Association Rules Mining association rules is a typical data mining problem. It can be modeled as follows:  Let I = { a 1 a 2 a 3  203 , a m be a set of items and DB 000\242 T 1 T 2 T 3 203 ,T n 000\262 be a transactions database with items in I A pattern is a set of items in I. The term itemset is used interchangeably with the term set of items or pattern The transaction represents an itemset that occurs in a database. The number of items in a pattern is called the length of the pattern Patterns of length k are sometimes called k-item patterns The support s\(A\ of a pattern A is defined as the number of transactions in the database DB containing A An association rule is an association relationship between a pair of patterns expressed in the form R: X 000\316 s 000D Y, which means X mplies Y, where X and Y are exclusive patterns X 000\210 Y 000\207  pre-pattern and post-pattern of rule R, respectively. s and 000D are the support and confidence of the rule R, respectively. The support s of rule R is defined as the support of the pattern obtained by joining X and Y \(s = s\(X 000\211 Y confidence 000D of rule R is defined as s\(X 000\211 Y X database DB, a minimum support threshold s min and a minimum confidence threshold 000D min the problem of finding the complete set of association rules with support and confidence no less than these support and confidence thresholds, respectively, is called the association rules mining problem A pattern A is a frequent pattern or a frequent set  less than a predefined minimum support threshold s min Also, given a transaction database and a minimum support threshold s min the problem of finding the com plete set of frequent patterns is called the frequent patterns mining problem  Techniques for discovering association rules have been studied extensively  Many approaches transform the association rules mining problem to the frequent patterns mining problem. The most popular one is the Apriori algorithm Many relevant studies adopt an Apriori-like approach The Apriori algorithm is based on the following property: if any k-length pattern is not a frequent pattern in the database then any k+1 length pattern that includes this pattern can never be a frequent pattern in the database. Using this property, any verified short frequent patte rns can help in screening longer candidate patterns. The algorithm works as follows I\220 = {x | x 000\217 I and x is a frequent item Find all frequent items by scanning DB P 1 1-length patterns {x} | x 000\217 I\220 For k = 2; P k-1 000z\000\207 k do begin C k apriori_gen\(P k-1  For any pattern c 000\217 C k c.count = 0 For all transactions T 000\217 DB For any pattern c 000\217 C k if \(c 000\216 T  P k c | c 000\217 C k c.count 000t s min  End Answer   000\211 P k Initially, P 1 gets all frequent patterns with a single item After that iteratively the alg orithm calls the function 215apriori_gen\216 that generates a complete set C k of k-length candidate patterns; then, th eir support is counted by scanning transactions containi ng these candidate patterns the set P k of all k-length patterns is generated by pruning C k to eliminate infrequent patterns. Once P k is empty, the iteration is terminat ed. The union of variable length frequent patterns 000\211 P k forms the complete set of frequent patterns in which association rules can be identified Each transaction is checked to see if it supports some candidate patterns To speed up this checking operation, a hash tree of candidate patterns is used 2.2 Parallel Algorithms and Load Balancing Several efforts have focused on the development of algorithms for data mining on parallel platforms 7 8 11 Several techniques have been developed for dynam ic load balancing using some form of load estim  In previous related research, all consideration of load balancing is based on an estim ation of the computation workload. For example, the workload of the join operation was estim equivalence Proceedings of the 2003 International Conference on Parallel Processing Workshops \(ICPPW\22203 1530-2016/03 $ 17.00 \251 2003 IEEE 


classes To count the support of candidate patterns, it was also suggested to estimate the related workloads. In [11 static load balancing was embedded in the data partition algorithm Many studies have proved that computing the counts of candidate patterns is th e most computationally expensive step in the algorithm. The only way to compute these counts is to scan the entire transactions database Most algorithms focus on computing the support of  s can be classified into two basic ty pes based on what types of data are partitioned. They are either count distribution \(CD algorithms or data distribution DD\ algorithms Ina CD algorithm 2], the entire candidate set is copied into all the nodes. Transaction data are partitioned and each node is assigned an exclusive partition The allocation of workload is controlled by partition of transaction. A DD algorithm partitions the set of candi date patterns for exclusive assignment to processing done in a round-robin fashion. Each node is responsible for computing the counts for its locally stored subset of the candidate patterns for all the transactions in DB The allocation of workload is controlled by partition of candidate. But both partition of candidates and partition of transactions are well workload estimation of the count Thus, load balancing based on load estimation cannot be perfect. To conclude, past approaches to load balancing for mining association rules in databases did not demonstrate the versatility of the dynamic load balancing technique that we propose in this paper. Our results also in Sections 4 and 5 support our claim 2.3 Parallel Implementation of Data Mining Many researchers have implemented relevant algorithms and evaluated their performance on parallel machines or supercomputers such as SGI and Cray T3D [11 Som e researchers experimented on both parallel machines and PC applied identical algorithms They did not consider adjusti ng the algorithm for the chosen computing platform for example, to reduce the effect of long delay on PC clust ers, they tried to improve the PC-interconnection network 3. A Super-Programming Model for Mining Association Rules 3.1 The Super-Programming Model In the super-programming model \(SPM\ is modeled as a single virtual machine with PCs as processing units. The workload is modeled as a collection of SIs. Like instructions for processors, SIs are expected to complete a task with limited wo rkload that can make the execution time quite predictab le. i.e. SPM is workloadoriented. An example of such an SI is \215compute the supports for a set of candidate patterns, where the number of patterns is no more than k, against a block of transaction data\216 k is a design parameter. SIs model atomic workload units The maximum execution time for each SI is well estimated. It is determi ned by design parameters Designers can choose the parameters so that all SIs have similar maximum execution time. Any large task is implemented by executing more than one SI Application programs are modeled as Super-Programs \(SPs They are composed of SIs A runtime environment supports the execution of SPs At run time, each SI is dynamically assigned to a PC to execute if and only if the PC has resource. Each SI can only be executed on a single node. SIs are executed parallel if they do not depend on each other. Extending the functional unit to handle multiple SIs makes the high-level parallelism not only to be determined by the algorithm but also by the ISA designer. Increasing the degree of highlevel parallelism makes easier the task of balancing the workload 3.2 Design Issues for the Super-Instruction Set There exist three main issues 1 SIs are implemented by software routines that can be executed on PC nodes Since SIs are dynamically assigned to PCs, they could be executed on any PC. This requires implementing SIs that are portable throughout PCs in the cluster For a heterogeneous system, it becomes a major task 2 Completeness and orthogonality of the SI set The SI set creates an abstract la yer. It should encapsulate the underlying support system. An application should be described completely by using these SIs. Thus, the SI set should provide all basic operations to support such abstractions. Considering the storage capacity and the programming capability of general-purpose computers the number of SIs can be unlimited. Also SIs can be as robust as needed. There is no real need to improve any resource in order to provide a larger instruction set. Thus, the SI set is open to expansion to match the application domain\220s requirements, as needed. In other words, the SI set is completely applicat ion dependent. To enhance software component reuse, it heavily depends on the application domain Another issue is the orthogonality of the SI set That is SIs should not have any functionality overlap in terms of the types of majo r tasks that they carry out This way the SI set will have reasonable size without any redundancy for better program maintenance, ease of algorithm development, efficiency and good portability 3 Member PCs are completely independent processing units Proceedings of the 2003 International Conference on Parallel Processing Workshops \(ICPPW\22203 1530-2016/03 $ 17.00 \251 2003 IEEE 


They may have different independent logical space. SIs are dynamically assigned to a PC. Thus, SP running on a cluster needs a global logical space. SIs should only reference data with names \(or Ids\in this space rather than reference their operands with local addresses on the underlying procedures implemented SIs.. The runtime environment will map the global Object to local space 3.4 Data Blocks for Mining Association Rules The operands for SIs are blocks of data. Such a data block is called a super-data block SDB primary entities for high-level super-programming; they are used as build-in data in ordinary programming. Highlevel super-programs build th eir data structures using SDBs. SIs manipulate these SDBs. Each SDB has its own global ID as data in an ordinary program have their own address. The data included in an SDB can be loaded/cached/stored at any node by runtime support systems.  The data blocks have limited maximum size Thus the workload of SIs is limited. This way, assigning a significantly large task to a single node is avoided For mining association rules we have designed a set of superdata blocks as shown in Table 1 Table 1 Summary of Super-data blocks types Name Content BlockOfItems A list of distinct items covering a continuous, exclusive partition BlockOfTransa ction A set of transaction data BlockOfJoinR esult A list of candidate patterns without checking for frequent sub-patterns BlockOfCandi dates A list of candidate patterns. All of their sub-patterns are frequent patterns BlockOfFreque ntSet A list of frequent patterns with the same length BlockOfRules A set of alreadymined association rules 3.5 ST Set for Mining Association Rules We have designed an SI set for mining association rules in large transaction databases. A summary is shown in Table 2 Table 2 Summary of a super-instruction set for mining association rules SI name Parameters Function LoadDataBlock Reference to the data source; an SDB of transactions; maximum size of the SDB Gets a block of data residi ng outside of the system CountItemSupport ID of the extr acted transaction block; ID of the map model Extracts all distinct items and counts the support of items appearing in a block of raw transaction data ShrinkItemBlock SDB of items; the th reshold of support Prunes items in an SDB of items GetFrequentItems Block A list of SDBs of items; SDB of frequent items; generated SDB of 1-length frequent patterns; mapping object Creates an SDB of frequent items and an SDB of 1-frequentItemSet ShrinkTransaction Block ID of the original SDB of transactions ID of the result SDB of transaction Shrinks a block of transactions MergeTransaction Block A list of IDs of pruned SDBs of transactions Merges a list of pruned SDB blocks of transactions into one SDB GenCandidates Block  or [A list of frequent patterns]; an SDB of frequent patterns; a global mapping object; the generated SDB Generates an SDB of candidate patterns FilterCandidates An SDB of candidates; an SDB of frequent patterns; the global mapping object Screens candidate patterns in a SDB by comparing their sub-patterns with frequent patterns stored in another SDB CountCandidates Block ID of an SDB of candidate patterns; ID of an SDB of transactions Counts the partial support of the candidate patterns in an SDB of candidates PruneCandidates Block An SDB of candidate patterns; the threshold of support Prunes an SDB of  candidates GetFrequentSet Block List of SDBs of candidate patterns a global mapping object Generates an SDB of frequent patterns from a list of SDBs of counted candidates CheckConfidence InBlock an SDB containing postpatterns; an SDB of pre-pattern; an SDB of rules; mapping object Extracts rules from a pair of SDBs of frequent patterns StoreResult SDB of rules Stores an SDB of rules in external storage Proceedings of the 2003 International Conference on Parallel Processing Workshops \(ICPPW\22203 1530-2016/03 $ 17.00 \251 2003 IEEE 


Proceedings of the Third International Conference on Machine Learning and Cybernetics Shanghai, 26-29 .August 2004 users and items pages Instead of being found passively items and pages can discover their potential Web users automatically and be recommended to these users actively in this general approach Acknowledgements This work is supported by the National Natural Science Foundation of China 60205007 Natural Science Foundation of Guangdong Province 001264 031558 Research Foundation of Science and Technology Plan Project in Guangdong Province 2003C50118 and Research Foundation of State Key Laboratory for Novel Software Technology at Nanjing University Referencis 11 Herlockei J Understanding and Improving Automated Collaborative Filtering Syaems Pb.D Thesis Computer Science Dept University of Minnesota Bamshad Mobasher Honghua Dai Tao Luo Miki Nakagawa Effective personalization based on association rule discovery from web usage data In the Proteedings of the ACM Workshop on Web 121 Information and Data Management WIDM2001 Pp 9-15 3 laronski W Bloemer I Vanhoof K Wets G Use of Bayesian belief networks to help understand online audience In Proceedings of Data Mining for Marketing Applications Workshop at ECWKDD 2001,3-7 4 Agganual C C Wolf J L Wu K and Yu P S Honing Hatches an Egg A New Graph-theoretic Approach to Collaborative Filtering In Proceedings of the ACM KDD99 Conference. San Diego, CA pp 201-212 Sarwar BM Karypis G Konstan JA and Riedl I Applicafion of Dimensionality Reduction in Recommender System  A Case Study In ACM WebKDD 2000 Web Mining for E-Commerce Workshop 6 Stefan Berchtold, Daniel A. Keim, Hans-Peter Kriegel The X-tree An Index Structure for High-Dimensional 222 Data Proceedings of the 22nd International Conference on Very Large Databases pp 28-39 F 223of\224 and S Muthukrisbnan Influence sets based on reverse nearest neighbor queries In Proc ACM SIGMOD Int Conf on Management of Data Dallas USA May 2000 5 7 I 1172 


12 Fin V Jensen Bayesian Networks and Decision Graphs Springer, New York 200 1 13 1 L Kolodner 223Maintaining Organization in a Dynamic Long-Term Memory,\224 Cognitive Science 7\(4 1983,243-280 14 C.M Kuok A Fu M.H Wong 223Mining Fuzzy Association Rules in Databases\223 ACM SIGMOD Record vol 27\(1 March 1998,4146 lS Lily Liang and Carl Looney 223Inference via fuzzy belief Petri nets,\224 Proc IEEE ACTAI 2003 Conf Sacramento 2003 5 10-5 14  Carl G Looney and Lily Rui Liang 223Cognitive Situation and Threat Assessment of Ground Battlespaces\224 Int J Information Fusion 4\(4 2003,297-308 1171 Carl Looney and Lily Liang 223Inference via Fuzzy Belief Networks\224 Proc ISCA International Conference San Diego Nov 2002  181 Carl G Looney, +\221Interactive clustering and merging with a new fuzzy expected value,\224 Pattern Recognition 35 2002 2413-2423 19 Carl G Looney Pattern Recognition Using Neural Networks Oxford University Press NY 1997 20 Carl Looney 223Fuzzy Petri Nets for Rule Based Decisionmaking,\224 3EEE Trans SMC 18 I Jan. 1988 22 A E Nicholson and J M Brady 223Dynamic belief networks for discrete monitoring,\224 ZEEE Trans SMC 24 I I 1994, 1593-1610 23 Z Pawlak Rough Sets Theoretical Aspects of Reasoning about Data Kluwer Academic, Dordrecht 1991 24 J Pearl Probabilistic Reasoning in intelligent Systems Networks of Plausible Inference Morgan Kaufmann Publishers San Mateo, Califomia 1988 25 R Schank Dynamic memory a theovy ofreminding and learning in computers and people Cambridge Universiv Press Cambridge UK 1982 26 G Shafer A Mathematical Theoy of Evidence Princeton University Press, Princeton 1976  Glenn Shafter and Judea Pearl Editors Uncertain Reasoning Morgan Kaufinan San Mateo,l990 Readings in 28 S Wright 223Correlation and causation,\224 J Agricultural Reseurch 20 1921,557-585 29 Y Xiang, \223Belief updating in multiply sectioned Bayesian networks without repeated local propagations,\224 Int J Approximate Reasoning 23,2000 1-21 30 Nevin L Zhang 223Computational properties of two exact algorithms for Bayesian networks,\224 Applied Intelligence 9 1998 173-183 1211 Sucheta Nadkami and Prakash Shenoy 223A Bayseian network approach to making inferences in causal maps,\224 European J Operations Research 128,2001,479-498 558 


tight t t= 1 Step 8: If  is null, then do the next step; otherwise, set r=r+1 and repeat Steps 6 to 8 1rL Step 9: Construct the association rules for all large q-itemset t with items \( ,  using the following substeps 1 2, ,...,t t a 1 1 1... ... qt t t? ? ? ? ? ?     \(8 b rules using 1 1 1 1  k k n j j j t t j n j t j           9 We take the fuzzy mining process of excessive air coefficient as an example. The excessive air coefficient value is taken when the unit is in 300MW stable load and the coal consumption is lower \(lower than 355g/kW.h history data. Then the data is standardized and is mined by fuzzy mining algorithm. The following rule is output: When the load is 100% and coal consumption eB  \(g/kW.h 37.50, 43.75], the excessive air coefficient  is [34.5 35.7], with a support value of 62% and a confidence of 81 It satisfies the requirement of minimum support and  Step 10: Output the rules with confidence value larger than or equal to the predefined minimum confidence After Step 10, the rules constructed are output and can act as the meta-knowledge for the given transaction. It is expressed based on reasoning and is easy to be understood 4.  The application of fuzzy data mining in operation optimization Based on the history data of a 300MW power plant unit, the typical parameters which are related to operation optimization are analyzed. Those parameters include load main steam pressure, main steam temperature, re-heated temperature, feed water temperature, exhaust gas temperature, excessive air coefficient and so on. The quantitative association rules are acquired by analyzing the history data. For example, load    1... nL L 1... nP P 1 1... mT T g gnB B related to low coal consumption are chosen as optimization values to optimal the electric industrial process. The rules to decide the optimal parameters values are expressed as 


to decide the optimal parameters values are expressed as 1 1   this way the optimal operation parameters are decided according to the load and the other related condition. The optimization values attained from data mining are reachable in operation and can reflect the actual status in operation According to the method mentioned above, the history data from recent three months are analyzed. A total of 4212 transactions consisted of the operation parameters of the typical stable load of 100%, 90% and 80% are obtained These transactions are standardized by formula \(1 minimum support value is set at 30% and the minimum confidence is set at 75%. The membership function of the parameters is shown in Figure 2 75 82 86 92686458  1 Low Middle High 0.5  Figure 2. The membership function 1645 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005 minimum confidence. This rule can be expressed as The corresponding range is This rule means that when the load is 300MW and the coal consumption is lower the optimal range of the excessive air coefficient is 1.342~1.350. The average of the interval can be adopted to decide the optimization point. The optimization value of excessive air coefficient 100 ,34.5,35.7eM ?&lt; &gt;?&lt 300 ,1.34eM MW ?&lt; &gt;?&lt gt gt;2,1.350 is set at 1.346 when the load is 300MW \(100 the method mentioned above, by utilizing the fuzzy association algorithm to mine the optimization value when the typical load of 100%, 90% and 75%, etc, a set of optimization values is obtained. The optimal values attained from the fuzzy data mining and the reference values attained in traditional way are listed in Table 1 300MW 16.67 Referenc Optimal Parameters Main steam temperature Reheat temperature Feed water temperature Flue gas temperature Excessive air coefficient Main Steam Pressure MPa 537 537 270.5 138.4 137.1 1.352 1.346 16.71 538.3 538.1 


538.1 268.1 270MW e/ Referenc Optimal 16.67 537 537 264.2 135.2 133.6 1.432 1.431 16.66 538.1 537.4 264.6 Table 1. Optimization value analysis result of 300MW power unit 225MW e/ Reference Optimal 13.89 537 537 254.1 129.8 130.4 1.480 1.484 13.96 537.2 535.4 258.4  The controllable parameters are optimized based on the results of the fuzzy data mining in power plant. The performance of the boiler improved obviously. The average boiler efficiency improved about 0.924% and the coal consumption reduced about 3.72g/kW.h. The optimization value is close to the reference value in trend and can be used to guide the industry process. The newly founded rules and knowledge can be added to model base or the knowledge base. The operation optimization base on data mining is an effective method to improve the efficiency in power plant The execution times of fuzzy association mining in different minimum support with a computer Pentium 1.7G/256M are shown in Figure 3. For a total of about 4000 transactions in the data set and the minimum support is set at 20%, the execution time is about 150s. The fuzzy association mining is high efficiency. The execution time of fuzzy association mining increases linearly with the transactions in data set. So it is applicable to large data sets 80% 60% 40% 20 50 100 150 200 minimum support tim e s  Figure 3. The time of fuzzy association mining in different minimum support 5. Conclusion The operation optimization is the mainly method to improve the performance in power plant and the decision of optimization value is the key point in operation optimization. In this paper, we proposed the operation optimization based on data mining and applied the fuzzy 


optimization based on data mining and applied the fuzzy association rules to find the optimization value from the history data of the equipments in power plant. The fuzzy sets theory was introduced into the association mining process in order to soften the partition boundary of the domain and generalize and abstract the data. Base on the history data in power plant, the optimization values are reachable in operation and easy to guide operation. The rules mined out exhibit quantitative regularity in large database and can be used to provide guides and suggestions to the appropriate operator. Experimental results with the data in a 300MW power plant show that the algorithm base on fuzzy set operation performs very well and can be used to guide the operating process to achieve a good performance References 1] Wang X Z  Automatic classification for mining process operational data  Ind. Eng. Chem. Res., 37 pp.2215-2222, 1998 2] Tony Ogilvie, B W Hogg  Use of data mining techniques in the performance monitoring and operation of a thermal power plant  IEEE Colloquium on Knowledge Discovery and Data Mining, 1998 3] Ren HaoRen, Li Wei  The analyze of operation index for the power unit under different loads  Proceedings of the CSEE, Vol 19, No. 9, pp.50-52,56, 1999 4] Agrawal R, Imielinski T, Swami A  Mining 1646 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005 association rules between sets of items in large database  Proc of the ACM SIGMOD conf on Management of data, Washingtong D.C, pp.207-216 May 1993 5] Agrawal R, Srikant R  Fast algorithms for mining association rules in large databases  The International Conference on Very Large Data Bases, Santiago, Chile pp. 487-499, 1994 6] Srikant R, Agrawal R  Mining quantitative association rules in large relational tables   Proceedings of the ACM SIGMOD International Conference on Management of Data, Montreal Canada, pp.1-12, 1996 7] Zou XiaoFeng, LU JianJian, Song ZiLin  Mining linguistic valued association rules  Journal of System Simulation, Vol 14, No. 9, pp.1130-1132, 2002 8] T. P. Hong, J. B. Chen  Finding relevant attributes and membership functions  Fuzzy Sets and Systems Vol 103, No. 3, pp.389-404, 1999 9] T.P.Hong, C.S.Kuo, S.C.Chi  Mining association rules from quantitative data  Intelligent Data Analysis, Vol 3, No. 5, pp.363-376, 1999   1647 pre></body></html 


0-7695-2263-7/05 $20.00  2005 IEEE pre></body></html 


n M L N n t n t n t n t L M L t L t L tt L t kkkk kkkk kkk kkkk kkkkkkkkP VK VK VK VK PP       kkP t 31 where L  s the error covariance associated with the state estimate t i    kkLX  tt kkk P1  00 0  0                     s s sss s s sss s s sssss N n t n t n 


n t n N n t n t n t n N n t n t n t n t n t n c t L kkkkkk kkkkk kP VKVK VKVK  32 4. Simulations One has run simulations comparing the sequential implementations of MSJPDA algorithm and the new algorithm here. A typical multisensor multitarget tracking environment is assumed in the simulations. According to article [1,3], One known that the performance of sequential MSJPDA is better than the performance of parallel MSJPDA. Therefore, the performance of parallel MSJPDA algorithm will not be compared here There are three sensors, which are fixed in three platforms. Regarding the 2nd sensor as fusion centre situation of the other sensors are: =?-500m?-500m 0m??N =?-500m? 500m?0m??The distance error of each sensor is: =300m, =200m, =100m?The bear error of each sensor is 0.03rad, =0.02rad, =0.01rad?The of sample is T=1s?The nonparametric model of clutter is used in the simulations, and expected number of false measurement is m=1.8 1 sN 3 s 1r 2 2r 3 3r 1 Simulations have been run for racking two targets. The true initialization state of the targets is X1?[-29500m,400m/s,34500m,-400m/s X ?[-26250m,296m/s,34500m,-400m/s]'? 2 The two targets will cross above 31seconds later. To evaluate tracking performance, 50 Monte Carlo runs were performed for three case of the target detection probability Pd=0.97 ? Pd=0.76 ? Pd=0.58. In every run, the total simulation time is 140 steps 


simulation time is 140 steps            Figure 1  RMS position error in case of Pd=0.97          Figure 2  RMS velocity error in case of Pd=0.97       Figure 3  RMS position error in case of Pd=0.76 567 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005         Figure 4  RMS velocity error in case of Pd=0.76         Figure 5  RMS position error in case of Pd=0.58          Figure 6  RMS velocity error in case of Pd=0.58  Table 1 The emanative times comparison for sequential MSJPDA and SD-CMSJPDA algorithm  Pd N A  0.97 0. 76 0.58 Sequential MSJPDA 2 11 17 SD-CMSJPDA 0 3 5 Pd denotes detection probability, N denotes emanative 


Pd denotes detection probability, N denotes emanative times, A denotes the kind of algorithm Table 1 shows the summation of emanative times for sequential MSJPDA and SD-CMSJPDA algorithm in 50 Monte Carlo simulations. From table 1 , it is shown that the stability of SD-CMSJPDA is better than that of sequential MSJPDA as the detection probability varied Figure 1,2 show the RMS errors for position and velocity in case of Pd=0. 97, respectively; Figure 3,4 show the RMS errors for position and velocity in case of Pd=0.76 respectively; Figure 3,4 show the RMS errors for position and velocity in case of Pd=0.58, respectively. From the figures we can see that the average RMS position error is lower for the SD-CMS JPDA algorithm. We also see that the state estimation precision of sequential MSJPDA get worse as the detection probability decreases The reasons for these simulation results lies:1 state estimation precision will get worse when the detection probability decrease;2 algorithm is to process measurement from each sensor using single sensor JPDA algorithm sequentially. Therefore the estimation error from each sensor will be accumulated Moreover, the sequential MSJPDA algorithm can  t improve the joint detection probability of the multisensor system The estimation error of the SD- CMSJPDA  algorithm will not be accumulated for it processes the measurement from each sensor directly in the mean time .What  s more the new method can greatly improve the joint detection probability of the multisensor system. Therefore, the tracking performance of SD-CMSJPDA algorithm is better than that of sequential MSJPDA. Algorithm All of the simulations are run in the personal computer with a 2.0G CPU and a 256M memory. The average cost time per step is 0.0251 in the sequential implementations of MSJPDA algorithm. And the average cost time per step is 0.0282 in the sequential implementations of MSJPDA algorithm. According to the results we can see that there is few difference in real time between the new method and the sequential   MSJPDA when there is not so many sensors and targets 568 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005  5. Conclusion In order to solve the problem of multisensor multi target tracking, a new centralized multisensor  joint probabilistic data association  algorithm is proposed in this paper. The simulation results shows that the tracking performance of the new algorithm is better than that of the sequential MSJPDA algorithm The computational complexity of the new method will increase as the number of sensors and targets grow Therefore, how to improve the real time of SD- CMSJPDA algorithm will be pay attention References 1] He You, Wang Guohong, Lu Dajin, Peng Yingning Multisensor Information Fusion With Application[M Publishion House of Electronics Industry. 2000, Beijing.  [11] B..Zhou and N.K.Bose Multitarget  Tracking in Clutter:Faste Algorithms for Data Association .IEEE Transaction on Aerospace and Electronic Systems 1993,29\(2 2] Bar-shalom,Y\(Ed Applications and Advances,2: Norwood,MA Artech  House, 1992 3] L.Y. Pao, C.W.Frei. A Comparison of Parallel and Sequential Implementation of a Multisensor Multitarget Tracking Algorithm. Proc. 1995 American Control Conf. Seattie, Washington,June 1995 1683~1687 


4] K. Chang, C. Chong, Bar-Shalom, Joint Probabilistic Data Association in Distributed Sensor Networks IEEE Transactions on Automatic Control, AC-31\(10 octobre 1986 5] Bar-shalom, Multitarget Multisensor Tracking Advanced Application ,:YBS Publishing, 1990 6] Bar-shalom, Multitarget Multisensor Tracking Principles and Techenices ,:YBS Publishing, 1995 7] Hu Wenlong, Mao shiyi, Multisensor Data Association Based on Combinatorial Optimization[J]. Journal of Systems Engineering and Electronics . 1997 NO.1,1~9 8] Pattipati K R, Passive Multisensor Data Association Using a New Relaxation Algorithm.In Multitarget-Multisensor Tracking: Advanced and Applications,Y.Barshalom,Norwood,MA:Aretech,199 0 9] Deb S,et al, An S-Dimentional Assignment Algorithm for Track Initiation ,Proc. Of the IEEE Int. Conf Systems Engineering, Kobe, Japan,Sept 1992 527~530 10] Deb S,et al, A Multisensor-Multitarget Data Association Algorithm for Heterogeneous Sensors[J].IEEE Trans. on AES 1993 ,29 \(2 560~568 12] Bar-shalom,Y.,and Fortmann,T.E  Tracking and Data Association  New York:Academic press,1988 13] J,A,Roecher and G.L.Phillis,Suboptimal Joint Probabilistic Data Association .IEEE Transaction on Aerospace Electronic Systems.1993,29\(2 14] J,A,Roecker,A Class of Near Optimal JPDA Algorithm IEEE Transaction on Aerospace and Electronic Systems,1994,30\(2 15] Han Yanfei, Analysis and Improvement of Multisensor Multitarget Probabbilistic Data Association Filting Algorithm[J].  Journal of Systems Engineering and Electronics, 2002, Vol.24, 36~38  569 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





