Extended Multi-word Trigger Pair Language Model Using Data Mining Technique Yong Chen and Kwok-Ping Chan Department of Computer Science  Information Systems, University of Hang Kong Pokfulam Road Hang Kong yongchen kpchan csis.hku.hk Abstract  a good language model is essential to a postprocessing algorithm for recognition systems Trigger pair model has been used to investigate long distance dependent relationship However previous trigger pair model has only one wordfor its trigger It is desirable that more words can 
be observed in the trigger for a betfer prediction of the triggered word In this work we view estoblishing triggerpair model as mining association rules in a large database and create a multiple words trigger pair model by using an adapted Apriori algorithm The new trigger pair model can be used in the stage offinding best path from a word lattice as traditional trigger pair model can Specially it can be used tu correct mistakes remaining in the final result as well Those mistakes would be unavoidable for other language 
models Keywords pattern recognition Chinese recognition language model, data mining 1 Introduction Language models LM are commonly used in post processing algorithms of recognition systems to improve the recognition rate 2][4 During postprocessing a language model serves as a knowledge base to assist the recognition system to make a better decision N-gram model widely used in recognition systems reflects the relationship between current word and \(N-1 immediately preceding words Due to data sparseness, the largest value of N is usually 3 This limits ow 
view on the history In other words we have a near-sighted vision on the history On the other hand to make a better prediction of the cment word we need to know more about the history In previous works researchers have made much effort towards this goal In 9][3 Kneser and Bahl created topic dependent language model that is for each predefmed topic they generated a separate N-gram model. When using the language model first the topic of current text has to be determined since for different topic different N-gram pattem will be used Iyer 
and Ostendolf used a similar approach in 8 except that they capture sentence-level topic characteristics instead of word-level characteristics Bellegarda used SVD technique to decompose a word by document matrix and to construct a latent semantic language model in 6 The trigger pair technique suggested by Lau R lo represents another research effort along this direction. The basic idea in trigger pair mechanism is to investigate the Mutual Information MI between trigger pistory and triggered word the predicted word which reflects the degree of dependency between them This information can help the recognizer 
to pick a correct word from the candidate list in the postprocessing stage However in previous research works on trigger pair model there is a common weakness  the trigger and triggered pair are both single word This situation is due to the formidable computational workload in obtaining trigger pairs whose trigger andor triggered word are composed of multiple words In practice we are only interested in those trigger pairs whose trigger is composed of multiple words and whose triggered word is a single word With this kind of trigger pair we can predict the current word triggered word based 
on the knowledge of what words occurred previously \(multiple words trigger\in history. Hereinafter we refer to this kind of trigger pair as multiple words triggerpair We can use the model not only in picking the best path from a word lattice but also in correcting mistakes remaining in the best path Those mistakes would otherwise be unavoidable for other language model 2 Introduction of Trigger Pair Model The idea of trigger pair is actually motivated by the reading experience of human beings When a reader is reading an illegible or mistakenly typed word helshe can usually guess the correct word 
by referring to one or more words in the context highly associated with it Typical highly associated word pairs are bo th...and movie  star etc This kind of dependent relationship can be used in predicting the current word based on the knowledge of previously occurred and highly associated words Usually, a trigger pair can he denoted as 4  B  where A represents the trigger and B the triggered word The distance between the trigger and the triggered word 262  0-7803-7952-7/03/$17.00 0 2003 IEEE 


may exceed 3 words. This is very meaningful for we can use the trigger pair model as a complement to the N-gram model which has short view on the history 2.1 Window sue for trigger pair model In this study we determined an appropriate window sue of 6 by establishing a family of distanced big models justasin[l3][12 2.2 Selection of trigger pairs Usually we use formula 1 or 2 to select trigger pairs Formula \(1 only takes the extent of correlation between the trigger and the triggered word into account In contrast formula 2 takes both the extent of correlation and utility of the trigger pair into account In this study we use formula 2 as the criteria for selecting the most interesting trigger pairs into the model Audiences can refer to 13][12][1 for a detailed discussion about the two formulas Where PW  the probability of event X P\(x  1  P\(X  the probability that event A does not occur Where P\(X I r  the conditional probability of X given Y P\(X,Y  the joint probability of event X and event Y occur together We use AMI to select the most meaningful trigger pairs while use MI I to measure the actnal degree of the dependent relationship between the trigger and triggered word We will use this value in ow later application where the numerator is the joint probability of  and B the dominator is the product of probabilityP\(A and P\(B 2.3 Distance-related or Distance-unrelated Given a trigger and a triggered word the distance between the trigger and the triggered word may he different measured by the number of words between them When establishing a trigger pair model we can neglect the difference in distance We simply consider those trigger pairs, which share the same trigger and the same triggered word but have different interval distances to be the same This kind of trigger pair model is called Distance-unrelated trigger pair model Certainly the information provided by such kind of trigger pair model is less accurate, since the information contained in trigger pairs of different interval distance is mixed together. If we create separate trigger pair model for different interval distance then we can obtain more accurate information We call this kind of trigger pair model as Distance-related trigger pair model In this study we established a Distance-related trigger pair model 3 Multi-word Trigger Pair As previously explained the multiple words trigger pair will contain multiple words in the trigger For ease of understanding we use a Tri-word trigger pair as an example in the following explanation It can be represented as 4,Ai  B  4 appears in the second last sentence A appears in the last sentence B appears in the current sentence We can refer to 4 as the first trigger A as the second trigger and B as the triggered word Such an arrangement enables us to capture long distance dependent relationship We only picked those Tri-word sequences which occurred more than a predefined times over a huge amount of training text Those Tri-word sequences might not occur by chance and should he considered quite stable Hence we can use those Tri-word pattems to correct mistakes in a sentence The details on such kind of application will be presented in section 5 Our method for establishing the Tri-word trigger pair model discussed below, can easily be extended for creating more complex trigger pairs where there are more constituent words for the trigger and their locations in the context are free For example, they may appear in previous sentences or in the same sentence as the triggered word is To further explain the Tri-word trigger pair we use the following example to illustrate what a Tri-word trigger pair looks lie Assume three sentences form following scenario tic c:c;c c c:c  c;c c:c:c:-c c  coco   c,c,-c,o-c,o 0 a where C represents a single Chinese character a group of Cs standing together represents a word Words in a sentence are separated by the sign _ C represents the I th character of the m-th sentence We can form a Tri-word trigger pair by randomly picking up three words respectively from the second last sentence the middle sentence and the current sentence For instance CiC  CiC and C:Cp can form a Tri-word trigger pair CiC is the first trigger AA C:C:C the second trigger 4  and C:Cp the triggered word B There are 32,000 words in our vocabulary Theoretically the number of possible three-word sequences a Tri-word trigger pair may be viewed as a three-word sequence is 32,000x32,000x32,0003.2768x10 It is 263 


computationally infeasible to investigate such huge amount of trigger pairs to select best ones We get around the problem by using an adapted Apriori algorithm The Apriori algorithm is used by researchers in data mining field to discover association rules from a huge amount of database records 3.1 Apriori algorithm The main task for data mining researcher is to mine out valuable association rules from database. These des can be used to guide later activities See 7 for a detailed explanation about the Apriori algorithm WI, can generate two word sequence W W and WI Wl for C2 since the fmally obtained trigger pair is sequential We can produce the two-word sequence by sim 4 scan the training database to count the number Of occurrences of each word sequence in Cz One occurrence of W Wi means W auuears in the urevious sentence CI 3.2 Adapted Apriori algorithm Our goal is to fmd a Tri-word trigger pair The trigger consists of two words one located in the second last sentence AA  the other located in the last sentence  4  the triggered word B is located in current sentence. Such kind of trigger pair can be viewed as an association rule looks like A and Ai  B Hence it is possible for us to use Apriori algorithm which possesses good pruning capability during searching to discover Tri-word trigger pair in a huge amount of tex However we have to adapt it before using it We can think each word inthe vocabulary as the item in the transaction records each sentence as the transaction record in the database and the corpus as the database. One difference between the two cases is that trigger pair is sequential while itemset is not Thus we made some changes in the Apriori algorithm to fit the trigger pair case The procedure is described as follows. Note that we will use word sequence instead of trigger pair in the middle steps, since those three-word patterns are not trigger pairs yet in the middle steps 1 Initially each word in the vocabulary is a member of CI We scan the training data to count the number of occurrences of each word count The notation W,AW represents the pattern Wi  __ 1   meanwhile Wi appears in the current sentence WI 6000 5 Put word sequences with count over or equal to 20 into followed by wk with n sentences apart For the study in this LZ WI w2 WI w3 wz w4 200 w2 w5 200 6 We generate member candidates for C3 by joining members of LZ in a different way than that used processing transaction records First we pick one member in 4 say W WJ Then, we see ifthere is another word sequence in Lr whose fust word happens to be W say W wk If such a word exists we can form a three-word sequence Wi Wj Wk]\(the last word of Wi Wj is the same as the first word of W Wk to be a candidate for C3 At this moment it is just a candidate for C3 Next we check if the count for W  W meets the support count requirement  L I   I I W"1 I 10 2 Suppose that the support count is 20 and totally 5 words in CI is eli ible for L pi 6000 w4 2000 2700 3 Form CZ by making all possible combinations of the members in L1 Note every two members in LI say WI and paper n=2 W W represents Wi followed by Wk with 2 sentence apart For example Wi appears in the second last sentence and Wk appears in the current sentence If Wi W wk meets this condition it is qualified for C3 For this purpose we have to generate a model containing the information about W Wk We use to represent this model Following the same reasoning line, if there are Wi Wj wk} and W wk W in L3 the candidate Wi Wj wk W for C4 can be formed since the fwst two words of Wj wk W are the same as the last two words of W W Wk If TAW meets the requirement of support count then W W Wk, Wl is a true member for C4 If we want to determine if Wl W2  W is a member of 6 fmt we see if the Wl Wz  Wm and WZ W3  W are 264 


both member of L then see if the count of W 4 W meets the support count requirement This framework for member generation keeps the property of downward closure 5 as in the original Apriori algorithm The property of downward closure means all subsets of a frequent itemset of sue i+l would also be frequent For the trigger pair case downward closure means all subsequences of a frequent word sequence trigger pair of sue i+l would also be frequent I count 7 Count the number of occurrences of members in C3 C3 8 There are two members meet the requirement on support count and go into L3 WI w2 w3 w 9 w2 w4 9 There is no overlap between the two members in L3 Hence the iteration terminates After obtaining frequent three word sequences we use the average mutual information to pick out the most interesting ones Take the word sequence WI W WS as an example Unliie the case of processing transaction records where many association rules may be derived from a single frequent itemset there is only one possible Tri word trigger pair rule can be derived from a three-word sequence For example from Wl W2 W5 we have wl~w2 Here, the trigger is Wl W2 and the triggered word is WSl The reason we use AMI as a metric for trigger pair selection is it is a good measure of the expected benefit provided by the trigger in predicting the triggered word in terms of information  We modified the formula 3 to \(4 in order to calculate the AMI of the Tri word trigger pair As described in step 6 the adapted algorithm significantly reduce the number of trigger pairs whose AMI 265 would be evaluated therefore the computer workload is alleviated Actually the adatapted Apriori algorithm is figured out based on following fact sequence W W2  Wm.l and W2 W3  W  being frequent and w;*w meeting the support count requirement are necessary conditions. The necessary conditions are minimal requirement for Wl W2  Wm  being frequent We use these necessary conditions to reduce the search space To carry out the search space reducing operation in step 6 two suppolting language models are needed one is such a language model that can reflect the concurrence information of two words with one word in the previous sentence and the other word in the current sentence; the other supporting language model is such a language model that can reflect the concurrence information of two words with one in the second last sentence and the other word in the current sentence, that is L  For later reference we call the former supporting language model as Supporting Language Model A and the latter one Supporting Language Model B 4 Construction of Models We established the traditional trigger pair model, the Tri word trigger pair model and the word bigram N-gram model, N=2 model all over a IS million characters training text It contains a wide variety of topics such as economy fmce law technology, stock etc As mentioned in section 2 we constructed a distance related trigger pair model with a window size of 6 words We basically follow the traditio~l method to construct the distance-related trigger pair model except that we did not construct a trigger pair model for the case that the trigger is immediately followed by the triggered word i.e d=l This kind of information is already captured by the bi-gram language model Hence we only constructed these trigger pair models with distance of 2,3,4,5 or 6 words First we established distance4 2,3,4,5,6 word bigram models. Based on these models we then construct trigger pair models We rule out those distance4 bigram patterns which occur less than 2 times since they are not stable In other words they occur by chance in the training text Additionally We set a threshold of 0.1 for AMI That is only these trigger pairs whose AMI values are greater than 0.1 will be collected and their MI values will be calculated We follow the adapted Apriori algorithm to construct the Tri-word trigger pair model The file size of Supporting Language Model A and Supporting Language Model B are 92862Ohytes and 859535bytes containing 70298 and 65102 entries respectively. The size of the file containing C is 56M or so including 3,826,403 entries In our study the support count is 30 The threshold for AMI is 0.1 The file size of Tri-word trigger pair model is about SOMbytes including around 1.2 million entries The MI value for the Tri-word trigger pair is calculated according to formula 9 


The bi-gram model has a size of 6.5Mbytes and smoothed by the Katz smoothing technique 5 Application The function of the special Tri-word trigger pair is two-fold in our research First we combine it with traditional N-gram model to fmd M best paths from a word lattice when we apply Dynamic Programming to the word lattice \(see section 6 for explanation of word lattice In this application, the MI values of the Tri-word trigger pair are used After we get M best candidates we pick the top best one. Most likely some mistakes remain in it The Tri-word trigger pair is utilized again but in a different way to correct the remaining mistakes With the special Tri-word trigger pairs we can correct the mistakes in three different ways 1 Based on the 4 in the second last sentence and 4 in the last sentence, correct mistakes in current sentence 2 Based on Ai in the last sentence and B in current sentence correct mistakes in the second last sentence 3 Based on the A in the second sentence and B in current sentence correct mistakes in the last sentence 2 and 3 are very special and attractive since it allow us to correctmistakes in the history which would othenvise never be salvaged As a matter of fact the three ways of correctipg error are the very motivation for establishing such a Tri-word trigger pair model with the 3 words in the second last sentence the last sentence and the current sentence respectively In the following description we illustrate how to correct mistakes in the current sentence in way I As for correcting mistakes in way 2 or 3 the process is the same Actually we focus on those positions in the top best path where there are at least two successive single character words appearing We have found that 80 percent mis recognized characters in the top best path are single character words.. Hence we put our effort on salvaging these errors caused\222by single character words Assume the recognition results of the second last sentence the last sentence and the current sentence form following scenario ctc c:c:c c c:c c:c c:c;c c:c c:c c:cp  cc,oc d  c  G Ct Ct and C are consecutive three words First we focus on C  It is the sixth character in the current sentence and in the character lattice we have produced 12 candidates for each characters Now we pick top k candidates from the sixth column in our system k=2 or 3 Then we interrogakthe Tri-word trigger pair model to see whether there are any trigger pairs which meet following requirements a their triggered word is a 2-character or 3 chracter word b the fust character of their triggered word is one of the top k candidates c the first trigger and second trigger of these trigger pairs satisfying a and b will be matched with one word in the second last sentence and one word in the last sentence respectively The triggered word of these trigger pairs satisfying above 3 conditions is treated as word candidate Next we focus on Ct  Find out all possible word candidates for this position in the same way This example is about a scenario containing consecutive 3 words We stop at the character C,\223 since a multiple character word consists of at least two characters. If we are facing consecutive h words we will stop the process at the fi-l character In the end alI possible word candidates as well as original words like C:Cp C:C:Ct Ct  C and C  form a word lattice We find the best path from the word lattice by using the combined model of the N-gram model and the trigger pair model As we can imagine the word candidates suggested by the matched trigger pairs may contain character outside the 12 candidates produced by the recognizer since only the first character in these word candidates is required to he one of the fmt k candidates. Hence it is possible the f recognition rate may exceed the top 12 recognition rate as shown in OUT experiment. After fmishing one iteration of correction in the way of l 2 and 3 we can conduct another since new words may turn up after finishing previous iteration The above description is the process of the error correcting in way I The implementations of error correcting in way 2\and 3 are the same The following example is to illustrate how the error in the current sentence can be corrected in the way I Second last sentence Last sentence 5\200 d e a Current testing sentence EPq  iB W m s5i 8 Topbestpath EP 5J E iB 87 I m ZB 8 Correctedresult EP 5J  67  Btl ZS5i 8 Used Tri-word trigger pair I E W RE r;il H4k is G Ea 53 Note that the charxter 221\221$5\222\222 is salvaged even though it is not included in the 12 candidates for column 8 266 


6 Experiments Text Original System System System Top 12 rate Arate ph B rate Crate rate 82.56 89.64 90.16 91.37 91.42   AI I IB I System C outperfom System B by 1.21 percent\(ahsolute1y on text A by 1.04 percent\(ahsolute1y on text B System C outperfom System A by 1.73 percent\(ahsolute1y on text A and by 1.53 percent\(absolute1y on text B 7 Conclusion In this study we successfully introduce the data mining technique into the language modeling field and construct a multiple words trigger pair language model Our experiments show that the new language model can he used not only in iinding the hest path from a word lattice but also in correcting errors remaining in the fmal result By introducing the data minimg technique we propose a new research direction for language modeling References l Abramson Norman Information Theory And Coding McGRAW-HILL Book Company 1963 Z Bahl Lalit R Peter F Brown Peter V de Souze and Robert L Mercer 1989 223A tree-based statistical language model for natural language speech recognition\224 IEEE Transactions on Acoustics Speech and Signal Processing Vol 37, pp1001-1008 July 3 Babl Lalit R et al 223The IBM large vocabulary continuous speech recognition system for the ARPA NAB news task\224 in Proc ARF\222A Workshop on Spoken Language Technology 1995 pp.121-126 I 4 Brown Peter F Vincent J Della Pietra, Peter V de Son Jennifer C Lai and Robert L Mercer 1992 223Class-based n-gram models of natural language\224 Computational Linguistics Vol 18 No.4 pp467-479 SI Brin S Motwani R Silverstein C 223Beyond market basket Generalizing associationrules to correlations\224 In proc 1997 ACM-SIGMOD Int Conf Management of Data SIGMOD\22297\Tucsion AZ May 1997 pp265-276 6 Bellegarda Jeromo R 223Exploriting Latent Semantic Information in Statistical Language Model 224 Proceedings ofthe IEEE. Vol.88 N0.8 August 2OOO.pp1279-1296 7 Han Jiawei Kamber Micheline Data Mining Concepts and Techniques 2001 Morgan Kaufmann Publishers 8 Iyer Rukmini M Ostendorf, Mari 223Modeling long Distance Dependence in Language Topic Mixtures Versus Dynamic Cache Models\224 IEEE Trans on Speech and Audio Processing Vo1.7 No.1 pp30-39 9 Kneser R. and Steinbiss V 223On the dynamic adaption of stochastic LM\224 in Proc. Int Conf Acoustics Speech Signal Processing 1993.Vo1.2 pp.586-589 IOILau R 223Maximum likelihood maximum entropy trigger language model 224 Bachelor\222s Thesis Massachusetts Institute of Technology MA 1993 Ill Mallat Stephen G 223A theory for multiresolution signal decompostion: The wavelet representation\224 IEEE Trans on Pattern Analysis and Machine Intelligence Vol.11 N0.7. July 1989. pp673-693  Ronald 223A maximum entropy approach to adaptive statistical language model\224 Computer speech and language Vol.10 1996 pp.187-228 I31 Zhou GuoDong Lua KimTeng 223Interpolation of n gram and mutual-information based trigger pair language models for Mandarin speech recognition\224 Computer Speech and Language Vo1.13 1999 pp125-141 267 


rule mining can be used to overcome these disadvantages  7. Conclusions  This paper, discusses the application of data mining techniques within Web documents to discover what users want. It introduces the concept of decision patterns in order to interpret decision rules in terms of association mining. It has proved that any decision pattern is a closed pattern. It also presents a new concept of rough association rules to improve of the quality of text mining. To compare with the traditional association mining, the rough association rules include more specific information and can be updated dynamically to produce more effective results The distinct advantage of rough association rules is that they can take away some uncertainties from discovered knowledge through updating supports and weight distributions of association rules. It also demonstrates that the proposed approach gains a better performance on both precision and recall, and it is a considerable alternative of association rule mining This research is significant since it takes one more step further to the development of data mining techniques for Web mining  References   M. L. Antonie and O R. Zaiane, Text document categorization by term association, 2 nd  IEEE International Conference on Data Mining Japan, 2002, 19-26  G Cha n g, M  J  Healey J   A. M  M c Hugh a n d J   T L Wang Mining the World Wide Web: An information search approach Kluwer Academic Publishers, 2001   M   E i rinaki and M. Vazirgiannis  W e b  mining for web personalization ACM Transactions on Internet Technology  2003 3\(1 1-27 4  D A  Evans et a l   CL ARIT  e x pe ri ment s i n  bat c h filtering: term selection and th reshold optimization in IR and SVM Filters TREC02 2002  U Fay y a d G. P i atetsky Shapiro, P  S m y t h a n d R Uthrusamy, eds Advances in knowledge discovery and data mining Menlo Park, California: AAAI Press/ The MIT Press, 1996  R. F e ldman and H. Hirsh Mining as sociation s in text in presence of background knowledge, 2 nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 1996, 343-346  J  W  Guan D. A Bell D Y L i u T h e rough s et approach to association rules, 3 rd IEEE International Conference on Data Mining USA, 2003, 529-532 8 J  D Hol t a n d S  M. Chung, M u ltipas s algor ithms  for mining association rules in text databases Knowledge and Information Systems 2001 3 168-183  L i and B. Liu, Learning to classify texts using positive and unlabeled data IJCAI 2003, 587-592  i and N Zhong, W e b mining model and its applications on information gathering Knowledge-Based Systems 2004 17 207-217  N Zhong Ca pturing evolving patterns for ontology-based IEEE/WIC/ACM International Conference on Web Intelligence Beijing, China, 2004, 256-263  Z hong Interp retations of association rules by granular computing, 3 rd  IEEE International Conference on Data Mining USA, 2003, 593-596  i ning ontology fo r automatically  acquiring Web user information needs IEEE Transactions on Knowledge and Data Engineering 2006 18\(4 554-568   Li u Y  Da i X L i  W S. Lee, and P. S. Yu, Building text classifiers using positive and unlabeled examples, 3 rd  IEEE International Conference on Data Mining USA, 2003 179-186  J. Mo staf a  W   Lam  and M. Palakal A m u ltil e v el approach to intelligent information filtering: model, system and evaluation ACM Transactions on Information Systems  1997 15\(4 368-399   Pawlak In purs uit of patterns in data re as oning from data, the rough set way 3 rd International Conference on Rough Sets and Current Trends in Computing USA, 2002 1-9  Sebas tiani M a chin e  learning in autom a te d text categorization ACM Computing Surveys 2002 34\(1 1-47  z vetkov X Yan a nd J. Han, TSP: Mining top-K closed sequential patterns 3 rd IEEE International Conference on Data Mining USA, 2003, 347-354  S.T W u  Y  Li Y X u  B  P h am and P Ch en Automatic pattern taxonomy exatraction for Web mining IEEE/WIC/ACM International Conference on Web Intelligence Beijing, China, 2004, 242-248  H  Y u  J H an, and K  C h ang, P E B L  pos itive exam pl e  based learning for Web page classification using SVM KDD02 2002, 239-248  n Y. Fu M i ning M u ltiple-level Association Rules in Large Databases IEEE Transactions on Knowledge and Data Engineering 1999 11\(5 798-805  G I. Webb and S  Z hang, K-op timal rule discovery  Data Mining and Knowledge Discovery 2004 10 39-79  Y Yao Y Zhao a nd R.B. Maguire, Explanation oriented association mining using rough set theory 9 th  International Conference on Rough Sets, Fuzzy Sets, Data Mining and Granular Computing China, 2003, 165-172 


33 3 xyz3 Itemset SUPPORT xyz3Closed Itemset Equivalence Class abcd abce abcde abde acde abd abe ade 1 11 12 2 2 2 cb  a bdbc de bcd cde d e4 4 4 4 4 4 4 3 3 3 3 6 6 6 acd ace2 2 2 2 Cfreq CM a b Figure 3. Equivalence classes of frequency when intersected by a CAM \(a b  constraint Example 1 In Figure 3\(a itemsets lattice with the frequency equivalence classes and the border of the theory of CAM ? sum\(X.price have that I1 = {?bc, 6?} while on the other hand I2 = {?ac, 3?, ?bc, 6?, ?bd, 4?, ?cd, 4?, ?ce, 4? ?cde, 3 What has happened is that some equivalence classes have been cut by the CAM constraint. With interpretation I1 we mine closed frequent itemsets and then we remove those ones which do not satisfy the CAM constraint: this way we lose the whole information contained in those equivalence classes cut by the CAM constraint. On the other hand, according to interpretation I2, we mine the set of itemsets which satisfy the CAM constraint and then we compute the closure of such itemsets collection: thus, by de?nition, the itemsets bd and cd are solutions because they satisfy CAM and they have not a superset in the result set with the same support and satisfying the constraint Which one of the two di?erent interpretations is the most reasonable? It is straightforward to see that interpretation I1 is not a condensed representation since it loses a lot of information. In extreme cases it could output an empty solutions set even if there are many frequent itemsets which satisfy the given set of user-de?ned constraints. On the other hand, interpretation I2, which corresponds to the de?nition Cl\(FThD\(Cfreq[D,?] ? CAM representation of FThD\(Cfreq[D,?] ? CAM Observe that I2 is a superset of I1: it contains all 


Observe that I2 is a superset of I1: it contains all closed itemsets which are under the CAM border \(as I1 plus those itemsets which arise in equivalence classes which are cut by the CAM border \(such as for instance ce and cde in Figure 3\(a Proposition 3 Cl\(FThD\(Cfreq ? CAM FThD\(Cfreq CAM Let us move to the dual problem. In Figure 3\(b show the usual equivalence classes and how they are cut by CM ? sum\(X.prices are upward closed, we have no problems with classes which are cut: the maximal element of the equivalence class will be in the alive part of the class. In other words when we have a CM constraint, the two interpretations I1 and I2 correspond Proposition 4 Cl\(FThD\(Cfreq ? CM FThD\(Cfreq CM The unique problem that we have with this condensed representation, is that, when reconstructing FThD\(Cfreq[D,?] ? CM care of testing itemsets which are subsets of elements in Cl\(FThD\(Cfreq ? CM not to produce itemsets which are below the monotone border B+\(Th\(CM not need to access the transaction dataset D anymore Since we mine maximal itemsets of the equivalence classes it is impossible to avoid this problem, unless we store, together with our condensed representation the border B+\(Th\(CM closed itemset. This could be an alternative. However since closed itemsets provide a much more meaningful set of association rules, we consider a good tradeo? among performance, conciseness and meaningfulness the use of Cl\(FThD\(Cfreq?CM resentation Finally, if we use free sets instead of closed, we only shift the problem leading to a symmetric situation. Using free sets interpretations I1 and I2 coincide when dealing with anti-monotone constraints because minimal elements are not cut o? by the constraint \(e.g. de in Fig. 3\(a constraints \(e.g. no free solution itemsets in Fig. 3\(b Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 4. Algorithms In this Section we study algorithms for the computation of MP5. We ?rst discuss separately how monotone and anti-monotone constraints can be pushed in the computation, then we show how they can be exploited together by introducing the CCIMiner algorithm 4.1. Pushing Monotone Constraints Pushing CAM constraints deep into the frequent itemset mining algorithm \(attacking the problem FThD\(Cfreq[D,?] ? CAM  since they behave exactly as Cfreq . The case is di?erent for CM constraints, since they behave exactly the opposite of frequency. Indeed, CAM constraints can be used to e?ectively prune the search space to a small downward closed collection, while the upward closed collection of the search space satisfying the CM constraints cannot be exploited at the same time. This tradeo? holding on the search space of the computational problem FThD\(Cfreq[D,?] ? CM extensively studied [18, 9, 4], but all these studies have failed to ?nd the real synergy of these two opposite types of constraints, until the recent proposal of ExAnte [6]. In that work it has been shown that a real synergy of the two opposites exists and can be exploited by reasoning on both the itemset search space and the transactions input database 


set search space and the transactions input database together According to this approach each transaction can be analyzed to understand whether it can support any solution itemset, and if it is not the case, it can be pruned In this way we prune the dataset, and we get the fruitful side e?ect to lower the support of many useless itemsets, that in this way will be pruned because of the frequency constraint, strongly reducing the search space. Such approach is performed with two successive reductions  reduction \(based on monotonicity reduction \(based on anti-monotonicity to  reduction we can delete transactions which do not satisfy CM , in fact no subset of such transactions satis?es CM and therefore such transactions cannot support any solution itemsets. After such reduction, a singleton item may happen to become infrequent in the pruned dataset, an thus it can be deleted by the ?reductions. Of course, these two step can be repeated until a ?xed point is reached, i.e. no more pruning is possible. This simple yet very e?ective idea has been generalized in an Apriori-like breadth-?rst computation in ExAMiner [5], and in a FP-growth [10] based depth-?rst computation in FP-Bonsai [7 Since in general depth-?rst approaches are much more e?cient when mining closed itemsets, and since FP-Bonsai has proven to be more e?cient than ExAMiner, we decide here to use a FP-growth based depth?rst strategy for the mining problem MP5. Thus we combine Closet [16], which is the FP-growth based algorithm for mining closed frequent itemset, with FPBonsai, which is the FP-growth based algorithm for mining frequent itemset with CM constraints 4.2. Pushing Anti-monotone Constraints Anti-monotone constraints CAM can be easily pushed in a Closet computation by using them in the exact same way as the frequency constraint, exploiting the downward closure property of antimonotone constraints. During the computation, as soon as a closed itemset X s.t  CAM \(X ered, we can prune X and all its supersets by halting the depth ?rst visit. But whenever, such closed itemset X s.t  CAM \(X e.g. bcd in Figure 3\(a some itemsets Y ? X belonging to the same equivalence class and satisfying the constraint may exist \(e.g bd and cd in Figure 3\(a ery such X in a separate list, named Edge, and after the mining we can reconstruct such itemsets Y by means of a simple top-down process, named Backward-Mining, described in Algorithm 1 Algorithm 1 Backward-Mining Input: Edge, C, CAM , CM C is the set of frequent closed itemsets CAM is the antimonotone constraint CM is a monotone constraint \(if present Output: MP5 1: MP5 = C split Edge by cardinality 2: k = 0 3: for all c ? Edge s.t. CM \(c 4: E|c| = E|c| ? {c 5: if \(k &lt; |c 6: k=c generate and test subsets 7: for \(i = k; i &gt; 1; i 8: for all c ? E|i| s.t. CM \(c 9: for all \(i? 1 10: if  Y ?MP5 | s ? Y 11: if CAM \(s 12: MP5 = MP5 ? s 13: else 


13: else 14: E|i?1| = E|i?1| ? s The backward process in Algorithm 1, generates level-wise every possible subset starting from the borProceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE der de?ned by Edge without getting into equivalence classes which have been already mined \(Line 10 such subset satis?es the constraint then it can be added to the output \(Line 12 reused later to generate new subsets \(Line 14 have a monotone constraint in conjunction, the backward process is stopped whenever the monotone border B+\(Th\(CM Lines 3 and 8 4.3. Closed Constrained Itemsets Miner The two techniques which have been discussed above are independent. We push monotone constraints working on the dataset, and anti-monotone constraints working on the search space. It  s clear that these two can coexist consistently. In Algorithm 2 we merge them in a Closet-like computation obtaining CCIMiner Algorithm 2 CCIMiner Input: X,D |X , C, Edge,MP5, CAM , CM X is a closed itemset D |X is the conditional dataset C is the set of closed itemsets visited so far Edge set of itemsets to be used in the BackwardMining MP5 solution itemsets discovered so far CAM , CM constraints Output: MP5 1: C = C ?X 2: if  CAM \(X 3: Edge = Edge ?X 4: else 5: if CM \(X 6: MP5 = MP5 ?X 7: for all i ? flist\(D |X 8: I = X ? {i} // new itemset avoid duplicates 9: if  Y ? C | I ? Y ? supp\(I Y then 10: D |I= ? // create conditional fp-tree 11: for all t ? D |X do 12: if CM \(X ? t 13: D |I= D |I ?{t |I  reduction 14: for all items i occurring in D |I do 15: if i /? flist\(D |I 16: D |I= D |I \\i // ?-reduction 17: for all j ? flist\(D |I 18: if supD|I \(j I 19: I = I ? {j} // accumulate closure 20: D |I= D |I \\{j 21: CCIMiner\(I,D |I , C,B,MP5, CAM , CM 22: MP5 = Backward-Mining\(Edge,MP5, CAM , CM For the details about FP-Growth and Closet see [10 16]. Here we want to outline three basic steps 1. the recursion is stopped whenever an itemset is found to violate the anti-monotone constraint CAM Line 2 2  and ? reductions are merged in to the computation by pruning every projected conditional FPTree \(as done in FP-Bonsai [7 Lines 11-16 3. the Backward-Mining has to be performed to retrieve closed itemsets of those equivalence classes which have been cut by CAM \(Line 22 5. Experimental Results The aim of our experimentation is to measure performance bene?ts given by our framework, and to quantify the information gained w.r.t. the other lossy approaches 


approaches All the tests were conducted on a Windows XP PC equipped with a 2.8GHz Pentium IV and 512MB of RAM memory, within the cygwin environment. The datasets used in our tests are those ones of the FIMI repository1, and the constraints were applied on attribute values \(e.g. price gaussian distribution within the range [0, 150000 In order to asses the information loss of the postprocessing approach followed by previous works, in Figure 4\(a lution sets given by two interpretations, i.e. |I2 \\ I1 On both datasets PUMBS and CHESS this di?erence rises up to 105 itemsets, which means about the 30 of the solution space cardinality. It is interesting to observe that the di?erence is larger for medium selective constraints. This seems quite natural since such constraints probably cut a larger number of equivalence classes of frequency In Figure 4\(b built during the mining is reported. On every dataset tested, the number of FP-trees decrease of about four orders of magnitude with the increasing of the selectivity of the constraint. This means that the technique is quite e?ective independently of the dataset Finally, in Figure 4\(c of our algorithm CCIMiner w.r.t. Closet at di?erent selectivity of the constraint. Since the post-processing approach must ?rst compute all closed frequent itemsets, we can consider Closet execution-time as a lowerbound on the post-processing approach performance Recall that CCIMiner exploits both requirements \(satisfying constraints and being closed ing time. This exploitation can give a speed up of about to two orders of magnitude, i.e. from a factor 6 with the dataset CONNECT, to a factor of 500 with the dataset CHESS. Obviously the performance improvements become stronger as the constraint become more selective 1 http://fimi.cs.helsinki.fi/data Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Information loss Number of FP-trees generated Run time performance 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 10 5 10 6 m I 2 I 1  PUMSB@29000 CHESS @ 1200 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 6 10 


10 1 10 2 10 3 10 4 10 5 10 6 10 7 m n u m b e r o f fp t re e s PUMSB @ 29000 CHESS @ 1200 CONNECT@11000 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 m e x e c u ti o n  ti m e  s e c  CCI Miner  \(PUMSB @ 29000 closet         \(PUMSB @ 29000 CCI Miner  \(CHESS @ 1200 closet         \(CHESS @ 1200 CCI Miner  \(CONNECT @ 11000 closet         \(CONNECT @ 11000 a b c Figure 4. Experimental results with CAM ? sum\(X.price 6. Conclusions 


6. Conclusions In this paper we have addressed the problem of mining frequent constrained closed patterns from a qualitative point of view. We have shown how previous works in literature overlooked this problem by using a postprocessing approach which is not lossless, in the sense that the whole set of constrained frequent patterns cannot be derived. Thus we have provided an accurate de?nition of constrained closed itemsets w.r.t the conciseness and losslessness of this constrained representation, and we have deeply characterized the computational problem. Finally we have shown how it is possible to quantitative push deep both requirements \(satisfying constraints and being closed process gaining performance bene?ts with the increasing of the constraint selectivity References 1] R. Agrawal, T. Imielinski, and A. N. Swami. Mining association rules between sets of items in large databases In Proceedings ACM SIGMOD, 1993 2] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules in LargeDatabases. InProceedings of the 20th VLDB, 1994 3] R. J. Bayardo. E?ciently mining long patterns from databases. In Proceedings of ACM SIGMOD, 1998 4] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Adaptive Constraint Pushing in frequent pattern mining. In Proceedings of 7th PKDD, 2003 5] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi ExAMiner: Optimized level-wise frequent pattern mining withmonotone constraints. InProc. of ICDM, 2003 6] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Exante: Anticipated data reduction in constrained pattern mining. In Proceedings of the 7th PKDD, 2003 7] F. Bonchi and B. Goethals. FP-Bonsai: the art of growing and pruning small fp-trees. In Proc. of the Eighth PAKDD, 2004 8] J. Boulicaut and B. Jeudy. Mining free itemsets under constraints. In International Database Engineering and Applications Symposium \(IDEAS 9] C. Bucila, J. Gehrke, D. Kifer, and W. White DualMiner: A dual-pruning algorithm for itemsets with constraints. In Proc. of the 8th ACM SIGKDD, 2002 10] J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In Proceedings of ACM SIGMOD, 2000 11] L.Jia, R. Pei, and D. Pei. Tough constraint-based frequent closed itemsets mining. In Proc.of the ACM Symposium on Applied computing, 2003 12] H. Mannila and H. Toivonen. Multiple uses of frequent sets and condensed representations: Extended abstract In Proceedings of the 2th ACM KDD, page 189, 1996 13] R. T. Ng, L. V. S. Lakshmanan, J. Han, and A. Pang Exploratory mining and pruning optimizations of constrained associations rules. In Proc. of SIGMOD, 1998 14] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering frequent closed itemsets for association rules In Proceedings of 7th ICDT, 1999 15] J.Pei, J.Han,andL.V.S.Lakshmanan.Mining frequent item sets with convertible constraints. In \(ICDE  01 pages 433  442, 2001 16] J. Pei, J. Han, and R. Mao. CLOSET: An e?cient algorithm formining frequent closed itemsets. InACMSIGMODWorkshop on Research Issues in Data Mining and Knowledge Discovery, 2000 17] J. Pei, J. Han, and J. Wang. Closet+: Searching for the best strategies for mining frequent closed itemsets. In SIGKDD  03, August 2003 18] L. D. Raedt and S. Kramer. The levelwise version space algorithm and its application to molecular fragment ?nding. In Proc. IJCAI, 2001 


ment ?nding. In Proc. IJCAI, 2001 19] R. Srikant, Q. Vu, and R. Agrawal. Mining association rules with item constraints. In Proceedings ACM SIGKDD, 1997 20] M. J. Zaki and C.-J. Hsiao. Charm: An e?cient algorithm for closed itemsets mining. In 2nd SIAM International Conference on Data Mining, April 2002 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


4] K. Chang, C. Chong, Bar-Shalom, Joint Probabilistic Data Association in Distributed Sensor Networks IEEE Transactions on Automatic Control, AC-31\(10 octobre 1986 5] Bar-shalom, Multitarget Multisensor Tracking Advanced Application ,:YBS Publishing, 1990 6] Bar-shalom, Multitarget Multisensor Tracking Principles and Techenices ,:YBS Publishing, 1995 7] Hu Wenlong, Mao shiyi, Multisensor Data Association Based on Combinatorial Optimization[J]. Journal of Systems Engineering and Electronics . 1997 NO.1,1~9 8] Pattipati K R, Passive Multisensor Data Association Using a New Relaxation Algorithm.In Multitarget-Multisensor Tracking: Advanced and Applications,Y.Barshalom,Norwood,MA:Aretech,199 0 9] Deb S,et al, An S-Dimentional Assignment Algorithm for Track Initiation ,Proc. Of the IEEE Int. Conf Systems Engineering, Kobe, Japan,Sept 1992 527~530 10] Deb S,et al, A Multisensor-Multitarget Data Association Algorithm for Heterogeneous Sensors[J].IEEE Trans. on AES 1993 ,29 \(2 560~568 12] Bar-shalom,Y.,and Fortmann,T.E  Tracking and Data Association  New York:Academic press,1988 13] J,A,Roecher and G.L.Phillis,Suboptimal Joint Probabilistic Data Association .IEEE Transaction on Aerospace Electronic Systems.1993,29\(2 14] J,A,Roecker,A Class of Near Optimal JPDA Algorithm IEEE Transaction on Aerospace and Electronic Systems,1994,30\(2 15] Han Yanfei, Analysis and Improvement of Multisensor Multitarget Probabbilistic Data Association Filting Algorithm[J].  Journal of Systems Engineering and Electronics, 2002, Vol.24, 36~38  569 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





