A High-Performance Distributed Algorithm for Mining Association Rules 000 Assaf Schuster Ran Wolff and Dan Trock Technion 226 Israel Institute of Technology Email 001 assaf,ranw,dtrock 002 cs.technion.ac.il Abstract We present a new distributed association rule mining D-ARM algorithm that demonstrates superlinear speedup with the number of computing nodes The algorithm is the 336rst D-ARM algorithm to perform a single scan over the database As such its performance is unmatched by 
any previous algorithm Scale-up experiments over standard synthetic benchmarks demonstrate stable run time regardless of the number of computers Theoretical analysis reveals a tighter bound on error probability than the one shown in the corresponding sequential algorithm 1 Introduction The economic value of data mining is today well established Most large organizations regularly practice data mining techniques One of the most popular techniques is association rule mining ARM which is the automatic discovery of pairs of element sets that tend to appear together 
in a common context An example would be to discover that the purchase of certain items say tomatoes and lettuce in a supermarket transaction usually implies that another set of items salad dressing is also bought in that same transaction Like other data mining techniques that must process enormous databases ARM is inherently disk-I/O intensive These I/O costs can be reduced in two ways by reducing the number of times the database needs to be scanned or through parallelization by partitioning the database between several machines which then perform a distributed ARM D-ARM algorithm In recent years much progress 
has been made in both directions The main task of every ARM algorithm is to discover the sets of items that frequently appear together 226 the frequent itemsets The number of datab ase scans required for the task has been reduced from a number equal to the size of 003 This work was supported in part by Microsoft Academic Foundation and by THE ISRAEL SCIENCE FOUNDATION founded by the Israel Academy of Sciences and Humanities the largest itemset in Apriori 3  t o t y p i cally ju st a s in g l e scan in modern ARM algorithms such as Sampling and DIC 
17 Much progress has also been made in parallelized algorithms With these the architecture of the parallel system plays a key role For instance many algorithms were proposed which take advantage of the fast interconnect or the shared memory of parallel computers The latest development with these is 18 in which each process m ak es just two passes over its portion of the database Parallel computers are however very costly Hence although these algorithms were shown to scale up to 128 processors few organizations can afford to spend such resources on data mining The alternative is distributed al 
gorithms which can be run on cheap clusters of standard off-the-shelf PCs Algorithms suitable for such systems include the CD and FDM algorithms 2 6 bot h p aral lelized versions of Apriori which were published shortly after it was described However while clusters may easily and cheaply be scaled to hundreds of machines these algorithms were shown not to scale well 15 The DDM algorithm 15 wh ich o v e rco m es th is scalab ility p r o b l em was recently described Unfortunately all the D-ARM algorithms for share-nothing m achines scan the database as many times as Apriori Since m 
any business databases contain large frequent itemsets long patterns these algorithms are not competitive with DIC and Sampling In this work we present a parallelized version of the Sampling algorithm called D-Sampling The algorithm is intended for clusters of share-nothing machines The main obstacle of this parallelization that of achieving a coherent view of the distributed sample at reasonable communication costs was overcome using ideas taken from DDM Our distributed algorithm scans the database once just like the Sampling algorithm and is thus more ef\036cient than any 
D-ARM algorithm known today Not only does this algorithm divide the disk-I/O costs of the single scan by partitioning the database among se veral machines but also uses the combined memory to linear ly increase the size of the sample This increase further improves the performance of the algorithm because the safet y margin required in Sam1 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


pling decreases when the globa l sample size increases Extensive experiments on standard synthetic benchmarks show that D-Sampling is superior to previous algorithms in every way When compared to Sampling 226 one of the best sequential algorithms known today 226 it offers superlinear speedup When compared to FDM it improves runtime by orders of magnitude Finally on scalability tests an increase in both the number of computing nodes and the size of the database does not degrade D-Sampling performance FDM on the other hand suffers performance degradation in these tests The rest of this paper is structured as follows We conclude this section with some notations and a formal definition of the D-ARM problem In the next section we present relevant previous work Section 3 describes the DSampling algorithm and section 4 provides the required statistical background Section 5 describes the experiments we conducted to verify D-Sampling performance We conclude with some open research problems in section 6 1.1 Notation and Problem De\336nition Let 000 002 004 006 b n 006 r n 017 017 017 n 006 022 024 be the items in a certain domain An itemset is a subset of 000 A transaction 025 is also a subset of 000 which is associated with a unique transaction identi\036er 226 026 000 031  A database 031 033 is a list of such transactions Let 031 033 002 036 031 033 b n 031 033 r n 017 017 017 n 031 033   be a partition of 031 033 into  parts Let  be a list of transactions which were sampled uniformly from 031 033 andlet  002 036  b n  r n 017 017 017 n    be the partition of  induced by 031 033  For any itemset  and any group of transactions       1 3 025 5  n  9 is the number of transactions in  which contain all the items of  and  3   5  n  9 002 A B C C E F G I J L M N O M O  We call  3   Q  n 031 033 S U the local frequency of  in partition 006 and  3   5  n 031 033 9 its global frequency  likewise we call  3   Q  n  S U the estimated local frequency of  in partition 006 and  3   5  n  9 its estimated global frequency  For some frequency threshold Z   006   3    a we saythatanitemset  is frequent in  if  3   5  n  9 c  006   3   and infrequent otherwise If  is a sample we say that  is estimated frequent or estimated infrequent  If  is a partition we say that  is locally frequent and if  is the whole database then  is globally frequent  Hence an itemset may be estimated locally frequent in the d G e partition globally infrequent etc The group of all itemsets with frequency above or equal to f 3 in  is called h i F j  k The negative border of h i F j  k is all those itemsets which are not themselves in h i F j  k but have all their subsets in h i F j  k  Finally for a pair of globally frequent itemsets  and m such that  n m 002 q  and some con\036dence threshold Z s  006  u 1  f  a wesaytherule  x m is con\336dent if and only if  3   5   m n 031 033 9 c  006  u 1  f   3   5  n 031 033 9  De\336nition 1 Given a partitioned database 031 033  and given  006   3   and  006  u 1  f  the D-ARM problem is to 336nd all the con\336dent rules between frequent itemsets in 031 033  2 Previous Work Since its introduction in 1993 1 t h e A R M probl em has been studied intensively Ma ny algorithms representing several different approaches were suggested Some algorithms such as Apriori Partition DHP IC and FP-growth 3 14 11 5 8 are bottom-up s t arting from items ets o f size a and working up Others like Pincer-Search 10  u s e a hybrid approach trying to guess large itemsets at an early stage Most algorithms including those cited above adhere to the original problem de\036nition while others search for different kinds of rules 5 16 13  Algorithms for the D-ARM problem usually can be seen as parallelizations of sequential ARM algorithms The CD FDM and DDM 2 6 15 alg o r ith ms p a rallelize A p r io ri 3 and P DM 12  parallelizes DHP 11   T he major d ifference between parallel algorithms is in the architecture of the parallel machine This may be shared memory as in the case of 18 di s t ri b u t e d s hared m emory a s i n  9 o r s hared nothing as in 2 6 15  One of the best sequential ARM algorithms 226 Sampling 226 was presented in 1996 by Toivonen 17 The i dea b ehind Sampling is simple A random sample of the database is used to predict all the frequent itemsets which are then validated in a single databas e scan Because this approach is probabilistic and therefore fallible not only the frequent itemsets are counted in the scan but also their negative border If the scan reveals that itemsets that were predicted to belong to the negative border are frequent then a second scan is performed to discover whether any superset of these itemsets is also frequent To further reduce the chance of failure Toivonen suggests that mining be performed using some 200 1 201 f 3 s  006   3    and the results reported only if they pass the original  006   3   threshold He also gives a heuristic which can be used to determine 200 1 201 f 3 Thecost of using 200 1 201 f 3 is an increase in the number of candidates The Sampling algorithm and the DIC algorithm Brin 1997 5 are t he onl y s i ngl e-s can AR M a l gori t h ms kno wn t oday  The performance of the two is thus unrivaled by any other sequential ARM algorithm The algorithm presented here combines ideas from several groups of algorithms It 036rst mines a sample of the database and then validates the result and can thus be seen as a parallelization of the Sampling algorithm 17 Th e sample is stored in a vertical tr ie structure that resembles the one in 14 4  a nd i t i s mi ned u s i ng modi 036 cat i ons of t h e DDM 15 algorithm which i s A priori-bas ed Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


3 D-Sampling Algorithm All distributed ARM algorithms that have been presented until now are Apriori based and thus require multiple database scans The reason why no distributed form of Sampling was suggested in the six years since its presentation may lie in the communication complexity of the problem As we have seen the communication complexity of D-ARM algorithms is highly dependent on the number of candidates and on the noise level in the partitioned database When Sampling reduces the database through sampling and lowers the 000 002 003 004 005 007 t threshold it greatly increases both the number of candidates and the noise level This may render a distributed algorithm useless This is the reason that the r educed communication complexity of DDM seems to offer an opportunity The main idea of D-Sampling is to utilize DDM to mine a distributed sample using n f 016 017 005 instead of 000 002 003 004 005 007 t After 021 023 024 026 027 030 032 033 035 has been identi\036ed the partitioned database is scanned once in parallel to 036nd the actual frequencies of 021 023 024 026 027 030 032 033 035 and its negative border Those frequencies can then be collected and rules can be generated from itemsets more frequent than 000 002 003 004 005 007 t  We added three modi\036cations to this scheme First although the given DDM is levelwise here it is executed on a memory resident sample Thus we could modify DDM to develop new itemsets on-the-\037y and calculate their estimated frequency with no disk-I/O Second a new method for the reduction of 000 002 003 004 005 007 t to n f 016 017 005 yielded two additional bene\036ts it is not heuristic i.e our error bound is rigorous and it produces many less candidates than the rigorous method suggested previously Third after scanning the database it would not be wise to just collect the frequencies of all candidates Since these candidates were calculated according to the lowered threshold few of them are expected to have frequencies above the original 000 002 003 004 005 007 t  Instead we run DDM once more to decide which candidates are frequent and which are not We call the modi\036ed algorithm D-Sampling Algorithm 1 3.1 Algorithm D-Sampling begins by loading a sample into memory The sample is stored in a trie 226 a lexicographic tree This trie is the main data structure of D-Sampling and is accessed by all its subroutines Each node of the trie stores in addition to structural information parents descendants etc the list of 036   s of those transactions that include the itemset associated with this node These lists are initialized from the sample for the 036rst level of the trie when a new trie node 226 and itemset 226 are developed the 036   lists of two of the parent nodes are intersected to create the 036   list of the new node The 036rst step of D-Sampling is to run a modi\036cation of DDM on the distributed sample Then in order to set n f 016 017 005  the algorithm enters a loop in each cycle through the loop it calls another DDM derivative called M-Max to mine the next 000 estimated-frequent itemsets 000 is a tunable parameter we set to about     After it 036nds those additional itemsets D-Sampling reduces n f 016 017 005 to the estimated frequency of the least frequent one and re-estimates the error probability using a formula described in section 4 When this probability drops below the required error probability the loop ends Then D-Sampling creates the 036nal candidate set  by adding to 021 023 024 026 027 030 032 033 035 its negative border Algorithm 1 D-Sampling For node 002 out of 003 Input 000 002 003 004 005 007 t  000 002 003  f 003 017    0  1  000  2  The set of con\036dent associations between globally frequent itemsets Main Set 3 007 005 005 f 005 5   n f 016 017 005 5 000 002 003 004 005 007 t Load a sample 033 0 of size 1 from   0 into memory Initialize the trie with all the size itemsets and calculate their 036   lists 021 023 024 026 027 030 032 033 035 5 000   000  000 002 003 004 005 007 t A While 3 007 005 005 f 005 D 2 1 021 023 024 026 027 030 032 033 035 5 021 023 024 026 027 030 032 033 035 F 000 000 H I  000 A 2 Set n f 016 017 005 to the frequency of the least frequent itemset in 021 023 024 026 027 030 032 033 035 3 Set 3 007 005 005 f 005 to the new error bound according to 000 002 003 004 005 007 t  n f 016 017 005 and 021 023 024 026 027 030 032 033 035 Let  be 021 023 024 026 027 030 032 033 035 F L 007 N H P 002 S 007  f 005 U 007 005 V 021 023 024 026 027 030 032 033 035 W Scan the database and compute 004 005 007 t V X Y   0 W for each X    Update the frequencies in the trie to the computed ones Compute 021  0  _ 030  a 032   035 by running 000   000  000 002 003 004 005 007 t A  this time with the actual frequencies If exists X  021  0  _ 030  a 032   035 such that X f 021 023 024 026 027 030 032 033 035 i.e from negative border report failure g 007 003 h i n 007 1 V 021  0  _ 030  a 032   035 Y 000 002 003  f 003 017 W Once the candidate set is established each partition of the database is scanned exactly once and in parallel and the actual frequencies of each candidate are calculated With these frequencies D-Sampling performs yet another round of the modi\036ed DDM In this round the original 000 002 003 004 005 007 t is used thus unless there is a failure this round should never develop a candidate which is outside the negative borProceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


der If indeed no failure occurs then all frequent itemsets will be evaluated according to the actual frequencies which were found in the database scan Hence after this round it is known which of the candidates in 000 are globally frequent and which are not In this cas e rules are generated from 001 003 004 006 b t 013 r 017 020 022 023 using the known global frequencies If an itemset belonging to the negative border of 001 024 025 027 030 t 017 031 023 does turn out to be frequent this means that DSampling has failed a superset of that candidate which was not counted might also turn out to be frequent In this case we suggest the same solution offered by Toivonen to create a group of additional candidates that includes all combinations of anticipated and unantic ipated frequent itemsets and then perform an additional scan The size of this group is limited by the number of anticipated frequent itemsets times the number of possible combinations of unanticipated frequent itemsets Since failures are very rare events and the probability of multiple failure is exponentially small the additional scan will incur costs that are of the same scale as the 036rst scan 3.2 DDM 320 A Modi\336ed Distributed Decision Miner The original DDM algorithm as described in 15 i s l e velwise When the database is small enough to 036t into memory the levelwise structure of the algorithm becomes super\037uous Modi\036ed Distributed Decision Miner or MDDM Algorithm 2 therefore starts by developing all the locally frequent candidates regardless of their size It then continues to develop candidates whenever they are required i.e when all their subsets are a ssumed frequent according to the local hypothesis 032  or when another node refers to the associated itemset The remaining steps in MDDM are the same as in DDM Each party looks for itemsets for which the global hypothesis and local hypothesis disagree and communicate their local counts to the rest of the parties When no such itemset exists the party passes it can return to activity if new information arrives If all of the parties pass the algorithm terminates and the itemsets which are predicted to be frequent according to the public hypothesis 033 are the estimated globally frequent ones If a message is received for an itemset which has not yet been developed it is developed on-the-\037y and its local frequency is calculated 3.3 M-Max Algorithm The modi\036ed DDM algorithm identi\036es all itemsets with frequency above 034 036 037      D-Sampling however requires a further decrease in the frequency of itemsets which are included in the database scan The reason for this as we shall see in section 4 is that three parameters affect the Algorithm 2 Modi\036ed Distributed Decision Miner For node 036 out of 037 Input   226 the target frequency  001 030 t 017 031 023 De\336nitions 032    031 004    1 3 4 5 7 8 8 031  8 8        031    031     1 3 4 5 7 8 8 031  8 8        031 004   031  033 C  F I J K M  1 3 4 5 7 8 8 031  8 8        031   M  1 3 4 5 7  031   G C  F SU V W X Y    036   Main Develop all the candidates which are more frequent than   according to 032 Do _ Choose a candidate  that was not yet chosen and for which either 033 C  F b   d 032    031 004  or 032    031 004  b   d 033 C  F _ Broadcast i l 036 n C  F         031 004  p _ If no such itemset exists broadcast q s t   v Until  032 t    n  x y z all  with 033 C  F    Return y When node 036 receives a message i from party   1 If i q s t   v insert  into 032 t    n 2 Else i l 036 n C  F         031   p If  204 032 t    n remove  from 032 t    n If  was not developed then develop it set 206 C  F U  Calculate  207 X 036 n 210 036  X by intersecting the 211 212 020 lists of two of  222s immediate subsets and set        031 004  215 5 216 217 004 221 024 004 222 217 215 215 223 225 215 Insert  to 206 C  F Recalculate 033 C  F and 032    031 004  Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


chances for failure These are the size of the sample 000  the size of the negative border and the estimated frequency of the least frequent candidate The 036rst parameter is given the second is a rather arbitrary value which we can calculate or bound and the last parameter is the one we can control The frequency of the least frequent candidate can be controlled by reducing 001 003 005 006 b  However this must be done with care lowering the frequency threshold increases the number of candidates This increase depends on the distribution of itemsets in the database and is therefore nondeterministic The larger number of candidates affects the scan time the more candidates you have the more comparisons must be made per transaction In a distributed setting the number of candidates is also strongly tied to the communication complexity of the algorithm To better control the reduction of 001 003 005 006 b  we propose another version of DDM called M-Max Algorithm 3 MMax increases the number of frequent itemsets by a given factor rather than decreasing t he threshold value by an arbitrary value Although worst case analysis shows that an increase of even one frequent ite mset may require that any number of additional candidates be considered the number of such candidates tends to remain small and roughly proportional to the number of additional frequent itemsets We complement this algorithm with a new bound for the error presented in section 4 The combined scheme is both rigorous and economical in the number of candidates The M-Max algorithm is based on the inference that changing the n f r 016 b 020 021 threshold to the 022 value of the n largest itemset 1 every time an itemset is developed or a hypothesis value is changed will result in all parties agreeing on the n most frequent itemsets when DDM terminates This is easy to prove Take any 036nal state of the modi\036ed algorithm The 022 value of each itemset is equal in all parties hence the 036nal n f r 016 b 020 021 is equal in all parties as well Now compare this state to the corresponding state under DDM with the static n f r 016 b 020 021 value set to the one 036nally agreed upon The state attained by M-Max is also a valid 036nal state for this DDM Thus by virtue of DDM correctness all parties must be in agreement on the same set of frequent itemsets As a stand-alone ARM algorithm M-Max may be impractical because a node may be required to refer to itemsets it has not yet developed If the database is large this would require an additional disk scan whenever new candidates are developed Nevertheless at the 001 003 005 006 b correction stage of D-Sampling the database is the memory-resident sample It is thus possible to evaluate the frequency of arbitrary itemsets with no disk-I/O 1 023 is used when the 024 largest 025 is zero Algorithm 3 M-Max For node f out of r Input 001 003 005 006 b  The M most frequent itemsets not yet in 026 027 030 032 033 034 035 036  De\336nitions same as for algorithm 3.2 Let  denote the initial size of 026 027 030 032 033 034 035 036   006 b  001 003 005 006 b Main Do 1 call  020  006 b 2 Choose  that was not yet chosen and for which either 022   1 006 b 3 5 6  8 036   or 5 6  8 036   1 006 b 3 022   Broadcast    f B   8 016 b 020 021 6  8 036   J 3 If no such itemset exists broadcast K M O   Q Until R 5 O   020 B R  000 T U all  in the trie with 022   Y 006 b which are not in 026 027 030 032 033 034 035 036  Return T When node f receives a message  from party   1 If   K M O   Q insert  into 5 O   020 B 2 Else    f B   8 016 b 020 021 6  8 036 c  J If  e 5 O   020 B remove  from 5 O   020 B If  was not developed then develop it set g    i  Calculate  j  f B 001 f   by intersecting the k m o lists of two of  222s immediate subsets and set 016 b 020 021 6  8 036    p q r s  u 027  w s p p x z p Insert  to g   Recalculate 022   and 5 6  8 036   call  020  006 b procedure  020  006 b  Do M times  Select the next most frequent itemset outside 026 027 030 032 033 034 035 036  and develop its descendants if they have not been developed yet Set 006 b to the 022 value of the last itemset selected For itemsets with 022   consider 5 instead Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


4 Statistical Analysis The M-Max subroutine requires that we estimate the probability of an error 226 i.e an itemset which is actually frequent but appears with frequency of less than the lowered frequency threshold in the sample An over estimation may result in an unnecassary decrease in 000 002 004 005 007 which would result in a larger than required number of candidates Here we describe the probability bound we have used in our implementation which outperforms the naive Chernoff bound discussed in the original paper which described the Sampling algorithm Let b n 005 007 n 016 be the frequency of some arbitrary itemset 017 in 020 022  Consider a random sample 023 of size 024 from 020 022  We will assume that transactions in the sample are independent Hence the number of rows in 023 which contain 017 can be seen as a random variable 025 027 022 032 033 034 024 037 005 007   The frequency of 017 in 024 transactions  005 007  025  024 is an estimate for 005 007  which improves as 024 increases The best-known way to bound the chance that  005 007 will deviate from 005 007 is with the Chernoff bound We use a tighter bound for the case of binomial distributions see Hagerup and Rub 7  007 034  005 007   005 007  0 2  4 6 8 016  005 007 016   005 007      A 8 005 007  005 007    A B D Lemma 1 Given a random uniform sample 023 of N transactions from 020 022  a frequency threshold E 032 033 F 007 H I thelowered frequency threshold 000 002 004 005 007  and the negative border of J K L N  A O 023 Q  denoted 024 022  the probability S  T U K V A W that any 017 Y 024 022 will have frequency larger than or equal to E 032 033 F 007 H I hence causing failure is bounded by  024 022   6 8 016  E 032 033 F 007 H I 016  000 002 004 005 007    K L N  A 8 E 032 033 F 007 H I 000 002 004 005 007  K L N  A B D For any speci\036c itemset in 024 022  the probability that this itemset will cause failure is the probability that its estimated frequency is below 000 002 004 005 007 while its actual frequency is above E 032 033 F 007 H I  Substituting E 032 033 F 007 H I for 005 007 and 000 002 004 005 007 for  005 007  the bound gives us  007 034  F 007 H I 034 017 037 020 022   F 007 H I 034 017 037 023   0 c  4 6 8 016  E 032 033 F 007 H I 016  000 002 004 005 007    K L N  A 8 E 032 033 F 007 H I 000 002 004 005 007  K L N  A B D As for the entire 024 022   007 034 e 017 Y 024 022 h 017 005 i 032 000   4 m n o D p  007 034 017 005 i 032 000   4  1 2 4 8 15 0 2 4 6 8 10 12 14 16 18  D\212Sampling Speedup Number of Computers Speedup T20.I6.D200M, MinFreq = 1 T10.I4.D375M, MinFreq = 0.5 T5.I2.D600M, MinFreq = 0.25 Linear Speedup  Figure 1 D-Sampling speedup  024 022  6 8 016  E 032 033 F 007 H I 016  000 002 004 005 007    K L N  A 8 E 032 033 F 007 H I 000 002 004 005 007  K L N  A B D Since calculating the negative border is in itself a costly process we choose to relax this bound by substituting  w   J K L N  A O 023 Q  for  024 022   Obviously any itemset in J K L N  A O 023 Q can only be extended by at most  w  items and thus this relaxed bound holds Corollary 1 Toivonen 1996 If none of the itemsets in the negative border caused failure then no other itemset can cause failure Any other itemset 017 outside J K L N  A O 023 Q and 024 022 must include a subset from 024 022  Hence its frequency must be less than or equal to the frequency of this subset It follows that if the frequency of each itemset in 024 022 is below E 032 033 F 007 H I  so is the frequency of 017  5 Experiments We carried out three sets of experiments The 036rst set tested D-Sampling to see how much faster it is to run the algorithm with the database split among 033 machines than to run it on a single node The second set compared DSampling DDM and FDM on a range of E 032 033 F 007 H I values The last one checked scale-up the change in runtime when the number of machines is increased together with the size of the database We ran our experiments on two clusters the 036rst cluster which was used for the 036rst second and fourth sets of experiments consisted of 15 Pentium computers with dual 1.7GHz processors Each of the computers had at least 1 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


 0.25 0.5 0.75 1 1.25 1.5 1.75 2 10 15 20 25 30 35 40 T5.I2.D600M, N=15 MinFreq Time \(Minutes FDM time DDM time D\212Sampling time 0.25 0.5 0.75 1 1.25 1.5 1.75 2 0 1 2 3 4 5 6 Number of Scans Figure 2 Runtime of D-Sampling DDM and FDM for varying MinFreq gigabyte of main memory The computers were connected via an Ethernet-100 network The second cluster which we used for the scale-up experiments was composed of 32 Pentium computers with a dual 500MHz processor Each computer had 256 megabytes of memory The second cluster was also connected via an Ethernet-100 network All of the experiments were performed with synthetic databases produced by the standard gen tool 000 002 000 005 007 000 t 007 f 016 020 021 023 025 027 027 031 032 033 033 035 035 035  000 007 023 000  f   005 t 023   021 023 033   033  f  027  The databases were built with the same parameters which were used by Toivonen in 17 The onl y c hange we made w a s t o enlarge the databases to about 18 gigabytes each had we used the original sizes the whole database would 036t hen partitioned into the memory of the computers The database T5.I2.D600M has 600M transactions each containing an average of 036ve items and patterns of length two T10.I4.D375M and T20.I6.D200M follow the same encoding When the database was to be partitioned we divided it arbitrarily by writing transaction number 1 2 3 into the 1 2 3 4  partition 5.1 Speedup Results The speedup experiments were designed to demonstrate that parallelization works well for Sampling We thus ran D-Sampling with  5 7 with  5 7  D-Sampling reverts to Sampling on a large database Then we tested how splitting the database between  computers affects the algorithm\222s performance As 036gure 1 shows the basic speedup of D-Sampling is slightly sublinear However when the number of candidates 1 4 8 12 16 20 24 28 32 0 5 10 15 20 25  D\212Sampling Scale\212up Number of Computers Time \(minutes  T5.I2.D1200M, MinFreq = 0.5 T10.I4.D750M, MinFreq = 0.75 T20.I6.D400M, MinFreq = 1.5 Figure 3 D-Sampling scale-up is large the speed-up become s superlinear This is because the global sample size increases with the number of computers This larger sample size translates into a higher 007 021 035 016 020 value and thus to a smaller number of candidates than with  5 7  5.2 Dependency on 9 005   020 f The second set of experiments 036gure 2 demonstrates the dependency of D-Sampling performance on 9 005   020 f  which determines the number and size of the candidates We compared the D-Samplin g runtime to that of both DDM and FDM D-Sampling turned out to be insensitive to the reduction in 9 005   020 f  its runtime increased by no more than 50 across the whole range On the other hand the runtime of DDM and FDM increased rapidly as 9 005   020 f is decreased This is because of th e additional scans required as increasingly larger ite msets become frequent Because it performs just one database scan D-Sampling is expected to be superior to any levelwise D-ARM algorithm just as Sampling is superior to all levelwise ARM algorithms 5.3 Scale-up The third set of tests was aimed at testing the scalability of D-Sampling Here the partition size was 036xed We used a database of about 1.5 gigabytes on each computer A scalable algorithm should have the same runtime regardless of the number of computers D-Sampling creates the sam e communication load per candidate as DDM However because it generates more Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


candidates it uses more communication As can be seen from the graphs in 036gure 3 D-Sampling is scalable in two of the tests In fact for mid-range numbers of computers DSampling runs even faster than with 000 001 003  this is due to the superlinear speed-u p discussed earlier The mild slowdown seen in 036gure 3c is due to the smaller average pattern size and the smaller number of candidates in T5.I2.D1200M The larger the number of candidates the greater the saving in candidates when the number of computers increases If there are enough large patterns this saving will compensate for the increasing communication overhead Such is not the case however with T5.I2.D1200M 6 Conclusions and Future Research We presented a new D-ARM algorithm that uses the communication ef\036ciency of the DDM algorithm to parallelize the single-scan Sampling algorithm Experiments prove that the new algorithm h as superlinear speedup and outperforms both FDM and DDM with any 004 006 000 007 b n f value The exact improvement in relation to previous algorithms depends on the number of database scans they require Experiments demonstrat e good scalability provided the database scan is the major bottleneck of the algorithm Some open questions still remain First it would be interesting to continue partitioning the database until every partition becomes memory resident This approach may lead to a D-ARM algorithm that mines a database by loading it into the memory of large number of computers and then runs with no disk-I/O at all Second it would be interesting to have a parallelized version of the other single-scan ARM algorithm 226 DIC 226 on a share-nothing cluster  or of the two-scans partition algorithm Finally we feel that the full potential of the M-Max algorithm has not yet been realized we intend to research a dditional applications for this algorithm References 1 R  A g r a w a l  T  I mie lin sk i a n d A  N  S w a mi M in in g a ssociation rules between sets of items in large databases In Proc of the 1993 ACM SIGMOD Int\222l Conference on Management of Data  pages 207\226216 Washington D.C June 1993 2 R  A g r a w a l a n d J  S h a fe r  P a ra lle l m in in g o f a sso c i a tio n rules IEEE Transactions on Knowledge and Data Engineering  8\(6\:962 226 969 1996 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules In Proc of the 20th Int\222l Conference on Very Large Databases VLDB\22294  pages 487 226 499 Santiago Chile September 1994 4 V  S  A nant hanar a yana D  K  S ubr amani a n and M  N  Murty Scalable distributed and dynamic mining of association rules In Proceedings of HiPC\22200  pages 559\226566 Bangalore India 2000 5 S  B rin  R M o tw a n i J Ullma n  a n d S Tsu r  D y n a mic ite mset counting and implication rules for market basket data SIGMOD Record  6\(2\:255\226264 June 1997 6 D  C heung J Han V  Ng A  F u and Y  F u A f ast d i s tributed algorithm for mining association rules In Proc of 1996 Int\222l Conf on Parallel and Distributed Information Systems  pages 31 226 44 Miami Beach Florida December 1996 7 T  H ager up and C  R ub  A gui ded t our of C h er nof f bounds Information Processing Letters  33:305 226 308 1989/90 8 J  H an J  P ei  a nd Y  Y i n Mi ni ng f r e quent pat t e r n s w i t hout candidate generation Technical Report 99-12 Simon Fraser University October 1999 9 Z  J ar ai  A  V i r mani  a nd L  I f t ode T o w ar ds a c ost ef f ect i v e parallel data mining approach Workshop on High Performance Data Mining held in conjunction with IPPS\22298 March 1998  D I  L i n a nd Z  M K e dem P i ncer sear ch A ne w a l gorithm for discovering the maximum frequent set In Extending Database Technology  pages 105\226119 1998  J S  Park M  S  Chen a nd P  S  Y u  A n e f f ect i v e h ashbased algorithm for mining association rules In Proc of ACM SIGMOD Int\222l Conference on Management of Data  pages 175 226 186 San Jose California May 1995  J S  Park M  S  Chen a nd P  S  Y u  E f 036 ci ent p aral l e l d at a mining for association rules In Proc of ACM Int\222l Conference on Information and Knowledge anagement  pages 31 226 36 Baltimor e MD November 1995  J P e i a nd J Han C a n w e push m or e c onst r ai nt s i nt o f r e quent pattern mining In Proc of the ACM SIGKDD Conf on Knowledge Discovery and Data Mining  pages 350\226354 Boston MA 2000  A S a v a ser e  E  O mi eci nski  a nd S  B  Na v a t h e An ef 036 c i e nt algorithm for mining association rules in large databases The VLDB Journal  pages 432\226444 1995  A S c hust e r a nd R  W o l f f  C o mmuni cat i onef 036 c i e nt di stributed mining of association rules In Proc of the 2001 ACM SIGMOD Int\222l Conference on Management of Data  pages 473 226 484 Santa Barbara California May 2001  R  S r i k ant a nd R  Agr a w a l  Mi ni ng gener a l i zed associ at i o n rules In Proc of the 20th Int\222l Conference on Very Large Databases VLDB\22294  pages 407 226 419 Santiago Chile September 1994  H T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In The VLDB Journal  pages 134\226145 1996 18 O R Za ia n e  M  E l-Ha jj a n d P  L u  F a st p a ra lle l a sso c i a tio n rules mining without candidacy generation In IEEE 2001 International Conference on Data Mining ICDM\2222001  pages 665\226668 2001 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


9 Here are some observations and explanations on the results 1\The total time of our comparison includes the time to write the association rules to a file; Bit-AssocRule is 2 to 3 orders of magnitude faster than the various Apriori algorithms \(64-221 times faster\he test data set, the big the time difference between the BitAssocRule and the various Apriori algorithms. We havenêt compared our algorithm with some of the other association rule algorithms such as VIPER  13,24] \(CHARM and CL OSE are base d on the closed frequent itemsets concept\d on their published comparison results with Apriori, our BitAssocRule is very competitive compared to them and a direct comparison will be conducted and reported in the near future 2\e or litter longer time than the various Apriori algorithms in constructing the 1-itemsets because of the extra cost of building the bitmaps for the 1-itemsets. But after the 1-itsemtset is done, Bit-AssocRule is significant faster than the Apriori algorithms in constructing large frequent itemsets because it only uses the fast bit operations \(AND COUNT and SHIFT\nd doesnêt need to test the subsets of the newly candidate 3\aps of the frequent items, and the bitmap storage \(uncompressed less than the original data set \(1/2 to 1/4 of the original data size The main reasons that Bit-AssocRule algorithm is significant faster than Apriori and its variations are 1\ocRule adopts the divide-and-conquer strategy, the transaction is decompose into vertical bitmap format and leads to focused search of smaller domain There is no repeated scan of entire database in BitAssocRule 2\snêt follow the traditional candidate-generate-and test approach, thus saves significant amount of time to test the candidates 3\basic operations are bit Count and bit And operations, which are extremely faster than the pattern search and matching operations used in Apriori and its variations 5. Conclusion The contributions of this paper are in two aspects:  we extend the application domains of bitmap techniques and introduce the bitmap techniques for complex DSS query optimization and association algorithm. We present a bitmap based query optimization algorithm to optimize complex query with multiple table join based on outer join operations and push the outer join operations from the data flow level to the bitmap level and achieve significant performance gain.   We introduce a novel algorithm to calculate the foundset for those tables involved in the prejoin table by using prejoin_bitmap_indexes and integrate this algorithm into the current commercial data flow based query engine seamlessly. Our query optimization can achieve an order of magnitude faster than conventional query engine Secondly we introduce the bitmap technique to the data mining procedure and develop a bitmap-based algorithm Bit-AssocRule to find association rules. Our BitAssocRule avoids the time-consuming table scan to find and prune the itemsets, all the operations of finding large itemsets from the datasets are the fast bit operations. The experimental result of our Bit-AssocRule algorithm with Apriori and AprioirHybrid algorithms shows Bit-AssocRule is2 to 3 orders of magnitude faster. This research indicates that bitmap technique can greatly enhance the performance for decision support queries and finding association rule, and bitmap techniques are very promising for the decision support query optimization and DM applications Bitmap technique is only one way to improve the performance of complex DSS queries and DM algorithm Parallelism is another crucial factor to improve the performance of DSS and data mining.  We are currently working on paralleling the bitmap-based algorithms and hope to report our findings in the near future 6. References  Agrawal R. Sri kant R., çFast al gorithm for mining association rulesé, Prod. of the 20 th VLDB Conf. 1994  Agrawal R., Mannila H., Sri kant R., Toivone n H., Verkamo A., çFast discovery of  association rulesé, in Advances in Knowledge Discovery and Data Mining, MIT 1996  AIP D Tec hnical P u blications In Syba se IQ Administration Guide, Sybase IQ Release  11.2 Collection, Sybase Inc Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


10  er-Yahia S., Johnson T., çOptimizing queries on compressed bitmapsé, Prod. of the 20 th VLDB  Conf  wa l, R., Gunopul os D  Constraint-based rule mining in large, dense databases Proc. of the 15th Int'l Conf. on Data Engineering \(ICDE1999 6] Bertino E., Ooi B.C., Sacks-Davis R. etc, çIndexing techniques for advanced database systems Kluwer Academic Publishers  n C Ioa nnidis Y B itm ap index design and evaluationé, Prod of the  SIGMOD-96  h atziant oni ou D Akinde  M, Johnson T Kim  S, çThe md-join: an operator for complex olapé. Prod of the 18 th Intêl Conference on Data Engineering \(ICDE2001  outer joiné, Prod of the 2nd International Conf. on Databases   Fre nch C  One size fits allê databa se arc hitecture do not work for dssé, Prof of the  SIGMOD-95  hal, A., çOute r join simplification and reordering for query optimizationé, ACM TODS, 22\(1\1997  G., çVolcano, an extensible and pa rallel query evaluation systemé, IEEE Transaction on Knowledge and Data Engineering, 6\(6  e i, J. Yin. Y  Mining fre quent patterns without candidate generation", Prod of the SIGMOD-2002    Hanusa R., çA lesso n in outer joins \(learned the hard way!\data Review, Spring 1998  aine C., Data A A novel inde x s u pporting high volume data warehouse insertioné Prod of the 25th VLDB Conf  on T , çPerform ance m easurem ents of compressed bitmap indicesé Prod. of the 25th VLDB conf   Y Data m i ni ng and m achine oriented modeling: a granular computing approaché, Journal of Applied Intelligence, Oct. 2000  Lin T.Y., çFi nding ass ociation rules using fast bit computation: machine-oriented modelingé ISMIS-2000  M., çGroup bit map index: a structure for association rules retrieval Prod. of the 4 th Intêl Conf. on Knowledge Discovery and Data Mining \(KDD-98  OêNeil P Graefe G., çM ulti-table joi ns  through bitmapped join indexesé, SIGMOD September 1995, 8-11   OêNeil P., Quass D., çIm prove d que ry pe rformance with variant indexesé, Prod of the SIGMOD-1997   Inform ix and inde xing s u pport for data warehouses,  Informix Whitepaper  a p join i n dex  http://technet.oracle.com/products/oracle9i/daily/a pr09.html  n, H. Lu, S. Ni shio, S. Ta ng, and D  Yang. "H-mine: hyper-structure mining of frequent patterns in large databases", Proc. The 2001 IEEE Intêl Conference on Data Mining  OêNeil P.,  OêNeil E.,  çBitSliced Index Arithmeticé, Prod of the SIGMOD2001  vase re , A Om iecinski E Na vat he S  An efficient algorithm for mining association rules in large databasesé, in Prod. of the 21 st VLDB conf  She noy P Bhal otia G Haritsa J B a wa M Sudarshan S., Shah D., çTurbo-charging vertical mining of large databaseé, Prod. of the SIGMOD2000  a processi ng for com plex queriesé, Red Brick/Informix White Paper  as and starjoi n technologyé Red Brick/Informix White Paper  P C be nchm ark d de cision s u pport  dard specification, Release 2.2. \(Transaction Processing Performance Council \(TPC  durie x P J oi n indexe s ACM TODS 12\(2\ 1987  W u M Buchm ann A  Encoding bitm ap indexing for data warehouseé, Proc. of the 14th  Intêl Conference on Data Engineering, 220-231, 1998   Gouda K., çF ast vertical usi ng diffsetsé, Tech report, Dept. of  computer science, RPI  Y., Des hpa nde P., Naughton J Shukla A.,  çSimultaneous optimization and evaluation of multiple dimensional queriesé, SIGMOD-98, 271282 Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


20% 4 4 4 12 12 30% 4 3 3 12 12 40% 3 3 3 12 11 50% 3 2 2 11 10 60% 2 1 2 11 10 70% 2 1 2 11 11 80% 2 2 2 10 11 90% 2 2 2 9 10 99% 0 1 1 6 10 Looking at Table 9, one may wonder why the representation determined for minSup = 70% has shorter longest elements in its Bd  GDFree \(here: length = 1 than the representation determined for minSup = 80 here: length = 2 border elements in Bd  GDFree, which are infrequent in the representation determined for minSup = 80 become frequent when lowering the support threshold to 70 7. Related work The most similar to the representations based on generalized disjunctive sets is the NDR representation which consists of all frequent non-derivable itemsets [7 8], and the representations based on ?-free sets [7, 9]. It was shown in [8], that for each non-empty itemset Z, one can derive the lower bound \(l\(Z u\(Z on sup\(Z from the fact that sup\(Z Non-derivable itemsets are those for which u\(Z Z gt; 0. If u\(Z Z sup\(Z supersets Y of a derivable itemset are also derivable and u\(Y Y Y such that neither sup\(Z Z Z Z It was proved in [7, 8] that u\(Z Z  u\(Z?{a Z?{a for |Z| ? 1 This important result was used in [7, 8] to determine the upper bound on the length of non-derivable itemsets namely: non-derivable itemsets are not longer than log2|D|? + 1 Hence, the bound on the length of non-derivable itemsets is identical to the bound on the length of generalized disjunction-free sets \(please see Theorem 3.2 It has been proved in [7] that ?-free sets are a subset of non-derivable itemsets, so their length is also bounded by ?log2|D|? + 1 There is a claim in [7, 9] that the generalized disjunction-free sets equal the ?-free sets. This claim however, is not correct, which we will prove by the example beneath. Table 4 contains all generalized disjunction-free sets. Among them, there is {fh}, the support of which equals 0. The support bounds for {fh are found as follows \(please, see [7] for the details  sup\({fh f h   sup\({fh f  sup\({fh h  sup\({fh Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE Hence, l\({fh fh fh sup\({fh free, is not a ?-free set Since the families of the generalized disjunction-free sets and ?-free sets may differ, finding the relationship between them or between the generalized disjunction-free sets and non-derivable itemsets remains a challenge 8. Conclusions The representations based on generalized disjunctionfree sets belong to the most concise ones among all lossless frequent patterns representations with borders. In 


lossless frequent patterns representations with borders. In practice, they are also much more concise than representations based on closed itemsets and approximate representations. In this paper, we offered three methods of deriving an upper bound on the length of sets in such representations. We proved that the upper bound on the length of a longest generalized disjunction-free set depends logarithmically on the number of records in the database. The obtained result is of high importance as it guarantees that any generalized disjunction-free set representation for all patterns \(both frequent and infrequent scans, where n is the number of records in the database irrespectively how strong or weak correlations among items in the database are and irrespectively of the lengths of records and number of distinct items The modifications of the basic estimation take into account the support threshold of the representation to be found or, additionally, the information on the length of longest sets of the representation already calculated for a higher support threshold. Though these estimations are more accurate than the basic one, they are still quite rough. Further improvements of the quality of estimating the length of longest itemsets in generalized disjunctionfree representations is subject to further research References 1] R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, A.I Verkamo  Fast Discovery of Association Rules   Advances in Knowledge Discovery and Data Mining AAAI, CA, 1996 2] J. Baptiste, J.-F. Boulicaut  Constraint-Based Discovery and Inductive Queries: Application to Association Rule Mining  Pattern Detection and Discovery, Springer London, UK, September 2002, pp. 110-124 3] E. Baralis, S. Chiusano, P. Garza  On Support Thresholds in Associative Classification  SAC, ACM, Taipei Taiwan, March 2004, pp. 553-558 4] Y. Bastide, N. Pasquier, R. Taouil, G. Stumme, L. Lakhal  Mining Minimal Non-redundant Association Rules Using Frequent Closed Itemsets  Computational Logic, 2000 pp. 972  986 5] J.-F. Boulicaut, A. Bykowski, C. Rigotti  Approximation of Frequency Queries by Means of Free-Sets  PKDD Springer, Lyon, France, September 2000, pp. 75-85 6] A. Bykowski, C. Rigotti  A Condensed Representation to Find Frequent Patterns  PODS, ACM, Santa Barbara USA, May 2001, pp. 267-273 7] T. Calders, Axiomatization and Deduction Rules for the Frequency of Itemsets, Ph.D. Thesis, University of Antwerp, 2003 8] T. Calders, B. Goethals  Mining All Non-derivable Frequent Itemsets  PKDD, Springer, Helsinki, Finland August 2002, pp. 74?85 9] T. Calders, B. Goethals  Minimal k-free Representations of Frequent Sets  ECML/PKDD, Springer, CavtatDubrovnik, Croatia, September 2003, pp. 71-82 10] J. Han, M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann Publishers, 2000 11] S.K. Harms, J. Deogun, J. Saquer, T. Tadesse  Discovering Representative Episodal Association Rules from Event Sequences Using Frequent Closed Episode Sets and Event Constraints  ICDM, IEEE Computer Society, San Jose, California, USA, November-December 2001, pp. 603  606 12] M. Kryszkiewicz  Closed Set Based Discovery of Representative Association Rules  IDA, Springer Cascais, Portugal, September 2001, pp. 350-359 13] M. Kryszkiewicz  Concise Representation of Frequent Patterns based on Disjunction  free Generators  ICDM IEEE Computer Society, San Jose, California, USA 


IEEE Computer Society, San Jose, California, USA November-December 2001, pp. 305  312 14] M. Kryszkiewicz M  Inferring Knowledge from Frequent Patterns  Soft-Ware, Springer, Belfast, Northern Ireland April 2002, pp. 247  262 15] M. Kryszkiewicz  Concise Representations of Association Rules  Pattern Detection and Discovery, Springer London, UK, September 2002, pp. 92-109 16] M. Kryszkiewicz, Concise Representation of Frequent Patterns and Association Rules, Habilitation Thesis Publishing House of Warsaw University of Technology 2002 Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE 17] M. Kryszkiewicz  Reducing Infrequent Borders of Downward Complete Representations of Frequent Patterns  Proc. of The First Symposium on Databases Data Warehousing and Knowledge Discovery, Scientific Publishers OWN, Baden-Baden, Germany, July, 2003, pp 29-42 18] M. Kryszkiewicz  Closed Set Based Discovery of Maximal Covering Rules  International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems Vol. 11. World Scientific Publishing Company, Singapore September 2003, pp. 15-29 19] M. Kryszkiewicz  Reducing Borders of k-Disjunction Free Representations of Frequent Patterns  SAC, ACM Taipei, Taiwan, March 2004, pp. 559  563 20] M. Kryszkiewicz  Reducing Main Components of k-Disjunction Free Representations of Frequent Patterns   Proc. of IPMU  2004 \(in print 21] M. Kryszkiewicz, M. Gajek  Concise Representation of Frequent Patterns based on Generalized Disjunction  Free Generators  PAKDD, Springer, Taipei, Taiwan, May 2002, pp. 159-171 22] M. Kryszkiewicz, M. Gajek  Why to Apply Generalized Disjunction-Free Generators Representation of Frequent Patterns  ISMIS, Springer, Lyon, France, June 2002, pp 383  392 23] M. Kryszkiewicz, H. Rybi?ski, M. Gajek  Dataless Transitions between Concise Representations of Frequent Patterns  Journal of Intelligent Information Systems JIIS Netherlands, 2004, pp. 41-70 24] N. Pasquier, Data mining: Algorithmes d  extraction et de R  duction des R  gles d  association dans les Bases de Donn  es, Th  se de Doctorat, Universit  Blaise Pascal   Clermont  Ferrand II, 2000 25] N. Pasquier, Y. Bastide, R. Taouil, L. Lakhal  Efficient Mining of Association Rules Using Closed Itemset Lattices  Journal of Information Systems, Vol. 24, No. 1 1999, pp. 25  46 26] N. Pasquier, Y. Bastide, R. Taouil, L. Lakhal  Discovering Frequent Closed Itemsets for Association Rules  ICDT, Springer, Jerusalem, Israel, January 1999 pp. 398  416 27] J. Pei, G. Dong, W. Zou, J. Han  On Computing Condensed Frequent Pattern Bases  ICDM, IEEE Computer Society, Maebashi City, Japan, December 2002 pp. 378-385 28] V. Phan-Luong V  Representative Bases of Association Rules  ICDM, IEEE Computer Society, San Jose California, USA, November-December 2001, pp. 639-640 29] A. Savinov  Mining Dependence Rules by Finding Largest Itemset Support Quota  SAC, ACM, Taipei Taiwan, March 2004, pp. 525-529 30] M.J. Zaki, C.J. Hsiao  CHARM: An Efficient Algorithm for Closed Itemset Mining  SIAM, Arlington, 2002 


for Closed Itemset Mining  SIAM, Arlington, 2002 Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


                                                  S J       


                                                      


                         L A                                        


          L A  Table 7. Table of Granules at left-hand-side is isomorphic to  at right- hand-side: By Theorem  3.1 one can ?nd patterns in either table as a single generalized concept  Internal points  are:[4]\(1, 1, 0, 0 tions; [5]\(0, 1, 1, 0  0, 1, 0, 1  0, 1, 1, 1  1, 1 1, 0  1, 1, 0, 1  1, 0, 1, 1 11]\(1, 1, 1, 1 form and simplify them into disjoint normal forms 1  T E N    S J    T E N    S J 2  T W E N T Y    L A    T H I R T Y   A 3  T W E N T Y      T H I R T Y   A 4  T W E N T Y            L A 5  T E N      T W E N T Y    L A    T E N    T W E N T Y   A   S J    T W E N T Y   A      T H I R T Y    L A     Y 7  T E N      T W E N T Y      T H I R T Y   L A    T E N   L A    S  J   A 8  T W E N T Y      T E N      T W E N T Y    L A    T E N   T W E N T Y      T H I R T Y 9  T W E N T Y    N Y    T E N    S J    T H I R T Y    L A      T W E N T Y    L A  1 0  T W E N T Y    N Y    T W E N T Y    L A    T H I R T Y   A    J 1 1  T W E N T Y          T W E N T Y    L A   T H I R T Y    L A    a l l If the simpli?ed expression is a single clause \(in the original symbols non-generalized the following associations 1   T E N     S J    T E N    S J  2. SJ   J 4   L A    T W E N T Y    L A    T H I R T Y    6 Conclusions Data, patterns, method of derivations, and useful-ness are key ingredients in AM. In this paper, we formalize the current state of AM: Data are a table of symbols. The patterns are the formulas of input symbols that repeat. The method of derivations is the most conservative and reliable one, namely, mathematical deductions. The results are somewhat surprising 1. Patterns are properties of the isomorphic class, not an individual relation - This implies that the notion of patterns may not mature yet and explains why there are so many extracted association rules 2. Un-interpreted attributes \(features can be enumerated 3. Generalized associations can be found by solving integral linear inequalities. Unfortunately, the number is enormous. This signi?es the current notion of data and patterns \(implied by the algorithms 4. Real world modeling may be needed to create a much more meaningful notion of patterns. In the current state of AM, a pattern is simply a repeated data that may have no real world meaning. So we may need to introduce some semantics into the data model [12],[10],[11 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE References 1] R. Agrawal, T. Imielinski, and A. Swami  Mining Association Rules Between Sets of Items in Large Databases  in Proceeding of ACM-SIGMOD international Conference on Management of Data, pp. 207216, Washington, DC, June, 1993 


216, Washington, DC, June, 1993 2] Richard A. Brualdi, Introductory Combinatorics, Prentice Hall, 1992 3] A. Barr and E.A. Feigenbaum, The handbook of Arti?cial Intelligence, Willam Kaufmann 1981 4] Margaret H. Dunham, Data Mining Introduction and Advanced Topics Prentice Hall, 2003, ISBN 0-13088892-3 5] Fayad U. M., Piatetsky-Sjapiro, G. Smyth, P. \(1996 From Data Mining to Knowledge Discovery: An overview. In Fayard, Piatetsky-Sjapiro, Smyth, and Uthurusamy eds., Knowledge Discovery in Databases AAAI/MIT Press, 1996 6] H Gracia-Molina, J. Ullman. &amp; J. Windin, J, Database Systems The Complete Book, Prentice Hall, 2002 7] T. T. Lee  Algebraic Theory of Relational Databases  The Bell System Technical Journal Vol 62, No 10, December, 1983, pp.3159-3204 8] T. Y. Lin  Deductive Data Mining: Mathematical Foundation of Database Mining  in: the Proceedings of 9th International Conference, RSFDGrC 2003 Chongqing, China, May 2003, Lecture Notes on Arti?cial Intelligence LNAI 2639, Springer-Verlag, 403-405 9] T. Y. Lin  Attribute \(Feature  The Theory of Attributes from Data Mining Prospect  in: Proceeding of IEEE international Conference on Data Mining, Maebashi, Japan, Dec 9-12, 2002, pp. pp.282-289 10] T. Y. Lin  Data Mining and Machine Oriented Modeling: A Granular Computing Approach  Journal of Applied Intelligence, Kluwer, Vol. 13, No 2, September/October,2000, pp.113-124 11] T. Y. Lin, N. Zhong, J. Duong, S. Ohsuga  Frameworks for Mining Binary Relations in Data  In: Rough sets and Current Trends in Computing, Lecture Notes on Arti?cial Intelligence 1424, A. Skoworn and L Polkowski \(eds 12] E. Louie,T. Y. Lin  Semantics Oriented Association Rules  In: 2002 World Congress of Computational Intelligence, Honolulu, Hawaii, May 12-17, 2002, 956961 \(paper # 5702 13  The Power and Limit of Neural Networks  Proceedings of the 1996 EngineeringSystems Design and Analysis Conference, Montpellier, France, July 1-4, 1996 Vol. 7, 49-53 14] Morel, Jean-Michel and Sergio Solimini, Variational methods in image segmentation : with seven image processing experiments Boston : Birkhuser, 1995 15] H. Liu and H. Motoda  Feature Transformation and Subset Selection  IEEE Intelligent Systems, Vol. 13 No. 2, March/April, pp.26-28 \(1998 16] Z. Pawlak, Rough sets. Theoretical Aspects of Reasoning about Data, Kluwer Academic Publishers, 1991 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207ñ216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intíl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intíl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





