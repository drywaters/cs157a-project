Proceedings of the Second International Conference on Machine Learning and Cybernetics Wan 2-5 November 2003 A MINING ALGORITHM FOR FUZZY WEIGHTED ASSOCIATION RULES BAO-YI WANG\222 SHAO-MIN ZHANG\222\224 Dept of Computer Science and Engineering North China Electric Power University, Baoding 071003, Hebei, China 2School of Computer Science and Technology, Xidian University, Xi\222an 710071, Shanxi, China E-MAIL wwangbaoyi@sohu.com, shaominzhangq@263.sina.com I Abstract The association rule mining is an important research subject of knowledge discovery Aiming at the common method of mining for attributes 
of quantitative type in database we analyze the existing defects and put fomard a method of applying fuzzy set tbeory to association rules mining Due to the problem that each attribute\222s importance is diNerent in specific purpose mining we put forward a solution by assigning corrwpondiug weight to attribute of diNerent importance Basing on the above idea we Put forward a mining deorithm nsine fuzzv weighted association probability p\(B 1 A Strong association rules are such rules 
which can satisfy the lowest threshold of support ratio and the lowest threshold of confidence ratio at the same time Algorithm of this paper draws lessons from this idea However in reality attribute relations in database usually are shown as fuzzy IekitiOnS fiuthermofe the importance of them is different  did not discuss these problems and the mining method of the association rules This paper bas researched into these problems rules and through-the given penmenc we testify the feasibility of the algorithm,and point ont the existing defect of the algorithm demanding improvement in future 2 Putting forward the problems of fuzzy 
weighted association rule mining Keywords Fuzy weighted association rules Fuzzy set Mining algorithm Frequent pattern Quantitative attribute Database 1 Introduction Association rule mining is to discover the interesting association or relevant relation among itemsets in a large quantity of data Ref.[l firstly put forward an idea of mining association rules and discussed the mining problem of association rules of Boolean attribute Suppose I={j,,i2;..,im is setofitems,task-relevantdataD is a set of transactions in database, each transaction T in D is a set of items and 
T c I Each transaction has an identifier called TID Suooose A is an itemset transaction T Because some attributes in database belong to quantitative type aimed at mining method for association rules of this kind of attribute at present the common method12\222 is to make the continuous data discrete thus we can transform the problem of association rules of quantitative attribute into one of the Boolean attribute and deal with it One method is to divide the discussing domain of attributes into some non-overlapped intervals then maps continuous data to these 
intervals Because obvious division of intervals might exclude some latent elements near certain intervals thereby some meaningful intervals might be neglected Aother method is to divide the discussing domain of attributes into some overlapped intervals then the elements situated near borders of intervals might be located in two intervals at the same time These elements contribute to two intervals at the same time which might result in excessively emphasizing the function   contains A when and only when A E T  Association rule is an implication formula A B in which A c 
I of these elements thus the meaning of intervals might also be excessively emphasized certain The defects of mentioned above two methods mainly I and A    the A in come from the stiff division of the border. In order to solve transaction set D and has SuPPOfl ratio this problem we can adopt fuzzy set defined in discussing SUP port\(d 2 B  p\(A U B domain of attributes to soften the borders The reason is of A U B in 
D that is probability that fuzzy set can provide much smoother changes between the elements in the set and the elements which are not in U 2221  and also has confidence ratio the set owing to these smooth changes almost all the confidence\(A 3 B  p\(B 1 A a percentage of A in elements near the borders would not any longer be transaction D B also in D nat is conditional excluded meanwhile these elements would not be the percentage 0-7803-7865-2/03/$17.00 02003 IEEE 2495 


Proceedings of the Second International Conference on Machine Learning and Cybernetics Wan 2-5 November 2003 1 I 20 excessively stressed either Table 1 Attribute table 60 86 28 Suppose an attribute set  A A  A  and a transaction database D={d,,d,;..,d,}in which di includes a transaction identifier tid and a data attribute set di tid,t t~dom\(A A A dom\(Aj is attribute A 222s value domain Definition 1 A fuzzy conception is said to be x  t,pU  px is membership grade denoting how much it belongs to X  Definition 2 A fuzzy conception set on D is said to be F=\(x,,x,;..,x  each fuzzy conception xi denotes a fuzzy set hi  for instance we defme three fuzzy conceptions to attribute 223salary\224 low medium high},then F  sakzvyiow sak~vy sa   Definition 3 Fuzzy pattem x is a group of non-empty fuzzy conception sets expressed as x A x A 221xk in which k denotes the length of x  and x is called k pattem pu,\(d  min{p d I xi EX  in which min\(set denotes the minimum element in set Definition 4 Support ratio of fuzzy pattern x is user-prescribed threshold ID1 denotes the number of transaction in D Definition 5 Support ratio of fuzzy association rule PAAA4 I P\223A4 2 4 C\(Pu,,,\(4 I PAd4 2 4 C\(P\224\(4 bA\(4 2 4 sup\(A 3 B  dcD ID1 confidence ratio of fuzzy association rule conf A 3 B  dcD drD Seeing that in database each attribute\222s importance to specific mining goal is different we can assign different weights to different attributes that is for I={A,,A,;..,A we associate each A with a 2496 


Proceedings of the Second International Conference on Machine Learning and Cybernetics Wan 2-5 November 2003 weight wi E 0,1 that is some weight will be assigned to fuzzy conceptions X  x2    xp in attribute Ai Definition.6 Suppose xi is a fuzzy conception in attribute Ai and xi E A AB  for any d ED  if pu,,,\(d then d s contribution to A>B is w PhB 4  mwwi x Pz 4 I xi E A U 4 3 thus d s contribution to A 3 B inclines to amibute of weight Definition 7 Support ratio and confidence ratio of fuzzy weighted association rule respectively are E\(w-pAABvj I w-~~~~\(d 2 6 ID1 w_sup\(A  E  dED 13 W-pA,,B\(d w-P~,,/\\B\(d w-con/\(A E  deD E\(w-flAd W-fl,M>E deD the prefix w represents weight similarly here in after The concrete weighted computing method to support ratio and confidence ratio is not described here, and you can find them in ReE[2 Definition 8 k frequent fuzzy pattern set L XI AX2 A...AXp ISUp\(X AX2 A...AX minsup  L UL  we call L frequent pattern set Theorem 1 i  1 sub-pattem of k frequent fuzzy pattem is i  1 frequent fuzzy pattern F'rooE from the definitions that transaction t has a sup&x to tizzy pattem we know thatt 3s support to the pattem x which has length k will always not go beyond that to X s any sub-pattem So support ratio of k frequent fuzzy pattem X I support ratio of i  1 sub-pattem of x  i  1 sub-pattem of Xis frequent fuzzy pattem 4 A mining algorithm for fuzzy weighted association rules Mining fuzzy weighted association rules have two I Generate a frequent itemset L in which the 2 Produce the wanted association mles by taking At fust we give a mining algorithm FWFPMA for steps support ratio of all members is higher than minsup advantage of L  frequent fuzzy pattem as follows Input transaction database D  fuzzy concept set F  the lowest support ratio threshold minsup weight set W Output frequent pattem set L  1 c  F  C is the set of fuzzy concepts 2 Foreach XEC do  s\(x 3 For each d E D do 4 For each x E c do 5 w-p,\(d then 6 Add d.fid,w-p,\(d to s\(x  7 L  x I x E c and w-sup\(x I min sup  8 sI  9  k:=2 L,-l  k  IO L  s  11 w sup\(x  0  w-suP\(x x d I x E L   C C=ql]q2]...qk-I]Q[k-l]lql Q[l];..,P[k  21 Q[k 21 P[k-I  Q[k  11 and PE L  Q E L and VC E Lk  where C is a k  1 sub-pattem ofC  1 2 For all c E c do  13 w-sup\(C 15 B  C[l]C[2]...C[k  2]C[k  select two 14 A=C[I]C[2]...C[k-2]C[k I k  1 sub-patterns of C and d,fid,w-p\(,\(d B d,.fid d2.tid   do 16 For each element pair d,fid,w pA d E S\(A Add d,.tid,min\(w-pu,\(d d2 w  sup C  w  sup\(C  to s\(C min\(w-~,\(dl d  17 L CEC Iw_sup\(C I8  I E Lk 1 2497 


lhcccdings of thc Sccond lnccrnational Confcrcncc on Machinc Lcarning and Cyhcrncdcs Wan 2-5 Novcmbcr 2003 2498 19 L=uLk In this algorithm L  set of frequent fuzzy weighted k patterns C  set of candidate fuzzy weighted k patterns C[i  the i th fuzzy concept of fuzzy pattem C sk  set of tid lists of L  L  set of all frequent fuzzy weighted patterns The s andLIare obtained by scanningD in steps I  and Lk and sk are obtained in steps 9 18 for k 2  C is firstly generated by Lk  For each candidate pattern C E c  s\(C can be derived by fuzzy set operations from two elements of s  If s\(C s\(c and C are inserted into sk and L respectively, otherwise s\(C is deleted from s  Moreover, for each fuzzy weighted pattern XI which meets the lowest support ratio the algorithm will generate an array s\(X   d.tid px d I px d 2 E d E D to save record number of records contributing to pattern X and px d contributing to patternX  After generating the set of frequent pattem, we can use algorithm GEN-FWAR to generate fuzzy association rules GEN-FWAR is as follows Input set of frequent pattem L  lowest confidence ratio min conf Output: set of rules R I R 2 For each k E L do  3 For each S c K  and S   If w  conf\(S 3 K  S t w min conf Add S,K-S to R 5 Result of the experiment and analysis of the occupied memory volume of the algorithm With VB6.0 we analyze the quantitative attribute taken from mushroom database in http://www.ics.uci.edu/-mleardMLSummary.htm1 According to quantity distribution of each attribute and application effects in reality we establish fuzzy concept big medium small or very big rather big medium rather small very small in each attribute and assign a weight within range of 0,1 to each attribute, with given minsup=0.3 We take records respectively as 500,1000,1500 at random to carry out the experiment Results have been shown in Figure I When selecting 1500 as record\222s number each record is 124 bytes, capacity of the database is 15OOx 124=18600 bytes and changing the value minsup, the results of the experiment are shown in Memory volume KB 10 5 IZL 500 1000 1500 Number of records Fieure I The exoeriment result when minsun=3 Memory volume KB t I 5  L minsup 0.3 0.4 0.5 Figure 2 The experiment result when different minsup In this example the occupied memory volume is 1 I S\(X  I x 4 bytes in which I s\(x  I denotes the number of records as d.tid,pu d from EL 1 EL the algorithm each record occupies 4 bytes Due to the number of records as d.tid u d generated when generating I pattem is the most and the occupied memory volume is the largest So the occupied memory volume shown in Figure 1 and Figure 2 are both volume occupied by generating records as d.tid fix d when generating I-pattern From above two Figures we can know that extra memory volume will become larger with the number records becoming larger, and it will also become larger with minsup\222s reduction For example when selecting 1500 as the number of database minsup4.3 


Proceedings of the Second International Conference on Machine La\223g and Cybernetics Xi\224 2-5 November 2003 XI s\(xi I 10691 the occupied memory volume is X,EL 42764 bytes which is probably 22.9 of the volume of the database We can see that the scale of database becomes much larger and the problem about extra memory volume will become more obvious so fwtlier research into the improvement of the algorithm is needed 6 Conclusion Due to many attributes in da+ase are quantitative type and their importance is different by defining fuzzy set in discussing domains of attribute to substitute for the common method of dividing 222 discussing domains of attribute the defects of stiff division borders can be overcome By assigning weights to the corresponding attributes which have different importance, we can discover some important rules which have been neglected previously Meanwhile owing to introducing fuzzy concept the ultimate rules can be understood more easily In this paper the mining algorithm for set of fuzzy weighted fiequent pattem and the algorithm for generating rules are put forward and by experiment we analyze and testify the algorithm and point out the algorithm\222s defects demanding improvement later References I Agrawal R Imielinski T Swami A Mining association rules between sets of items in large database Proceedings of the ACM SIGMOD Intemational Conference on the Management of Data Washington D.C pp.207-216 1993 2 JIAN-JIANG LU Mining Boolean and general fuzzy weighted association rules in database Systems Engineering-Theory  Practice, pp.28-32,2002\(2 3 NlNG CHEN AN CHEN LONG-XIANG ZHOU Efficient algorithms for mining fuzzy rules in large relational datahases Joumal of software vol 12\(07 pp.949-959,2001 4 Chan ManKuok Fu Ada Won Man Hon Mining fuzzy association rules in database SIGMOD Record vol 27\(1 pp.41-46 1998 5 Chen Ming-syan Han Jia-wei Yu Philip Data Mining an overview from a database perspective IEEE Transactions on Knowledge and Data Engineering vol 8\(6\pp 866-883 1996 2499 


required in the model. We therefore develop an algorithm to form the reduced database table effectively and hence to discover fuzzy association rules, which uses the idea of dropping those entries \(rows joint probability is less than a reasonably threshold G. It is namely effective reduced database algorithm [3 Definition 3: Any entry with Prjoint &lt; G in the reduced database table is eliminated by the effective reduced database algorithm ERDB inputted by user The 2005 IEEE International Conference on Fuzzy Systems781 The joint probability threshold G could be either a certain value for each reduced database table, or the maximum probability of each reduced database table with a certain percentage. We call the former threshold probability threshold, and the latter maximum probability threshold Because the maximum probability threshold is adjusted with joint probability of the table, it has no problem with the curse of dimensionality for large number of attributes The effective reduced database algorithm \(ERDB  illustrated below EFFECTIVE REDUCED DATABASE ALGORITHM \(ERDB Calculate all one attribute only reduced database tables Choose that one with least rows Do loop until all attributes are extended 1 more attribute for attributes not already chosen 2 database tables 3 End do loop When a reduced database table extends a new attribute, the rows of attribute values in the new reduced database table formed will initially be all combinations of the rows of attribute values in the previous reduced database table plus the extended attribute  s one attribute only reduced database table Then we calculate the joint probability distribution of the new table. Finally forming the new reduced database table will be fully completed, after dropping the rows whose joint probabilities are less than the threshold G. The example of effective reduced database algorithm \(ERDB the procedure of extending all one attribute only reduced database tables to form the final reduced database table shown in Fig. 4 medium Joint Probability Distribution small large B 0.3 0.7 0 medium Joint Probability Distribution small large C 0 0.1 0.9 medium Joint Probability Distribution small large A 0.6 


0.6 0.4 0 medium Joint Probability Distribution small A  medium small B medium small medium small  Joint Probability Distribution small B  mediumsmall C  A Fig. 5: The example of effective reduced database algorithm Notice: the joint probability distribution of each new reduced database table is not simply multiplied with probabilities in reduced database tables formed in previous loop, but calculated from the original database every time If there are k attributes and equally i fuzzy sets of each attribute, entries \(rows such as 33 = 27 entries in Fig. 4. However, those entries with small joint probability are removed to help to reduce noisy rules. The removed entries will not be extended to form the next new reduced database table with one more attribute D E Less space and time complexity, transparency, knowledge based Effective reduced database algorithm provides the advantage of forming reduced database table with less space and time complexity As what we discussed in section III.A and III.C, there would be i k possible entries in reduced database table in total which will be huge number in practice. However, the effective reduced database algorithm can significantly reduce space and time complexity without losing too much information. For example, in them first example of supermarket basket analysis application in section IV, because there are 11 attributes in database and 3 fuzzy sets in each attribute, the possible combinations in total would be 311 = 177,147. It is a huge hypothesis space to search the right set of fuzzy association rules. However, there are only 9 entries left to generate fuzzy association rules by using the effective reduced database algorithm The reduced database table formed by using the effective reduced database algorithm is transparent for users. Each combination of words on its left side represent a rule discovered from original database, and the joint probability distribution on its right side will tell users how often the related combination of words in the same row occurs in the original database. And we can also work out more measurement of those rules according to the joint probability distribution, such as credibility of fuzzy association rules Those words associated with joint probability distribution are also very easy to understand for a human being The reduced database table can also be known as a knowledge based table contained the main information of 


knowledge based table contained the main information of original database, and it can be further used to discover other fuzzy models in the soft computing and machine learning such as fuzzy association rules, fuzzy decision tree [9], fuzzy Bayesian net [10], etc Fuzzy association rules from reduced database table After we get the final reduced database table by using the effective reduced database algorithm, we need determine attributes to be on the right side of rules as so to form the set of fuzzy association rules, and then drop those rules whose credibility is less than credibility threshold H from the reduced database table For example, if we assume the joint probability threshold G is 0.01 and select attribute C as the attribute on the right side of rules, we can respectively form rule 4 from the 2nd row and rule 5 from 3rd row in the reduced database table in Fig. 4 IF \(A is small B is small C is medium The 2005 IEEE International Conference on Fuzzy Systems782 IF \(A is small B is small C is large As the nature of reduced database table, the support of each fuzzy association rule is the joint probability associated with the row to form the rule on the right side of reduced database table. Therefore, supportrule 4 = Pr joint-2 = 0.018 and supportrule 5 Pr joint-3 = 0.162. According to the Definition 2, p = Pr joint-1 Pr joint-2 + Pr joint-3 = 0 + 0.018 + 0.162 = 0.18, therefore credibilityrule 4 = Pr joint-2 y p = 0.1 and credibilityrule 5 = Pr joint-3 y p = 0.9. If the credibility threshold H is 0.9, we only keep rule 5 in the end. This procedure is also applied to the right side of rules containing more than one attribute F. A simple experiment to prove fuzzy association rules from reduced database table In this simple experiment, we create a 3-attribute 289example original dataset \(attributes X, Y ? [0, 8 values of X, Y attributes are symmetrically selected with the interval 0.5 and values of Z attribute are inferred using consistent fuzzy association rules in Fig. 6, where every attributes use 3 equal space fuzzy sets defined in definition 2 IF \(X is small Z is small IF \(Y is large Z is small IF \(NOT \(X is small Y is small Z is large IF \(NOT \(X is small Y is medium Z is medium Fig. 6: The original set of fuzzy association rules Then, we discover another set of fuzzy association rules from the original dataset formed previously, by using the effective reduced database algorithm with probability threshold G = 0.035 and credibility threshold H = 0.5. After unifying the set of rules we discovered, we get the exactly same set of fuzzy association rules as the original set of rules in Fig. 6 IV A SUPERMARKET BASKET ANALYSIS APPLICATION For supermarket basket analysis, association rules can be used to adjust store layouts, cross-selling, promotions catalogue design and to identify customer segments based on buying patterns [11]. However, traditional rules are only restricted in the interested relations of certain groups of items consistently purchased together, so that the original database has to only use discrete attributes. This brings the big complexity problem, while analysing the relationship of hundreds of items or more Fuzzy association rules can deal with continuous attributes in database. We therefore develop a new approach associated with continuous attributes for supermarket basket analysis, in addition to using discrete attributes as well. For instance, in the first example of supermarket basket example database, we discover the set of fuzzy association rules to describe customer  s favourite by using the relations among spending or shopping frequency and nutrient contents. It therefore reduces the number of features or attributes to be analysed. In the second example, we discover the rules combined both discrete and continuous attributes as so to cover more important 


and continuous attributes as so to cover more important features for analysis, compared with traditional association rules The example database and data set The supermarket basket database contains three tables illustrated in Fig. 7 a customers, such as name, sex, age, postcode, etc b panel data, which is randomly generated by data generator according to several certain shopping behaviours of customers. It includes UPC \(universal product code number of items purchased, total price, etc c information published in the website [12], such as energy, fat content, sugar content of each product in per 100 gram Fig. 7: Supermarket basket database example B 1 The parameters and examples To mining fuzzy association rules using the effective reduced database algorithm from prepared data set, we set up maximum probability threshold G as 0.15, which drops those entries whose joint probability is less than 15% of the maximum probability of each reduced database table, and credibility threshold H as 0.9 Nutrient contents not items We use SQL to select an customer shopping history to form 2 data sets, which contains his total spending or the amount of items purchased in each day separately and 10 kinds of nutrient contents of products purchased in that shopping in total, to discover fuzzy association rules In the first data set, we discover 9 fuzzy association rules and one of them is shown below IF \(sum_of_energy is high sum_of_protein is high sum_of_carbohydrate is high sum_of_vitamins is medium sum_of_fat is high sum_of_sodium is high sum_of_sugar is medium sum_of_fibre is high sum_of_starch is high sum_of_iron is medium THEN \(total_spending is high In the second data set, we discover 8 fuzzy association rules, and one of them is shown below The 2005 IEEE International Conference on Fuzzy Systems783 IF \(sum_of_energy is high sum_of_protein is high sum_of_carbohydrate is high sum_of_vitamins is medium sum_of_fat is high sum_of_sodium is high sum_of_sugar is medium sum_of_fibre is medium sum_of_starch is high sum_of_iron is medium THEN \(amount_of_items_purchased is big We can use this method to analyse the favourite poroduct or behaviour of a certain customer or a certain group of customers. The new approach of using nutrient contents instead of items in the relations gives much less complexity of association rules, especially to analyse hundreds or thousands of items in the supermarket 2 V Both discrete and continuous attributes We give another simple example below that includes both discrete and continuous attributes in the fuzzy association rule which is discovered from the data set containing customer  s sex, age and the sum of sugar content of products he/she purchased each time IF \(sex is male age is old sum_of_sugar is medium As it can be seen, if we use crisp sets as special fuzzy sets in fuzzy association rules, we still have the traditional association rules Combining discrete attributes such as customer details \(sex postcode, city, etc attributes such as nutrient contents, spending, price, stock, cost and customer  s age in the association rules is very helpful in 


and customer  s age in the association rules is very helpful in various kinds of business analysis. It provides alternative new approach to find some good relations for analyser CONCLUSION AND FUTURE WORK Compared with traditional association rules, fuzzy association rules provide good linguistic explanation, and can deal with both discrete attributes and continuous attributes. It provides an alternative new approach in the applications of association rules, which not only can be used to reduce the complexity of association rules, but also to cover more important attributes in the rules In order to solve complexity problem of mining fuzzy association rules, we discover those rules by using the effective reduced database algorithm \(ERDB The reduced database table \(RDBT database algorithm are good methods to build a transparent and knowledge based fuzzy model with less space and time complexity. Afterwards it can be used to discover various kinds of fuzzy models, such as fuzzy association rules, fuzzy decision tree, fuzzy Bayesian net, etc In the end, we only keep those fuzzy association rules whose credibility is not less than credibility threshold H, which are discovered from the final reduced database table It is helpful to either use feature selection before running the ERDB algorithm or combine it with the ERDB algorithm which can drop redundant and unimportant attributes to reduce the dimension of the final reduced database table. This improvement is our future work ACKNOWLEDGMENT This work is supported by my supervisor Jim F. Baldwin The author thanks him for proposing the reduced database table \(RDBT ERDB REFERENCES 1] R. Agrawal, T. Imielinski, A. Swarmi  Mining Association Rules between Sets of Items in Large Databases  in proceedings of the ACMSIGMOD 1993 International Conference on Managementof Data Washington D. C., U.S.A., pp. 207-216, 1993 2] Chan Man Kuok, Ada Fu, and Man Hon Wong  Mining Fuzzy Association Rules in Databases  SIGMOD Record, pp. 41-46, Vol 27 No. 1, 1998 3] Jim Baldwin, http://www.enm.bris.ac.uk/teaching/enjfb/emat31600 4] Lotfi A. Zadeh  Fuzzy Logic = Computing with Words  IEEE Transaction on Fuzzy Systems, Vol 4, No. 2, May 1996 5] J. F. Baldwin, T. P. Martin and B. W. Pilsworth  Fril  Fuzzy and Evidential Reasoning in Artificial Intelligence  Research Studies Press Ltd. \(John Wiley 6] E.H. Ruspini  A new approach to clustering  Information and Control vol 15, 1969, p. 22-32 7] J. F. Baldwin, Dong \(Walter  Simple Fuzzy Logic Rules based on Fuzzy Decision Tree for Classification and Prediction Problem   Intelligent Information Processing II, Published by Springer, ISBN 0387-23151-X \(HC 8] R. Agrawal, R. Srikant  Fast algorithms for mining association rules   in proceedings of the 20th VLDB Conference, pages 487--499 Santiago, Chile, 1994 9]   Jim Baldwin, Sachin Karale  New concepts for fuzzy partitioning defuzzification and derivation of probabilistic fuzzy decision trees   North American Fuzzy Information Processing Society \(NAFIPS 10]   J. F. Baldwin, E. Di Tomaso  Bayesian networks for continuous values and uncertainty in the learning process  the EUSFLAT Conference Zittau, Germany, September 2003 11] XLMiner, http://www.resample.com/xlminer/help 12] Sainsbury  s online, http://www.sainsburys.co.uk/healthyeating The 2005 IEEE International Conference on Fuzzy Systems784 pre></body></html 


accurate estimates for distinct value queries and event reports This algorithm outperforms other estimators that are based on uniform sampling[8 9 e v e n when using much less sample space We also provide comparison with our Implication Lossy Counting ILC algorithm described in Section 5.1 which is based in the Lossy Counting algorithm introduced in For this series of experiments we used a real dataset of eight dimensions which was given to us by an OLAP company whose name we cannot disclose due to our agreement The cardinalities of the dimensions are presented in Table 3 The parameters of the algorithms are presented in Table 5 For NIPS/CI we used 64 concurrent bitmaps with a fringe size of four thus requiring memory enough to hold  2 4  1   64  K  1920 itemsets We expect that the averaging\([5 o v e r these man y bitmaps will result in an error less than 10 We used the exact same sample space for DS The bound parameter t for DS was set to  1920  50  following the suggestion in F o r ILC we used an approximation parameter   0  01 which increases the memory requirements of ILC relative to those of NIPS/CI or DS On the average ILC used more than twice the memory that NIPS/CI and DS used For example for the experiment in Figure 7\(B it used more than 8,000 entries We evaluate the results of the algorithms with respect to the number of tuples the cardinality of the participating dimensions and the implication conditions To simulate a real data stream scenario we tracked the conditional implication counts of A  B  E  F and the unconditional B  E using the aforementioned algorithms The rst workload corresponds to quite large compound cardinality while the second to very moderate cardinalities Table 4 presents the actual aggregates for various instances of the stream for   5 and  1  60 We believe that most workloads fall somewhere in the middle with respect to the complexity of the wanted implications and the size of the returned counts Figure 7\(A depicts the relative error as the stream evolves for workload A using the algorithms DS NIPS/CI and ILC for different implication parameters In Figure a we show the results for minimum support   5 and  1  60 or  1  80 The different  1 are encoded in the parentheses next to identication of the algorithm in the legend of the graph In Figure b we increased the minimum support to   50 We observe that the behavior of DS varies widely while NIPS/CI remains always below the expected 10 error DS actually keeps a sample of the distinct elements seen so far and tries to scale the implication count that holds for that sample to the whole set of distinct elements In most cases the data in the sample is not representative of the implication The situation for DS is exacerbated when the minimum support increases where quite a lot of samples do not participate in the count making the scaling even more errorprone Algorithm ILC in all cases returned very erroneous results although it used much more space than NIPS/CI and DS since it tries to store not the implication counts but the actual implicated itemsets In these workload the implicated itemsets overwhelm its available memory which is actually larger than the amount given to NIPS/CI and DS In gure 7\(B we present the results of the algorithms for workload B The situation is still in favor of NIPS/CI whose relative error remains always close to the expected 10 unlike DS who returns highly skewed errors even though the domain cardinalities are much smaller and therefore keeps in the sample space much more data As expected from the analysis the error guarantees of NIPS/CI are virtually unaffected by changes in the cardinalities or the number of tuples seen so far in the stream ILC returns very erroneous results although now the cardinalities and the implicated items are much smaller compared to those of workload A The reason is not only because it keeps too much information in memory i.e all the implicated itemsets while both NIPS/CI and DS only hold a mantissa for the count but also because the constraint    rel is broken as the number of tuples increases 7 Related Work There are unique challenges in query processing for the data stream model Most challenges are the result of the streams being potentially unbounded in size Therefore the amount of the storage required in order to get an exact size may also grow out of bounds Another equally important issue is the timely query response required although the volumes of data the need to be processed is continually augmented at a very high rate Essentially the amount of computation per data item received should not add a lot of latency to each item Otherwise any such algorithm wont be able to keep up with the data stream In many cases accessing secondary storage such as disks is not even an option In  there is a discussion of what queries can be answered e xactly using bounded memory and queries that must be approximated unless disk access is allowed Sketching techniques\([14 ha v e been introduced to build summaries of data in order to estimate the number F 0 of distinct elements in a dataset In three algorithms that      approximate the F 0 are described with various space and time requirements Distinct is dri v e n by hashing functions similar to those studied in 14 and provides highly accurate results for distinct value queries compared to those taken by uniform sampling by using only a fraction of their sample size In the algorithms Stick y Sampling and Lossy Counting are introduced that estimate frequency counts with application to association rules and iceberg cubes In a framework for performing set expression on continuously updated streams based on sketching techniques is presented In a general framework over multiple granularities is presented for both range-temporal and spatio-temporal aggregations In a framework for identifying hierarchical heavy hitters i.e hierarchical objects like network addresses whose prexes denes a hierarchy with a frequency above a given threshold is described The effect of impications between columns has been emphasized in the system that identies correlated pairs of columns and soft-dependencies and has been proved very useful in query optimization 8 Conclusions We have presented a generalized and parameterized framework that can accurately and efciently estimate implication counts and can be applied to many scenarios To the best of our knowledge this is the rst practical and truly scalable approach to the problem Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


 Dimension Cardinality A 1557 B 2669 C 2 D 2 E 3363 F 131 G 660 H 693 Table 3 Cardinalities Workload A Workload B Tuples A  B  E  G E  B 134,576 608 50 672,771 12,787 125 1,344,591 34,816 152 2,690,181 84,190 165 4,035,475 132,161 182 5,381,203 187,584 188 Table 4 Impl counts w.r.t tuples NIPS/CI bitmaps 64 NIPS/CI K 2 DS sample size 1920 DS bound t 39 ILC  0.01 Table 5 Algorithm Parameters           0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILC \(.8             0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILS \(.8           0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6 ILC \(.8             0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILC \(.8   a   5 b   50 a   5 b   50 A Workload A B Workload B Figure 7 Relative Error vs stream size of online estimation within small errors of complex implication and non-implication counts between attributes of a data stream under severe memory and processing constraints and even in the presence of noise We prove that the complement problem of estimating non-implication counts can be      approximated when the size of the fringe zone is xed appropriately We demonstrate that existing algorithms for estimating frequent itemsets or sampling cannot be applied to the problem since they lose the cumulative effect of small implications In addition through an extensive set of experiments on both synthetic and real data we have shown that NIPS/CI always remains very close to the actual implication count capturing even very small implications whose total contribution is signicant References  R Agra w al A Evmie vski and R Srikant Information sharing across private databases In ACMSIGMOD  2003  N Alon Y  Matias and M Sze gedy  The space comple xity of approximating the frequency moments Journal of Computer and System Sciences  58:137 147 1999  A Arasu B Babcock S Bab u J McAlister  and J W idom Characterizing memory requirement for queries over continuous data streams In Proceedings of the Twenty-rst ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems  2002  B Babcock S Bab u M Datar  R  Motw ani and J W idom Models and Issues in Data Stream Systems In Proceedings of the Twenty-rst ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems  2002  Z Bar Y ossef T  Jayram R K umar  D  S i v akumar  and L T r e visan Counting Distinct Elements in a Data Stream In RANDOM  pages 110 2002  A Belussi and C F aloutsos Estimating the selecti vity of spatial queries using the correlation fractal dimension In VLDB95 Proceedings of 21th International Conference on Very Large Data Bases September 11-15 1995 Zurich Switzerland  pages 299310 1995  J Bunge and M Fitzpatrick Estimating the number of species A r e vie w  Journal of American Statistical Association  88:364373 1993  M Charikar  S  Chaudhuri R Motw ani and V  Narasayya T o w ards estimation error guaranties for distinct values In ACMPODS  pages 268279 2000  S Chaudhuri R Motw ani and V  Narasayya Random sampling for histogram construction How much is enough In ACMSIGMOD  pages 436 447 1998  G Cormode F  K orn S Muthukrishnan and D Sri v astana Finding hierar chical heavy hitters in data streams In VLDB  2003  M Datar  A  Gionis P  Indyk and R Motw ani Maintaing stream statistics over sliding windows In Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms  2002  A Deshpande M Garof alakis and R Rastogi Independence is Good Dependency-Based Histogram Synopses for High-Dimensional Data In ACM SIGMOD  2001  C Estan S Sa v age and G V a r ghese Automatically inferring patterns of resource consumption in network trafc In ACMSIGCOMM  2003  P  Flajolet and N Martin Probabilistic Counting Algorithms for Data Base Applications Journal of Computer and System Sciences  pages 182209 1985  N Friedman L Getoor  D  K oller  and A Pfef fer  Learning probabilistic relational models In IJCAI  pages 13001309 1999  S Ganguly  M  Garof alakis and R Rastogi Processing set e xpressions o v e r continuous update streams In ACM SIGMOD  pages 265276 2003  P  Gibbons Distinct sampling for highly-accurate answers to distinct v alues queries and event reports In VLDB  pages 541550 2001  P  J Haas J F  Naughton S Seshadri and L Stok es Sampling-based estimation of the number of distinct values of an attribute In VLDB  1995  I Ilyas V Markl P  Haas P  Bro wn and A Aboulnaga CORDS Automatic Discovery of Correlations and Soft Functional Dependencies In ACMSIGMOD  2004  J Jung B Krishnamurthy  and M Rabino vich Flash cro wds and denial of service attacks Characterization and implications for cdns and web sites In ACMWWW  2002  J Ki vinen and H Mannila Approximate dependenc y inference from relations Theoritical Computer Science  149:129149 1995  G Manku and R Motw ani Approximate frequenc y counts o v e r data streams In VLDB  2002  V  Poosala P  J Haas Y  E Ioannidis and E J Shekita Impro v e d histograms for selectivity estimation of range predicates In ACMSIGMOD  pages 294305 1996  S Stolfo W  Lee P  Chan W  F an and E Eskin Data mining-based intrusion detectors An overview of the columbia ids project ACMSIGMOD Record  30\(4 2001  H W ang D Zhang and K G Shin Detecting SYN Flooding Attacks In INFOCOM 2002  2002  K Whang B V ander Zander  and H T aylor  A Linear Time Probabilistic Counting Algorithm for Database Applications ACM Transactions on Database Systems  pages 209229 1990  D Zhang D Gunopulos V  Tsotras and B  See ger  T emporal and spatiotemporal aggregations over data streams using multiple time granularities Information Systems  28\(1-2 2003 The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ofcial policies either expressed or implied of the Army Research Laboratory or the U S Government Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


Figure 3a and 3b, is negative since CBA and PART achieved a higher classification rate against this particular dataset A comparison of the knowledge representation produced by our method, PART and CBA has been conducted to evaluate the effectiveness of the set of rules derived. Figure 4 represents the classifiers generated form the hyperheuristic datasets. Analysis of the rules sets indicated that MMAC derives a few more rules than PART and CBA for the majority of the datasets. In particular, the proposed method produced more rules than PART and CBA on 8 and 7 datasets, respectively. A possible reason for extracting more rules is based on the recursive learning phase that MMAC employs to discover more hidden information that most of the associative classification techniques discard, since they only extract the highest confidence rule for each frequent item that survives MinConf Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 6. Conclusions A new approach for multi-class, and multi-label classification has been proposed that has many distinguishing features over traditional and associative classification methods in that it \(1 that contain rules with multiple labels, \(2 evaluation measures for evaluating accuracy rate, \(3 employs a new method of discovering the rules that require only one scan over the training data, \(4 introduces a ranking technique which prunes redundant rules, and ensures only high effective ones are used for classification, and \(5 discovery and rules generation in one phase to conserve less storage and runtime. Performance studies on 19 datasets from Weka data collection and 9 hyperheuristic scheduling runs indicated that our proposed approach is effective, consistent and has a higher classification rate than the-state-of-the-art decision tree rule \(PART and RIPPER algorithms. In further work, we anticipate extending the method to treat continuous data and creating a hyperheuristic approach to learn  on the fly   which low-level heuristic method is the most effective References 1] R. Agrawal, T. Amielinski and A. Swami. Mining association rule between sets of items in large databases In Proceeding of the 1993 ACM SIGMOD International Conference on Management of Data, Washington, DC May 26-28 1993, pp. 207-216 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rule. In Proceeding of the 20th International Conference on Very Large Data Bases, 1994, pp. 487   499 3] M. Boutell, X. Shen, J. Luo and C. Brown. Multi-label semantic scene classification. Technical report 813 Department of Computer Science, University of Rochester Rochester , NY 14627 &amp; Electronic Imaging Products R &amp D, Eastern Kodak Company, September 2003 4] A. Clare and R.D. King. Knowledge discovery in multilabel phenotype data. In L. De Raedt and A. Siebes editors, PKDD01, volume 2168 of Lecture Notes in Artificial Intelligence, Springer - Verlag, 2001,  pp. 42-53 5] P. Cowling and K. Chakhlevitch. Hyperheuristics for Managing a Large Collection of Low Level Heuristics to Schedule Personnel. In Proceeding of 2003 IEEE conference on Evolutionary Computation, Canberra Australia, 8-12 Dec 2003 6] R. Duda, P. Hart, and D. Strok. Pattern classification Wiley, 2001 7] E. Frank and I. Witten. Generating accurate rule sets without global optimisation. In Shavlik, J., ed., Machine Learning: In Proceedings of the Fifteenth International 


Learning: In Proceedings of the Fifteenth International Conference, Madison, Wisconsin. Morgan Kaufmann Publishers, San Francisco, CA, pp. 144-151 8] J. Furnkranz. Separate-and-conquer rule learning Technical Report TR-96-25, Austrian Research Institute for Artificial Intelligence, Vienna, 1996 9] W. Li, J. Han and J. Pei. CMAR: Accurate and efficient classification based on multiple class association rule. In ICDM  01, San Jose, CA, Nov. 2001, pp. 369-376 10 ] T. Joachims. Text categorisation with Support Vector Machines: Learning with many relevant features. In Proceeding Tenth European Conference on Machine Learning, 1998,  pp. 137-142 11] T. S. Lim, W. Y. Loh and Y. S. Shih. A comparison of prediction accuracy, complexity and training time of thirtythree old and new classification algorithms. Machine Learning, 39, 2000 12] B. Liu, W. Hsu and Y. Ma. Integrating Classification and association rule mining. In KDD  98,  New York, NY, Aug 1998 13] J.R. Quinlan. C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann, San Francisco, 1993 14] J.R. Quinlan. Generating production rules from decision trees. In Proceeding of the 10th International Joint Conferences on Artificial Intelligence,  Morgan Kaufmann San Francisco, 1987, pp. 304-307 15] R. Schapire and Y. Singer, "BoosTexter: A boosting-based system for text categorization," Machine Learning, vol. 39 no. 2/3, 2000, pp. 135-168 16] F. Thabtah, P. Cowling and Y. Peng. Comparison of Classification techniques for a personnel scheduling problem. In Proceeding of the 2004 International Business Information Management Conference, Amman, July 2004 17]Y. Yang. An evaluation of statistical approaches to text categorisation. Technical Report CMU-CS-97-127 Carnegie Mellon University, April 1997 18] X. Yin and J. Han. CPAR: Classification based on predictive association rule. In  SDM  2003, San Francisco CA, May 2003 19]CBA:http://www.comp.nus.edu.sg/~dm2/ p_download.html 20] Weka: Data Mining Software in Java http://www.cs.waikato.ac.nz/ml/weka 21] M. J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. New algorithms for fast discovery of association rules. In Proceedings of the 3rd KDD Conference, Aug. 1997 pp.283-286 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


Low Medium 1.152e3 1.153e3 5.509e3 3.741e3 20 20.0 393 0.647 0.9900 0.0092 1.0000 0.0093 R=1000 1e-4 0.2 High 1.153e3 1.154e3 3.180e3 2.698e3 45 45.0 597 0.214 0.9900 0.0071 1.0000 0.0162 2e-4 2e-4 Low 0.4017 0.4024 109.291 71.994 4 4.0 39.7 0.996 0.9960 0.0075 0.9995 0.0012 2e-3 1.0 1e-3 Medium Medium 0.4128 0.4129 17.488 19.216 6 6.0 29.6 0.992 0.9938 0.0018 0.9987 0.0034 R=0.1 0.01 4e-3 High 0.4612 0.4722 4.1029 5.410e3 12 12.0 28.0 0.992 0.9902 0.0045 1.0000 0.0132 0.04 1e-8 Low 0.0250 0.0272 253.917 155.263 3 3.0 38.0 1.002 0.9991 0.0001 0.9994 0.0008 1e-3 1e4 1e-7 High Medium 0.0338 0.0290 17.303 12.837 3 3.0 14.5 0.332 0.9900 0.0058 0.9951 0.0045 R=1e-5 1e-2 1e-6 High 0.0918 0.0557 1.6071 1.225e5 8 8.0 9.9 0.992 0.9906 0.0034 0.9994 0.0142 1e-3 TABLE II SIMULATION RESULTS FOR: TRACKING REGIME PD=1, Q=100, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 2e-4 Low 0.0408 0.0407 2.9255 2.0060 5 5.0 30.3 0.996 0.9933 0.0016 0.9998 0.0020 0.02 1.0 1e-3 Medium Medium 0.0446 0.0446 0.5143 333.138 10 10.0 27.8 0.996 0.9900 0.0042 0.9995 0.0093 R=0.01 0.1 4e-3 High 0.0763 0.1062 0.1580 3.402e3 84 84.0 90.9 0.793 0.9900 0.0099 1.0000 0.0334 0.4 TABLE III SIMULATION RESULTS FOR: TRACKING REGIME PD=0.9, Q=1000, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 0.05 Low 1.166e3 1.167e3 1.073e4 3.357e4 11 12.2 257 0.542 0.9900 0.0089 1.000 0.0073 5e-5 1e-4 0.1 Low Medium 1.167e3 1.167e3 5.509e3 1.481e4 21 23.2 309 0.858 0.9900 0.0067 1.0000 0.0052 R=1000 1e-4 0.2 High 1.168e3 1.170e3 3.180e3 6.979e3 46 50.6 475 0.798 0.9900 0.0089 1.000 0.0022 2e-4 3] T. Fortmann, Y. Bar-Shalom, Y. Scheffe, and S. B. Gelfand  Detection Thresholds for Tracking in Clutter- A Connection Between Estimation and Signal Processing  IEEE Trans Auto. Ctrl., Mar 1985 4] S. B. Gelfand, T. Fortmann, and Y. Bar-Shalom  Adaptive Threshold Detection Optimization for Tracking in Clutter  IEEE Trans. Aero. &amp; Elec. Sys., April 1996 5] Ch. M. Gadzhiev  Testing the Covariance Matrix of a Renovating Sequence Under Operating Control of the Kalman Filter  IEEE Auto. &amp; Remote Ctrl., July 1996 6] L. C. Ludeman, Random Processes: Filtering, Estimation and Detection, Wiley, 2003 7] L. Y. Pao and W. Khawsuk  Determining Track Loss Without Truth Information for Distributed Target Tracking Applications  Proc. Amer. Ctrl. Conf., June 2000 8] L. Y. Pao and R. M. Powers  A Comparison of Several Different Approaches for Target Tracking in Clutter  Proc Amer. Ctrl. Conf., June 2003 9] X. R. Li and Y. Bar-Shalom  Stability Evaluation and Track Life of the PDAF Tracking in Clutter  IEEE Trans. Auto Ctrl., May 1991 10] X. R. Li and Y. Bar-Shalom  Performance Prediction of 


10] X. R. Li and Y. Bar-Shalom  Performance Prediction of Tracking in Clutter with Nearest Neighbor Filters  SPIE Signal and Data Processing of Small Targets, July 1994 11] X. R. Li and Y. Bar-Shalom  Detection Threshold Selection for Tracking Performance Optimization  IEEE Trans. on Aero. &amp; Elect. Sys., July 1994 12] D. Salmond  Mixture Reduction Algorithms for Target Tracking in Clutter  SPIE Signal and Data Processing of Small Targets, Oct. 1990 13] L. Trailovic and L. Y. Pao  Position Error Modeling Using Gaussian Mixture Distributions with Application to Comparison of Tracking Algorithms  Proc. Amer. Ctrl. Conf., June 2003 4323 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





