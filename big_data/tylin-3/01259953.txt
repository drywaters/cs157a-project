Proceedings of the Second International Conference on Machine Learning and Cybernetics Xi 2-5 November 2003 INCREMENTAL UPDATE FOR FUZZY ASSOCIATION RULES JIANJlANG LU'Z2,BAO-WEN XULz  Department of Computer Science and Engineering, Southeast University, Nanjing, 210096, China PLA University of Science and Technology, Nanjing, 210007, China Jiangsu Institute of Software Quality, Nanjing, 210096, China E-MAIL jjlu@seu.edu.cn Abstract In practice many databases are not static but changeable When some records are added or deleted 
in a database it could potentially invalidate some existing rules or introduce new rules Thus maintenance of the association rules is an important problem An incremental update algorithm for the fuzzy association rules is presented this algorithm uses the last mining results to maintain the collection of frequent fuzzy attribute sets and its negative border along with their support count incrementally The experimental results show that the incremental update algorithm can effeetively save the time due to avoiding the repeated cost introduced by doing mining task directly in the whole database Keywords Association rules; Incremental update 
1 Introduction Data mining Fuzzy clustering Negative border The mining of association rules is one of the most important issues in the field of data mining. The problem of mining Boolean association rules is introduced in 1,2 Parallel mining and weighted mining of Boolean association rules have also been studied in 3,4 In practice many databases are not static but changeable When some records are added or deleted in a database it could potentially invalidate some existing rules or introduce new rules Thus maintenance of the association rules is 
an important problem Ref SI introduces the problem of maintaining Boolean association rules and gives the FUP algorithm that can maintain Boolean association rules when new transactions are added into a database Ref 6 proposes a more general algorithm FUP2 which can maintain Boolean association rules when some records are inserted deleted or updated in the database Ref 7 uses negative borders to maintain rules Ref 8 proposes an incremental algorithm for maintaining constrained Boolean association rules Recently people have got interested in quantitative attributes The problem of 
mining quantitative association rules is introduced in 9 The algorithm in 9 finds association rules by partitioning the amibute domain combining adjacent partitions and then transforming the problem into a binary one Although this mining algorithm for quantitative association rule can solve problem introduced by quantitative attributes it introduces some other problems The first problem is that eqni-depth partitioning cannot embody the actual distribution of the data The second problem is caused by the sharp boundary between intervals Ref IO uses fuzzy set to soften partition boundary of 
the domains and presents the concept of fuzzy association rules but it does not present partitioning algorithm which can embody the actual distribution of the data and does not present the mining algorithm for fuzzy association rules which fits for large databases Ref Ill uses linguistic clouds to soften partition boundary of the domains and presents mining algorithm for association rules with linguistic cloud models Ref I21 uses the relational fuzzy c-means algorithm to partition the quantitative attributes into several fuzzy sets then the problem of mining fuzzy 
association rules is introduced by combining fuzzy sets The relational fuzzy c-means algorithm can embody the actual distribution of the data Furthermore fuzzy sets can soflen partition boundary But combining fuzzy sets can obtain excessive fuzzy association rules so the mining algorithm cannot fit for large database Liu Hsu and Ma I propose classification based on associations \(CBA\which uses iterative method to find the frequent and accurate possible rule set and then uses method of elicitation to 
build classification system. But it is important to note that the intervals involved in quantitative association rules may not he concise and meaningful A fuzzy approach that can be used for mining interesting rules for classification with degree of membership is in 14,15 This approach represents the revealed regularities and exceptions using linguistic terms The use of linguistic terms allows human users to understand the discovered rules better because of the affinity with the human 0-7803-7865.2/03/$17.00 02003 IEEE 2591 


knowledge representation Furthermore this approach is capable of finding interesting relationships among attributes without any subjective input required of the users Some results show that the classification methods in 14,15 have good accuracy and interpretability than C4.5 and CBA In this paper Apriori algorithm is first improved for mining fuzzy association rules, and this improved mining algorithm can fit for large database Then an incremental update algorithm for the fuzzy association rules is presented this algorithm uses the last mining results to maintain the collection of frequent fizzy attribute sets and its negative border along with their support count incrementally The experimental results show that the incremental update algorithm can effectively save the time due to avoiding the repeated cost introduced hy doing mining task directly in the whole database The rest of this paper is organized as follows In section 2 Apriori algorithm is improved for mining fuzzy association rules In section 3 an incremental update algorithm for fuzzy association rules is presented The conclusions are drawn in section 4 2 An algorithm for mining fnzzy association rules Let T t,,t,,...,t be a relational database fj represents the j-th record in T let I=\(i,,i2,.-,im be the attribute set where ij denotes a Boolean categorical or quantitative attribute tj[ik represents value of the j-th record in attribute ik  Values of the record in attribute need to be partitioned into several fuzzy sets for mining fuzzy association rules Let A and A be two values of the record in Boolean attribute, then two values can be partitioned into two fuzzy sets A and A where I,x=A  O,x  A Categorical attribute with fewer values can be Partitioned into several fuzzy sets with the same method We adopt fuzzy c-means \(FCM\algorithm I6 to partition quantitative attribute into several fuzzy sets Comparing with 12 there are two differences One is that FCM algorithm costs less time than the relational fuzzy c-means algorithm, another is that fuzzy sets are not expressed with normal fuzzy numbers as in  121 In order to mine fuzzy association rules we consmct a new database through original database T In this new database, attributes are fuzzy sets; values of the record in attributes are obtained as follows Let i be a fuzzy set of attribute  i is an attribute in new database Value of qe j-th record in attribute i is i:\(t,[ik  i:\(tj[ik Is membership degree of tj with respect to fuzzy set i  In this new database, because attributes are fuzzy sets we call attributes fuzzy attributes next Let I still be the fuzzy attribute set let t,\(y represent value of the j-th record in fuzzy attribute y  tj\(y falls in OJ Let x ty,,-,yp y  tYp'lr....Yp+q c I XnY=Q A fuzzy association rule is an implication of the form Y The support and confidence of fuzzy association Definition 1 Let fuzzy attribute set X rule are defined as Ref 12 X y y c1,thesupportofXisdefinedasfollows where tfitj\(ym is called the support count of 1=11-1 fuzzy attribute set X Fuzzy attribute sets with at least a minimum support are called frequent fuzzy attribute sets Definition 2 The support of X 2 Y is defined as follows f Htj\(YJ sup  j=lm n Definition 3 The confidence of X  Y is defined as follows Because tj\(yk falls in OJ we can know that all subsets of a frequent fuzzy attribute set must also be frequent according to definition 1 With the above finding it is easy to modify Apriori algorithm I to mine fiizzy association rules In this section we will discuss an Abalone database from UCI Machine Learning Repository which bas nine attributes 4177 instances The quantitative attributes including length, diameter height whole weight shucked weight viscera weight shell weight and rings are respectively denoted as 11  IS To simplify we mine the fuzzy association rules with the consequent IS The attribute sex is a categorical attribute with three values M F and I Three values are partitioned into three fuzzy sets Proceedings of the Second International Conference on Machine Learning and Cybernetics an 2-5 November 2003 2592 


Procee of the Second International Conference on Machine Le&ning and Cyberneacs Xi\224 2-5 November 2003 M F and I Fwzy set M is defined as The definitions of fuzzy set F and I are similar Each quantitative attribute is partitioned into three fuzzy sets large middle and small which are denoted them as integer I 2 and 3 Each element in fuzzy association rules is represented by attribute fuzzy set such as 41 I Figure I shows the time for mining frequent fuzzy attribute set with different minimum support Not including the cost of the FCM algorithm ai5 ai 0.~6 o m Figure 1 Costing time 3 lncremental update for fuzzy association rules In practice the database is updated frequently rather than static After being inserted deleted and updated new fuzzy association rules will present and some others will become inapplicable So the fuzzy association rules must he maintained Update can he looked as a process of two steps that delete old records first and then insert new ones So we only consider the two conditions, insert and delete In order to find fuzzy association rules in the updated database a naive way is directly to run mining algorithm again Since there exist much unnecessary repeated computing cost the efficiency of this method is lower An incremental update algorithm for fuzzy association rules is presented in this section This algorithm uses the last mining results and incrementally updates fuzzy association rules to avoid the repeated cost introduced by doing mining task directly in the whole database Because fuzzy association rules can be easily obtained from tkequent fuzzy attribute sets the rule maintenance problem becomes the problem of maintaining frequent fuzzy attribute sets 3.1 Addition of new records When new records are added to the database some old frequent fuzzy attribute sets can potentially become infiequent in the updated database Similarly some old infrequent fuuy attribute sets can potentially become frequent In order to solve the update problem efficiently the membership value of fuzzy sets in the new records should be known One approach is using FCM algorithm to cluster again in the updated database But when the volume of data is very huge this approach will cost much time Because the problem of updating fuzzy association rules usually assumes that only few new records are added to the database we can consider that the original cluster centers vi will not change approximately So we can compute the membership value directly by the equation ut  1 x\(d  dl m-\222  1,2  c Definition 4 The negative border NBd\(FL of a collection of fuzzy attribute sets FL is defined as follows Given a collection FL c P\(F of fuzzy attribute closed with respect to the set inclusion relation, in which F is the collection of all fuzzy attributes and P\(F denotes the subset space of F the negative border NBd\(FL of FL consists of the minimal fuzzy attribute sets X E F not in FL In the rest of this section DB denotes the original database db denotes the added records and DE denotes the updated database Also FLDu  FLdb and FLDB denote the collection of frequent fuzzy attribute sets and NRd\(FLDa  NBd\(FLdb and NBd\(FLDB\222 denote the negative border of the original database incremental database and the updated database We maintain the collection of frequent fuzzy attribute sets and its negative border along with their support count in the database On one hand we check if there exist some old frequent fuzzy attribute sets to become infrequent in the updated database Let f E FL since we know the support count forfin DB and db the support count forfin DRudb can he easily obtained Iffdoes not have minimum support in DBudb it is removed from FL\224  On the other hand some new fuzzy attribute sets may become frequent in the updated database There exist two conditions I If none of the fuzzy attribute set inNRd\(FLDB gets the minimum support no new fuzzy attribute set will he added to FLDB\222 2 If some fuzzy attribute sets in NBd\(Ff\224 gets the minimum support move them to FL?\222 and recompute the negative border If  FLDB\222 U NBd\(FLnB\222  FLD8 U NBd\(FLDn We have to find the negative border closure of FLDB and scan the entire database DE once to find the updated frequent fuzzy attribute set and its negative border Further 2593 


Proceedings of the Second International Conference on Machine Learning and Cybernetics Wan 2-5 November 2003 detail can be obtained in Algorithm 1 An incremental update algorithm lor frequent fuzzy attribute sets Input the original collection of frequent fuzzy attribute sets and its negative border along with their fuzzy support count cluster centers of each quantitative attribute db minimum support Output updated frequent fuzzy attribute sets and its negative border I Compute membership values of each record in db according to the original fuzzy cluster centers and then compute the frequent fuzzy attribute sets in db 2 Compute support count of the original collection of frequent fuzzy amibute sets and its negative border in db  3 Compute the total support count of tbe original collection of frequent fuzzy attribute sets and its negative border in DBudb Check if they have minimum support add them to updated frequent fuzzy attribute sets 4 For each frequent fuzzy attribute set in db if it is presented in original negative border, check its total support If it has minimum support add it to updated frequent fuzzy attribute sets 5 Compare FLDB with FLDB if they are not equal recompute the updated negative border else terminate the algorithm 6 Compare FL U NBd\(FLDB with FLDn U NBd\(FLDB if they are not equal, recompute the closure of updated negative border and scan the whole database once to obtain the final updated frequent fuzzy attribute set and compute its negative border 3.2 Experimental analysis In the Abalone database we select 4000 records to conslruct the original database remain 177 records as added records to construct the incremental database db Set minimum fuzzy support to be 0.22 and minimum fuzzy confidence to be 0.5 total 11 fuzzy association rules listed in table 1 are obtained using incremental update method and mining the whole database respectively The results are equal The former 10 rules are presented in the original database and the last rule is an added rule These results show that incremental update method can discovery the updated fuzzy association rules Tlie experiment an3 aone at PI1 233MHZ with 32M RAM the time for mihing in the whole database is 389 seconds while the incremental update method only use 2 seconds Ohviously incremental update method can effectively save the time Table 1 Fuuy association rules I Incrementalupdatemining I Mining in the whole 1 4 Conclusions In this paper Apriori algorithm is improved for mining fuzzy association rules which improved mining algorithm can fit for large database with many quantitative attributes Then an incremental update algorithm for fuzzy association rules is presented This algorithm uses the last mining results and incrementally updates fuzzy association rules to avoid the repeated cost introduced by doing mining task directly in the whole database Acknowledgements This work was supported in part by the National Natural Science Foundation of China NSFC\\(60073012 National Grand Fundamental Research 973 Program of China \(2002CB312000\National Research Foundation for the Doctoral Program of Higher Education of China Natural Science Foundation of Jiangsu Province China BK2001004 Opening Foundation of State Key Laboratory of Software Engineering in Wuhan University and Opening Foundation of Jiangsu Key Laboratory of Computer Information Processing Technology in Soochow University References I R Agrawal T Imieliski and A Swami Mining association rules between sets of items in large databases Proceedings of ACM SIGMOD Conference on Management of Data. Washington, DC pp207-216,1993 2594 


Proceedings of the Second International Conference on Machine Learning and Cybernedcs Wan 2-5 November 2003 2 R Agrawal R Srikant 223Fast algorithms for mining association rules\224 Proceedings of the 1994 International Conference on Very Large Databases Santiago, Chile pp487-499 1994 3 R Agrawal J.C Shafer 223Parallel mining of association rules design implementation and experience\224 Special Issue on Data Mining IEEE Transactions on Knowledge and Data Engineering 1996,8\(6 962-969  Lu Jianjiang 223Research on algorithms of mining association rules with weighted items\224 Jod of Computer Research and Development 2002 39\(10 I28 1  1286 5 D.W.L Cheng J Han V.T Ng 223Maintenance of discovered association rules in large databases An incremental update technique\224 Proc of the 12th ICDE New Orleans, Louisiana pp106-114 1996 6 D.W.L Cheng S.D.Lee B Kao 223A general incremental technique for maintaining discovered association rules\224 Proc of the Fifth International Conference on Database Systems for Advanced Applications. Melboume, Australia, pp185-194 1997 7 S Thomas S Bodagala K Alsabti et al 223An efficient algorithm for the incremental updation of association rules in large databases\224 Proc KDD\22297 Newport Beach, California, USA pp263-266 1997 8 Kou Yu-jing Wang Cbun-hua Huang Hou-han 223An incremental algorithm for maintaining constrained association rules\224 Journal of Computer Research and Development, 2001,38\(8\947-951 9 R Srikant R Agrawal 223Mining quantitative association rules in large relational tables\224 Proceedings of the ACM SIGMOD Conference on Management of Data Montreal Canada ppl-12 1996 IO M.K Cban F Ada H.W Man 223Mining fuzzy association rules in database\224 Proceedings of the ACM Sixth International Confer\223 on Information and Knowledge Management Las Vegas Neveda pp10-14 1997 Lu Jianjiang Qian Zuoping Song Ziling 223Application of normal cloud association rules on prediction\224 Journal of Computer Research and Development 2000,37\(1 I 1317-1320 I21 Lu Jianjiang Qian Zuoping Song Zilin 223Mining linguistic valued association rules\224 Journal of Software, 2001,12\(4\607-611 I31 B Liu W Hsu Y Ma 223Integrating classification and association rule mining\224 Proceedings of the International Conference on Knowledge Discovery and Data Mining New York pp80-86,1998 I41 W.H Au K.C.C Chan, \223Classification with degree of membership A fuzzy approach\224 Proc of the 1st IEEE Int\222l Conf on Data Mining San Jose CA 3542,2001 I51 Zou Xiaofeng Lu Jianjiang Song Zilin 223Classification system based on fuzzy class association rules\224 Joumal of Computer Research and Development 2003 40\(5 651-656 I61 R.J Hathaway J.W Davenport J.C Bezdek 223Relational dual of the c-means algorithms\224 Pattern Recognition, 1989,22\(2\205-212 2595 


Proceedings of the Second IEEE International Conference on Cognitive Informatics \(ICCI03 0-7695-1986-5/03 $17.00  2003 IEEE 


Proceedings of the Second IEEE International Conference on Cognitive Informatics \(ICCI03 0-7695-1986-5/03 $17.00  2003 IEEE 


Proceedings of the Second IEEE International Conference on Cognitive Informatics \(ICCI03 0-7695-1986-5/03 $17.00  2003 IEEE 


Publishing Company, 200 1 Structuring and Categorization as a First Step in Summarizing Legal Cases. An International Journal of Information Processing and Management, 33\(6 pp. 727-737 13] T. Mori. A Term Weighting Method based on Information Gain Ratio for Summarizing Documents retrieved by IR System. Journal of NLP, 9\(4 I41 M. Okumura, H. Nanba. New Topics on Automated Text Summarization. Journal of NLP, 9\(4 I51 K. Tsuda, M. Shishibori &amp; J. Aoe. An Efficient String Pattern-matching Algorithm. The Application of Reducing 103 pre></body></html 


 10 20 30 40 50 60 70 80 90 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 100 200 300 400 500 600 700 800 900 Implication Count 0.06 0.08 0.1 Mean Error Bounded Fringe Unbounded Fringe 10 20 30 40 50 60 70 80 90 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 100 200 300 400 500 600 700 800 900 Implication Count 0.06 0.08 0.1 Mean Error Bounded Fringe Unbounded Fringe  A   100  A   1  000  A   100  A   1  000 1000 2000 3000 4000 5000 6000 7000 8000 9000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 20000 40000 60000 80000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 1000 2000 3000 4000 5000 6000 7000 8000 9000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 20000 40000 60000 80000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe  A   10  000  A   100  000  A   10  000  A   100  000 Figure 4 DataSet One with c  1 Figure 5 DataSet One with c  2 and therefore these itemsets participate in the implication count The number of tuples created by this step is S  50  c  1   2  4   The rest of the steps create itemsets that should not participate in the count We create three different kind of tuples that break one implication condition The relative weight of each kind is 1  3 Generate   A  S   3 pairs  a i  b j  where each a i is different than all itemsets of A created before As in the previous step for each a i create at most c different b j  For each combination  a i  b j  write 50 tuples Then for each a i create eight b  j different than all b j s created before And write the eight tuples  a i  b  j   This step creates   A  S   3 new itemsets of A that should not participate in the implication count because they do not satisfy the minimum top-condence level although they satisfy both the minimum support and the maximum multiplicity constraint The number of tuples created by this step is   A  S   3  50  c  1   2  8   Generate   A  S   3 pairs  a i  b j  where each a i is different than all itemsets of A previously generated and each one appears with u different b j  where c  1  u  c  10 Write 50 such tuples This step creates   A  S   3 new itemsets of A that should not participate in the implication count because they do not satisfy the maximum multiplicity condition The number of tuples created by this step is   A  S   3  50  c  5  5  Generate   A  S   3 pairs  a i  b j  where each a i is different than all itemsets of A previously generated For each pair  a i  b j  write 40 tuples This step creates   A  S   3 new itemsets of A that should not participate in the implication count because they do not satisfy the minimum support requirement The number of tuples created by this step is   A  S   3  40 Shufe the output le This step just demonstrates that the operation of the algorithm is independent to the ordering of the tuples Estimate the implication count using algorithm NIPS/CI with a fringe size of four and also without a bounded fringe Perform one hundred such experiments and calculate the mean and the standard deviation of both estimations The total number of tuples for each experiment can be derived by adding the partial number of tuples created in each step For example for  A   10000 S  5000 and c  4 the average number of tuples for the corresponding experiment was 015 3  108  333 A minimum support of 50 tuples for this case corresponds to only 015  001 of the tuples demonstrating that in the implication count contribute even implications that hold for a very small number of tuples Figures 4,5 and 6 show the results for c  1  2  4 for varying cardinalities  A   The x-axis corresponds to the actual implication count of the dataset as that was imposed by the creation process The y-axis denotes the mean relative error as it is calculated by running one hundred experiments We used the following formula to estimate the mean relative error relative error   Actual S  Measured S  Actual S  Graphs Bounded Fringe express the experimental results for the case of a fringe with size F  4 while graphs Unbounded Fringe demonstrate the result for the case of an arbitrarily large fringe The error bars correspond to the statistical deviation of the mean error as that was computed by one hundred such experiments.The deviation is generally negligible which means that the error of the estimated S is always very close to the mean error We also observe that the difference between the estimation using a bounded fringe of size four and a unbounded one is negligible for a very wide range of implication counts and therefore a size of four for the fringe zone is sufcient to provide very accurate results for most applications 10 20 30 40 50 60 70 80 90 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe Figure 6 Dataset One with c  4 6.2 Real-world datasets  Algorithmic comparison We compare our estimates with the results taken using Distinct Sampling DS which has been sho wn pro vide highlyProceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


accurate estimates for distinct value queries and event reports This algorithm outperforms other estimators that are based on uniform sampling[8 9 e v e n when using much less sample space We also provide comparison with our Implication Lossy Counting ILC algorithm described in Section 5.1 which is based in the Lossy Counting algorithm introduced in For this series of experiments we used a real dataset of eight dimensions which was given to us by an OLAP company whose name we cannot disclose due to our agreement The cardinalities of the dimensions are presented in Table 3 The parameters of the algorithms are presented in Table 5 For NIPS/CI we used 64 concurrent bitmaps with a fringe size of four thus requiring memory enough to hold  2 4  1   64  K  1920 itemsets We expect that the averaging\([5 o v e r these man y bitmaps will result in an error less than 10 We used the exact same sample space for DS The bound parameter t for DS was set to  1920  50  following the suggestion in F o r ILC we used an approximation parameter   0  01 which increases the memory requirements of ILC relative to those of NIPS/CI or DS On the average ILC used more than twice the memory that NIPS/CI and DS used For example for the experiment in Figure 7\(B it used more than 8,000 entries We evaluate the results of the algorithms with respect to the number of tuples the cardinality of the participating dimensions and the implication conditions To simulate a real data stream scenario we tracked the conditional implication counts of A  B  E  F and the unconditional B  E using the aforementioned algorithms The rst workload corresponds to quite large compound cardinality while the second to very moderate cardinalities Table 4 presents the actual aggregates for various instances of the stream for   5 and  1  60 We believe that most workloads fall somewhere in the middle with respect to the complexity of the wanted implications and the size of the returned counts Figure 7\(A depicts the relative error as the stream evolves for workload A using the algorithms DS NIPS/CI and ILC for different implication parameters In Figure a we show the results for minimum support   5 and  1  60 or  1  80 The different  1 are encoded in the parentheses next to identication of the algorithm in the legend of the graph In Figure b we increased the minimum support to   50 We observe that the behavior of DS varies widely while NIPS/CI remains always below the expected 10 error DS actually keeps a sample of the distinct elements seen so far and tries to scale the implication count that holds for that sample to the whole set of distinct elements In most cases the data in the sample is not representative of the implication The situation for DS is exacerbated when the minimum support increases where quite a lot of samples do not participate in the count making the scaling even more errorprone Algorithm ILC in all cases returned very erroneous results although it used much more space than NIPS/CI and DS since it tries to store not the implication counts but the actual implicated itemsets In these workload the implicated itemsets overwhelm its available memory which is actually larger than the amount given to NIPS/CI and DS In gure 7\(B we present the results of the algorithms for workload B The situation is still in favor of NIPS/CI whose relative error remains always close to the expected 10 unlike DS who returns highly skewed errors even though the domain cardinalities are much smaller and therefore keeps in the sample space much more data As expected from the analysis the error guarantees of NIPS/CI are virtually unaffected by changes in the cardinalities or the number of tuples seen so far in the stream ILC returns very erroneous results although now the cardinalities and the implicated items are much smaller compared to those of workload A The reason is not only because it keeps too much information in memory i.e all the implicated itemsets while both NIPS/CI and DS only hold a mantissa for the count but also because the constraint    rel is broken as the number of tuples increases 7 Related Work There are unique challenges in query processing for the data stream model Most challenges are the result of the streams being potentially unbounded in size Therefore the amount of the storage required in order to get an exact size may also grow out of bounds Another equally important issue is the timely query response required although the volumes of data the need to be processed is continually augmented at a very high rate Essentially the amount of computation per data item received should not add a lot of latency to each item Otherwise any such algorithm wont be able to keep up with the data stream In many cases accessing secondary storage such as disks is not even an option In  there is a discussion of what queries can be answered e xactly using bounded memory and queries that must be approximated unless disk access is allowed Sketching techniques\([14 ha v e been introduced to build summaries of data in order to estimate the number F 0 of distinct elements in a dataset In three algorithms that      approximate the F 0 are described with various space and time requirements Distinct is dri v e n by hashing functions similar to those studied in 14 and provides highly accurate results for distinct value queries compared to those taken by uniform sampling by using only a fraction of their sample size In the algorithms Stick y Sampling and Lossy Counting are introduced that estimate frequency counts with application to association rules and iceberg cubes In a framework for performing set expression on continuously updated streams based on sketching techniques is presented In a general framework over multiple granularities is presented for both range-temporal and spatio-temporal aggregations In a framework for identifying hierarchical heavy hitters i.e hierarchical objects like network addresses whose prexes denes a hierarchy with a frequency above a given threshold is described The effect of impications between columns has been emphasized in the system that identies correlated pairs of columns and soft-dependencies and has been proved very useful in query optimization 8 Conclusions We have presented a generalized and parameterized framework that can accurately and efciently estimate implication counts and can be applied to many scenarios To the best of our knowledge this is the rst practical and truly scalable approach to the problem Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


 Dimension Cardinality A 1557 B 2669 C 2 D 2 E 3363 F 131 G 660 H 693 Table 3 Cardinalities Workload A Workload B Tuples A  B  E  G E  B 134,576 608 50 672,771 12,787 125 1,344,591 34,816 152 2,690,181 84,190 165 4,035,475 132,161 182 5,381,203 187,584 188 Table 4 Impl counts w.r.t tuples NIPS/CI bitmaps 64 NIPS/CI K 2 DS sample size 1920 DS bound t 39 ILC  0.01 Table 5 Algorithm Parameters           0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILC \(.8             0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILS \(.8           0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6 ILC \(.8             0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILC \(.8   a   5 b   50 a   5 b   50 A Workload A B Workload B Figure 7 Relative Error vs stream size of online estimation within small errors of complex implication and non-implication counts between attributes of a data stream under severe memory and processing constraints and even in the presence of noise We prove that the complement problem of estimating non-implication counts can be      approximated when the size of the fringe zone is xed appropriately We demonstrate that existing algorithms for estimating frequent itemsets or sampling cannot be applied to the problem since they lose the cumulative effect of small implications In addition through an extensive set of experiments on both synthetic and real data we have shown that NIPS/CI always remains very close to the actual implication count capturing even very small implications whose total contribution is signicant References  R Agra w al A Evmie vski and R Srikant Information sharing across private databases In ACMSIGMOD  2003  N Alon Y  Matias and M Sze gedy  The space comple xity of approximating the frequency moments Journal of Computer and System Sciences  58:137 147 1999  A Arasu B Babcock S Bab u J McAlister  and J W idom Characterizing memory requirement for queries over continuous data streams In Proceedings of the Twenty-rst ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems  2002  B Babcock S Bab u M Datar  R  Motw ani and J W idom Models and Issues in Data Stream Systems In Proceedings of the Twenty-rst ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems  2002  Z Bar Y ossef T  Jayram R K umar  D  S i v akumar  and L T r e visan Counting Distinct Elements in a Data Stream In RANDOM  pages 110 2002  A Belussi and C F aloutsos Estimating the selecti vity of spatial queries using the correlation fractal dimension In VLDB95 Proceedings of 21th International Conference on Very Large Data Bases September 11-15 1995 Zurich Switzerland  pages 299310 1995  J Bunge and M Fitzpatrick Estimating the number of species A r e vie w  Journal of American Statistical Association  88:364373 1993  M Charikar  S  Chaudhuri R Motw ani and V  Narasayya T o w ards estimation error guaranties for distinct values In ACMPODS  pages 268279 2000  S Chaudhuri R Motw ani and V  Narasayya Random sampling for histogram construction How much is enough In ACMSIGMOD  pages 436 447 1998  G Cormode F  K orn S Muthukrishnan and D Sri v astana Finding hierar chical heavy hitters in data streams In VLDB  2003  M Datar  A  Gionis P  Indyk and R Motw ani Maintaing stream statistics over sliding windows In Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms  2002  A Deshpande M Garof alakis and R Rastogi Independence is Good Dependency-Based Histogram Synopses for High-Dimensional Data In ACM SIGMOD  2001  C Estan S Sa v age and G V a r ghese Automatically inferring patterns of resource consumption in network trafc In ACMSIGCOMM  2003  P  Flajolet and N Martin Probabilistic Counting Algorithms for Data Base Applications Journal of Computer and System Sciences  pages 182209 1985  N Friedman L Getoor  D  K oller  and A Pfef fer  Learning probabilistic relational models In IJCAI  pages 13001309 1999  S Ganguly  M  Garof alakis and R Rastogi Processing set e xpressions o v e r continuous update streams In ACM SIGMOD  pages 265276 2003  P  Gibbons Distinct sampling for highly-accurate answers to distinct v alues queries and event reports In VLDB  pages 541550 2001  P  J Haas J F  Naughton S Seshadri and L Stok es Sampling-based estimation of the number of distinct values of an attribute In VLDB  1995  I Ilyas V Markl P  Haas P  Bro wn and A Aboulnaga CORDS Automatic Discovery of Correlations and Soft Functional Dependencies In ACMSIGMOD  2004  J Jung B Krishnamurthy  and M Rabino vich Flash cro wds and denial of service attacks Characterization and implications for cdns and web sites In ACMWWW  2002  J Ki vinen and H Mannila Approximate dependenc y inference from relations Theoritical Computer Science  149:129149 1995  G Manku and R Motw ani Approximate frequenc y counts o v e r data streams In VLDB  2002  V  Poosala P  J Haas Y  E Ioannidis and E J Shekita Impro v e d histograms for selectivity estimation of range predicates In ACMSIGMOD  pages 294305 1996  S Stolfo W  Lee P  Chan W  F an and E Eskin Data mining-based intrusion detectors An overview of the columbia ids project ACMSIGMOD Record  30\(4 2001  H W ang D Zhang and K G Shin Detecting SYN Flooding Attacks In INFOCOM 2002  2002  K Whang B V ander Zander  and H T aylor  A Linear Time Probabilistic Counting Algorithm for Database Applications ACM Transactions on Database Systems  pages 209229 1990  D Zhang D Gunopulos V  Tsotras and B  See ger  T emporal and spatiotemporal aggregations over data streams using multiple time granularities Information Systems  28\(1-2 2003 The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ofcial policies either expressed or implied of the Army Research Laboratory or the U S Government Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


Figure 3a and 3b, is negative since CBA and PART achieved a higher classification rate against this particular dataset A comparison of the knowledge representation produced by our method, PART and CBA has been conducted to evaluate the effectiveness of the set of rules derived. Figure 4 represents the classifiers generated form the hyperheuristic datasets. Analysis of the rules sets indicated that MMAC derives a few more rules than PART and CBA for the majority of the datasets. In particular, the proposed method produced more rules than PART and CBA on 8 and 7 datasets, respectively. A possible reason for extracting more rules is based on the recursive learning phase that MMAC employs to discover more hidden information that most of the associative classification techniques discard, since they only extract the highest confidence rule for each frequent item that survives MinConf Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 6. Conclusions A new approach for multi-class, and multi-label classification has been proposed that has many distinguishing features over traditional and associative classification methods in that it \(1 that contain rules with multiple labels, \(2 evaluation measures for evaluating accuracy rate, \(3 employs a new method of discovering the rules that require only one scan over the training data, \(4 introduces a ranking technique which prunes redundant rules, and ensures only high effective ones are used for classification, and \(5 discovery and rules generation in one phase to conserve less storage and runtime. Performance studies on 19 datasets from Weka data collection and 9 hyperheuristic scheduling runs indicated that our proposed approach is effective, consistent and has a higher classification rate than the-state-of-the-art decision tree rule \(PART and RIPPER algorithms. In further work, we anticipate extending the method to treat continuous data and creating a hyperheuristic approach to learn  on the fly   which low-level heuristic method is the most effective References 1] R. Agrawal, T. Amielinski and A. Swami. Mining association rule between sets of items in large databases In Proceeding of the 1993 ACM SIGMOD International Conference on Management of Data, Washington, DC May 26-28 1993, pp. 207-216 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rule. In Proceeding of the 20th International Conference on Very Large Data Bases, 1994, pp. 487   499 3] M. Boutell, X. Shen, J. Luo and C. Brown. Multi-label semantic scene classification. Technical report 813 Department of Computer Science, University of Rochester Rochester , NY 14627 &amp; Electronic Imaging Products R &amp D, Eastern Kodak Company, September 2003 4] A. Clare and R.D. King. Knowledge discovery in multilabel phenotype data. In L. De Raedt and A. Siebes editors, PKDD01, volume 2168 of Lecture Notes in Artificial Intelligence, Springer - Verlag, 2001,  pp. 42-53 5] P. Cowling and K. Chakhlevitch. Hyperheuristics for Managing a Large Collection of Low Level Heuristics to Schedule Personnel. In Proceeding of 2003 IEEE conference on Evolutionary Computation, Canberra Australia, 8-12 Dec 2003 6] R. Duda, P. Hart, and D. Strok. Pattern classification Wiley, 2001 7] E. Frank and I. Witten. Generating accurate rule sets without global optimisation. In Shavlik, J., ed., Machine Learning: In Proceedings of the Fifteenth International 


Learning: In Proceedings of the Fifteenth International Conference, Madison, Wisconsin. Morgan Kaufmann Publishers, San Francisco, CA, pp. 144-151 8] J. Furnkranz. Separate-and-conquer rule learning Technical Report TR-96-25, Austrian Research Institute for Artificial Intelligence, Vienna, 1996 9] W. Li, J. Han and J. Pei. CMAR: Accurate and efficient classification based on multiple class association rule. In ICDM  01, San Jose, CA, Nov. 2001, pp. 369-376 10 ] T. Joachims. Text categorisation with Support Vector Machines: Learning with many relevant features. In Proceeding Tenth European Conference on Machine Learning, 1998,  pp. 137-142 11] T. S. Lim, W. Y. Loh and Y. S. Shih. A comparison of prediction accuracy, complexity and training time of thirtythree old and new classification algorithms. Machine Learning, 39, 2000 12] B. Liu, W. Hsu and Y. Ma. Integrating Classification and association rule mining. In KDD  98,  New York, NY, Aug 1998 13] J.R. Quinlan. C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann, San Francisco, 1993 14] J.R. Quinlan. Generating production rules from decision trees. In Proceeding of the 10th International Joint Conferences on Artificial Intelligence,  Morgan Kaufmann San Francisco, 1987, pp. 304-307 15] R. Schapire and Y. Singer, "BoosTexter: A boosting-based system for text categorization," Machine Learning, vol. 39 no. 2/3, 2000, pp. 135-168 16] F. Thabtah, P. Cowling and Y. Peng. Comparison of Classification techniques for a personnel scheduling problem. In Proceeding of the 2004 International Business Information Management Conference, Amman, July 2004 17]Y. Yang. An evaluation of statistical approaches to text categorisation. Technical Report CMU-CS-97-127 Carnegie Mellon University, April 1997 18] X. Yin and J. Han. CPAR: Classification based on predictive association rule. In  SDM  2003, San Francisco CA, May 2003 19]CBA:http://www.comp.nus.edu.sg/~dm2/ p_download.html 20] Weka: Data Mining Software in Java http://www.cs.waikato.ac.nz/ml/weka 21] M. J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. New algorithms for fast discovery of association rules. In Proceedings of the 3rd KDD Conference, Aug. 1997 pp.283-286 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


Low Medium 1.152e3 1.153e3 5.509e3 3.741e3 20 20.0 393 0.647 0.9900 0.0092 1.0000 0.0093 R=1000 1e-4 0.2 High 1.153e3 1.154e3 3.180e3 2.698e3 45 45.0 597 0.214 0.9900 0.0071 1.0000 0.0162 2e-4 2e-4 Low 0.4017 0.4024 109.291 71.994 4 4.0 39.7 0.996 0.9960 0.0075 0.9995 0.0012 2e-3 1.0 1e-3 Medium Medium 0.4128 0.4129 17.488 19.216 6 6.0 29.6 0.992 0.9938 0.0018 0.9987 0.0034 R=0.1 0.01 4e-3 High 0.4612 0.4722 4.1029 5.410e3 12 12.0 28.0 0.992 0.9902 0.0045 1.0000 0.0132 0.04 1e-8 Low 0.0250 0.0272 253.917 155.263 3 3.0 38.0 1.002 0.9991 0.0001 0.9994 0.0008 1e-3 1e4 1e-7 High Medium 0.0338 0.0290 17.303 12.837 3 3.0 14.5 0.332 0.9900 0.0058 0.9951 0.0045 R=1e-5 1e-2 1e-6 High 0.0918 0.0557 1.6071 1.225e5 8 8.0 9.9 0.992 0.9906 0.0034 0.9994 0.0142 1e-3 TABLE II SIMULATION RESULTS FOR: TRACKING REGIME PD=1, Q=100, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 2e-4 Low 0.0408 0.0407 2.9255 2.0060 5 5.0 30.3 0.996 0.9933 0.0016 0.9998 0.0020 0.02 1.0 1e-3 Medium Medium 0.0446 0.0446 0.5143 333.138 10 10.0 27.8 0.996 0.9900 0.0042 0.9995 0.0093 R=0.01 0.1 4e-3 High 0.0763 0.1062 0.1580 3.402e3 84 84.0 90.9 0.793 0.9900 0.0099 1.0000 0.0334 0.4 TABLE III SIMULATION RESULTS FOR: TRACKING REGIME PD=0.9, Q=1000, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 0.05 Low 1.166e3 1.167e3 1.073e4 3.357e4 11 12.2 257 0.542 0.9900 0.0089 1.000 0.0073 5e-5 1e-4 0.1 Low Medium 1.167e3 1.167e3 5.509e3 1.481e4 21 23.2 309 0.858 0.9900 0.0067 1.0000 0.0052 R=1000 1e-4 0.2 High 1.168e3 1.170e3 3.180e3 6.979e3 46 50.6 475 0.798 0.9900 0.0089 1.000 0.0022 2e-4 3] T. Fortmann, Y. Bar-Shalom, Y. Scheffe, and S. B. Gelfand  Detection Thresholds for Tracking in Clutter- A Connection Between Estimation and Signal Processing  IEEE Trans Auto. Ctrl., Mar 1985 4] S. B. Gelfand, T. Fortmann, and Y. Bar-Shalom  Adaptive Threshold Detection Optimization for Tracking in Clutter  IEEE Trans. Aero. &amp; Elec. Sys., April 1996 5] Ch. M. Gadzhiev  Testing the Covariance Matrix of a Renovating Sequence Under Operating Control of the Kalman Filter  IEEE Auto. &amp; Remote Ctrl., July 1996 6] L. C. Ludeman, Random Processes: Filtering, Estimation and Detection, Wiley, 2003 7] L. Y. Pao and W. Khawsuk  Determining Track Loss Without Truth Information for Distributed Target Tracking Applications  Proc. Amer. Ctrl. Conf., June 2000 8] L. Y. Pao and R. M. Powers  A Comparison of Several Different Approaches for Target Tracking in Clutter  Proc Amer. Ctrl. Conf., June 2003 9] X. R. Li and Y. Bar-Shalom  Stability Evaluation and Track Life of the PDAF Tracking in Clutter  IEEE Trans. Auto Ctrl., May 1991 10] X. R. Li and Y. Bar-Shalom  Performance Prediction of 


10] X. R. Li and Y. Bar-Shalom  Performance Prediction of Tracking in Clutter with Nearest Neighbor Filters  SPIE Signal and Data Processing of Small Targets, July 1994 11] X. R. Li and Y. Bar-Shalom  Detection Threshold Selection for Tracking Performance Optimization  IEEE Trans. on Aero. &amp; Elect. Sys., July 1994 12] D. Salmond  Mixture Reduction Algorithms for Target Tracking in Clutter  SPIE Signal and Data Processing of Small Targets, Oct. 1990 13] L. Trailovic and L. Y. Pao  Position Error Modeling Using Gaussian Mixture Distributions with Application to Comparison of Tracking Algorithms  Proc. Amer. Ctrl. Conf., June 2003 4323 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





