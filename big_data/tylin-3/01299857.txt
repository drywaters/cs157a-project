Data Mining Rules Using Multi-Objective Evolutionary Algorithms Beatriz de la Iglesia Mark S Philpott,Anthony J Bagnall and Vie J Rayward-Smith University of East Anglia Norwich NR4 7TJ En g 1 and bli,m.s.philpott,ajb,vjrs Abstract In data mining nugget discovery is the dis covery of interesting classification rules that apply to a target class In previous research heuristic meth ods Genetic algorithms, Simulated Annealing and Tabu Search have been used to optimise a single measure of interest This paper proposes the use of multi objective optimisation evolutionary algorithms to allow the user to interactively select a number of interest measures and deliver the best nuggets an approxima tion to the Pareto-optimal set according to those mea 
sures Initial experiments are conducted on a number of databases using an implementation of the Fast Eli tist Non-Dominated Sorting Genetic Algorithm NSGA and two well known measures of interest Compar isons with the results obtained using modern heuristic methods are presented Results indicate the potential of For a categorical attribute a conjunct al is a test that can take the following forms Simple value AT  v where v is a value from the do main of AT Domj for some l  j  n A record z satisfies this test if z[AT  v Subset of values AT E wl 
  vk}J  j  n where U    vk is a subset of values in the domain of AT for some 1  j  n A record z satisfies this test if z[AT E VI  vk z satisfies this test if z[AT  U Inequality test AT  U for some 1  j  n A record For a numeric attribute a conjunct ap is a test that can take the following form multi-objective evolutionary algorithms for the task of nugget discovery Simple value A 
 w I I n as for categorical attributes 1 Introduction 1.1 Defining a nugget Classification is a common task for data mining In classi fication a model is sought which can assign a class to each instance in the dataset Classification algorithms often use overall classification accuracy as the guiding criteria to con struct a model Partial classification 2 also termed nugget discovery seeks to find patterns that represent a description of a particular class This data mining task is particularly relevant when some of the classes in a database are minor ity classes i.e those with few representative instances in the database as in those cases high overall classification accuracy can 
be achieved even when minority classes are misclassified by the model induced A nugget or partial description is often presented as a simple conjunctive rule a  0 where the precondition or antecedent of the rule a represents a conjunction of tests on the attributes or fields AT   AT of the database D and the postcondition or consequent of the rule 0 rep resents the class assignment. Association rules are of a sim ilar format but the consequent is not fixed to he a particular attribute-value pair Also in association rule induction I the target of the discovery is to generate all rules that meet certain constraints 
In the case of conjunctive rules the antecedent is of the following form a  ai Aa2 A  Aa Binarypartition AT  w or AT 2 U for some 1 5 j 5 n 21 E Dom A record z satisfies these tests if z[ATj  w orz[ATj 2 U respectively Rangeof values 211  AT  uz or AT E U~,WZ for some 1 5 j  n and v1 w2 E Dom A record z satisfies this test if VI  z[AT 5 UZ A record z satisfies 
a conjunction oftests al Aa2 A  A a if z satisfies all the tests al a2   arn  The consequent of the rule is just the specification of the class that the rule is describing, chosen from a set of pre defined classes Note that other rule formats can he defined however this would represent an increase in the size of the search space and also the rules found may he more complex It is as sumed in this research that conjunctive rules are sufficient for a class description Note also that although the rules described use 
a rela tively simple syntax, the introduction of numeric attributes without pre-discretisation and the use of disjunction of nom inal attribute values increases the complexity of the problem dramatically Many of the commonly proposed algorithms for rule induction e.g.[l 3,4 can only discover rules us ing simple value tests For such a simple rule we can define some measures based on the cardinalities of the different sets defined by the rule Each conjunct defines a set of data points or records for which the test specified by the conjunct is true, and the intersection of all those sets or the set of points for which 0-7803-7804-0 03/$17.00 0 2003 IEEE  1552 


all the conjuncts are true defines the applicability of the rule in the database We will refer to this set as A and IAI  a The set of data points for which the consequent of the rule is true or in other words the set of data points that belong to the class specified by the rule will be referred to as B and lBl  b Finally the set of points for which both the antecedent and consequent of the rule are true will be called C and IC1  c In summary A  z E Dla\(z B  z E Dlp\(z and C  z E Dla\(z A p\(z Note that c _ a and c 5 b as C C A and C C B Also a _ d and b 5 d since A B C D In nugget discovery both 6 and dare fixed 1.2 Measuring Interest In order to assess the quality of a nugget the following prop erties are often examined Accuracy Confidence Acc\(r  2 This measure represents the proportion of records for which the prediction of the rule \(or model in the case of a complete classification is correct, and it is one of the most widely quoted measures of quality, specially in the context of complete classification Coverage Cov\(r f This measure is defined here as the proportion of the target class covered by the rule Since b is fixed in nugget discovery coverage is proportional to c There are other measures proposed and used by data mining algorithms and they can often be presented in terms of a bl c and d It is intuitive that we wish to find nuggets that maximise both accuracy and coverage We can define a partial order ing with respect to accuracy and coverage such that if two rules had the same accuracy, the more covering rule would be preferred Equally if two rules had the same cov erage the more accurate rule would be preferred Such a partial ordering is discussed in 7 A similar ordering was also proposed in 3 in the context of association rules The partial ordering would define an upper accuracylcoverage border which contains potentially interesting rules Nugget discovery algorithms should be able to sample this front ef fectively looking for,accurate rules or for general rules We propose to use multi-objective metaheuristic meth ods for the problem of nugget discovery from classification datahases with nominal and numerical attributes For the bi-objective problem of discovering nuggets of high accu racykoverage the multi-objective treatment should allow the algorithm to return an approximation to the upper accu racylcoverage border containing solutions that are spread across the border Other objectives, such as simplicity can be incorporated in the search if the approach proves effec tive 2 Modern Heuristics for Nugget Discovery In previous research 6 141 we presented an interest mea sure thefitness measure f\(r  Xc  a where X E 8 which was capable of partially ordering rules according to accuracy and coverage under certain constraints The X pa rameter establishes an accuracy threshold defined by f above which the fitness measure orders rules correctly with respect to the i partial ordering Importantly when two rules cannot be compared under the partial ordering the X parameter can be used to establish a preference for more accurate rules or more widely covering rules At low val ues of the X parameter accurate rules will be fitter than widely covering rules whereas at high values of the X pa rameter widely covering rules will be fitter than highly ac curate rules Generally the X range we experiment with are between l,a/b Hence, by running modern heuristics techniques such as Simulated Annealing SA Tabu Search TS and Genetic Algorithms GAS to search for nuggets that maximise the fitness measure with different levels of X we are able to approximate the upper accuracykoverage border for a particular nugget discovery problem Results of applying the modern heuristic algorithms across a range of datasets from the UCI repository showed that interesting nuggets could be obtained at different levels of accuracylcoverage We now propose to extend this research by using Multi-objective Evolutionary Algorithms MOEA  This should allow us to deliver a set of rules that lie in the Pareto optimal front or in the upper accuracylconfidence border if we choose those as the measures to be optimised in one sin gle run of the algorithm Additionally if MOEA proves to be more successful than the standard GA/SA/TS approach then other measures of interest could be introduced in the search as additional objectives The disadvantages to the modern heuristics approach that we may wish to overcome by using other type of MOEAs are the following First we need to run the al gorithm several times with varying levels of X as it is not known in advance what kind of nuggets may be obtainable by a particular X value and it is generally not known in ad vance what are the preferences of the decision maker Sec ond only accuracy and coverage can he taken into consid eration with the present approach To optimise nuggets in terms of simplicity for example we run a post-processing algorithm which removes conjuncts in a greedy manner choosing the one that produces no deterioration in accu racy one at a time Hence there is no simple way to in troduce other interest criteria other than by replacing the fitness measure by a different measure which encapsulates the alternative criteria. Also there is no real encouragement for the algorithm to produce different rules along the upper coveragelaccuracy border hence many copies of the same rule may be produced even with variations in the X parame ter 1553 


3 MOEAs for Nugget Discovery In order to create a nugget discovery system that is more flexible we propose to use Pareto-based MOEA to deliver nuggets that are in the Pareto optimal set according to some measures of interest which can be chosen by the user The proposed system will present to the user a number of measure of interest from which some measures can he selected As well as those presented in section 1.2 mea sures of the simplicity of a nugget based for example on the Minimum Description Length Principle 131 or simply on the number of conjuncts of the nugget may also he used The system will represent nuggets using the same approach as in 6 71 The process of nugget evaluation may also be similarly conducted The main difference is that the over all objective of the MOEA will be to find a set of diverse non-dominated solutions which should constitute a good approximation to the Pareto optimal front Note that in most cases we do not know the exact composition of the Pareto optimal front We expect that there may be some merit in evaluating the performance of various Pareto-based MOEA approaches for this problem, for example the Niched Pareto Genetic Algo rithm 9 the Pareto Archive Evolution Strategy PAES al gorithm proposed by Knowles and Corne l l and the Fast Elitist Non-Dominated Sorting Genetic Algorithm NSGA proposed by Deb et al SI However in this paper we limit ourselves to the evaluation of one of the MOEA, the NSGA version IIj algorithm SI against the conventional GNSAITA algorithms Hence we are trying to establish the benefit of using a Pareto-based approach against the aggre gating function approach In this paper we also limit ourselves to the use of accu racy and coverage as our measures of interest Since results are encouraging we plan to extend the research by using other measures 4 Implementation of the NSGA I1 for Nugget Discovery The NSGA I1 algorithm SI is a variation of the original NSGA algorithm proposed in 151 The new algorithm ad dresses some of the criticisms of the previous version in cluding the lack of elitism the need for specifying a shar ing parameter to ensure diversity in the population and, the high computational complexity of non-dominated sort The NSGA I1 algorithm is fully explained in SI The initial population is created by the initialisation procedure described in a later section The first iteration is slightly dif ferent to the rest In the main function a child population is created at each stage using binary tournament selection single-point cross-over and mutation Both populations are combined and the resulting population of size 2N where N is the size of the initial population is then sorted ac cording to non-domination into different fronts Hence so lutions that belong to the first front are non-dominated so lutions Those are then discounted to find the second front  1554 and so.on Within each front, solutions are sorted accord ing to crowding distance The crowding distance specifies the size of the largest cuboid enclosing a point in the pop ulation i but not including any other. points in the popula tion A new population is then created by taking solutions from the combined population in terms of rank first and crowding distance as a secondary sorting criterion, until the new population is of size N The process goes hack then to the creation of a child population and continues through an specified number of generations In the following sections we discuss some of the de tails of the implementation that are necessary to adapt the NSGA I1 algorithm to solve the nugget discovery problem as presented before Most of these implementation details are shared with the modern heuristic algorithms 4.1 Representing a Solution The solution to be represented is a conjunctive rule or nugget following the syntax described previously A binary string is used for this as follows The first part of the string is used to represent the nu meric fields or attributes Each numeric attribute is rep resented by a set of Gray-coded lower and upper limits where each limit is allocated a user-defined number of bits p p  10 is the default There is a scaling procedure that transforms any number in the range ofpossible values using p bits 0,2P  11 to a number in the range of values that the attribute can take The procedure works as follows When the data is loaded the maximum value maxi and minimum value min for each attribute i.are stored A weight for each attribute is then calculated as    When the string representing a nugget is decoded, the uppex and lower limit values for each attribute are calculated by hit  ss  tu nun where ss represents the decimal vaiue of an p bit Gray coded substring extracted from the binary string, which cor responds to one of the limits The second part of the string represents categorical at tributes, with each attribute having U number of bits where v is the number of distinct values or the number of labels that the categorical attribute can take If a bit assigned to a categorical attribute is set to 0 then the corresponding label is included as an inequality in one ofthe conjuncts 4.2 Initialising the Population The initial approach to this problem was to initialise each solution randomly with the help of a random number gen erator This however proved to be ineffective as many of the solutions obtained were of very poor quality and NSGA ll was unable to produce good results A similar problem was encountered in the implementation of the modern heuristics 


A more effective initialisation procedure was to use mu tated forms of the default rule as initial solutions The de fault rule is the rule in which all limits are maximally spaced and all labels are included In other words it predicts the class without any pre-conditions For the initialisation all solutions in the population pool were initialised to be copies of the default rule and then some of the bits were mutated according to a parameter representing the probability of mu tation Experimentation was carried out to establish a good setting for this parameter    ments were implemented for this algorithm 4.3 Evaluating a Solution To evaluate a solution the bit string is first decoded and the When a bit string is decoded as a nugget it will acquire was found that the three algorithms performed similarly with Tabu Search slightly outperforming the others in Some occasions even though only a simple implementation was produced In,the experiments that follow therefore we will focus on the Tabu Search algorithm to produce the set of results to be compared to those of NSGA II The TS used included very simple recency and frequency memory structures and aspiration criteria No intensifica tion or diversification techniques, or any other TS enhance At each step all neighbours of a solution can be gener ated, or alternatively a subset of x neighbours can be gen erated where x is set by the user as a parameter various neighbourhood operators were tested including flip-one-bit, flip-two-bits swap reverse and move opera and the algorithm performed well With all those in terms of the quality of solutions produced If efficiency is considered measured by the number of evaluations re quired to reach the best solution along with quality of so lutions the flip-one-bit or move operators seem to give best results The neighbourhood operators were tested using recency memory with a tabu tenure of 10 iterations, and a subset of 20 neighbours generated The recency memory was imple mented by recording previously visited solutions i.e the whole solution in a list of size n where is the abu tenure and jisallowing a visit After this frequency memory was tested on the best three operators Rip.one.bit fip.two.bits and move F quency memory used a threshold of 20 that is a single el ement or in a binary representation a bit can only be changed 20 times further changes on that element are disallowed For all three neighbourhood operators there was no marked improvement in the quality of solutions but there was an improvementin the average umber ofevalua tions to reach a good solution when frequency memory was used data is scanned through the following format IF 11  AT 5 ul AND 12 5 AT2 5 ZLZ AND li 5 AT 5 ui AND   AT  labelX AND AT  labelY THEN Classj where 11 is given by the first P bits Ofthe binary string u1 is given by the following p bits, etc If a lower limit for any attribute i is set to its lowestpossible value for.the at tribute mini or the upper limit is set to its highest possible value max then there is no need to include that limit in the decoded nugget. If both limits are excluded in that way then the attribute is obviously also excluded Equally if a categorical attribute has a value of 1 for all the bits allocated to its labels, then there is no need to include the attribute In this way the algorithm can perfom its own fawe dection by ignoring some attributes any solution in the tabu list For each record the values of the fields are compared winst the nuggets and the class is Ompared The Next different tabu tenures were tried in the context of COUntS Of c and a are updated accordingly The Counts Of b and d are known from the data loading stage the data has been examined the measures Of in terest in this case the coverage and accuracy are calculated for each nugget In the modern heuristic approach at this stage the fitness of the rule would he calculated 5 Parameter Experimentation In this section we introduce the parameters that were ex perimented with in order to improve the performance of the NSGA II algorithm and their effect on the quality of solu tions We also discuss the parameters that were used to run the modern heuristic algorithms 5.1 Parameters for the Modern Heuristics In our previous work 171 three different algorithms a Ge netic Algorithm Simulated Annealing and Tabu Search TS were used to'solve the nugiet discovery problem It recency memory T of 15 and 20 were tried in corn bination with the three neighbourhood operators previously chosen The solution quality or efficiency of the search did not improve mar,,edly with greater tabu tenure An extra set of experiments was carried observe the effect of increasing the size of the subset of neighbours produced and it was found that this did not produce any clear gains Increasing the threshold that controls the fre quency memory produced no clear gains either The final parameters chosen and used in ourexperiments are Once Neighbourhood operator flip-one-bit Recency memory with a tabu tenure of 10 e Frequency memory with a threshold of 20  A subset of 20 neighbours generated 1555 


Stopping after 250 iterations without change in the best solution value 5.2 Parameters for the MOEA algorithms The parameters that were experimented with in this case were Cross-overrate: This was varied from 60 to 90 in steps of 10 Stopping condition in this algorithm the stopping condition was implemented as a prefixed number of generations set by the user This was varied from 50 to 100 in steps of IO Population size This was varied from 100 to 160 in steps of io Initial mutation rate This affects the mutation of in dividual bits when solutions are created initially Mu tation also occurs when solutions are reproduced to form new solutions It was varied between 0 and 490 in steps of 1  In terms of parameter experimentation it was found for all the databases that the mutation rate had the most pro found effect on solution quality A rate of 2 produced the hest results in each case consistently Other values had a marked effect on the quality of the fronts produced For example with mutation rate of 4 the quality of solutions was distinctly inferior in each case. This may be due to the use of mutation in the initialisation procedure in which var ious copies of the default rule are altered according to it It may point to the importance of a strong initial population hence the performance of the algorithm may he improved in the future by introducing some known 223good rules in the initialisation process The other parameters did not seem to affect the quality of solutions as dramatically hence the algorithm was found to he robust to changes on those parameters Best results were obtained with cross-over rates that varied between 60 and 80 stopping criteria between 80 and 100 generations, and a population of about\222l20 solutions Name Adult Mushroom Contraception 6 Results Records Class Num Cat 45,222 2 68 8,124 2 0 21 1.471 3 27 The databases used for this experiments were extracted from the UCI repository Table 1 represents their main char acteristics including number of classes and number of nu merical and categorical attributes for more details see  121 Both the TS and the NSGA I1 algorithms were used to ex tract knowledge from the databases of Table I The TS algorithm was run using the parameters de scribed in the previous section with a range of X values for each dataset This was done in order to compare the spread and quality of solutions obtainable by varying the X value with those obtainable using NSGA 11 For each dataset X was set at the value of 1.01 initially equivalent to an accu racy threshold of 99 and incremented in small steps of 0.05 in most cases\to the value of 2 and in some occasions to higher values in order to obtain widely covering solutions of higher accuracy than the default rule For each X value 5 runs of the algorithm were performed and the accuracy and coverage of the best rule obtained in each run was recorded The modem heuristics can at times get stuck in inferior so lutions so in past experiments the hest of 5 runs was chosen as the hest nugget obtainable with a particular X value For this exercise however we include all the solutions in order to observe the overall quality and spread of the solutions produced No attempt has been made to prune any of the inferior or suboptimal solutions obtained It is worth not ing that as quite often the algorithm returns copies of the same solution even at different values of A the number of distinct solutions may be low and in the visualisation\222of results, overlapping solutions are not noticeable For the NSGA I1 algorithm the solutions plotted are those that form the first non-dominated front at the end of the execution of the algorithm that is all those solutions that are non-dominated in the final population We choose to evaluate the performance of our algorithms by using visualisation This seems quite intuitive when there is just two dimensions to visualise As the true Pareto front is not known it is not possihle to compare to it in order to evaluate performance Throughout this section we present the results of both algorithms in 2-dimensional graphs, plotting accuracy of the rules found in the y-axis and coverage in the x-axis Each graph contains a series labeled MOMH Multi-objective Meta Heuristic\which represents the results for the NSGA I1 algorithm and a seriesJabeled TS which represents the Tabu Search Results Note that the scales of the y-axis varies in some graphs where the solu tions found lie in a reduced accuracy range This is to allow for improved visualisation of results However all solutions are always presented in the graphs The results presented in the graphs are those of the train data for both algorithms Checks on the test data have shown no marked signs of overfitting for either algorithm except in the case of some of the very accurate and specific patterns The Adult database contains census information from adults in the USA Any records with missing values were re moved from this database prior to the algorithm application The database has two classes one of them corresponds to 223Income  5011\224 with 24 default accuracy on the training data the other corresponds to 223Incomes 50K\224 with 76 default accuracy on the training data The results for the class 223Income  50K\224 are shown on Figure 1 The fronts generated by both algorithms are very similar in terms of 15 22156 


 Figure I Adult database class 223 50 K\224    LI U I    221 h Figufe 2 Adult database class 221\2215 50 K the quality of solutions obtained However TS generates a higher proportion of solutions which are dominated by both other TS solutions as well as NSGA I1 solutions Also the spread of solutions in the front is better with the NSGA I1 algorithm although this may he expected as it is a feature of the search in chis particular algorithm   The results for the class 223Income 5 50 K are shown on Figure 2 Again the results of the two algorithms are of similar quality hut in this case TS marginally.outper forms NSGA I1 in general Only a very small proportion of the TS solutions are dominated The TS solutions are clus tered in small areas of the front with some regions being un-represented hence the NSGA I1 algorithm has a better spread of solutions over the front The mushroom dataset was obtained by mushroom records drawn from The Audubon Society Field Guide to North American Mushrooms IO This data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family The Mushroom database has two classes the 223Poi sonous\224 class with a default accuracy of48 and the 223Edi ble\224 class with a default accuracy of 52 For this database rules can he found that are nearly 100 accurate with nearly 100 coverage. hence we have some information about the Pareto front it is expected to be small and concentrated on the right and upper border of the graph:Results for the 223Poi sonous\224 class are given in Fig 3 The TS algorithm finds 222 Figure 3 Mushroom database c1ass\222\224Poisonous\224   _I  x f c Figure 4 Mushroom database class \223Edible\224 a number of dominated solutions The\222best accurate rule is found by both algorithms The NSGA I1 algorithm finds a cluster of rules of high coverage which are not found by the TS Hence despite the Pareto front for this problem he ing confined to a small region the NSGA I1 is successful in finding distinct solutions in the front The results, for the 223Edible\224 class are presented in Fig 4 The accuracy of solutions in the non-dominated front is within a very small range \(less than 1 In this class both algorithms produce similar quality solutions The NSGA I1 algorithm produces more varied solutions in the front The contraceptive database is a subset of the I987 Na tional Indonesia Contraceptive Prevalence Survey The samples are married women who were either not pregnant or do not knowlf they were at the time of interview The proh lem is to predict the current contraceptive method choice of a woman no use longlterm methods, or short-term meth ods with default accuracies of43 22 and 35 respec tively based on her demographic and socio-economic char acteristics Results for the class 223no use\224 are given in Fig 5 Both algorithms produce non-dominated solutions in dif ferent areas ofthe Pareto front although the spread of non dominated solutions across the front is better for the NSGA algorithm 1557 


mxXf  r Figure 5 Contraception database class 223no use\224 Figure 7 Contraceptive database class 223Short-term\224 222 Figure 6 Contraceptive database class \223Long-term\224 Results for the class 223long-term use\224 are presented in Fig 6 In these results TS produces more non-dominated solutions and the spread of solutions in the front is reason able except for those of high coverage greater than 65 where NSGA I1 finds a large number of solutions Results for the class 223short-term use\224 are presented in Fig 7 In the Pareto front area of high accuracy above 65 and low coverage below 25 the NSGA I1 algo rithm finds all but one of the non-dominated solutions However in the other area the algorithms perform very sim ilarly both in terms of solution spread and quality 6.1 Efficiency Comparisons To compare the execution time of both algorithms is not straightforward in terms of computing time since they were run on different machines However one method of com parison is to establish the number of evaluations that each algorithm performed to arrive to a particular set of solu tions This is possible because both algorithms use the same underlying evaluation function for a solution This can only give a rough approximation as both algorithms per form other operations However evaluation is the dominat ing operation in both algorithms so it can be an initial point of comparison The comparison was established using the Adult database For this database, the TS required approximately 5000 evaluations on average to find one of the solutions that is to find a single rule which is represented as one point in the graphs above The algorithm was run 5 times per X value, with a range of X values previously described This means that 138 experiments were run to get the set of solutions shown in Figure 2 Hence the total number of evaluations performed is nearly 700,000 Even if only one run per x\222 value was performed which may deteriorate the quality of the front obtained approximately 140,000 eh uations would have to he performed On the other hand the NSGA II\222algorithm evaluates a population of 2n solu tions, in this case n  120 at each generation and the ex periments shown on graph 2 were run for I00 generations Hence the total number of evaluations performed by this al gorithm is 24,000 Therefore we can conclude that the TS algorithm is a much less efficient way to achieve a set of results of similar quality, in terms ofthe number of evalua tions performed 7 Conclusions and further work In this paper we propose\222the use of Pareto-based MOEA in the extraction of rules from databases This is novel and complements previous research in nugget discovery using heuristic techniques The ability to present the user with a number of interest measures which may be selected, and then to search for a set of solutions which represent an ap proximation to the Pareto optimal front for those measures is desirable for a partial classification algorithm We have implemented the NSGA I1 Pareto-based MOEA and compared it to the TS algorithm We have performed parameter experimentation for the former and having found a range of suitable parameters we have applied the algo rithm to a range of well known classification databases The results have shown that in terms of quality of indi vidual solutions both algorithms are comparable However the spread of solutions across the approximated Pareto-front found by the NSGA I1 algorithm was always better Also in terms of efficiency the NSGA I1 algorithm significantly outperforms the TS approach Therefore we can conclude 1558 


that in order to find an approximation to the Pareto front it is best to use the NSGA I1 algorithm Given our results the potential of NSGA I1 for nugget discovery is clear The flexibility of this algorithm will make it advantageous for solving this type of data mining problem As the subject of further research, the first step is to com pare the performance of the NSGA I1 algorithm with the true Pareto optimal front We are already working in this area and results will be reported shortly It may be interest ing to introduce more objectives into the search, such as the simplicity of a rule It may also be worth while to compare the performance of other Pareto-based algorithms against NSGA I1 for this particular problem  Bibliography I R Agrawal T Imielinski and A Swami Database mining A performance perspective In Nick Cercone and Mas Tsuchiya editors Special Issue on Learning and Discover in Knowledge-Based Databases num her 6 in 5 pages 914-925 Institute of Electrical and Electronics Engineers, Washington U.S.A 1993 2 S Ah K Manganaris and R Srikant Partial clas sification using association rules In D Heckerman H Mannila,,D Pregibon and R Uthurusamy, editors Proceedings of the Thid.Int Cunf on Knowledge Dis coven andData Mining pages 115-1 18 AAAI Press 1997 3 R J Bayardo and R Agrawal Mining the most inter esting rules In S Chaudhuri and D Madigan. editors Proceedings of the Fiffli ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining pages 145 155. New York USA 1999 ACM 4 R J Bayardo R Agrawal and D Gunopulos Constraint-based rule mining in large, dense datasets In Proc of the 15th lnt Con on Data Engineering pages 188-197,1999 5 C.A Coello Coello A comprehensive suri'ey of evolutionary-based multiobjective-optimization tech niques Knowledge and Information Systems 1\(3 1999 161 B de la Iglesia J C W Debuse and V J Rayward-Smith Discovering knowledge in commer cial databases using modem heuristic techniques In E Sirnoudis J W Han and U M Fayyad edi tors Proceedings of the Second Int Con on Knowl edge Discover\andData Mining pages 4449 AAAI Press 1996  7 B de la Iglesia and V J Rayward-Smith The discov ery of interesting nuggets using heuristic techniques In H A Abbass R A Sarker and C S Newton ed itors Data Mining a Heuristic Approach pages 72 96 Idea group Publishing USA 2002 SI K Deb S Agrawal A. Pratap and T Meyarivan A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization NSGA-11 In Pmceed ings of the Parallel Problem Solving from Nature VI Conference Lecture Notes in Computer Science, No 1917 pp 849-958,2000 Springer 9 J Horn N Nafpliotis and D E Goldberg A Niched Pareto .Genetic Algorithm for Multiobjective Opti mization In Pioceedings of the First IEEE Conference on Evolutiona~y Coniputation IEEE World Congress on Computational Intelligence volume 1 pages 82 87 Piscataway, New Jersey 1994 IEEE Service Cen ter  A A Knopf The Audubon SocieQ Field Guide to North AniericalMushrooms G H Lincoff New York 1981 I 11 J Knowles and D Come The Pareto archived evo lution strategy A new baseline algorithm for Pareto multiobjective optimisation In Peter J Angeline Zbyszek Michalewicz Marc Schoenauer Xin Yao and Ali Zalzala editors Proceedings of the Congress on Evolutionary Computation volume 1 pages 98 105 Mayflower Hotel Washington D.C USA 6-9 1999 IEEE Press I21 C J Merz and P M Murphy UCI repos itory of machine learning databases Uni versity of California Irvine Dept of In formation and Coumputer Sciences 1998 http://www.ics.uci.edu/-mleardMLRepositoryhtmI  131 J R Quinlan and R L Rivest Inferring decision trees using the minimum description length principle Infor mation and Coniputation 80:227-248,1989 I41 V J Rayward-Smith J C W Dehuse, and B de la Iglesia Using a genetic algorithm to data mine in the financial services sector In A Macintosh and C Cooper editors Applications and Innovations in Expert Sysfenis 111 pages 237-252 SGES Publica tions 1995 IS N Srinivas and Kalyanmoy Deb Multiobjective op timization using nondominated sorting in genetic al gorithms Evolutionary Coniputation 2\(3 1-248 1994  161 X Yang and M. Gen Evolution program for bicriteria transportation problem In M Gen and T Kobayashi editors Proceedings of the 16th International Confer ence on Computers and Industrial Engineering pages 451454 Ashikaga, Japan, 1994 1559 


  Query creates a table of product-month pairs for which there are more than 00 sales transactions in the Sales table.  Query 2 creates a table ReducedSales which extracts from the Sales fact table only those sale transactions that involve product-month pairs resulting from Query The result of Query 2, a reduced Sales table ReducedSales is based on question B.  This reduced table is used in Query 3 to create a table of product-month-zip triples for which there are more 00 sales transactions in the Sales fact table and later in Query 4 to create a table ReducedSales2 that extracts all sale transactions that involve product-month-zip triples resulting from Query 3.  We did not have to use the original Sales fact table, because we know that all possible transactions that satisfy conditions of Query 3 and 4 are replicated in much smaller ReducedSales table Finally, Query 5 performs a join of the ReducedSales2 table with itself in order to find the final result  6. Experiments  In this section we describe an experimental performance study of mining real world data that uses the system and methods described in previous sections of this paper For our experiments we used Oracle 9i relational DBMS running on a 28KB Linux machine.  The data we used reflects the aggregated daily purchases for a major US-based retailer during a six-month period and conforms to the schema shown in Figure 2.  The sizes of the tables are as follows The Product table has around 5000 tuples The Location table has around 2000 tuples with about 50 different Regions The Calendar table has around 200 tuples The Sales table has around 3 million tuples As we discussed in Section 4, standard association rules are not defined for such data.  However, as we also outlined in Section 4, we can still mine association rules by approximating or redefining the meaning of 223frequently together.\224  In our experiments we mined both association rules and extended association rules.  In this paper, we present the results of two queries that are typical for our experiments The first query finds regular association rules based on the following question Find all pairs of items that appear together in more than 9000 transactions Our implementation of this query \(code available by contacting the authors\ involves a pruning of all infrequent items as described in Section 5.2.  The result of the query is thirteen pairs of items shown in Table 2  Table 2  8130 13380 64 521 3060 8130 10226 13890 15240 64 561 13890 15150 9498 8130 13380 11192 123660 123690 54 565 600 8130 48 521 PRODUCTID1 PRODUCTID2 SUM PRODUCTID1 PRODUCTID2 REGIONID SUM 7740 8130 9811 8130 8280 12473 8130 8310 13717 8130 8550 9057 8130 13890 11906 8130 15150 9642 8130 15240 11541 13890 15240 11157 15150 15240 10749  The second query finds extended association rules using the Approach Section 4\o approximating 223frequently together\224, and it is based on the following question  Find all pairs of items that appear together in more than 300 transactions in the same region Our implementation of this query \(code available by contacting the authors\volves pruning all item-region pairs that do not appear in more than 300 transactions The result of this query, shown in Table 3, is a set of fifteen rules, where each rule involves two items and a region  Table 3  15150 18420 62 562 123660 123720 53 556 123660 123780 54 507 123690 123720 53 736 123690 123720 54 803 123690 123840 53 535 123720 123840 53 1081 164490 168420 62 599 167520 167640 17 530 168120 168420 62 555 168420 169650 62 575  Comparing the two queries and their implementation in our system we make the following observations.  The extended association rule mining takes significantly less Proceedings of t he 36th Ha waii International Conf erence on S y st e m Sciences \(HICSS\22203 0 76 95 18 745/0 3 1 7.00 251 20 02  I E EE 600 8130 16853 


 time to produce about the same number of rules as association rule mining.  Furthermore, in order to find a similar \(manageable\ber of rules, the support threshold for the association rules had to be set significantly larger than the threshold for extended rules This fact supports our claim that association rules find coarser granularity correlations among items while extended rules discover finer patters The experiments also validate the viability of our tightly-coupled integration with the relational DBMS The running times for all of our queries are measured in seconds and minutes; and there is still a room for significant performance improvement by, for example upgrading hardware or through the addition of indexing In practice, in cases when a data warehouse is heavily utilized with OLAP and reporting requests, a separate data mart dedicated exclusively to data mining can be a good alternative in order to minimize the hits on the enterprise data warehouse and improve overall performance  7. Conclusions  In this paper, we presented a new data-mining framework that is tightly integrated with the data warehousing technology.  In addition to integrating the mining with database technology by keeping all query-processing within the data warehouse, our approach introduces the following two innovations Extended association rules using the other non-item dimensions of the data warehouse, which results in more detailed and ultimately actionable rules Defining association rules for aggregated \(nontransactional\ata We have shown how extended association rules can enable organizations to find new information within their data warehouses relatively easily, utilizing their existing technology.  We have also defined several exact approaches to mining repositories of aggregated data which allows companies to take a new look at this important part of their data warehouse We have conducted experiments that implement our approach on real-life aggregated data and the results support the viability of our integration approach as well as the appropriateness of extended association rules In our future work we plan to elaborate on the optimization algorithm.  We also intend to undertake a further performance study with larger data sets, using different hardware platforms and various types of indexes  References    A g ra wa l R., Im ie linsk i T. a n d A  Sw a m i.  Mining  Association Rules Between Sets of Items in Large Databases Proceeding of ACM SIGMOD International Conference  993\, 207-2 6  2 A g ra wa l R. a nd Srik a n t R Fa st A lg o rithm s f o r Mining  Association Rules Proceeding of International Conference On Very Large Databases VLDB  994\, 487-499  3 R. A g ra wa l, H. Ma nnila R Srik a n t, H. T o iv one n, a n d A   Verkamo.  Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining AAAI/MIT Press  996  4 Berry M. an d L i n o f f   G    Data Mining Techniques for Marketing, Sales and Customer Support Wiley 997  5 C h a u dhr i  S. a n d D a y a l, U  A n ov e r v i ew of  D a ta Warehousing and OLAP Technology ACM SIGMOD Record  26   997\, 65-74  6 H ilde r m a n, R.J Ca rte r C L H a m ilton, H J a nd Ce rc one   N.  Mining Association Rules from Market Basket Data Using Share Measures and Characterized Itemsets International Journal of Artificial Intelligence Tools 7 \(2 998 89-220  7 In m o n  W  H  Building the Data Warehouse Wiley 996  8 K i m b a ll, R., Re e v e s L Ros s M., a nd T hor nthw hite W   The Data Warehouse Lifecycle Toolkit Wiley 998  9 L e a v itt, N. Da ta Mi ning f o r the C o rp ora te Ma sse s IEEE Computer 2002\, 22-24     S  S a r a w a gi  S  Th o m a s  R  A gr a w a l  I n t e gr at i n g M i n i n g  with Relational Database Systems: Alternatives and Implications Proceedings of ACM SIGMOD Conference   998\, 343-354    S  T s ur, J. Ullm a n S. A b ite b oul C. Clif ton  R. M o tw a n i, S  Nestorov, A. Rosenthal.  Query Flocks: A Generalization of Association-Rule Mining Proceedings of ACM SIGMOD Conference  998 2   2 W a ng K H e Y  a n d H a n J  Mi ni ng Fr e que nt I t e m s e ts Using Support Constraints Proceedings of International Conference on Very Large Databases VLDB 2000\, 43-52   3 W a ts on H  J A nni no D   A a nd W i x o m  B  H  C u r r e nt  Practices in Data Warehousing Information Systems Management  8 200   Proceedings of t he 36th Ha waii International Conf erence on S y st e m Sciences \(HICSS\22203 0 76 95 18 745/0 3 1 7.00 251 20 02  I E EE 


 11 B IOGRAPHY  Ying Chen is a Senior Consultant with Booz Allen Hamilton Inc. She has 5 years of professional experience in system design and development with J2EE and SOA technologies. She is now involved in a research and development project in support of United States Intelligence Community, designing and implementing Advanced Information Sharing and Collaboration solutions. She holds an MS degree in Computer Sc ience from Virginia Tech and BS degree in Computer Science from Fudan University in Shanghai, China  Brad Cohen is an Associate with Booz Allen Hamilton Inc He has 8 years of professional experience in developing and implementing enterprise-class systems for both commercial and government applications.  Currently he is serving as the technical manager on a research and development project in support of the United States Intelligence Community.  He holds an MBA and MS in Information Systems, and a BS in Decision and Information Scien ces from the University of Maryland, College Park   


Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 7 Conclusion and Future Work References frontier  pages 487\320499 Morgan Kaufmann 1994  W orkshop on frequent itemset mining implementations 2003 http://\336mi.cs.helsinki.\336/\336mi03  W orkshop on frequent itemset mining implementations 2004 http://\336mi.cs.helsinki.\336/\336mi04  J  Han J  Pei and Y  Y in Mining frequent patterns without candidate generation In Proceedings of 20th International Conference on Very Large Data Bases VLDB VLDB Journal Very Large Data Bases Data Mining and Knowledge Discovery An International Journal Lecture Notes in Computer Science  2004  J W ang and G Karypis Harmon y Ef 336ciently mining the best rules for classi\336cation In The Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD\32504 Symposium on Principles of Database Systems 2 b Average time taken per frequent itemset shown on two scales T10I4D100K is increased and hence the number of frequent items decreases Figure 5\(c also shows that the maximum frontier size is very small Finally we reiterate that we can avoid using the pre\336x tree and sequence map so the only space required are the itemvectors and the minSup SIAM International Conference on Data Mining required drops quite quickly as ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery 2000 ACM SIGMOD Intl Conference on Management of Data Figure 5 Results  8\(3\3204 2000  F  P an G C ong A T ung J Y ang and M Zaki Carpenter Finding closed patterns in long biological datasets In  2121:236 2001  M Steinbach P N T an H Xiong and V  K umar  Generalizing the notion of support In a Runtime ratios T10I4D100K c Number of Itemvectors needed and maximum frontier size T10I4D100K  pages 1\32012 ACM Press May 2000  F  K orn A Labrinidis Y  K otidis and C F aloutsos Quanti\336able data mining using ratio rules  Morgan Kaufmann 2003  J Pei J Han and L Lakshmanan Pushing convertible constraints in frequent itemset mining We showed interesting consequences of viewing transaction data as itemvectors in transactionspace and developed a framework for operating on itemvectors This abstraction gives great 337exibility in the measures used and opens up the potential for useful transformations on the data Our future work will focus on 336nding useful geometric measures and transformations for itemset mining One problem is to 336nd a way to use SVD prior to mining for itemsets larger than  pages 205\320215 2005  We also presented GLIMIT a novel algorithm that uses our framework and signi\336cantly departs from existing algorithms GLIMIT mines itemsets in one pass without candidate generation in linear space and time linear in the number of interesting itemsets Experiments showed that it beats FP-Growth above small support thresholds Most importantly it allows the use of transformations on the data that were previously impossible  That is the space required is truly linear  D Achlioptas Database-friendly random projections In  2001  R Agra w al and R Srikant F ast algorithms for mining association rules In  8:227\320252 May 2004  J Pei J Han and R Mao CLOSET An ef 336cient algorithm for mining frequent closed itemsets In  pages 21\32030 2000  S Shekhar and Y  Huang Disco v ering spatial colocation patterns A summary of results 


mator from sensor 1 also shown 6. CONCLUSIONS This paper derives a Bayesian procedure for track association that can solve a large scale distributed tracking problem where many sensors track many targets. When noninformative prior of the target state is assumed, the single target test becomes a chi-square test and it can be extended to the multiple target case by solving a multidimensional assignment problem. With the noninformative prior assumption, the optimal track fusion algorithm can be a biased one where the regularized estimate has smaller mean square estimation error. A regularized track fusion algorithm was presented which modifies the optimal linear unbiased fusion rule by a less-than-unity scalar. Simulation results indicate the effectiveness of the proposed track association and fusion algorithm through a three-sensor two-target tracking scenario 7. REFERENCES 1] Y. Bar-Shalom and W. D. Blair \(editors Tracking: Applications and Advances, vol. III, Artech House, 2000 2] Y. Bar-Shalom and H. Chen  Multisensor Track-to-Track Association for Tracks with Dependent Errors  Proc. IEEE Conf. on Decision and Control, Atlantis, Bahamas, Dec. 2004 3] Y. Bar-Shalom and X. R. Li, Multitarget-Multisensor Tracking Principles and Techniques, YBS Publishing, 1995 4] Y. Bar-Shalom, X. R. Li and T. Kirubarajan, Estimation with Applications to Tracking and Navigation: Algorithms and Software for Information Extraction, Wiley, 2001 5] S. Blackman, and R. Popoli  Design and Analysis of Modern Tracking Systems  Artech House, 1999 10 15 20 25 30 35 40 45 50 55 60 2 4 6 8 10 12 14 Time N o rm a li z e d e s ti m a ti o n e rr o r s q u a re d Target 1 Sensor 1 Centralized Est Track Fusion 10 15 20 25 30 35 40 45 50 55 60 0 2 


2 4 6 8 10 12 14 Time N o rm a li z e d e s ti m a ti o n e rr o r s q u a re d Target 2 Sensor 1 Centralized Est Track Fusion Fig. 7. Comparison of the NEES for centralized IMM estimator \(configuration \(i estimators \(configuration \(ii sensor 1 also shown 6] H. Chen, T. Kirubarajan, and Y. Bar-Shalom  Performance Limits of Track-to-Track Fusion vs. Centralized Estimation: Theory and Application  IEEE Trans. Aerospace and Electronic Systems 39\(2  400, April 2003 7] H. Chen, K. R. Pattipati, T. Kirubarajan and Y. Bar-Shalom  Data Association with Possibly Unresolved Measurements Using Linear Programming  Proc. 5th ONR/GTRI Workshop on Target Tracking Newport, RI, June 2002 8] Y. Eldar, and A. V. Oppenheim  Covariance Shaping Least-Square Estimation  IEEE Trans. Signal Processing, 51\(3 pp. 686-697 9] Y. Eldar  Minimum Variance in Biased Estimation: Bounds and Asymptotically Optimal Estimators  IEEE Trans. Signal Processing, 52\(7 10] Y. Eldar, A. Ben-Tal, and A. Nemirovski  Linear Minimax Regret Estimation of Deterministic Parameters with Bounded Data Uncertainties  IEEE Trans. Signal Processing, 52\(8 Aug. 2004 11] S. Kay  Conditional Model Order Estimation  IEEE Transactions on Signal Processing, 49\(9 12] X. R. Li, Y. Zhu, J. Wang, and C. Han  Optimal Linear Estimation Fusion  Part I: Unified Fusion Rules  IEEE Trans. Information Theory, 49\(9  2208, Sept. 2003 13] X. R. Li  Optimal Linear Estimation Fusion  Part VII: Dynamic Systems  in Proc. 2003 Int. Conf. Information Fusion, Cairns, Australia, pp. 455-462, July 2003 14] X. D. Lin, Y. Bar-Shalom and T. Kirubarajan  Multisensor Bias Estimation Using Local Tracks without A Priori Association  Proc SPIE Conf. Signal and Data Processing of Small Targets \(Vol 


SPIE Conf. Signal and Data Processing of Small Targets \(Vol 5204 15] R. Popp, K. R. Pattipati, and Y. Bar-Shalom  An M-best Multidimensional Data Association Algorithm for Multisensor Multitarget Tracking  IEEE Trans. Aerospace and Electronic Systems, 37\(1 pp. 22-39, January 2001 pre></body></html 


20 0  50  100  150  200  250  300 Pe rc en ta ge o f a dd iti on al tr af fic Cache size 200 clients using CMIP 200 clients using UIR c Figure 6. The percentage of additional traf?c the cache at every clock tick. A similar scheme has been proposed in [13], which uses fv, a function of the access rate of the data item only, to evaluate the value of each data item i that becomes available to the client on the channel If there exists a data item j in the client  s cache such that fv\(i j replaced with i A prefetch scheme based on the cache locality, called UIR scheme, was proposed in [7]. It assumes that a client has a large chance to access the invalidated cache items in the near future. It proposes to prefetch these data items if it is possible to increase the cache hit ratio. In [6], Cao improves the UIR scheme by reducing some unnecessary prefetches based on the prefetch access ratio \(PAR scheme, the client records how many times a cached data item has been accessed and prefetched, respectively. It then calculates the PAR, which is the number of prefetches divided by the number of accesses, for each data item. If the PAR is less than one, it means that the data item has been accessed a number of times and hence the prefetching is useful. The clients can mark data items as non-prefetching when PAR &gt; b, where b is a system tuning factor. The scheme proposes to change the value of b dynamically according to power consumption. This can make the prefetch scheme adaptable, but no clear methodology as to how and when b should be changed. Yin et al. [19] proposed a power-aware prefetch scheme, called value-based adaptive prefetch \(VAP the number of prefetches based on the current energy level to prolong the system running time. The VAP scheme de?nes a value function which can optimize the prefetch cost to achieve better performance These existing schemes have ignored the following characteristics of a mobile environment: \(1 query some data items frequently, \(2 during a period of time are related to each other, \(3 miss is not a isolated events; a cache miss is often followed by a series of cache misses, \(4 eral requests in one uplink request consumes little additional bandwidth but reduces the number of future uplink requests. In this paper, we addressed these issues using a cache-miss-initiated prefetch scheme, which is based on association rule mining technique. Association rule mining is a widely used technique in ?nding the relationships among data items. The problem of ?nding association rules among items is clearly de?ned by Agrawal et al. in [5]. However in the mobile environment, one cannot apply the existing association rule mining algorithm [4] directly because it is too complex and expensive to use This makes our algorithm different from that of [4] in 


This makes our algorithm different from that of [4] in twofold. First, we are interested in rules with only one data item in the antecedent and several data items in the consequent. Our motivation is to prefetch several data items which are highly related to the cache-miss data item within Proceedings of the 2004 IEEE International Conference on Mobile Data Management \(MDM  04 0-7695-2070-7/04 $20.00  2004 IEEE the cache-miss initiated uplink request. We want to generate rules where the antecedent is one data item, but the cache-missed data item and the consequent is a series of data items, which are highly related to the antecedent. If we have such rules, we can easily ?nd the data items which should also be piggybacked in the uplink request. Second in mobile environment, the client  s computation and power resources are limited. Thus, the rule-mining process should not be too complex and resource expensive. It should not take a long time to mine the rules. It should not have high computation overhead. However, most of the association rule mining algorithms [4, 5] have high computation requirements to generate such rules 5. Conclusions Client-side prefetching technique can be used to improve system performance in mobile environments. However, prefetching also consumes a large amount of system resources such as computation power and energy. Thus, it is very important to only prefetch the right data. In this paper, we proposed a cache-miss-initiated prefetch \(CMIP scheme to help the mobile clients prefetch the right data The CMIP scheme relies on two prefetch sets: the alwaysprefetch set and the miss-prefetch set. Novel association rule based algorithms were proposed to construct these prefetch sets. When a cache miss happens, instead of sending an uplink request to only ask for the cache-missed data item, the client requests several items, which are within the miss-prefetch set, to reduce future cache misses. Detailed experimental results veri?ed that the CMIP scheme can greatly improve the system performance in terms of increased cache hit ratio, reduced uplink requests and negligible additional traf?c References 1] S. Acharya, M. Franklin, and S. Zdonik. Prefetching From a Broadcast Disk. Proc. Int  l Conf. on Data Eng., pages 276  285, Feb. 1996 2] S. Acharya, M. Franklin, and S. Zdonik. Balancing Push and Pull for Data Broadcast. Proc. ACM SIGMOD, pages 183  194, May 1997 3] S. Acharya, R. Alonso, M. Franklin, and S. Zdonik. Broadcast disks: Data Management for Asymmetric Communication Environments. Proc. ACM SIGMOD, pages 199  210 May 1995 4] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In J. B. Bocca, M. Jarke, and C. Zaniolo editors, Proc. 20th Int. Conf. Very Large Data Bases, VLDB pages 487  499. Morgan Kaufmann, 12  15 1994 5] R. Agrawal, Tomasz Imielinski, and Arun Swami. Mining Association Rules Between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207  216, Washington, D.C May 1993 6] G. Cao. Proactive Power-Aware Cache Management for Mobile Computing Systems. IEEE Transactions on Computers, 51\(6  621, June 2002 7] G. Cao. A Scalable Low-Latency Cache Invalidation Strategy for Mobile Environments. IEEE Transactions on Knowledge and Data Engineering, 15\(5 ber/October 2003 \(A preliminary version appeared in ACM MobiCom  00 8] K. Chinen and S. Yamaguchi. An Interactive Prefetching Proxy Server for Improvement of WWW Latency. In Proc INET 97, June 1997 9] E. Cohen and H. Kaplan. Prefetching the means for docu 


9] E. Cohen and H. Kaplan. Prefetching the means for document transfer: A new approach for reducing web latency. In Proceedings of IEEE INFOCOM, pages 854  863, 2000 10] R. Cooley, B. Mobasher, and J. Srivastava. Data preparation for mining world wide web browsing patterns. Knowledge and Information Systems, 1\(1  32, 1999 11] C. R. Cunha, Azer Bestavros, and Mark E. Crovella. Characteristics of WWW Client Based Traces. Technical Report TR-95-010, Boston University, CS Dept, Boston, MA 02215, July 1995 12] D. Duchamp. Prefetching hyperlinks. In USENIX Symposium on Internet Technologies and Systems \(USITS  99 1999 13] V. Grassi. Prefetching Policies for Energy Saving and Latency Reduction in a Wireless Broadcast Data Delivery System. In ACM MSWIM 2000, Boston MA, 2000 14] S. Hameed and N. Vaidya. Ef?cient Algorithms for Scheduling Data Broadcast. ACM/Baltzer Wireless Networks \(WINET  193, May 1999 15] Q. Hu and D. Lee. Cache Algorithms based on Adaptive Invalidation Reports for Mobile Environments. Cluster Computing, pages 39  48, Feb. 1998 16] Z. Jiang and L. Kleinrock. An Adaptive Network Prefetch Scheme. IEEE Journal on Selected Areas in Communications, 16\(3  11, April 1998 17] V. Padmanabhan and J. Mogul. Using Predictive Prefetching to Improve World Wide Web Latency. Computer Communication Review, pages 22  36, July 1996 18] N. Vaidya and S. Hameed. Scheduling Data Broadcast in Asymmetric Communication Environments. ACM/Baltzer Wireless Networks \(WINET  182, May 1999 19] L. Yin, G. Cao, C. Das, and A. Ashraf. Power-Aware Prefetch in Mobile Environments. IEEE International Conference on Distributed Computing Systems \(ICDCS 2002 Proceedings of the 2004 IEEE International Conference on Mobile Data Management \(MDM  04 0-7695-2070-7/04 $20.00  2004 IEEE pre></body></html 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





