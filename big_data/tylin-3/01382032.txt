Proceedings of the Third International Conference on Machine Learning and Cybernetics Shanghai 26-29 August 2004 TEXT CATEGORIZATION BASED ON FREQUENT PATTERNS WITH TERM FREQUENCY XIAO-YUN CHEN'",YI CHEN',LEI WANG YUN-FA W  Department of Computer and information technology  Fudan University Shanghai 200433, China Department of Mathematics, Fuzhou University  Fuzhou 350002 China 2 E-MAIL c-xiaoyun@Zlcn.com Abstract The association categorization technology based on frequent patterns is recently presented which build the 
elasdRcation rules by frequent patterns in various categories and classii the new text employing these des But the current association classification methods exist shortage in two aspects when it is applied to classii text da one is the method iguored the information about word's frequency in a text  the other is the method need pruning des when the mass rules are generated but that lead to the veracity of classifying dropped. Therefore 
this paper present a texi categorization algorithm based on frequent patter0 with term frequency and obtains higher performance than other association categorization methods and some current text classification methods Our study provides evidence that association rule mining can he used for the construction of fast and effective classifiers for automatic text Categorization Keyword Association Rnles;Text categorization; Frequent Pattern 1 Introduction Automatic text categorization has always been an important application and research topic since the inception of digital documents 
Today text categorization is a necessity due to the very large amount of text documents that we have to deal with daily Most of the existing approaches for text categorization are based on Statistics and Machine Learning Among these approaches the famous are Niiive Bayes[l k-Nearest Neighbor KNN  Linear Least Squares Fit LLSF  Neural Network[4 Support Vector Machine SVM and so on Recently,. some new algorithms of classification are introduced which apply association rule 7 to classifying data for example CBA IO CMAR 9 and ARC Ill 
These classification algorithms discover association rules in the training samples and use these rules to build classifier But when these algorithms are used in text datasets there are some shortages 1 term frequency information of text 0-7803-8403-2/04/520.00 WOO4 IEEE is neglected in all text classification methods based on association rules The term frequency is important feahm of text datasets so it can't be ignored especially to association rules defined based on probability of words occurrence 2 among most of the existing classification methods based on 
association rules rule pruning strategies are applied when the mass rules are generated Although that can speed up the categorization process the classifying accuracy is reduced at the same time In this paper we present a new text association categorization method  text categorization based on association rules with term frequency We mainly make the following contributions I during the training process we introduce term frequent into frequent pattem and put forward to frequent pattem with term frequency mining algorithm 2 we apply frequent pattems with term frequency to 
build text classifier and store the rules in classification rules tree CR-tree Our method can achieve better efficiency even without pruning rules and avoids the problem of classification accuracy drop due to rule pruning in other method 9 1 I 2 The Feature Vector Expression for Documents Text categorization based on association rule usually consists of the following steps fustly documents feature vectors are generated The unshuctured text data is transformed into structured text data that is expressed by the document feature vector Secondly association rules are 
found by Apriori-lie algorithm once the entire set of rules has been generated an important step is to prune rules in order to reduce the amount of rules The last step is to classify new documents applying the association rules generated in the second step In order to convert the text data into the document feature vector we apply N-gram language model and x'statistics technology to selecting features of training document set If D is a training document set C  C 1610 


Proceedings of the Third International Conference on Machine Learning and Cyhemetics Shanghai 26-29 August 2004 C  Cm is a set of categories a document dj E D is assigned to category Ci  dj r,\(w w2 _ t.\(w is used to model the document d,,where wi is the term frequency of keyword ti it indicate the occurrence frequency of ti in the document di  We discover the term frequency in a text dataset generally gather in the specifically range only terms frequency of a few documents are higher So we define maximum term frequency threshold M\(t for term rk to limit term frequency at lower level The term frequency over the maximum threshold will he replaced with the threshold Maximum threshold strategy can prohibit from generating excessive rules 3 Generating Classification Rules 3.1 The Approach for Association Rules Discovery Association rule mining is a data mining task that discovers relationships among items in a transaction database Association rules have been extensively studied in the literature. Formally, association rules are defined as follows Let I i l,....in he a set of items and D be a set of transactions where each transaction consists of a subset of items in I An association rule is an implication of the formX-tY whereXcI,YcIandXnY=0 The rule X t Y holds in D with confidence c if c of transactions in D that contain X also contain Y The rule has a support s in the transaction set D if s of the transactions in D contains X U Y 171 3.2 Mining Frequent Pattern with Term Frequency The general algorithms of association rules mining don't involve term frequency so we can't directly applied them to mining frequent term sets in the document set with term frequency We fmtly introduce some symbol Definition 1 Given terms tj\(wj wj 1 We say ti\(wi and t,\(wj are similar whether wj  wi 2 If wc  wj  ti\(wi and tj\(wj are equal written hold or not written ti  t,\(WJ  t,\(W Given terms tj\(wj andtj\(wi 3 If I 2tj and w hvj we say term t,\(wi including rj\(wj written t,\(wi  ti\(wj 4 If t Qjandw wi we say term rj\(wt belong to ti\(wj written rt\(wj C rj\(wi Example 1 IC91  11 2 111 5 2 M5 U1\(5 M2 11 3 Iz\(1 111 VI 111 91 Definition 2 Given a term t the term frequency of t represents the time of t occurs in the document d or the term set X  written by R\(t d or R\(t X respectively Definition 3 Given a document d that contains the term-set X  the support count 9 of X in d is Example 2 Given a document d 11\(10 12\(3 13\(6 the support count of the term-set 11\(5 12\(1 for d n The support count p would be 1 when the frequency of all terms is equal 1 Definition 4 Given a document set D  every document d E D has the following form d  t,\(w wz  t.\(w the support of term set X in D is 2 kX,d ID1 P\(X D E Definition 5 Given the minimum support threshold E if q\(x.0 holds then term set X is frequent in D The frequent term set X is said tofrequent pattern too Lemma 1 If a term set X with term frequency is frequent then any superset of X must also he frequent The proof is easy so we skip over But the superset must consider the term frequency according to definition 1 For example, term t,\(3 is the superset of t,\(2 ort,\(l Algorithm 1 Find frequent pattern with term frequency Input A subset 0 of the Document set D where C is the category attached to the document d  A minimum support threshold E Output the frequent pattern set F of category C Method 1 For each term t E 0 do begin 2 NtI MdR\(t d  z   3 j=1 4 repeat 5 t j  dtW,Q 6 7 j+l F t i I t j  1611 


Proceedings of the Third International Conference on Machine Learning and Cybernetics Shanghai 26-29 August 2004 8 until\(j=l i<M[t]andr\(j-l 9 End IO For i=Z Fj.l 0.i  do begin 12 13 Foreach X inTj dobegin 15 End 16 17 End 18 F+U,\(X E i Ii  1 1 1 Tj=\(Fi.i W FiJ Tj=Ti X I i-l Fi.I 14 x support  dX.d Fi XE I X  support>_  Note the i-term set indicate the term-set consist of i different keywords. such as ab\(3 and cd 1 are.2-term sets Algorithm 1-6 steps generate frequent I-termset Fl the keyword t maybe have many frequent I-termset for example t\(l 2 3 and so on if t\(2 is not belong to F then t i i>2 1 can not belong to FI  In step 7-12 all the fresuent i-term sets Fj are generated the candidate i-term set Ti is generated by joining two different frequent i-1 term sets x.x'~Fi The joining condition of frequent i-1 term sets include 1 two frequent ill term sets X and x have and only have one different keyword 2 The term frequency is same if the keyword is same in X and x  Example 3 Given the training documents of the category in C as following, assume the minimum support count is 4 di=\(I1\(3 I MI dz=\(Iz\(l Id4 M2 ddIi\(4 Iz\(5 b\(3 b=\(I1\(2 Is\(1 MI 4=\(11\(3  I Id2 11 Iz 13 b 1s d132001 dz 0 14 0 2 d34503O a2010 1 434200 the term frequency matrix is showed as follows The frequent pattern set for C is F=~Il\(I I l If the document set D=U:,D includes m classes the category collectionc cl,c2,,..cm whereD consists of all documents belonging to C  The frequent pattern set found from document subsets of various categories are different but they are likely to intersect each other Definition 5 the classification rule for category C,\(i  12  m is formal as X X and X is frequent in D  X is the condition part of the de C is the consequence I5~l~.~l~~~~~z~~~.~l~~~~z Il\(l Z IZ\(1 10 Ii\(2 IZ\(2 I part of the rule Definition 6 The confidence of rule x x is defined by 3 In accordance with definition 6, we know that the higher confidence of a rule is the larger probability that the term set X occur in certain documents of the category c is contrarily the smaller the probability that the term set x appear in other categories is The classification rules that are built by frequent term sets in example 1 as following ldl Ir\(l MI 4\(I II\(Z 1d2 I~\(l l Idl 2 IAl I Z We considered two approaches to building a text classifier based on term association The first one is to extract association rules from the entire training set 9,10 but it is difficult to handle categories that are small or overlapping As a result we adopt the local approach to solve the problems that is classification rules are generated from each category respectively 4 4.1 Building Classifier Classifying the New Document Using the CR-tree We expect the algorithm can handle the pmblem of the multi-class categorization so following notions are introduced Define 7 Given a document d to classify class confidence for category C is the sum of rules confidence that match with document d and point to category C that is 4 Where R is the set of rules that match with d and its class label is C Define 8 If there are n rules matching with document d which ml of them have class label CI m2 of them have class label C2  and mk of them have class label Cr the chs diferencefuctor of category C for document d is Ma+\(Q\(d C,d  Q\(d,C i j  l,Z m L61 5 Difference factor shows difference between all other classes and the highest confidence class When solve the problem of multiple-class classification those categories that difference factor is less than given threshold 6 are selected contrarily, in the case of single-class classification only the highest confidence category is selected The detailed classifying process is elabrated in the following 6\(d,CJ  Mux\(Q\(d,C 1612 


Proceedings of the Third International Conference on Machine Learning and Cybemetics Shanghai 26-29 August 2004 Step 1 Find all matched rules with the new document d the matched rules of d are denote those rules that condition part appear in d Step 2 Group all matched rules of d by category label and the pups are sorted order by the class confidences P descending Step 3 Only those categories that exceed given threshold 8 areselected Obviously step 1 is the step that spends the most time in whole process of classifying 4.2 Search Matching Rules Based CR-tree The existing association classification method speeds up the process of classify by pruning rules 8,9,11,12 hut that will lead to lower the accuracy of categorization Wherefore we search the matched rules by classification rule tree and don\222t prune any rules Theorem 1 1 If a document d to classify doesn\222t match with the rule R T X  then the document doesn\222t match with the super rules of RI T X  2 If a document d doesn\222t match with the pattem q then the document don\222t match with the super-pattem of TI The amount of rules that must be scanned during the classification phase can be reduced by theorem 1 For example if the rulea\(3 A b\(l does not match with the document d, its supper rule 43 A b\(l A d\(Z have no use for being review because it must not match with the document d We stored all classification rules in Classification Rules Tree CR-tree which is a prefix tree structure The construction process of CR-tree is explained as follows 1 A CR-tree has a mot node 2 all nodes in the path from root node to no root node denote the term appearing at left hand side of rules, called Panern Node if the path consist of all terms that occur at left hand side of a rule then the lasted pattem node is Rule Node which can denote a rule 3 the node that denote super pattem is the child of the node that denote sub pattem. Since CR-trees are applied to store classification rules rule nodes must store the category label and the confidence of the rule Example 4 Assume a classification rule set R found in training phase is show as tahle\(a tahle\(a table b I Rule U I 12 Rule U 1 1  b\(l Z 60 1  a\(Z l Firstly, all terms appearing at left hand side of rules are sorted according to frequency descending order the result set is L:\(b\(l 2 c\(l d\(l We reordered rules in R by L and obtain rules set R\222 is show in table 3 Then building CR-tree by R as shown in Figure 1   a\(Z l b\(l 2 l 60 Wl l 70 b\(l I 70 b\(l l 60 b\(l l 60 5 c\(l Figure1 Classification rules tree CR-tree for example 4 The sub-pattem and super-pattem share same prefix nodes in CR-tree leads to a lot of storage space he saved According to theorem 1,all children node pattems impossibly appear in d if the father node pattems don\222t appear in document d So we needn\221t consider the sub-tree of the rule node or pattem node that doesn\222t match with the new document and can review directly its right sibling node If there isn\222t the right sibling node the algorithm backtracks to the father node of the current node and considers the right sibling node of father node 5 Experimental Results and Algorithm Analysis In order to test performance of our approach we implement this algorithm Data set is subset of news in People Daily from 1993 to 1997 which consist of 2000 training documents and 2315 testing documents the data set is divided into ten categories The measure used is micro-average of F  Table 1 shows a comparison between our TRARC classifier and other well known methods The text features are obtained by N-Gram technology and x2 statistic method the number of feature is 60 1613 


Proceedings of the Third International Conference on Machine Learning and Cybernetics Shanghai 26-29 August 2004  TRARC Ourapproach ARC Training time\(sec 196 51 Classi&ing tiWsec 19 46 The number of rules 5834 1641 MicmaveragoFdB 88.4 84.86 We conclude as following Our method performs well as compared to the other methods It outperforms most of the conventional methods such as KNN Bayes and SVM Specially classification accuracy of TRARC algorithm is better than ARC due to considering the term frequency in TRARC Although the classification rules generated by TRARC are further more than ARC TRARC based on CR-tree is faster than ARC Training time of TRARC is longer than ARC since TRARC must review more candidate term se6 after using term frequency The performance of association classification method is higher than Bayes KNN and SVM under lower feature numbers KNN SVM 0.04 0.06 18 60 92 14    80.57 75.4 80.3 CR-tree is obviously better than algorithms no applying CR-tree whether super rules are pruned or not Figure 2 shows the effect of using maximum term frequency From figure 2\(a we can find with increase of maximum term frequency F1 value has upward trend Figure 2\(b illuminates augment of maximum term freqnency result in the training time increase By figure 2\(c we found although the number of rules and training times are increased the classification time of TRARC maintain invariableness because of applying CR-Tree in TRARC, whereas the classification time of RARC without using CR-tree is obvious increased Figure3 shows that training time and classification time descend evidently with the increase of support threshold and the efficiency of TRARC is better than one of RARC Without CR-k In table 2 shows the efficiency of algorithms using With CR-Vee  The number of Rules Classification lime\(Sec Miemverage-Fl Table 2 The effect of using CR-tree or super rules pruning Super NkS Super Nk super NleF super des 434 5834 434 5834 40 52 5.5 19 80.4 88.9 80.4 889  91 88 815m I 615 c 60    413 85 e 15 I 9 IW  80  2 40 b 87 g 215 2 20  1 j 0  Y 86 I 0 012345618910 0 123 4 3 6 78 910 0 1234 56 78910 a  C Figure2 The effect of maximum term frequency on micro-average F1 value training time minsup=ZO and classification time 1614 


Proceedings of the Third International Conference on Machine Leaming and Cybernetics, Shanghai 26-29 August 2004 IQ 20 30 40 50 60 10 20 30 40 50 60 IQ 20 30 40 50 60 uinimun SYIIWlt himun suo~rttx uinimn supwrt\(X IdTRARC mc I a 3 C maximum term frequency is 5 Figure 3 The effect of minimum support threshold on micro-average F1 value training time and classification time 6 Conclusions In this paper we have presented a method applying frequent patterns with term frequency to build the classifier TRARC and classification rules are stored by CR-Tree The maximum term frequency threshold is introduced to limit the amount of rules discovered Our experiment proves the method can greatly improve association classifier\222s performance and avoid the problem of classification accuracy declining that is brought by pruning rules Acknowledgements This paper is supported by the National Natural Science Foundation 69933010 and Science Foundation of Education Bureau of Fujian Province \(JB02069 References I Lewis D.D Nave Bayes at forty  The independence assumption in information retrieval In Machine Leaming ECML98, the IO European Conference on Machine Learining, 1998.4-15 Yang Y  Liu,X 1999 re-examination of text categorization methods ACM Conference on Research and Development in Information Retrieval 42-49 3 Yang Y and Chute C.G An example-based mapping method for text categorization and retrieval ACM Transaction on Information Systems TOIS 1994 12\(3 252-271 4 Wiener E A neural network approach to topic spotting In Proceedings of the Annual Symposium on Document Analysis and Information Retrieval SDAIE 99,1995 2 5 J0achims.T 1998 categorization with support vector Machines Learning with Many Relevant Features Proceedings of the European Conference on Machine Learning \(ECML\Springer  Yang,Y.\(1999 evaluation of statistical approaches to text categorization Journal of Information Retrieval Vol 1, pp67-88 7 R.Agrawal and R.Srikant Fast algorithms for mining association rules In Proceeding of the 1994 International Conference on Vary Large Data Bases Santiago Chile 1994.487-499  G.Dong and J.Li Efficient mining of emerging pattems Discovering trends and differences In proc 1999 Int Conf Knowledge Discovery and Data Mining KDD99 pages43-52,San Diego CA Aug.1999 9 W.Li J.Han and J.pei.CMAR Accurate and efficient classification based on multiple classification rules In IEEE International Conference on Data Mining ICDM\222OI San Jose California November 29-December 2001 IO B.Liu W.Hsu and Y.Ma Integrating classification and association rule mining In ACM Int Conf on Knowledge Discovery and Data Mining SIGKDD\22298 pages 80-86 NewYork City NY August 1998  1 I 0.R.Zalane and M.L.Antonie Classifying text documents by associating terms with text categories In Thirteenth Australasian Database Conference ADC\22202 pages 215-222 Melbourne Australia January 2002 12 K.Wang,S.Zhou and S.C.Liew Buidmg Hierarchical Classifiers Using Class Proximity In the 25 VLDB Conference, pages 363-374 Edinburgh Scotland,l999 I31 J.HanJ.pei,and Y.Yin Mining frequent patterns without candidate generation In SIGMODOO,DaJlas,TX,May 2000 1615 


among items that appear in the transactions document as shown in Figure 3. Let us set the minimum support minsup  let $src:=document  transactions.xml   let $minsup:=2 let $total:=count\($src let $c:=distinct-values\($src let $l:=\(for $itemset in $c let $items:=\(for $item in $src where $itemset=$item return $item let $sup:=\(count\($items where $sup&gt;=$minsup return &lt;largeItemset&gt lt;items&gt; {$items} &lt;/items&gt lt;support&gt; {$sup} &lt;/support&gt lt;/largeItemset&gt let $L:=$l return &lt;largeItemsets&gt;{FP-growth\($l,$L,$minsup,$total,$src lt;/largeItemsets&gt  The XQuery expressions for the FP-growth are as follows define function FP-growth\(element $l, element $L, element minsup, element $total, element $src let $f-item:=first item in $L let $l-item:=last item in $L let $T:=getl-itemsets\($src, $total, $minsup return &lt;items&gt join \($l-item, $T lt;/items&gt let $l:=$l-items let $L:=remove\($l for f-item in $L where $f-item != $l-item return  FP-growth \($l, $L, $minsup, $total, $src   define function getl-itemsets\(element $src, element $total element $minsup let $l:=\(for $itemset in $src where $itemset=$l-item return $itemset let $sup:=\(count\($l where $sup&gt;=$minsup return &lt;largeItemset&gt lt;items&gt; {$item} &lt;/items&gt lt;support&gt; {$sup} &lt;/support&gt lt;/largeItemset&gt   Figure 4 shows all the large itemsets generated by our XQuery queries  lt;largeItemsets&gt lt;largeItemset&gt lt;items&gt lt;item&gt; B &lt;/item&gt lt;/items&gt lt;support&gt; 7 &lt;/support&gt lt;largeItemset&gt lt;largeItemset&gt lt;items&gt lt;item&gt; A &lt;/item&gt lt;/items&gt lt;support&gt; 4 &lt;/support&gt lt;largeItemset&gt  lt;largeItemset&gt lt;items&gt 


lt;items&gt lt;item&gt; C &lt;/item&gt  lt;/items&gt lt;support&gt; 4 &lt;/support&gt lt;largeItemset&gt  lt;largeItemset&gt lt;items&gt lt;item&gt; D &lt;/item&gt lt;/items&gt lt;support&gt; 2 &lt;/support&gt lt;largeItemset&gt  lt;largeItemset&gt lt;items&gt lt;item&gt; E &lt;/item&gt lt;/items&gt lt;support&gt; 2 &lt;/support&gt lt;largeItemset&gt lt;/largeItemsets&gt  Figure 4 : Large Itemsets document \(large.xml  Item    conditional  conditional    frequent pattern pattern base    FP-tree           generated E     {\(BAC:1 BA:1 BAE:2 D    {\(B:1 BA:1 C    {\(BA:2 B:2 BAC:2 A    {\(B:4}                      &lt;B:4&gt;              BA:4  Table 3: Mining FP-tree  7. Conclusion  In this paper, we presented the XQuery expression of the FP-growth method from XML data We use the FP-growth algorithm to generate the large itemset document, and then select all association rules from the large item set where support is greater than or equal to the minimum support. We have proposed frequent pattern tree \(FP-tree growth method, FP-growth. There are several advantages of FP-growth over other approaches. It constructs a highly compact FP-tree, which is usually substantially smaller than the original database, and thus saves the costly database scans in the subsequent mining processes. It applies a pattern growth method which avoids costly candidate generation and test by successively concatenating frequent 1-itemset found in the \(conditional divide-and-conquer method which dramatically reduces the size of the subsequent conditional pattern bases and conditional FP-trees. To our knowledge, the FP-growth method has been implemented using the query language XQuery, without pre-processing or post-processing of the data. Many issues remain open, one of the issues concerns the structure of the XML data. Since the structure of the XML data can be  very complex and irregular, identifying the mining context on such XML data becomes difficult. Our current and future research in this area focuses on investigating what other standard data mining algorithms can be expressed in XQuery  References  1] PMML 2.1 Predictive Model Markup Language http://www.dmg.org, March 2003 2] World Wide Web Consortium. Extensible Markup Language \(XML Second Edition 


Language \(XML Second Edition Recommendation. http://www.w3.org/XML 3] Xyleme. http://www.xyleme.com 4] Lucie Xyleme. A dynamic warehouse for XML data of the web. IEEE Data Engineering Bulletin, 2001 5] World Wide Web Consortium. XQuery 1.0: An   XML Query Language \(W3C Working Draft http://www.w3.org/TR/2002/WD-xquery-20020816, Aug 2002 6] World Wide Web Consortium. http://www.w3.org 7] R. Agrawal, T. Imielinski, and A. Swami. Mining association rules between sets of items in large databases. In P Buneman and S. Jajodia, editors, SIGMOD93, pages 207  216 Washington, D.C., USA, May 1993 8] R. Agrawal and R. Srikant. Fast algorithms for mining association rules in large databases. In J. B. Bocca, M. Jarke and C. Zaniolo, editors, Proceedings of 20th International Conference on Very Large Data Bases, pages 487  499 Santiago, Chile, September 12-15 1994 9] D. Braga, A. Campi, M. Klemettinen, and P. L.  Lanzi Mining association rules from XML data. In Proceedings of the 4th International Conference on Data Warehousing and Knowledge Discovery \(DaWaK 2002 Provence, France 2002 10] T. Imielinski and A. Virmani. MSQL: A query language for database mining. 1999 11] R. Meo, G. Psaila, and S. Ceri. A new SQLlike operator for mining association rules. In The VLDB Journal, pages 122  133, 1996 12] A. Termier, M.-C. Rousset, and M. Sebag. Mining XML data with frequent trees. In DBFusion Workshop  02, pages 87  96        pre></body></html 





 0 100 200 300 400 500 600 1 21416181 Time \(epoch counts 0.6 0.9 1.0 a MinMaxLoad WC 0 1000 2000 3000 4000 5000 6000 7000 8000 1 21416181 Time \(epoch counts 0.6 b SS2 Figure 4 Space needed at node R to store answer synopsis S A  non-distributed single-stream case In contrast when SS2 is used to set the precision gradient Figure 4b the space requirement is almost an order of magnitude greater This difference in synopsis size occurs because in SS2 frequency counts are only pruned from synopses at leaf nodes so counts for all items that are locally frequent in one or more local streams reach the root node No pruning power is reserved for the root node and therefore no count in S A is ever discarded irrespective of the  value The same situation occurs if Algorithm 2b is used instead of Algorithm 2a This result underscores the importance of setting  1  in order to limit the size of S A  as discussed in Section 2.1 3.4 Communication Load Figure 5 shows our communication measurements under each of our two metrics for each of our three data sets under each of the ve strategies for setting the precision gradient listed in Table 3 First of all as expected the overhead of SS1 is excessive under both metrics Second by inspecting Figure 5a we see that strategy MinRootLoad does indeed incur the least load on the root node R in all cases as predicted by our analysis of Section 2.1.2 Under this metric MinRootLoad outperforms both simple strategies SS1 and SS2 by a factor of 0 500 1000 1500 2000 2500 3000 3500 4000 INTERNET2 AUCTION BBOARD SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC 247k 10k 296k 132k 20k counts transmitted \(per epoch a Load on root node R 0 500 1000 1500 2000 2500 3000 3500 4000 INTERNET2 AUCTION BBOARD SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC 43k    21k 52k   19k 23k   7k counts transmitted \(per epoch b Maximum load on any link Figure 5 Communication measurements k denotes thousands ve or more in all cases measured However MinRootLoad performs poorly in terms of maximum load on any link as shown in Figure 5b because no early elimination of counts for infrequent items is performed and consequently synopses sent from the grand-children of the root node to the children of the root node tend to be quite large As expected MinMaxLoad NWC performs best under that metric on all data sets For the AUCTION data set even though SS2 outperforms MinMaxLoad WC to be expected because of the high  value our hybrid strategy MinMaxLoad NWC is superior to SS2 by a factor of over two For the I NTERNET 2 and BBOARD data sets the improvement over SS2 is more than a factor of three On the negative side total communication not shown in graphs is somewhat higher under MinMaxLoad WC than under SS2 increase of between 7.5 and 49.5 depending on the data set 4 Related Work Most prior work on identifying frequent items in data streams 6 8 9 19 only considers the single-stream case While we are not aware of any work on maintaining frequency counts for frequent items in a distributed stream setting work by Babcock and Olston does adProceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


dress a related problem In the problem is to monitor continuously changing numerical values which could represent frequency counts in a distributed setting The objective is to maintain a list of the top k aggregated values where each aggregated value represents the sum of a set of individual values each of which is stored on a different node The work of assumes a single-le v e l communication topology and does not consider how to manage synopsis precision in hierarchical communication structures using in-network aggregation which is the main focus of this paper The work most closely related to ours is the recent work of Greenwald and Khanna which addresses the problem of computing approximate quantiles in a general communication topology Their technique can be used to nd frequencies of frequent items to within a con“gurable error tolerance The work in focuses on providing an asymptotic bound on the maximum load on any link our result adheres to the same asymptotic bound It does not however address how best to con“gure a precision gradient in order to minimize load which is the particular focus of our work 5 Summary In this paper we studied the problem of nding frequent items in the union of multiple distributed streams The central issue is how best to manage the degree of approximation performed as partial synopses from multiple nodes are combined We characterized this process for hierarchical communication topologies in terms of a precision gradient followed by synopses as they are passed from leaves to the root and combined incrementally We studied the problem of nding the optimal precision gradient under two alternative and incompatible optimization objectives 1 minimizing load on the central node to which answers are delivered and 2 minimizing worst-case load on any communication link We then introduced a heuristic designed to perform well for the second objective in practice when data does not conform to worst-case input characteristics Our experimental results on three real-world data sets showed that our methods of setting the precision gradient are greatly superior to na  ve strategies under both metrics on all data sets studied Acknowledgments We thank Arvind Arasu and Dawn Song for their valuable input and assistance References  R Agra w a l and R Srikant F ast algorithms for mining association rules In VLDB  1994  I Akamai T echnologies Akamai http://www akamai.com   A Ak ella A Bharambe M Reiter  and S Seshan Detecting DDoS attacks on ISP networks In PODS Workshop on Management and Processing of Data Streams  2003  A Arasu and G S Manku Approximate quantiles and frequency counts over sliding windows In PODS  2004  B Babcock and C Olston Distrib uted top-k monitoring In SIGMOD  2003  M Charikar  K  Chen and M F arach-Colton Finding frequent items in data streams In International Colloquium on Automata Languages and Programming  2002  E Cohen and M Strauss Maintaining time-decaying stream aggregates In PODS  2003  G Cormode and S Muthukrishnan What s hot and whats not Tracking frequent items dynamically In PODS  2003  E D Demaine A Lopez-Ortiz and J I Munro Frequency estimation of internet packet streams with limited space In European Symposium on Algorithms  2003  DynaServ er R UBis and R UBBos http://www.cs rice.edu/CS/Systems/DynaServer   eBay Inc eBay  http://www.ebay.com   C Estan and G V a r ghese Ne w directions in traf c measurement and accounting In SIGCOMM  2002  M F ang N Shi v akumar  H  Garcia-Molina R Motwani and J Ulmann Computing iceberg queries ef“ciently In VLDB  1998  P  B Gibbons and Y  Matias Ne w sampling-based summary statistics for improving approximate query answers In SIGMOD  1998  P  B Gibbons and S T irthapura Estimating simple functions on the union of data streams In Symposium on Parallel Algorithms and Architectures  2001  M Greenw ald and S Khanna Po wer conserving computation of order-statistics over sensor networks In PODS  2004  J Han J Pei G Dong and K W ang Ef cient computation of iceberg queries with complex measures In SIGMOD  2001  Internet2 Internet2 Abilene Netw ork http abilene.internet2.edu   R M Karp S Shenk er  and C H P apadimitriou A simple algorithm for nding frequent elements in streams and bags ACM Trans Database Syst  2003  H.-A Kim and B Karp Autograph T o w ard automated distributed worm signature detection In Proceedings of the 13th Usenix Security Symposium  2004  A Manjhi V  Shkapen yuk K Dhamdhere and C Olston Finding recently frequent items in distributed data streams Technical report 2004 http://www cs.cmu.edu/˜manjhi/freqItems.pdf   G S Manku and R Motw ani Approximate frequenc y counts over data streams In VLDB  2002  Open Source De v elopment Netw ork Inc Slashdot http://slashdot.org  Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


12] S. Fujiwara, J. D. Ullman and R. Motwani, Dynamic Miss-Counting Algorithms: Finding Implications and Similarity Rules with Confidence Pruning. Proceedings of the IEEE ICDE \(San Diego, CA 13] F. Glover  Tabu Search for Nonlinear and Parametric Optimization \(with Links to Genetic Algorithms  Discrete Applied Mathematics 49 \(1-3 14] M. Klemettinen, H. Mannila, P. Ronkainen, H Toivonen, and A. Verkamo, Finding interesting rules from large sets of discovered association rules. Proceedings of the ACM CIKM, International Conference on Information and Knowledge Management \(Kansas City, Missouri 1999 15] C. Ordonez, C. Santana, and L. de Braal, Discovering Interesting Association Rules in Medical Data. Proceedings of the IEEE Advances in Digital Libraries Conference Baltimore, MD 16] W. Perrizo, Peano count tree technology lab notes Technical Report NDSU-CS-TR-01-1, 2001 http://www.cs.ndsu.nodak. edu /~perrizo classes/785/pct.html. January 2003 17] Ron Raymon, An SE-tree based Characterization of the Induction Problem. Proceedings of the ICML, International Conference on Machine Learning \(Washington, D.C 275, 1993 18] N. Shivakumar, and H. Garcia-Molina, Building a Scalable and Accurate Copy Detection Mechanism Proceedings of the International Conference on the Theory and Practice of Digital Libraries, 1996 19] P. Tan, and V. Kumar, Interestingness Measures for Association Patterns: A Perspective, KDD  2000 Workshop on Post-processing in Machine Learning and Data Mining Boston, 2000 20] H.R. Varian, and P. Resnick, Eds. CACM Special Issue on Recommender Systems. Communications of the ACM 40 1997 Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE pre></body></html 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


