Proceedings of the Third International Conference on Machine Learning and Cybernedcs Shanghai, 26-29 Aygust 2004 AN IMPROVED ALGORITHM OF MINING FROM FP-TREE YONG QIU YONGJIE LAN QING-SONG XIE Infomation Electronic Engineering School Shandong Institute of Business and Technology  YanTai 264005 China E-MAIL sdytqy@l63.com Abstract Discovering association rules is a basic prnhlem in data mining. Finding frequent item sets is the most expensive step in assnciation rule discovery Analysing a frequent pattern gmwth FP-growth method 
is effieient and sealable for mining both long and short frequent patterns without candidate generation And proposing a new efficient algorithm QFP-growth not only heirs all the advantages in FP-growth method but also avoids it\222s bottleneck in generating a huge nnmber of conditional FP-trees By Using the technology of temporary mot QFP-growth rednee the processing time and memory spaee for miniog frequent item sets significantly Performance study also shows that the 
QFP-gmwtb method is effieient and scalable for mining large databases or data warehouses Moreover the algorithm generates frequent item sets in order so that the result can be used expediently Keywords fi.equent item sets 1 Introduction Knowledge diseovery data mining assoeiation des Association rule shows relationships among sets of items in a bansaction database Association rule discovery has been an active research area since its introduction in Agrawal, Imielinski and Swami 1993  The process of mining association rules consists of two main steps 1 
Find the frequent item sets or large item sets with a minimum support 2 Use the large item sets to generate association des that meet a confidence threshold Step 1 is the more expensive of the two since the number of item sets grows exponentially with the number of items A large number of increasingly efficient algorithms to mine frequent item sets have been developed over the years Most of the proposed pattem-mining algorithms are a variant of Apriori  2 3 4 5 61 which is based on 
a bottom-up breadth-fmt search that enumerates every single frequent itemset. Apriori uses the downward closure propeay of itemset support to prune the search space  the property that all subsets of a frequent itemset must themselves be frequent Thus the essential idea is to iteratively generate the set of candidate pattems of length cl from the set of frequent patterns of length k and check their corresponding occurrence frequencies in the database The Apriori heuristic achieves good performance gained by possibly significantly reducing the size of candidate However in situations with prolific frequent 
patterns long patterns or quite low minimum support thresholds an Apriori-like algorithm may still suffer from the following two nontrivial costs[lll 1 is costly to handle a huge number of candidate sets For example if there are lo4 frequent I-itemsets the Apriori algorithm will need to generate more than IO\222 length-2 candidates accumulate and test their occurrence frequencies Moreover to discover a frequent pattem of size 100 such as al  a100 it must generate more than 2\222\224 IO\224 candidates in total This is the 
inherent cost of candidate generation no matter what implementation technique is applied 2 is tedious to repeatedly scan the database and check a large set of candidates by pattern matching which is especially hue for mining long patterns Recent studies show FP-growth[l,l I algorithm is one of the most efficient frequent pattem mining methods As a divide-and-conquer method this method partitions projects the database into partitions recursively, but does not generate candidate sets This method also makes use of Apriori property 2 if any length pattern is not frequent in the database its length superpattems can never be frequent It 
counts frequent pattems in order to decide whether it can assemble longer pattems The FP-growth method adopts the divide-and-conquer strategy as follows a large database is compressed into a highly condensed, much smaller frequent pattem tree or FP-tree for short which is an extended prefix-@ee structure storing crucial quantitative information about frequent patterns Then an FP-tree-based pattern fragment growth mining method is adopted which starts from a frequent length-1 pattern as an initial suffix pattem examines only its conditional pattem base a 223sub-database\224 which consists of the set of frequent items 0-7803-8403-2/04/$20.00Q2004 IEEE 1665  


co-occurring with the suffix pattem constructs its conditional FP-tree and performs mining recursively with such a tree The pattem growth is achieved via concatenation of the suffix pattem with the new ones generated from a conditional FP-tree However, if we study the FP-growth method carefully it also has performance bottleneck and it must generate a huge number of conditional FP-trees recursively h process of mining so the efficiency of FP-growth remains unsatisfactory After some careful study and examination we proposed a new improved efficient algorithm QFP-growth  and an extension of the FP-growth method which not only heirs all the advantages in the FP-growth method but also avoids it\222s bottleneck in generating a huge number of conditional FP-trees By using the technology of temporary root the algorithm is compendious and satisfying in space and performance QFP-growth reduces the processing time and memory space for mining frequent item se significantly Experimental results also show that QFP-growth is competitive The remaining of the paper is organized as follows Section 2 introduces the method of FP-tree construction and 221tn FP-tree-based frequent pattem mining algorithm FP-growth Section 3 presents our algorithm of QFP-growth Section 4 discusses its scalability and perfomiance. Section 5 summarized our study and points out our some future research issues 2 FP-Growth Algorithm Xl Frequent Pattern Mining Problem The frequent pattem mining problem was fust introduced by R Agrawal et al in AIS931 as mining association rules between sets of items[l,2 Let I  al a2  am be a set of items and a transaction database DB  T1 T2  Tn  where Ti i E l..n is a transaction which contains a set of items in I Every transaction has a key label, called TID. The supportl or occurrence frequency of a pattem A which is a set of items is the number of transactions containing A in DB A is a frequent pattem if the support of A is no less than a predefined minimum support threshold 5 Given a transaction database DB and a minimum support threshold 5 the problem of fmdmg the complete set of frequent patterns is called the frequent pattem mining problem 22 FP-tree construction Algorithm To avoid candidate-generation-and-test in Apriori an Algorithm called FP-growth was proposed in ACM2000][1,111 the Algorithm based on the following observations 1 Since only the frequent items will play a role in the frequent pattern mining it is necessary to perform one scan of DB to identify the set of frequent items with frequency count obtained as a by-product 2 If we store the set of frequent items of each transaction in some compact structure it may avoid repeatedly scanning of DB 3 If multiple transactions share an identical frequent item set they can be merged into one with the number of occurrences registered as count It is easy to check whether two sets are identical if the frequent items in all of the transactions are sorted according to a fued order 4 If two transactions share a common prefix according to some iorted order of frequent items the shared parts can be merged using one prefix structure as long as the count is registered properly If the frequent items are sorted in their frequency descending order there are better chances that more prefix strings can be shared Algorithm 1 FP-tree construction  Input A transaction database DB and a minium support threshold 5 Output Its frequent pattem tree, FP-Tree Method The F\224-tree is constructed in the following steps I Scan the transaction database DB once. Collect the set of frequent items F and their supports Sort F in support descending order as L the list of frequent items 2 Create the mot of an FP-tree T and label it as 223null\224 for each transaction in DB do the following. Select and sort the frequent items in transaction according to the order of L Let the sorted frequent item list in transaction be PIP where p is the fxst element and P is the remaining list Call insert-tree PIP T Function insert-tree PIP T If T has a child N such that N.item-name  p.item-name Then increment Ns count by 1 Else do create a new node N Ns count  1 Ns parent link be linked to T Ns node-link be linked to the nodes with the same item-name via the node-link structure If P is nonempty Call insert-tree P N I Example  Let the transaction database DB be Table 1 and 5  3 Table 2 is ordered frequent items after fmt scan of database We can construct the FP-tree of Example database as Figure 1 Proceedings of the Third International Conference on Machine Learning and Cybemetics Shanghai, 26-29 August 2004 1666 


Proceedings of the Third International Conference on Machine Learning and Cybemetics Shanghai 26-29 August 2004 P II fcam:2 cb:l Table 1 A transaction database TID I Items c:3 Figure 1 FP-tree of Example database 23 Mining Frequent Patterns From FP-tree The process of FP-tree mining is as follows begin with the length-1 frequent pattern, construct its conditional pattern base which is a 223sub-database\224 constructed with the prefix sub-path set co-occurring with the suffix pattern in the FP-tree Then construct its conditional FP-me and mine recursively in this tree The pattern growth is realized by linking the suffix pattern and its conditional FP-tree Algorithm 2 W-growth Mining frequent patterns with FP-tree by pattern fragment growth  Input FP-tree and a mini\223 support threshold 6  Output The complete set of frequent patterns Method: call FP-growth\(FP-tree, null Procedure FP-growth\(Tree a m b a C  fca:2 fcabl J fca:2 f:l c:l fc:3 JIb  fc:4  fc:4  t4 J  fca:3 Jlm  t4 IC I f I Empty Empty There are several advantages of FP-growth over other approaches 1 It constructs a highly compact FP-tree which is usually substantially smaller than the original database and thus saves the costly database scans in the subsequent mining processes 2 It applies a pattern growth method which avoids costly candidate generation and test by successively concatenating frequent 1-itemset found in the conditional FP-trees This ensures that it never generates any combinations of new candidate sets which are not in the database because the itemset in any transaction is always encoded in the corresponding path of the FP-trees In this context mining is not Apriori-like restricted generation-and-test but frequent 222 pattern fragment growth only The major operations of mining are count accumulation and prefur path count adjustment 1667 


Profeedings of the Third International Conference on Machine Learning and Cybemetics Shanghai 26-29 August 2004 which are usually much less costly than candidate generation and pattem matching operations performed in most Apriori-like algorithms 3 It applies a partitioning-based divide-and-conquer method which dramatically reduces the size of the subsequent conditional pattem bases and conditional FP-tree 3 conditional FP-trees construction QFP-growth miniig frequent pattern without However as discussed above FP-growth algorithm must generate a huge number of conditional FP-trees recursively in process of mining it may take too much time and space. Can we avoid conditional FP-trees generation in frequent pattern mining To attack this problem we develop QFP-growth a extended FP-growth method for frequent pattem mining from FP-tree The algorithm is described bellow fm3 fcm3 fcam3 fam3 cam3 cm3, am3, p3,cp3 There is no order in the frequent pattems serial And By using QFP-growth algorithm the frequent pattems are f5,fc4,fca4,fcam3,fcm3,fa4,fam3,fm3,c5,ca4,cam3,cb3 m3 cp3 a4 am3,b4,m3,p3 The frequent pattems serial is ordered descending by head table of FP-tree 3 To avoid more Conditional FP-tree constructing FP-growth algorithm have to check whether a sub-tree is a single path tree But in QFP-growth  because of no conditional FP-tree constructed there is no reason to differentiate single path tree so the algorithm is simplified 4.The QFP-growth can be called only in condition of sum of count of temp-root\222s child minimum support this can reduce the call number of procedure evidently Figure 2 to 5 show temporary root construction of example database Executing procedure of QFP-growth is showed in table 4  Algorithm 3 QFP-growth Mining frequent pattems with FP-tree by pattem fragment growth Input FP-tree  and a minimum support threshold 6  Output: The complete set of frequent pattems Method call QFP-growth\(root null  Procedure QFP-growth\(RO0T Q FPgrefix a U in Q in top down order   Mining multipath FP-tree 3 sum all count of ai under Q 4 if ai suppor then  5 generate pattern B  a U ai with suppor ai support 6 conshuct temp-root with ai\222s child node 7 if temp-root bas child then call QFP-growth\(temp-root p  8 if sum of count of temp-root\222s child>=c then call QFP-growth\(temp-root p  9 I The improvement of QFP-growth algorithm 1 Instead of building Conditional pattemlbase and Conditional tree QFP-growth algorithm simply construct a dynamic temporary root so it can improve perfohance and efficiency 2 In FP-growth algorithm 223generate pattem B  ai U U with support  ai support\224  but in QFP-growth algorithm 223generatepattern B U U ai withsupper ai support\224  The frequent pattems are generated in order so they can be used expediently For example By using FP-growth algorithm, frequent pattems mined from FP-tree in figure 1 are f5 c5 fc4,a4 fa4 ca4, fca4 b4 cb3, m3 into ai support Figure 2 Temporary mt after generating f4 Figure 3 Temporary root after generating fc4 1668 


Proceedup bf the Tbird International Conference on Machine Laming and Cybernetics, Shanghai 26-29 August 2004 FvmI I WEL3 FP __   _____ __  4 Experiments and Results The goal of the experiments is to find owthe extent of different dataset properties that could affect the performance of QFT-growth algorithms and the relative performance compares with FP-growth algorithm Datasets are generated with the data generator by IBM QUEST FT-growth is provided by the original authors Experiments are performed on a Pentium 4 1.8GHz PC with 512Mb RAM running on Window XP All programs are complied with the same compiler. Experimental results show that QFT-growth is competitive it is efficient and scalable for mining large databases or data warehouses See figure 5 and figure 6 Figure 4 Temporary root after generating fcd No 1  CALLLEVELO I CALLLEvEL.1 1 CALLL  I PARAM IFP IPARAM IFP I PARAM IFP IPARAn root\(f5 cl llll I f5 J I I I I I  L root\(c4,bl fc4 4 3 root\(a4 fc fa4 J 4 mt\(m2.b2 4  61 fCap2 x I 71 I I I I I fcab2 x fed J I I I I I in I I I 1 fnAJ I I I I 91 92 x 1669 


Proceedqs of the Third International Conference on Machine Le-g and Cybernetics Shanghai 26-29 August 2004 50 45 40  35 Z 30 E 25  20 2 15 10 5 0 U  C t 0 05 1 15 2 25 3 Support threshold\($8 Figure 5 Scalability of QFP-growth with respect to support with single run 512 I E t i 12 05 1 15 2 Supportthreshold  Figure 6 Memory Comparison 5 Conclusions We have proposed an improved method QFP-growth for constructing frequent pattems from FP-tree Comparing with FF-growth metliod, QFP-growth can generate frequent pattems without condtional pattem-base and conditional FP-tree generation and the frequent pattems are generated  121 www.cs.sfu.cd-peijianIpersonaYpublications in order. It can be used to mine frequent pattems efficiently in 1 databases Our future research works is to use the QFF-growth method into implementation of SQLbased highly scalable Fp-bee structure constraint-based mining of frequent pattems sequential pattems max-pattems partial periodicity and other interesting frequent pattems mining Acknowledgements This paper is supported by ShanDong Physical Science Foundation\(Y2002G08 References I Han J and Kkber M Data mining Concepts and Techniques, Academic Press 2001 2 R Agrawal and R Srikant Fast algorithms for mining association rules In VLDB\2221994 pp 487-499 3 G Grahne L Lakshmanan and X Wang Efficient mining of constrained correlated sets In ICDE2OOO 4 M Hemettinen H Mannila P Ronkainen H Toivonen and A.I Verkamo.\222 Finding interesting rules from large sets of discovered association rules In CIKM\2221994 pp 401-408 5 B Lent A Swami and J Widom Clustering association rules In ICDE1997 pp 220-231 6 R Ng L V S Lakshmanan J Han and A Peng Exploratory mining and pruning optimizations of constrained associations rules In SIGMOD 1998 pp 13-24 171 J.S Park M.S Chen and P.S Yu An effective hash-based algorithm for mining association rules In SIGMOD1995 pp 175-186 8 S Sarawagi S Thomas and R Agrawal Integrating association rule mining with relational database systems Alternatives and implications In SIGMOD\2221998 pp. 343-354 9 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large database In VLDB\222 1995 pp. 432-443 lo R Srikant Q Vu and R Agrawal. Mining association rules with item constraints In KDD\222 1997 pp 594-605 11 Jiawei Han Jian Pei and Yiwen Yin Mining Frequent Patterns without Candidate Generation http I 1670 


I] Hu. K. and Xia S.W., Data mining based on large data warehouse, Journal of software, Vol 9,No.l,pp.53-63,Jan.1998 2] Agrawal. R., Mining association rules between sets of items in large database, Proc. ACM SIGMOD int  l conf. Management of data, Washington DC, pp 207-216.May.1993 3] Alex. B. and Stephen. IS., Data warebouse, Data mining and OLAP, McGraw-Hill Book Co. 1999 4] Chen. M.S. and Philip. S., Data mining: an overview from database perspective, IEEE Transaction on knowledge and data engineering pp.866-883 Aug.1996 Roberto. J., Efficiently mining long pattems from databases, Proceedings of the 1998 ACM-SIGMOD int  l conf. on management of data pp.85-93,1998 6] Duda. R.O., Hart. P.E. and Stork. D.J., Pattem recognition, Wiley, New York, 2001 7] Zaki, M.J., Ogibara. M. and Li. W., New algorithms for fast discovery of association rules, Proceedings of the third Int  l conf. on knowledge discovery in database and data mining, pp.283-286.1997 8] Goulboume. G.,  Coenen. F. and Leng. P., Algorithms for computing association rules using a partial-support tree, Knowledge-Based Systems, Vol 13 pp.141-149,2000 9] Yingwu Fang, Guangpeng Zhang, Dewei Wu and Wang Yi, Research on distributive data mining calculating process-DDCP algorithm, Joiirnal of university of elechonic science and technology, Vo132 No. 1, pp.80-84, Feb.2003 5 1660 pre></body></html 


It should be noted that after the above process the resulting support constraint set may become inconsistent Thus in the next round the value c   m i 1 z i may be larger If that happens the larger value c does not interpret as the privacy condence level Instead it should be interpreted as an indicator for inconsistency of the support constraint set Thus the above privacy deletion procedure should only be carried out one time We should note that even if the condence level is higher that is c   m i 1 z i is small there is still possibility that the condential information specied by  I,s,S  is leaked in theory That is for each transaction database D that satises the constraints S wehave support  I D    s S   However no one may be able to recover this information since it is NP hard to infer this fact Support constraint inference has been extensively studied by Calders in 2 3 It would be interesting to consider conditional privacypreserving synthetic transaction database generations That is we say that no private information is leaked unless some hardness problems are solved efciently This is similar to the methodologies that are used in public key cryptography For example we believe that RSA encryption scheme is secure unless one can factorize large integers In our case we may assume that it is hard on average to efciently solve integer linear programs Based on this assumption we can say that unless integer linear programs could be solved efciently on average no privacy specied in P is leaked by S if the computed condence level c   m i 1 z i is small 5 Conclusions In this paper we discussed the general problems regarding privacy preserving synthetic transaction database generation for benchmark testing purpose In particular we showed that this problem is generally NP hard Approximation algorithms for both synthetic transaction database generation and privacy leakage condence level approximation have been proposed These approximation algorithms include solving a continuous variable linear program According to 6 l i n ear probl ems ha vi ng hundreds of t housands of continuous variables are regularly solved Thus if the support constraint set size is in the order of hundreds of thousands then these approximation algorithms are efcient on regular Pentium-based computers If more constraints are necessary then more powerful computers are needed to generate synthetic transaction databases References 1 R  A g r a w al T  Imilien sk i an d A  S w a mi Min in g association rules between sets of items in large databases In Proc of ACM SIGMOD International Conference on Management of Database  pages 207216 1993  T  C a lders  Axiomatization and Deduction Rules for the Frequency of Itemsets  PhD Thesis Universiteit Antwerpen 2003  T  C a l ders  C omput at i onal compl e x i t y of i t e ms et frequency satisability In Proc 23rd ACM PODS 04  pages 143154 ACM Press 2004  R  F agi n J  Hal pern and N Me gi ddo A l ogi c f or reasoning about probabilities Information and Computation  87 1,2\78128 1990  G Geor gak opoul os  D  K a v v a di as  a nd C  P a padi mitriou Probabilistic satisability J of Complexity  4 111 1988  Li near P r ogrammi ng F r equent l y As k e d Q ues t i ons  http://www-unix.mcs.anl.gov/otc Guide/faq/linear-programming-faq html  T  M ielik  ainen On inverse frequent set mining In Proc of 2nd Workshop on Privacy Preserving Data Mining PPDM  pages 1823 IEEE Computer Society 2003  C  P o t t s  A nal ys i s of a l i n ear programmi ng heuri s t i c for scheduling unrelated parallel machines Discrete Appl Math 10 155164 1985 9 G  R amesh  W  Man iatty  a n d M Zak i Feasib le itemset distributions in data mining theory and application In Proc 22nd ACM PODS  pages 284295 2003  Y  W a ng X W u  a nd Y  Zheng P r i v ac y p res ervi ng data generation for database application performance testing In Proc 1st Int Conf on Trust and Privacy in Digital Business TrustBus 04 together with DEXA  LNCS 3184 pages 142-151 2004 Springer-Verlag  X W u  Y  W u Y  W a ng and Y  Li P ri v a c y a w are mar ket basket data set generation a feasible approach for inverse frequent set mining In Proc 5th SIAM International Conference on Data Mining  April 2005  Z Zheng R  K oha vi  a nd L Mas on R eal w o rl d performance of association rule algorithms In Proc of the ACM-SIGKDD International Conference on Knowledge Discovery and Data Mining  pages 401 406 ACM Press 2001 8 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDM05 1550-4786/05 $20.00  2005 IEEE 


Discovery, 8, 2004, pp. 7-23 20] W. Teng, M. Hsieh, and M. Chen. On the Mining of Substitution Rules for Statistically Dependent Items Proceedings of IEEE International Conference on Data Mining \(ICDM  02 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


14] Kryszkiewicz, M., and Rybinski, H. \(1999  Incomplete Database Issues for Representative Association Rules  ISBN: 3-540-65965-X, pp. 583-591 15] Little, R.J.A., and Rubin, D.B. \(2002 analysis with missing data, Wiley, New York, ISBN 0471183865 16] Omiecinski, E.R. \(2003  Alternative interest measures for mining associations in databases  IEEE TKDE vol. 15, no. 1, pp.57-69 17] Piramuthu, S. \(1998  Evaluating feature selection methods for learning in data mining applications  in the Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, vol. 5, pp. 294-301 18] Pyle, D. \(1999 Morgan Kaufmann Publishers, Inc.ISBN:1-55860-529-0 19] Ragel, A., and Cremilleux, B. \(1999  MVC - A preprocessing Method to deal with missing values   knowledge based system, vol. 12, pp.285-291 20] Ramoni, M., and Sebastiani, P. \(2000  Bayesian Inference with Missing Data Using Bound and Collapse   Journal of Computational and Graphical Statistics, vol. 9, no 4, pp. 779-800 21] Ramoni, M., and Sebastiani, P. \(2001  Robust Bayes Classifiers  AI, vol. 125, no. 1-2, pp. 207-224 22] Ramoni, M., and Sebastiani, P. \(2001  Robust Learning with Missing Data  Machine Learning, vol. 45, no 2 , pp. 147-170 23] Scott, R.E. \(1993 Logic and Practice, SAGE Publications, ISBN: 0803941072 24] Zaki, M.J., and Hsiao, C.J. \(2002  CHARM: An efficient algorithm for closed itemset mining  in the Proceedings of the Second SIAM International Conference on Data Mining Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE pre></body></html 


13: else 14: E|i?1| = E|i?1| ? s The backward process in Algorithm 1, generates level-wise every possible subset starting from the borProceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE der de?ned by Edge without getting into equivalence classes which have been already mined \(Line 10 such subset satis?es the constraint then it can be added to the output \(Line 12 reused later to generate new subsets \(Line 14 have a monotone constraint in conjunction, the backward process is stopped whenever the monotone border B+\(Th\(CM Lines 3 and 8 4.3. Closed Constrained Itemsets Miner The two techniques which have been discussed above are independent. We push monotone constraints working on the dataset, and anti-monotone constraints working on the search space. It  s clear that these two can coexist consistently. In Algorithm 2 we merge them in a Closet-like computation obtaining CCIMiner Algorithm 2 CCIMiner Input: X,D |X , C, Edge,MP5, CAM , CM X is a closed itemset D |X is the conditional dataset C is the set of closed itemsets visited so far Edge set of itemsets to be used in the BackwardMining MP5 solution itemsets discovered so far CAM , CM constraints Output: MP5 1: C = C ?X 2: if  CAM \(X 3: Edge = Edge ?X 4: else 5: if CM \(X 6: MP5 = MP5 ?X 7: for all i ? flist\(D |X 8: I = X ? {i} // new itemset avoid duplicates 9: if  Y ? C | I ? Y ? supp\(I Y then 10: D |I= ? // create conditional fp-tree 11: for all t ? D |X do 12: if CM \(X ? t 13: D |I= D |I ?{t |I  reduction 14: for all items i occurring in D |I do 15: if i /? flist\(D |I 16: D |I= D |I \\i // ?-reduction 17: for all j ? flist\(D |I 18: if supD|I \(j I 19: I = I ? {j} // accumulate closure 20: D |I= D |I \\{j 21: CCIMiner\(I,D |I , C,B,MP5, CAM , CM 22: MP5 = Backward-Mining\(Edge,MP5, CAM , CM For the details about FP-Growth and Closet see [10 16]. Here we want to outline three basic steps 1. the recursion is stopped whenever an itemset is found to violate the anti-monotone constraint CAM Line 2 2  and ? reductions are merged in to the computation by pruning every projected conditional FPTree \(as done in FP-Bonsai [7 Lines 11-16 3. the Backward-Mining has to be performed to retrieve closed itemsets of those equivalence classes which have been cut by CAM \(Line 22 5. Experimental Results The aim of our experimentation is to measure performance bene?ts given by our framework, and to quantify the information gained w.r.t. the other lossy approaches 


approaches All the tests were conducted on a Windows XP PC equipped with a 2.8GHz Pentium IV and 512MB of RAM memory, within the cygwin environment. The datasets used in our tests are those ones of the FIMI repository1, and the constraints were applied on attribute values \(e.g. price gaussian distribution within the range [0, 150000 In order to asses the information loss of the postprocessing approach followed by previous works, in Figure 4\(a lution sets given by two interpretations, i.e. |I2 \\ I1 On both datasets PUMBS and CHESS this di?erence rises up to 105 itemsets, which means about the 30 of the solution space cardinality. It is interesting to observe that the di?erence is larger for medium selective constraints. This seems quite natural since such constraints probably cut a larger number of equivalence classes of frequency In Figure 4\(b built during the mining is reported. On every dataset tested, the number of FP-trees decrease of about four orders of magnitude with the increasing of the selectivity of the constraint. This means that the technique is quite e?ective independently of the dataset Finally, in Figure 4\(c of our algorithm CCIMiner w.r.t. Closet at di?erent selectivity of the constraint. Since the post-processing approach must ?rst compute all closed frequent itemsets, we can consider Closet execution-time as a lowerbound on the post-processing approach performance Recall that CCIMiner exploits both requirements \(satisfying constraints and being closed ing time. This exploitation can give a speed up of about to two orders of magnitude, i.e. from a factor 6 with the dataset CONNECT, to a factor of 500 with the dataset CHESS. Obviously the performance improvements become stronger as the constraint become more selective 1 http://fimi.cs.helsinki.fi/data Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Information loss Number of FP-trees generated Run time performance 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 10 5 10 6 m I 2 I 1  PUMSB@29000 CHESS @ 1200 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 6 10 


10 1 10 2 10 3 10 4 10 5 10 6 10 7 m n u m b e r o f fp t re e s PUMSB @ 29000 CHESS @ 1200 CONNECT@11000 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 m e x e c u ti o n  ti m e  s e c  CCI Miner  \(PUMSB @ 29000 closet         \(PUMSB @ 29000 CCI Miner  \(CHESS @ 1200 closet         \(CHESS @ 1200 CCI Miner  \(CONNECT @ 11000 closet         \(CONNECT @ 11000 a b c Figure 4. Experimental results with CAM ? sum\(X.price 6. Conclusions 


6. Conclusions In this paper we have addressed the problem of mining frequent constrained closed patterns from a qualitative point of view. We have shown how previous works in literature overlooked this problem by using a postprocessing approach which is not lossless, in the sense that the whole set of constrained frequent patterns cannot be derived. Thus we have provided an accurate de?nition of constrained closed itemsets w.r.t the conciseness and losslessness of this constrained representation, and we have deeply characterized the computational problem. Finally we have shown how it is possible to quantitative push deep both requirements \(satisfying constraints and being closed process gaining performance bene?ts with the increasing of the constraint selectivity References 1] R. Agrawal, T. Imielinski, and A. N. Swami. Mining association rules between sets of items in large databases In Proceedings ACM SIGMOD, 1993 2] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules in LargeDatabases. InProceedings of the 20th VLDB, 1994 3] R. J. Bayardo. E?ciently mining long patterns from databases. In Proceedings of ACM SIGMOD, 1998 4] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Adaptive Constraint Pushing in frequent pattern mining. In Proceedings of 7th PKDD, 2003 5] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi ExAMiner: Optimized level-wise frequent pattern mining withmonotone constraints. InProc. of ICDM, 2003 6] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Exante: Anticipated data reduction in constrained pattern mining. In Proceedings of the 7th PKDD, 2003 7] F. Bonchi and B. Goethals. FP-Bonsai: the art of growing and pruning small fp-trees. In Proc. of the Eighth PAKDD, 2004 8] J. Boulicaut and B. Jeudy. Mining free itemsets under constraints. In International Database Engineering and Applications Symposium \(IDEAS 9] C. Bucila, J. Gehrke, D. Kifer, and W. White DualMiner: A dual-pruning algorithm for itemsets with constraints. In Proc. of the 8th ACM SIGKDD, 2002 10] J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In Proceedings of ACM SIGMOD, 2000 11] L.Jia, R. Pei, and D. Pei. Tough constraint-based frequent closed itemsets mining. In Proc.of the ACM Symposium on Applied computing, 2003 12] H. Mannila and H. Toivonen. Multiple uses of frequent sets and condensed representations: Extended abstract In Proceedings of the 2th ACM KDD, page 189, 1996 13] R. T. Ng, L. V. S. Lakshmanan, J. Han, and A. Pang Exploratory mining and pruning optimizations of constrained associations rules. In Proc. of SIGMOD, 1998 14] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering frequent closed itemsets for association rules In Proceedings of 7th ICDT, 1999 15] J.Pei, J.Han,andL.V.S.Lakshmanan.Mining frequent item sets with convertible constraints. In \(ICDE  01 pages 433  442, 2001 16] J. Pei, J. Han, and R. Mao. CLOSET: An e?cient algorithm formining frequent closed itemsets. InACMSIGMODWorkshop on Research Issues in Data Mining and Knowledge Discovery, 2000 17] J. Pei, J. Han, and J. Wang. Closet+: Searching for the best strategies for mining frequent closed itemsets. In SIGKDD  03, August 2003 18] L. D. Raedt and S. Kramer. The levelwise version space algorithm and its application to molecular fragment ?nding. In Proc. IJCAI, 2001 


ment ?nding. In Proc. IJCAI, 2001 19] R. Srikant, Q. Vu, and R. Agrawal. Mining association rules with item constraints. In Proceedings ACM SIGKDD, 1997 20] M. J. Zaki and C.-J. Hsiao. Charm: An e?cient algorithm for closed itemsets mining. In 2nd SIAM International Conference on Data Mining, April 2002 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


4] K. Chang, C. Chong, Bar-Shalom, Joint Probabilistic Data Association in Distributed Sensor Networks IEEE Transactions on Automatic Control, AC-31\(10 octobre 1986 5] Bar-shalom, Multitarget Multisensor Tracking Advanced Application ,:YBS Publishing, 1990 6] Bar-shalom, Multitarget Multisensor Tracking Principles and Techenices ,:YBS Publishing, 1995 7] Hu Wenlong, Mao shiyi, Multisensor Data Association Based on Combinatorial Optimization[J]. Journal of Systems Engineering and Electronics . 1997 NO.1,1~9 8] Pattipati K R, Passive Multisensor Data Association Using a New Relaxation Algorithm.In Multitarget-Multisensor Tracking: Advanced and Applications,Y.Barshalom,Norwood,MA:Aretech,199 0 9] Deb S,et al, An S-Dimentional Assignment Algorithm for Track Initiation ,Proc. Of the IEEE Int. Conf Systems Engineering, Kobe, Japan,Sept 1992 527~530 10] Deb S,et al, A Multisensor-Multitarget Data Association Algorithm for Heterogeneous Sensors[J].IEEE Trans. on AES 1993 ,29 \(2 560~568 12] Bar-shalom,Y.,and Fortmann,T.E  Tracking and Data Association  New York:Academic press,1988 13] J,A,Roecher and G.L.Phillis,Suboptimal Joint Probabilistic Data Association .IEEE Transaction on Aerospace Electronic Systems.1993,29\(2 14] J,A,Roecker,A Class of Near Optimal JPDA Algorithm IEEE Transaction on Aerospace and Electronic Systems,1994,30\(2 15] Han Yanfei, Analysis and Improvement of Multisensor Multitarget Probabbilistic Data Association Filting Algorithm[J].  Journal of Systems Engineering and Electronics, 2002, Vol.24, 36~38  569 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





