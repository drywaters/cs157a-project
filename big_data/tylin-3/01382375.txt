Proceedings of the Third International Conference on Machine Learning and Cybernetics, Shanghai 26-29 August 2004 MINING FREQUENT PATTERNS BASED ON IS+-TREE HAI-BING MA JIN ZHANG YlNG-JIE FAN YUN-FA W Computer and Information Technology Department Fudan University, Shanghai 200433 China E-MAIL martin0721  163.com harper@cableplus.com.cn fan74@ 163.com, yfhu@fudan,edu.cn Abstract Frequent patterns mining play an important role in data mining research It's groundwork of other data mining tasks In this paper a novel 
algorithm is presented for mining frequent patterns based on static IS+-tree, and is compared extensively with other classical algorithms such as Apriori and FP-growth The algorithm builds frequent patterns directly instead of wing high-cost candidate sets generation-and-test method adopted hy Apriork it works on a static IS+-tree instead of costly dynamic trees adopted by FP-growth it consume smaller size of main memory and is more efficient than others Above all IS-tree is 
a general index model and has been widely wed in full text storage and index time series patterns mining and many other fields Keywords Frequent pattern: frequent item set IS LS-tree 1 Introdnetion Frequent pattems mining is a key step in many data mining problems such as association rules mining 11-41  time series patterns  classification[6 and so on Let I=/i,,i2 i be a set of items. Let database D be a set of transactions where each transaction 
T is a set of items such that Tc I Let X be an item set A transaction T is said to contain X if XZT The support of X is the numbers of transactions that contains X in D X is frequent when the support of X is no less than user defined threshold value E Frequent patterns mining is usually divided into two categories in terms of the methods being adopted One is to find frequent pattems by candidate sets generation-and-test The other is to build frequent pattems 
directly Apriori algorithm is the most famous one of the first category l Its essential idea is to iteratively generate the set of candidate pattems of length k+l from the set of frequent pattems of length k Most of the previous studies adopt an Apriori-like approach such as 12.3.41 Whereas the candidate sets generation-and-test is always a bottleneck of these methods FP-growth is one of the most efficient methods among the second category 5 It calculates the frequent-1 item set first which forms a compact frequent patterns tree tree A partitioning-based, divide-and-conquer method is used to 
decompose the mining task into a set of smaller tasks of mining confined pattems in conditional databases Whereas, FP-growth method needs to build the conditional FP-tree dynamically The number of them grows in proportion of the length of pattems which costs too much both for time and space In this paper a FP-tree-lie but more concise and more extensive mathematical model-IS'-tree is developed to assist the mining task The algorithm generates frequent item set directly, instead of using the high-cost candidate sets generation-and-test it works on a static IS+-tree instead of creating trees dynamically as FP-growth The empirical results show that our algorithm is 
more efficient especially on dense databases at all levels of support threshold and it consumes smaller size of main memory as well Above all IS-tree model is a general index model and used widely in full text storage and index, mining time series and many other The paper is organized as follows: basic concepts and definitions about the IS-tree model are discussed in section 2 The algorithm for mining frequent patterns using IS+-tree is developed in section 3 It is empirically compared with FP-growth and Apriori method in section 4 Section 5 concludes this paper 2 Basic 
concepts and def~tions of ISt-tree Inter-Relevant Successive Trees\(IS-tree is a novel mathematical model we proposed recently for solving the storage and index problem in full text datahaseL71 It makes use of the property of redundancy and lexicographic order which naturally exits in full text database. Just E tkc text there also exists much redundancy among the transactions If we think the transaction database as full text every transaction a sentence, every item a character all distinctive items an alphabet, and introduce the concept of order into 0-7803-8403-2/04/$20.00 ZOO4 IEEE 1208 


Proceedings of the Third International Conference on Machine Learning and Cybernetics Shanghai 26-29 August 2004 transaction database we can also extend the Inter-Relevant Successive Tree model to transaction databases It's interesting that a FP-tree is essentially an IS-tree The reason we call it IS+-tree rather than FP-tree is that, first it is extension of IS-tree model into transaction database second it's more concise not having the link pointer as FP-tree Definition 1 Partial order relation A relation 4 on a set S is called a partial order relation if it is reflexive antisymmetric and transitive The set S together with the partial order 4 is called a partial order set or simply a poset Definition 2 ISt-tree Suppose there he a relation 4 among items in transaction database D IS'Ltree is a compressed representation of D It is a ordered tree which has a root node 4 and a strictly order exists among predecessor and successor and among brothers Suppose introducing a puppet item 4in every transaction and each transaction being a poset then each transaction is corresponding to a path from root 4 to some node and same prefix of transactions shares same path If each item has a unique identity ID then each node of IS+-tree has two field field ID is used to identify an item field count to indicate the numbers of transactions which share the path string from root to that node Definition 3 Leaf-node vector All the led-nodes n associated with an item U constitute a vector 5 called leaf-node vector of item a And ni is called an entry Property 1 Leaf-node property For the last item U among the alphabet in transaction database D all transactions that contain it can be obtained by starting from the leaf-node vector Z and tracing the IS+-uee up to the rwt We call the traversed part of the tree sub IS+-tree And its associated database projected database Suppose fj be removed a new leaf-node vector ai being available and ai being the nearest item from a in the alphabet which forms another sub IS"-tree This process can be performed recursively Thus all leaf-node vector of static IS'-tree and its associated sub IS+-tree can he available Let the sub IS+-tree that associated with vector 5 be denoted as IS  then there also exists led-node vector bound on IS we denoted it as b a the bound leaf-node vector also form a sub IS+-tree and this process'also can be performed recursively We denote these bound sub tree as IS  o and the bound leaf-node vector as b IS oj,,,o represents the projected database that is composed of all transactions that contains item set a  ajJ Each element in leaf-node vector has two fields one is a pointer pointed to the entry; the other is base count which    plays the same role as the count field of node entries For convenience we use b ID denotes the item ID that associated with ii _ Lemma 1 base count number The base count of an element in a bound leaf-node vector on a sub IS'-tree is the same as that of the bottom-most leaf-node vector that forms the sub IS+-tree Lemma 2 count merger Let n is some node of a sub IS'-tree which formed from a \(bound led-node vector 5 the count of it is sum of all the base connts of elements that are descendants of n in z Example 1 Consider the transaction datahase\(in lexicographic order of table 1 that consists of a alphabet I O,u,b,c,d,efl Its IS'-tree is like figure 1 Transaction Item set b a b c b  a c d T3 b b c d T4 b b c d A a:2 n Figure 1 IS+-tree of example 1 heorem 1 Completeness IS+-tree contains all information related to mining frequent pattems and frequent closed patterns Proof is omitted Ming frequent patterns based on ISt-tree How to mine bequent pattems based on static ISt-tree 3 It's natural that we adopt a divide-and-conquer method mining the frequent pattems on the static IS+-tree in a bottom-up depth-first and pattern-growth way What we called pattem-growth is Let IS q,,,q be a bound sub IS'-tree and if a is a frequent item in the projected database associated with IS 4 4  then 0 U\(b  bkJ is a frequent patterns When this process perfomed recursively all combinations of frequent pattems can be generated 1 209 


Proceedings of the Third International Conference on Machine Learning and Cybernetics Shanghai 26-29 August 2004 Definition 4 Ordered leaf-node queue A queue containing leaf-node vectors which ranked by frrquency ascending order of its associated item is called ordered leaf-node queue Example 2 Consider a transaction database of table 2 that contains 6 transactions The transaction has been ranked by frequency order with a alphabet  6 f,c,a,b,m,p and items that do not exit in alphabet has been pruned  Suppose the minimum support threshold is 3 It\222s associated IS+-tree is like figure 2 Table 2 database for example 2 I Transaction I Itemset m Figure 3-g Figure 3 h Figure 3-i Figure 3 leaf-node queues of example 2 Then each vector in the queue should be popped up and its associated projected database should be mined by bottom-up and depth fust strategy, until the global queue is At present only the leaf-nodes contained in the top vector is complete The top vector p is popped from global queue, and it becomes the current vector The item associated with it is put out as a frequent-I item All nodes contained in vector p trace up one step\(imagine they are removed from the tree and thus new leaf-nodes are obtained which are I and ma These nodes are pushed into global queue til the same time a local ordered queue associated with the current item p is created which represents the projected database associated with p These new leaf-nodes are pushed into it also The queues formed look like figure-3h and figure-3c For queue p the leaf-nodes contained in the top vector m are complete and popped up from it The support of its associated item is summed up from these nodes It is 3 Since it is qualified the combined string pm is put out as frequent-2 itemset AI1 nodes contained in vector E trace up one step and new leaf-nodes are obtained, which are a and ca These new led-nodes are pushed into queue p and another new ordered queue associated with pm\Current queues look like figure 3-d and figure 3-e Notes fustly according to lemma 1 the number of transactions share the same path is recorded in the base count field of bottom-most nodes. Secondly what stored in vectors in queues is the node\222s Object ID that is the object address or pointer Hence it incurs small space cost Thirdly it should be avoided that the same node is put into same queue twice This is the case that when tracing up different nodes of the same item may meet at same node empty 1210 


Proceedmgs of the Third International Conference on Machine Learning and Cybernetics Shanghai 26-29 August 2004 For example when b and b trace up step by step they will meet at node fa Thus fo should only be put into the queue associated with b once\(it is easy to implement through the Object ID of fo without extra data structure However the support they bring should be summed up according to lemma 2 For queue pm the top vector 5 is popped and the support of the associated item is calculated Notice that though the support of item a is 3 it is indeed 2 This is because a@s support is decided by p on the same path Because it\222s not qualified no new local queue is created nothing is put out All nodes contained in vector E still need trace up one step and new leaf-nodes are obtained It\222s CO It\222s only be pushed into the queue pm The cnrrent queue \(pm\looks lie figure 3-f Since queue \(pm is not empty it should be performed continually Now the leaf-nodes contained in the top vector z are complete and popped up and their support is calculated It\222s 3 which is brought from p an p Because it is qualified the string pmc is put out as frequent3 itemset All nodes contained in vector C still need trace up one step and new leahodes are obtained Now c reach 4 thus only one new node is obtained It is f and pushed into both the queue m and a new queue associated with pmc Current queues look like figure 3-g and figure 3-h Next, queue pmc\should be handled The top vector is popped, and because the support of p that is at the same path with f is only 2 it\222s not qualified. The nodes contained in vector f are traced up one step Since fo meets e no node is pushed into the queue The queue is looks like of figure 34 Now queue of pmc is empty and process about it is completed The procedure IeNIIIs to the state of figure 3-g The queue of pm should be performed continually. The top vector f is popped and the support of its associated item is not qualified. The queue becomes empty and process about it is completed The procedure returns to the state of figure 3-d Now the top vector of queue associated with p is 5 It can he handled just likeE This process continues until queue of p is empty The procedure will return to the state of figure 4-b The queue of d will be handled continually In this course 5 C f are pushed into the queue one after another And the leaf-nodes contained in the top vector will be complete They are handled just like p When the global queue is empty the procedure is   completed Having example 2 it\222s not difficult for us to present algorithm 1 The algorithm uses IdList the union of item ID to denote a string of a item set The gQueue is used to denote global queue The lQueue is a local queue and nQueue is a new queue Algorithm 1 Frequent itemset mining based on 1s+-tree Input an tree IS minimum support threshold E Output complete frequent item set steps I 2 3 4 5 6 7 8 Initialize IdLisf with 4  and gQueue with current leaf-nodes While gQueuefnull do frequent-I item Fetch the top vector i putting i ID as IdLisf  IdLisr U T ID Create lQueue associated with IdLisr tracing up one step from I and getting new nodes Put new nodes into both gQueue and lQueue for lQueue recording the base count brought by the nodes in I  End call mineIS\(ldLisr lQueue e return to step 1 and continue Function: mineIS\(ldLisr lQueue E  Input an string IdLisr its associated queue lQueue Output All frequent itemset that can be extended minimum support threshold E from IdLisr steps 1 While lQueuefnul1 do 2 3 4 If i 2 then 5 IdList  IdList U i ID 6 put out frequent item set IdLisr I Create a new queue nQueue associated with IdLisr 8 Trace up one step from f getting new nodes 9 Put new nodes into both 1Queue and nQueue recording the base count brought by the nodes in I Fetch the top vector i iethe support of i   10 Call mineIS\(ZdLisr nQueue h 11 12 Else I is  E  Trace up one step from i getting new nodes 1211 


Procee of the Third International Conference on Machine Laming and Cybernetics Shanghai 26-29 August 2004 Dataset items transactions Chess 7613,196 Connect 130/67,557 Pumsb  2,114/49,046 13 Put the new nodes into Queue recording the base count  14 End I retumto 1,continue The algorithm is simple clear and efficient Notice that since the mining task is performed on a compact static IS+-tree avoiding to allocate memory and then release them for dynamic trees which may cost too much both for time and space of memory when mining long pattems The ordered leaf-node queue only stores the pointer of nodes in ISf-tree  so it incurs small size of memory and reduce the  space consumption 4 Experimental evaluation 4.1 Environment and dataset characteristics Pattem-number\(supp 255,985\(0.6\1,272,932\(0.5 6,472,980\(0.4\7,501.582\(0.3 27.127\(0.9 0.8 4,130,671\(0.7 0.6 2,608\(0.9 0.8 2,698,761\(0.7\9,537.36X0.6 120/8,124 23,596,649\(0.02 1,000 12,301\(0.004 92,472\(0.02 4.2 Experimental results We give experimental results for every dataset. The result is showed in figure 4 and figure 5 The horizontal ordinate records mining time with seconds, including the time putting out to file The vertical ordinate records support which is normalized with range from zero to one For dense database the Apriori algorithm is degraded too quickly to be compared with the other two For example when support is set to 0.6 for chess, Apriori has cost 653 second, whereas IS and FP-growth only cost 8 second and 13 second respectively Aprion is 81.6 times slower than ISf Therefore for dense datasets our algorithm is only compared with FP-growth as figure 4 shows As we can see for dense datasets IS is always outperforms Fp-growth and when the support threshold is lowered, the gap between them widens For sparse dataset the result is showed in figure 5 When the support value is high Apriori is as competitive as the other two This is because the generated frequent patterns is usually short 4 and Aprioir need not scan the database many times However when the value of support is lowered, the number of the pattems become great and the length become long which makes the performance of Apriori degrade sharply IS is only trivially faster than FP-growth on sparse dataset since Fp-growth need not create too many dynamic trees on it The memory usage peak value of IS is always smaller than FP-growth in all cases 800 600 200 800 400 200 n 400 no0 600 f 4 FP-growth 0 7 0 6 0 5 0.4 0.3 Figure 4-a chess 1212 


Proceedings of the Third International Conference on Machine Laming and Cybernetics Shanghai 26-29 August 2004 1200 tFP-growth 400 200 0 1000 800 600 400 200 0 1600 1400 1200 1000 800 600 400 200 0 90 80 70 60 50 40 30 20 10 0 0 9 0.8 0 7 0 6 Figure 4-b connect t FP-growth 3 1 0 08 0 06 0 04 0 02 Figure 4-c mushroom 1 FP-growth IS 0.9 0.8 0 I 0.6 Figure 4-d pumsb IS S-Apriori 0.01 0.008 0.006 0.004 0.002 Figure 5 T25110D50K 5 Conclusions In this paper we present a novel algorithm for mining frequent items sets based on IS+-tree model Compared with classical algorithm Apriori and FP-grow our algorithm is the more efficient on both dense datasets and sparse datasets IS+-tree model has become part of our general model we are studying that is adapted well to many types of data Acknowledgements This paper is supported by national natural science fund\(60173027\China References R Agrawal and R Srikant Fast Algorithms for Mining Association Rules Proc Int 1 Conf Very Large Data Bases pp. 487-499. Sep.1994 J.S Park M.S Chen and P.S Yu An Effective Hash-Based Algorithm for Mining Association Rules Proc ACM-SIGMOD Int 1 Conf Management of Data pp 175-186. May.1995 A.Savasere,E.Omiecinski and S.Navathe An efficient algorithm for mining association rules in large databases In Proc 1995 Int.Conf VLDB\22295 pp 432-443, Sep. 1995 H Toivonen Sampling large databases for association rules Proceedings of the 22nd International Conference on Very Large Database, Bombay, India Sep 1996 Jiawei Han, Jian Pei and Yiwen Yi Mining Frequent Pattems without Candidate Generation Proc 2000 ACM-SIGMOD Int Conf Management of Data\(SIGM0D 00 1-12 May 2000 B.Liu and W.Hsu and Y.Ma Integrating classification and association rule mining. Knowledge Discovery and Data Mining\(KDD\22298 York Hu Yun-fa IS-tree A New Mathematics Model for Full-Text Database Tech Rep:TR0243l,Depamnent of Computing and Information Technology Fudan University, Mar 2002 Zeng Hai-quan Nu Yun-fa Mining Time-series Pattems Based on Inter-Relevant Successive Tree Journal of of Pattems recognition and Artificial intelligence pp 299-306 No.3 2003 1213 


For each un-instantiated variable Xij of ci  Do For each link t = H\(t  Xij =d\(t, Xij interprets ci  Do Compute the score sijk of t according to Eq.\(1 2 If sijkts  Put \(t,sijk Find in each Sij an element \(tij, sij H\(ti1 ti2  H\(tih and the sum of scores 6j sij is maximal Replace the missing value of Xi1, Xi2  Xih in ci by Xi1=d\(ti1,Xi1 ti2,Xi2  Xih =d\(tih,Xih Return D  the updated D Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE Incomplete case di Incomplete case di Does t H\(t  Xj=zjk exist in T Does t H\(t  Xj=zjk exist in T For each missing datum Xj=? in di For each missing datum Xj=? in di Find elements in L1 or L2 that contain {Xj =zjk} and can consistently interpret di Find elements in L1 or L2 that contain {Xj =zjk} and can consistently interpret di Compute score\(t according to Eq.\(1 2 Compute score\(t according to Eq.\(1 2 Does H\(t consistently interpret di Does H\(t consistently interpret di Apply the value to complete the missing data Apply the value to complete the missing data Figure 4. The proposed completing procedure Table  1. \(a b transactional form of the dataset Case X1 X2 X3 X4  TID Items c1 2 1 ? ?  t1 X1=2,X2=1 c2 2 1 2 1  t2 X1=2,X2=1, X3=2, X4=1 c3 2 1 2 1  t3 X1=2,X2=1, X3=2, X4=1 c4 1 2 1 ?  t4 X1=1,X2=2, X3=1 c5 1 2 1 2  t5 X1=1,X2=2, X3=1, X4=2 c6 ? 1 2 1  t6 X2=1, X3=2, X4=1 c7 1 ? 1 ?  t7 X1=1, X3=1 c8 1 2 1 1  t8 X1=1,X2=2, X3=1, X4=1 c9 1 ? 1 ?  t9 X1=1, X3=1 c10 1 2 1 2  t10 X1=1,X2=2, X3=1, X4=2 Table  2. Data associations of D ID Data association supp conf r1 {X3=1  X1=1} 0.60 1.00 r2 {X2=2, X3=1  X1=1} 0.40 1.00 r3 {X2=2  X1=1} 0.40 1.00 r4 {X2=1  X1=2} 0.30 0.75 r5 {X3=2, X4=1  X2=1} 0.30 1.00 r6 {X3=2  X2=1} 0.30 1.00 r7 {X1=2  X2=1} 0.30 1.00 r8 {X4=1  X2=1} 0.30 0.75 r9 {X1=1  X3=1} 0.60 1.00 


r9 {X1=1  X3=1} 0.60 1.00 r10 {X2=2  X3=1} 0.40 1.00 r11 {X1=2,X2=2  X3=1} 0.40 1.00 r12 {X2=1, X4=1  X3=2} 0.30 1.00 r13 {X2=1  X3=2} 0.30 0.75 r14 {X4=1  X3=2} 0.30 0.75 r15 {X2=1, X3=2  X4=1} 0.30 1.00 r16 {X3=2  X4=1} 0.30 1.00 r17 {X2=1  X4=1} 0.30 0.75 5. Experiments In order to test the effectiveness of our method, several experiments are performed and the results are compared with RBE. The datasets, DA and DB, are generated syntactically. In DA, data are randomly generated and the variables in DA have no particular dependency relations. In DB, three variables, X1, X2 and X3, are defined and each variable has three different instantiations, xij, i=1,2,3 and j=1,2,3. Cases in DB are generated \(1 X1=x11  X2=x22}, {X2=x21  X3=x32}, and X3=x33  X1=x13}, and \(2 associations do not hold. The datasets, Monk, TAE and Solaris are from [4]. Table 3 describes the basic information of these datasets. In these datasets, we randomly remove some data from completed cases and test if our method can successfully recover the missing ones. For nm missing data, the accuracy of recovery D is defined as 1- nw/nm if nw data are incorrectly guessed From the experimental results shown in Table 4, our method is more accurate than RBE is Table 3. Description of the test datasets Dataset ID cases variables niv missing ratio source DB -1 15 DB -2 30 3 3 20 DA -1 15 DA-2 100 6 2 20 Syntactic Monk -1 10 Monk -2 415 7 3 20 TAE -1 20 TAE -2 151 5 3 30 Solaris -1 5 Solaris -2 1066 13 4 10 UCI [4 niv: # of instances in each variables \(in average Table 4. Experimental results Our method RBEDataset ID \(s, c, nda s, c, nda DB -1 \(25,65, 6 15,50,8 DB -2 \(25,65, 6 15,50,8 DA -1 \(20,20,350 30,40,179 DA-2 \(20,20,350 30,40,179 Monk -1 \(15,15,115 10,35,181 Monk -2 \(15,15,115 10,35,181 TAE -1 \(10,30,73 5,60,115 TAE -2 \(10,30,73 5,60,115 Solaris-1 \(40,40,2926 60,60,988 Solaris-2 \(40,40,2926 60,60,988 s, c, nda data associations and nda  is the number of data associations obtained 6. Discussions &amp; Conclusion We presented in this paper a new method for completing missing data using data associations. The basic concept is that association rules describe the dependency relationships among data entries in a Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE 


0-7695-2291-2/04 $ 20.00 IEEE dataset and missing data should hold the similar relationships. One of the advantages of using this approach for completing missing data is that data associations can be easily and reasonably obtained Most reasonable datasets include reasonable association rules. Clearly, data associations truly describe the cross-relation among data instantiations more accurately. The score function that we presented in Eq.\(1 2 There may exist other measures, such as [16], that reveal more information for the applicability of data associations. The score function can be further improved. Unfortunately, the accuracy of the proposed method depends on the number of data associations When the threshold of minsupp and minconf are low there will be a large number of data associations being generated, resulting in inefficiency of the completing procedure. The future work includes developing a better informative score function and reducing the number of data associations and determining suitable values of minsupp and minconf. Currently, in this paper, only discrete data are discussed and the data existing in datasets are assumed to be correct and noise-free. We are extending the proposed method to handle non-discrete, noisy, and incorrect datasets 7. References 1] Agrawal, R., and Srikant, R. \(1994  Fast algorithm for mining association rules  in the Proceedings of the International Conference on VLDBases, pp. 487-499 2] Agrawal, R., Imielinksi, T., and Swami, A. \(1993  Dataset mining: a performance perspective  IEEE TKDE vol. 5, no. 6, pp. 914-925 3] Agrawal, R., Srikant, R., and Vu, Q. \(1997  Mining association rules with item constraints  in the Proceedings of the Third International Conference on KDD, Newport Beach, California, pp. 67-73 4] Black, C., Keogh, E., and Merz, C.J. \(1999 repository of machine learning databases, URL http://www.ics.uci.edu/~mlearn/MLRepository.html 5] Buntine, W.L. \(1994  Operations for Learning with Graphical Models  Journal of AI, vol. 2, pp. 159-225 6] Chen, M.S., Han, J., and Yu, P.S. \(1996  Data mining an overview from a database perspective  IEEE TKDE, vol 8, no. 6, pp.866-883 7] Coenen, F., Goulbourne, G., and Leng, P. \(2004  Tree structures for mining association rules  Data Mining and Knowledge Discovery, vol. 8, no. 1, pp.25-51 8] Dempster, A. P., Laird, N.M., and Rubin, D.B. \(1977  Maximum likelihood from incomplete data via the EM algorithm  Journal of the Royal Statistical Society, vol. 39 no. 1, pp.1-38 9] Frick, J.R., and Grabka, M.M. \(2003  Missing Income Information in Panel Data: Incidence, Imputation and its Impact on the Income distribution  in the Proceedings of the Workshop on Item-Non-response and Data Quality in Large Social Surveys, October 9-11 10] Giudici, P., and Castelo, R. \(2003  Improving Markov Chain Monte Carlo model search for data mining   Machine Learning, vol. 50, no. 1-2, pp.127-158 11] Hern  ndez, M.A., and Stolfo, S.J. \(1998  Real-world Data is Dirty: Data Cleansing and The Merge/Purge Problem  DMKD, vol. 2, no. 1, pp.9-37 12] Intelligent CAM Systems Laboratory  Methodologies for dealing with the Missing Data   http://www.eng.uc.edu/icams/resources/missing_data.htm 13] Kryszkiewicz, M., \(2000  Probabilistic Approach to Association Rules in Incomplete Databases  Web-Age Information Management, pp. 133-138 14] Kryszkiewicz, M., and Rybinski, H. \(1999 


14] Kryszkiewicz, M., and Rybinski, H. \(1999  Incomplete Database Issues for Representative Association Rules  ISBN: 3-540-65965-X, pp. 583-591 15] Little, R.J.A., and Rubin, D.B. \(2002 analysis with missing data, Wiley, New York, ISBN 0471183865 16] Omiecinski, E.R. \(2003  Alternative interest measures for mining associations in databases  IEEE TKDE vol. 15, no. 1, pp.57-69 17] Piramuthu, S. \(1998  Evaluating feature selection methods for learning in data mining applications  in the Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, vol. 5, pp. 294-301 18] Pyle, D. \(1999 Morgan Kaufmann Publishers, Inc.ISBN:1-55860-529-0 19] Ragel, A., and Cremilleux, B. \(1999  MVC - A preprocessing Method to deal with missing values   knowledge based system, vol. 12, pp.285-291 20] Ramoni, M., and Sebastiani, P. \(2000  Bayesian Inference with Missing Data Using Bound and Collapse   Journal of Computational and Graphical Statistics, vol. 9, no 4, pp. 779-800 21] Ramoni, M., and Sebastiani, P. \(2001  Robust Bayes Classifiers  AI, vol. 125, no. 1-2, pp. 207-224 22] Ramoni, M., and Sebastiani, P. \(2001  Robust Learning with Missing Data  Machine Learning, vol. 45, no 2 , pp. 147-170 23] Scott, R.E. \(1993 Logic and Practice, SAGE Publications, ISBN: 0803941072 24] Zaki, M.J., and Hsiao, C.J. \(2002  CHARM: An efficient algorithm for closed itemset mining  in the Proceedings of the Second SIAM International Conference on Data Mining Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE pre></body></html 


13: else 14: E|i?1| = E|i?1| ? s The backward process in Algorithm 1, generates level-wise every possible subset starting from the borProceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE der de?ned by Edge without getting into equivalence classes which have been already mined \(Line 10 such subset satis?es the constraint then it can be added to the output \(Line 12 reused later to generate new subsets \(Line 14 have a monotone constraint in conjunction, the backward process is stopped whenever the monotone border B+\(Th\(CM Lines 3 and 8 4.3. Closed Constrained Itemsets Miner The two techniques which have been discussed above are independent. We push monotone constraints working on the dataset, and anti-monotone constraints working on the search space. It  s clear that these two can coexist consistently. In Algorithm 2 we merge them in a Closet-like computation obtaining CCIMiner Algorithm 2 CCIMiner Input: X,D |X , C, Edge,MP5, CAM , CM X is a closed itemset D |X is the conditional dataset C is the set of closed itemsets visited so far Edge set of itemsets to be used in the BackwardMining MP5 solution itemsets discovered so far CAM , CM constraints Output: MP5 1: C = C ?X 2: if  CAM \(X 3: Edge = Edge ?X 4: else 5: if CM \(X 6: MP5 = MP5 ?X 7: for all i ? flist\(D |X 8: I = X ? {i} // new itemset avoid duplicates 9: if  Y ? C | I ? Y ? supp\(I Y then 10: D |I= ? // create conditional fp-tree 11: for all t ? D |X do 12: if CM \(X ? t 13: D |I= D |I ?{t |I  reduction 14: for all items i occurring in D |I do 15: if i /? flist\(D |I 16: D |I= D |I \\i // ?-reduction 17: for all j ? flist\(D |I 18: if supD|I \(j I 19: I = I ? {j} // accumulate closure 20: D |I= D |I \\{j 21: CCIMiner\(I,D |I , C,B,MP5, CAM , CM 22: MP5 = Backward-Mining\(Edge,MP5, CAM , CM For the details about FP-Growth and Closet see [10 16]. Here we want to outline three basic steps 1. the recursion is stopped whenever an itemset is found to violate the anti-monotone constraint CAM Line 2 2  and ? reductions are merged in to the computation by pruning every projected conditional FPTree \(as done in FP-Bonsai [7 Lines 11-16 3. the Backward-Mining has to be performed to retrieve closed itemsets of those equivalence classes which have been cut by CAM \(Line 22 5. Experimental Results The aim of our experimentation is to measure performance bene?ts given by our framework, and to quantify the information gained w.r.t. the other lossy approaches 


approaches All the tests were conducted on a Windows XP PC equipped with a 2.8GHz Pentium IV and 512MB of RAM memory, within the cygwin environment. The datasets used in our tests are those ones of the FIMI repository1, and the constraints were applied on attribute values \(e.g. price gaussian distribution within the range [0, 150000 In order to asses the information loss of the postprocessing approach followed by previous works, in Figure 4\(a lution sets given by two interpretations, i.e. |I2 \\ I1 On both datasets PUMBS and CHESS this di?erence rises up to 105 itemsets, which means about the 30 of the solution space cardinality. It is interesting to observe that the di?erence is larger for medium selective constraints. This seems quite natural since such constraints probably cut a larger number of equivalence classes of frequency In Figure 4\(b built during the mining is reported. On every dataset tested, the number of FP-trees decrease of about four orders of magnitude with the increasing of the selectivity of the constraint. This means that the technique is quite e?ective independently of the dataset Finally, in Figure 4\(c of our algorithm CCIMiner w.r.t. Closet at di?erent selectivity of the constraint. Since the post-processing approach must ?rst compute all closed frequent itemsets, we can consider Closet execution-time as a lowerbound on the post-processing approach performance Recall that CCIMiner exploits both requirements \(satisfying constraints and being closed ing time. This exploitation can give a speed up of about to two orders of magnitude, i.e. from a factor 6 with the dataset CONNECT, to a factor of 500 with the dataset CHESS. Obviously the performance improvements become stronger as the constraint become more selective 1 http://fimi.cs.helsinki.fi/data Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Information loss Number of FP-trees generated Run time performance 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 10 5 10 6 m I 2 I 1  PUMSB@29000 CHESS @ 1200 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 6 10 


10 1 10 2 10 3 10 4 10 5 10 6 10 7 m n u m b e r o f fp t re e s PUMSB @ 29000 CHESS @ 1200 CONNECT@11000 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 m e x e c u ti o n  ti m e  s e c  CCI Miner  \(PUMSB @ 29000 closet         \(PUMSB @ 29000 CCI Miner  \(CHESS @ 1200 closet         \(CHESS @ 1200 CCI Miner  \(CONNECT @ 11000 closet         \(CONNECT @ 11000 a b c Figure 4. Experimental results with CAM ? sum\(X.price 6. Conclusions 


6. Conclusions In this paper we have addressed the problem of mining frequent constrained closed patterns from a qualitative point of view. We have shown how previous works in literature overlooked this problem by using a postprocessing approach which is not lossless, in the sense that the whole set of constrained frequent patterns cannot be derived. Thus we have provided an accurate de?nition of constrained closed itemsets w.r.t the conciseness and losslessness of this constrained representation, and we have deeply characterized the computational problem. Finally we have shown how it is possible to quantitative push deep both requirements \(satisfying constraints and being closed process gaining performance bene?ts with the increasing of the constraint selectivity References 1] R. Agrawal, T. Imielinski, and A. N. Swami. Mining association rules between sets of items in large databases In Proceedings ACM SIGMOD, 1993 2] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules in LargeDatabases. InProceedings of the 20th VLDB, 1994 3] R. J. Bayardo. E?ciently mining long patterns from databases. In Proceedings of ACM SIGMOD, 1998 4] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Adaptive Constraint Pushing in frequent pattern mining. In Proceedings of 7th PKDD, 2003 5] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi ExAMiner: Optimized level-wise frequent pattern mining withmonotone constraints. InProc. of ICDM, 2003 6] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Exante: Anticipated data reduction in constrained pattern mining. In Proceedings of the 7th PKDD, 2003 7] F. Bonchi and B. Goethals. FP-Bonsai: the art of growing and pruning small fp-trees. In Proc. of the Eighth PAKDD, 2004 8] J. Boulicaut and B. Jeudy. Mining free itemsets under constraints. In International Database Engineering and Applications Symposium \(IDEAS 9] C. Bucila, J. Gehrke, D. Kifer, and W. White DualMiner: A dual-pruning algorithm for itemsets with constraints. In Proc. of the 8th ACM SIGKDD, 2002 10] J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In Proceedings of ACM SIGMOD, 2000 11] L.Jia, R. Pei, and D. Pei. Tough constraint-based frequent closed itemsets mining. In Proc.of the ACM Symposium on Applied computing, 2003 12] H. Mannila and H. Toivonen. Multiple uses of frequent sets and condensed representations: Extended abstract In Proceedings of the 2th ACM KDD, page 189, 1996 13] R. T. Ng, L. V. S. Lakshmanan, J. Han, and A. Pang Exploratory mining and pruning optimizations of constrained associations rules. In Proc. of SIGMOD, 1998 14] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering frequent closed itemsets for association rules In Proceedings of 7th ICDT, 1999 15] J.Pei, J.Han,andL.V.S.Lakshmanan.Mining frequent item sets with convertible constraints. In \(ICDE  01 pages 433  442, 2001 16] J. Pei, J. Han, and R. Mao. CLOSET: An e?cient algorithm formining frequent closed itemsets. InACMSIGMODWorkshop on Research Issues in Data Mining and Knowledge Discovery, 2000 17] J. Pei, J. Han, and J. Wang. Closet+: Searching for the best strategies for mining frequent closed itemsets. In SIGKDD  03, August 2003 18] L. D. Raedt and S. Kramer. The levelwise version space algorithm and its application to molecular fragment ?nding. In Proc. IJCAI, 2001 


ment ?nding. In Proc. IJCAI, 2001 19] R. Srikant, Q. Vu, and R. Agrawal. Mining association rules with item constraints. In Proceedings ACM SIGKDD, 1997 20] M. J. Zaki and C.-J. Hsiao. Charm: An e?cient algorithm for closed itemsets mining. In 2nd SIAM International Conference on Data Mining, April 2002 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


4] K. Chang, C. Chong, Bar-Shalom, Joint Probabilistic Data Association in Distributed Sensor Networks IEEE Transactions on Automatic Control, AC-31\(10 octobre 1986 5] Bar-shalom, Multitarget Multisensor Tracking Advanced Application ,:YBS Publishing, 1990 6] Bar-shalom, Multitarget Multisensor Tracking Principles and Techenices ,:YBS Publishing, 1995 7] Hu Wenlong, Mao shiyi, Multisensor Data Association Based on Combinatorial Optimization[J]. Journal of Systems Engineering and Electronics . 1997 NO.1,1~9 8] Pattipati K R, Passive Multisensor Data Association Using a New Relaxation Algorithm.In Multitarget-Multisensor Tracking: Advanced and Applications,Y.Barshalom,Norwood,MA:Aretech,199 0 9] Deb S,et al, An S-Dimentional Assignment Algorithm for Track Initiation ,Proc. Of the IEEE Int. Conf Systems Engineering, Kobe, Japan,Sept 1992 527~530 10] Deb S,et al, A Multisensor-Multitarget Data Association Algorithm for Heterogeneous Sensors[J].IEEE Trans. on AES 1993 ,29 \(2 560~568 12] Bar-shalom,Y.,and Fortmann,T.E  Tracking and Data Association  New York:Academic press,1988 13] J,A,Roecher and G.L.Phillis,Suboptimal Joint Probabilistic Data Association .IEEE Transaction on Aerospace Electronic Systems.1993,29\(2 14] J,A,Roecker,A Class of Near Optimal JPDA Algorithm IEEE Transaction on Aerospace and Electronic Systems,1994,30\(2 15] Han Yanfei, Analysis and Improvement of Multisensor Multitarget Probabbilistic Data Association Filting Algorithm[J].  Journal of Systems Engineering and Electronics, 2002, Vol.24, 36~38  569 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





