Fuzzy Methodology for Enhancement of Context Semantic Understanding Yu Sun 002  Fakhri Karray 002  Otman Basir 002  Jiping Sun 002  and Mohamed Kamel 002 002 Dept of Electrical and Computer Engineering University of Waterloo Waterloo Ontario N2L 3G1 Email  yusun karray obasir jiping mkamel  pami.uwaterloo.ca Abstract  One of the many issues that confront traditional 
statistical approaches of natural language understanding NLU is on how to overcome the insuf\036cient co-occurrence information caused by the limited boundary of statistical approaches Researchers have long used the imparting of human knowledge into statistical approaches including de\036nition of rules and collections of hierarchy of concepts However these are dif\036cult to de\036ne even for a domain expert They are also very much people and domain dependent This study proposes a fuzzy approach to tackle these issues in a way as to provide a methodology for logical reorganizing context in order to tackle the issue of boundary limitation to create the more reasonable and understandable word association which will be referenced 
as membership degree in latter stage and to make the processes of imparting of human knowledge easier and less domain dependent The accomplishment of these tasks could be achieved through the concept of Precisiated Natural Language PNL I I NTRODUCTION Due to issues such as word synonymy and polysemy understanding of natural language NLU is semantically related and very much dependent on the sentence organization As such use of World Knowledge or human knowledge is a common assistant to automatic NLU However the apporach of how to effectively apply World Knowledge on NLU is still a challenging problem One of the major difculties is how to impart machine understanding with the unbounded 
nature of human knowledge Human cognition is built upon a procedure of granulation When a particular event happens the event triggers all related memory and the brain reconstructs the knowledge pertaining to the events environment Unfortunately there doesnt exist yet a powerful algorithm to mimic exactly the human way of reconstructing knowledge from previous experience The current popular approach to tackle this issue is to use frequency based measures to predict the semantic understanding given the assumption that the historical words relations remains always true in the unpredicted context However the traditional statistical approaches cannot deal completely with this issue because any statistical approach for instance 
n-gram approaches to natural language is sub-sufcient This is due to the fact that the estimated parameters are based on insufcient information for instance only the near neighbors occurrence is considered during the word frequency analysis and therefore might lead to imprecise outcome As such we propose to apply the fuzzy logic-based World Knowledge rst to reorganize the planning of the original context before using the frequency based measure to compute the semantic coefcients between words The reason behind the use of fuzzy techniques instead of human-supplied lexicon and rules is intuitive In fact even a domain expert cannot provide a comprehensive lexicon 
and domain rules Fuzzy World Knowledge functions as a compensation to insufciency of human-supplied lexicon so that the missing data can be compensated through the fuzzysupplied knowledge Through the tuning of human-supplied lexicon by fuzzy Word Knowledge we try to compensate for the gaps of statistical approach when applied to NLU Throughout this paper the fuzzy based World Knowledge is synonymous of Precisiated Natural Language PNL  In particular  we propose three kinds of sets of w ord knowledge to tackle the issue of insufcient information discussed above The remainder of the paper is organized as follows In Section 2 the classical statistical and fuzzy approaches are 
reviewed In Section 3 we describe a novel formalism of human knowledge through three sets of knowledge which forms the basis of the proposed methodology In Section 4 we illustrate with an example to explain how to apply the proposed methodology to enhance a statistical measure to accurately compute the semantic coefcients of words In Section 5 experimental results illustrate the performance of the proposed approach followed by comments and discussions Finally conclusions are given in Section 6 II O VERVIEW AND B ACKGROUND Statistical approaches are widely used today by NLU researchers For instance the corpus approach 4 has 
been the earliest language model based on a pure statistical model The assumption that underlies the theory is that a document might be represented by a hyperspace vector consisting of signicant words selected by a certain metric The major issue of the corpus approach pertains the lack of capability in tackling the difculties caused by synonymy and polysemy within the English language domain as argued in Meanwhile the issue of sparse v ector representation  must also be considered In order to better understand the semantic meaning of words researchers have used the 0-7803-9158-6/05/$20.00  2005 IEEE The 2005 IEEE International Conference on Fuzzy Systems 143 


machine readable semantic lexicons WordNet  8 Oxford Advanced Learner  10 Longman Dictionary of Contemporary English  and Funk and Wagnalls F&W Dictionary  Other semantic approaches 13 14 took advantage of knowledge from the predened syntactic patterns and semantic slots called Heuristic However the issue of how to optimize the right semantic level arises as well the issue of boundary limitation has always existed Besides the semantic approaches are inevitably geared toward the representation of static knowledge They are more suitable for dening for instance the semantic ontology with static human-dened semantic hierarchy However actions in other words events organized by logical relations are difcult to depict by static semantic denitions In essence actions can only be dynamically described and accordingly represented by continuous states Fuzzy logic represents a good framework supporting continuous transition of states In earlier research work on fuzzy logic as applied to natural language processing researchers for instance Zadeh in focused on modelling adverb modiers in natural language through fuzzy sets However this approach has its limitation as it depicts modiers only In order to characterize other types of words meanings by fuzzy sets Zadeh introduced hierarchies of fuzzy concepts in Besides considering the relation between possibility and fuzzy sets Zadeh in  took adv antage of possibility of a proposition to dene its corresponding fuzzy set However the previous research work was mainly limited to simple proposition transitions and focused on characterization of modiers and the words with obvious distinguishing degree such as hot red and tall  This by itself cannot cope with natural sentences Other researchers for instance in 20 ha v e used a fuzzy relation system to depict semantic association through construction of a fuzzy thesaurus for a certain domain However the main issue is that the fuzzy thesaurus are in essence static so that a lexine in such static lexicons cannot dynamically adjust its original conceptual denition and relations when a new situation arises More recently Zadeh introduced the theory of fuzzy information granulation TFIG where Zadeh suggested a novel formalism for natural language processing which he called Precisiated Natural Language PNL III P ROPOSED A PPROACH The modelling of natural language  i.e free context represents the main topic of this paper An appropriate model must satisfy two basic requirements 1 enable human knowledge to translate into statistical approaches 2 rules to simplify manual annotation of the training corpus We base our modelling approach through integration of three kinds of sets of knowledge extracted from words Zadehs recent ndings on Epistemic Lexicon EL in Semantic pattern and Predicate dictionary as well generic rules A Predicate Dictionary In our research we believe verbs predicate are more important than nouns As such we dene a dictionary particularly for the interested predicates The dictionary is differentiated from others in the sense of direction which will be used in the latter stage for construction of membership functions Some examples are listed in Table I B Epistemic Lexicon EL The words association is described by Epistemic Lexicon EL In an EL denes a granulation le xicon in a format described as EL Word  attribute=[Word fuzzy  The parameters are w ords in the language relations R between words and the attributes of words An example of granulated attribute relations in the form of EL is shown in Fig 1 where A i is the i th attribute of lexine representing a super concept and G i  is a group of granulated concepts with a fuzzy measurement In our research we adopt this denition and further update Predicate Class Assigned Words Agree agree afliate ally coincide Against refuse damage cessation argue challenge Action-up boost increase bounce develop Action-down decrease reduce discourage dump fall Action-in buy import offer,proposal order Action-out disclose exclude explore export pay release TABLE I Examples for Predicate Dictionary 1n GG Attribute Granular members 1n Profitability Technology Capital Human Resource Cash / Mid Stock/ High Sales / High Manager / Mid Bio-tech / Hight e-commerce/ Mid Example Format  Lexine A A  Fig 1 An Example of Granulated Attribute Relations in the Form of Epistemic Lexicon The 2005 IEEE International Conference on Fuzzy Systems 144 


it as a normal form as in 1 EL   L 1 L 2   L n  1a L i   L j  Final   a Fuzzy Measurement  1b where L j  002 L 1 L n  Final   Predicate class  Concept  1c The lexine L i depicted by EL is usually difcult to be assigned to a certain concept as described in WordNet As compared with the denition of lexine in EL we manually dene common-sensed concepts And in order to tackle the issue of hierarchical denitions of concepts like in WordNet in this paper we adopt the at denition Some examples are depicted in Table II C Semantic Pattern In order to enable the system to capture semantic meanings we are interested we adopt pairs of semantic patterns introduced in F or instance to depict the k e y word appointed we have a pair of semantic patterns 1  subject  passive-verb  for instance   Gareth L Reed  has been appointed indicates  Gareth L Reed  is the subject and belongs to the concept of PEOPLE 2 passive-verb  drt direct objective   for instance has been appointed  president   indicates  president  is the drt and belongs to the concept of MANAGEMENT-STAFF As such this pair of semantic patterns are triggered if the system matches the two semantic patterns with word appointed D Rules Besides the denitions of the knowledge discussed above we design rules to enhance the static EL The rules dened for the selected domain are summarized as follows 1 One lexine might belong to multiple concepts classes 2 Any lexine in predicate dictionary is bi-sense-directed and takes only one of two directions positive or negative depending on its context positive direction  indicates the tendency of up or in and negative direction  indicates the tendency of down or out respectively 3 In a given sentence suppose the number of directions D i is n in practice we restrict n 003 2  As such Concept Assigned Words Equity share stock debt stake option Organization company bank Corp rm subsidiary spinoff venture Trading-place stock exchange Money Market bond treasury security warrant Legal court regulation Business Type service manufacture PC Mac computer gas oil TABLE II Examples for Concept Dictionary a complicated sentence is usually divided into subsentences Then the overall direction of the given sentence is decided by 002 n i 1 D i  where 003 D i  0 ifD i is a positive direction D i  0 ifD i is a negative direction 4 In the process of reorganizing the context the subject in the same cluster must remain consistent so does the direction 5 Lexines relationship can be inherited by its attributes For instance lexine Institute has the following attributes management-staff  performance  legal  scandal which are dened by the original EL If action Against  Institute  then Against  Institute.managementpeople  Institute.performance  Institute.legal  Institute.scandal  And vice versa IV M ETHODOLOGY B ASED ON FORMALIZED K NOWLEDGE We describe a methodology based on the formalized knowledge discussed above and illustrate an implementation in a selected domain 1 Selection of document For simplicity the implementation is based on uniquetopic documents selected from WSJ Ten documents are randomly selected from WSJ with the unique topic on Company-acquisition 2 Manual denition of three sets of knowledge Given the selected documents we dene predicate dictionary and semantic patterns In this paper we dene 6 predicate classes and manually assign appropriate words for each class according to the semantic association degree to the class Then we manually dene EL such as depicted in Table III In essence EL is a static word association We will see later that the original denition of EL has been adjusted by applying the rules described earlier Throughout this paper the lexis we are interested in include Buyer  Seller  Management-change  Companyoperation  and Protability  Finally in order to let the system automatically capture the interested contents we manually dene semantic patterns discussed above 3 Reorganizing of the original documents First we use of the shelf semantic parser to parse the Lexine Attribute Value in Fuzzy Measurement Buyer Organization very possible Management-staff possible Seller Organization very possible Management-staff possible TABLE III Example of Components of Epistemic Lexicon The 2005 IEEE International Conference on Fuzzy Systems 145 


sentences in the original documents with the assistance of the predicate dictionary and semantic patterns Next we apply the dened generic rules to adjust the parsed output depicted as reorganize the context Finally we apply a statistical tool TRUST in to compute the statistical corpus matrix Here we illustrate with an example on how this methodology enhances the system to correctly identify context Consider the sentence Beech-Nut Corp damaged its image over the sale of apple juice that turned out to be water in which the word sale is the targeted word Here we show step by step how the system correctly understands the sale drops down Procedure of Deduction Given by original denitions of the predicate dictionary and EL Against\(damage and Companyoperation\(image sale  Start Against\(damage image 004 Against\(damage Company-operation.image By denition of EL 004 Against\(damage Company-operation By 5 th rule 004 Against\(damage Company-operation.sale By 5 th rule  004 Against\(sale 004 Performance-down\(sale V R ESULTS AND D ISCUSSIONS A Results In this section we apply the statistical tool TRUST on the parsed outputs As an example we only discuss the spacial distribution of company which belongs to one of two possible concepts Buyer or Seller companies are sold  Fig 2\(a shows two different distributions of company given company belongs to Buyer or Seller before tuning tuning means applying the proposed methodology In the gure the origin denotes the central word company X-axis denotes all other words in the selected corpus due to the limits of the page only 8 words with their concepts are listed Y-axis denotes the distance between the central word company and other words on X-axis The larger the distance between company and another word the less correlated these two words circles denote the distribution of company belonging to concept Buyer and stars denote the distribution of company belonging to concept Seller For instance the gure shows before tuning company belonging to concept Buyer is more related to word board than company belonging to Seller before tuning  Fig 2\(b shows two different distributions of company given company belongs to Buyer or Seller after tuning In the gure the origin the X-axis and the Yaxis are similar to what has been dened in Fig 2\(a Comparing Fig 2\(b and Fig 2\(a we discover that some distributions have changed For instance Fig 2\(b shows after tuning company belonging to concept Seller is more related to word board than company belonging to buyer which is contrary to the results shown in Fig 2\(a After re-checking the original documents we nd that this seemingly contradictory result is in accordance with human intuition when a company is being sold the decision must be passed by the director board of the sold company and accordingly the word board instead of the sold company usually appears within the context However when a company plans to takeover another company the word board seldom appears In order to more clearly understand the tuning effect on the distributions we change the views and outline the results in Fig 3 The statistical outputs by applying of the proposed methodology might be viewed as the semantic coefcients of words These semantic coefcients can be directly applied on construction of fuzzy membership function in the latter stage which used to be achieved by domain experts arbitrary experience The proposed approach can be also used to identify the semantic meaning of a given word word sense disambiguation Consider the sentence The juice scandal forced Beech-Nut to pay a 2  2 million ne and 7  5 million to settle a lawsuit in the domain of company-acquisition the system can accurately tell that Beech-Nut is a sold company because the tuned EL shows scandal pay ne and lawsuit are more related to a seller than a buyer Other possible applications of the proposed approach include 1 operation as an assistant to a semantic parser  e.g anaphor resolution  and 2 operation as an assistant to a syntactic parser  e.g pp attachment  B Comments and discussions The main features of the proposed methodology that makes it an efcient tool for NLU can be summarized as follows 1 simplify manual annotation Humans annotation is much more accurate than any intelligent machine However manual annotation is tedious and time consuming Even for a domain expert it is difcult to outline a collection of concepts which is good enough to cover most of the corpus Besides even with the available set of concepts it is difcult to assign a word to its appropriate concept\(s because the belongingness of the word is multi-valued and dynamic To tackle this issue the methodology is proposed in a way where  the issue of assigning words belongingness might be tackled through the three sets of knowledge  by using the three sets of knowledge the system dynamically differentiates words directions which will be applied on construction of membership functions 2 imparts human knowledge into statistical universe Considering the cons of statistical tools researchers have tried to impart human knowledge into statistical universe to enhance The 2005 IEEE International Conference on Fuzzy Systems 146 


0 1 2 3 4 5 6 7 8 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Distance \025\025\025\025\025> larger Distribution of Concepts Referring Buyer and Seller before Tuning company in Buyer company in Seller action\025up expand action\025out sell action\025in purchase agree approve perf\025up sale action\025in buy manage board legal court company wrong   a Distribution of Concepts before Tuning 0 1 2 3 4 5 6 7 8 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Distance \025\025\025\025\025> larger Distribution of Concepts Referencing Buyer and Seller after Tuning company in Buyer company in Seller company action\025up expand action\025out sell action\025in purchase agree approve perf\025up sale action\025in buy manage board legal court correct   b Distribution of Concepts after Tuning Fig 2 Distribution of Concepts Referring Buyer:company and Seller:company Before v.s After Tuning 0 1 2 3 4 5 6 7 8 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Distance \025\025\025\025\025> larger Distribution of Concepts Referencing Buyer Before and After Tuning company in Buyer Before Tuning company in Buyer After Tuning company action\025up expand action\025out sell action\025in purchase agree approve perf\025up sale action\025in buy manage board legal court a Distribution of Concepts Referring Buyer:company 0 1 2 3 4 5 6 7 8 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Distance \025\025\025\025\025> larger Distribution of Concepts Referencing Seller Before and After Tuning company in Seller before Tuning company in Seller after Tuning company action\025up expand action\025out sell action\025in purchase agree approve perf\025up sale action\025in buy manage board legal court b Distribution of Concepts Referring Seller:company Fig 3 Distribution of Concepts Referring Buyer:company and Seller:company Before and After Tuning NLU through the fuzzy technique In previous research work however the major bottleneck was on the difculties encountered in generating the membership functions MF of natural words In this work we provide a solution to this through the proposed methodology into a statistical tool We use the local knowledge  i.e denition of EL and Predicate Dictionary to tune the global corpus matrix  i.e words distance distribution and concept distance distribution As such the original EL human-dened incomprehensive and inaccurate is tuned and enlarged so that EL becomes closer to human common sense and in accordance with its objective distribution refer to Table IV After achieving the tuned EL the system can easily generate the MF for interested words in the selected corpus 3 free from the boundary limitation For statistical approaches only the near neighbors occurrences can be taken into account For instance 3  gram only calculates the co-occurrences of 3 words on the central words left and right respectively If the window size increases up to 4 the complexity of computation will increase several orders The 2005 IEEE International Conference on Fuzzy Systems 147 


Lexine Attribute Value in Fuzzy Measurement Buyer action-in very possible agree very possible management-staff not very possible legal not very possible Seller action-out very possible legal possible management-staff possible sale hard to tell TABLE IV Example of Components of Epistemic Lexicon after Tuning of magnitude depending on the corpus size However the proposed approach tackles this issue by logically reorganizing related sentences together no matter how far away these sentences are from each other For instance according to the words direction dened by predicate dictionary and lexines attributes dened by EL the generic rules reasonably connects sentences which have the same subject and direction 4 free from decision rules The decision rules are the pivot component for both statistical and fuzzy approaches However the decision rules are difcult to discover for statistical approaches and require a great deal of domain knowledge for fuzzy approaches In this paper we solve such issue by simply dening 5 generic rules and the experiment shows they are satisfactory What is more important is that they are less domain dependent On the other hand the proposed approach is still domain dependent in some sense Indeed for each domain we have to re-dene EL concepts and semantic patterns In practice it is preferable to constrain the technique to a narrowly dened domain and make use of a large text corpus VI C ONCLUSION In this paper a fundamental research on how to formalize human knowledge through three sets of knowledge is thoroughly discussed The proposed work can either tackle issues challenging the traditional statistical approaches such as how to overcome the insufcient co-occurrence information caused by the limited window size or strengthen machines the capability to impart human knowledge into statistical approaches Based on this research the experiment is also presented to demonstrate the potential capabilities in tackling issues existing in traditional statistical measures and classic fuzzy approaches The results presented indicate that the use of proposed approach enhances the obtained words semantic coefcients Although the proposed approach decreases the domain experts impact on the nal results it is still necessary to apply domain knowledge to help the approach more accurately depict the semantic relations between concepts and words In order to make the approach compatible to multi-domain in future research work we will focus on expanding the unique topic domain to more complex ones Besides we will explore how to construct membership function based on words semantic coefcients through the proposed approach in our future research work R EFERENCES  L Zadeh T o w ard a theory of fuzzy information granulation and its centrality in human reasoning and fuzzy logic Fuzzy Sets and Systems  vol 90 no 2 pp 111127 1997  L Zadeh Enhanced by nlp using preciated natural language  2003 University of Waterloo  K S Jones  A statistical interpretation of term specicity and its application in retrieval Journal of Documentation  vol 28 no 1 1972  G Salton and C Buckle y  T erm-weighting approaches in automatic text retrieval Information Processing and Management an International Journal  vol 24 no 5 pp 513523 1988  D Hull J Pedersen and H Schuetze Document routing as statistical classication AAAI Spring Symposium on Machine Learning in Information Access  vol 12 March 1996  T  Joachims Learning to Classify Text using Support Vector Machines Methods Theory and Algorithms  Kluwer Academic Publishers May 2002  R Barzilay and M Elhadad Using le xical chains for te xt summarization in Proceedings of the Intelligent Scalable Text Summarization Workshop ISTS97  1997  H Li and N Abe Generalizing case frames using a thesaurus and the mdl principle Computational Linguistics  vol 24 no 2 1998  M Lesk  Automatic sense disambiguation using machine readable dictionaries How to tell a pine cone from an ice cream cone in Proceedings of SIGDOC  1986 pp 2426  S Banerjee and T  Pedersen  An adapted lesk algorithm for w ord sense disambiguation using wordnet in Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics CICLING-02  February 2002 pp 1722  R Kro v etz and W  B Croft Le xical ambiguity and information retrieval ACM Transactions on Information Systems TOIS  vol 10 no 2 pp 115141 1992  G Jan W ilms Using an on-line dictionary to e xtract a list of sensedisambiguated synonyms in Proceedings of the 30th annual Southeast regional conference  1992 pp 1522 ACM Press  S G Soderland Learning te xt analysis rules for domain-specic natural language processing Tech Rep UM-CS-1996-087 1996  W  Lehnert Symbolic/subsymbolic sentence analysis Exploiting the best of two worlds in Advances in Connectionist and Neural Computation Theory  1990 pp 135164 Ablex Publishers NJ  L Zadeh  A fuzzy-set-theoretic interpretation of linguistic hedges  Jour of Cybernetics  vol 2 pp 434 1972  L Zadeh Quantitati v e fuzzy semantics  Inf Sci  vol 3 pp 159176 1971  L Zadeh Pruf and its application to inference from fuzzy propositions in Proc IEEE Conf Decision control  1977 pp 13591360  L Zadeh Prufa meaning representation language for natural language Fuzzy Reasoning and its Applications  pp 166 1981  G Akri v as and G Stamou Fuzzy semantic association of audio visual document descriptions in Proc of Int Workshop on Very Low Bitrate Video Coding VLBV  2001  P  Subasic and A Huettner   Af fect analysis of te xt using fuzzy semantic typing IEEE Transactions on Fuzzy Systems Special Issue  August 2001  S Soderland D Fisher  J Aseltine and W  Lehnert CR YST AL Inducing a conceptual dictionary in Proceedings of the Fourteenth International Joint Conference on Articial Intelligence  Chris Mellish Ed San Francisco 1995 pp 13141319 Morgan Kaufmann  Y  Sun F  Karray  and J.P  Sun Semantic kno wledge e xtraction by trust Tech Rep Department of System Design Engineering University of Waterloo Waterloo 2003 The 2005 IEEE International Conference on Fuzzy Systems 148 


      T10.I4.D100k.d m P2 0 5 10 15 20 25 20 40 60 80 100  db transaction number k  Running tine \(sec 0.20 0.40 Figure 5. Scalability with the transaction number of db T10.I4.D z d100k.P2 0 5 10 15 20 25 100 200 400 600 800 1000  DB transaction number k  Running time\(sec 0.20 0.40 Figure 6. Scalability with the transaction number of DB 5. Conclusions In the real world, databases are periodically and continually updated.  Therefore, mining must be repeated Valid patterns and rules must to be efficiently generated Incremental mining must usually involve the original database and the new added transactions.  Scanning the original database is very expensive, so the proposed method outperforms others by avoiding the rescanning of the original database This investigation has presented a new method, NFUP for incremental mining.  NFUP does not require the rescanning of the original database and can determine new frequent itemsets at the latest time intervals.  The proposed method uses information available from a following partition to avoid the rescanning of the original database; it requires only the incremental database to be scanned.  In reality, the transaction number of the incremental database is very small in contrast to the original database.  The running time of NFUP rises almost in direct proportion with the transaction number of the incremental database Accordingly, NFUP is suited frequently updated databases In the future, the authors will consider the extension of the NFUP algorithm to sequence rules References  R. C. Agarw al, C  C. Agg arwal and V. V. V   Prasad, A tree projection algorithm for generation of frequent itemsets Journal of Parallel and Distributed Computing Vol. 61, No 3, pp. 350-361, 2000  C. C. Aggar wal and P  S. Y u Mining associations with the collective strength approach IEEE Trans. Knowledge and Data Engineering Vol. 13, No. 6, pp. 863-873, 2001  R. Agraw al T. Imielinski, and A. S w ami, Mining association rules between sets of items in large databases In Proc 1993 ACM SIGMOD Intl. Conf. on Management of Data Washington, D.C., pp. 207-216, May 1993  wa l a n d R Srika nt Fa st a l gorithms for mining association rules, In Proc. 20th Intl. Conf. on Very Large Data Bases Santiago, Chile, pp. 487-499, Sep. 1994  N. F. A y a n, A. U Tansel and E. Arkun, An eff icient algorithm to update large itemsets with early pruning, In Proc. 5th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining San Diego, CA, pp. 287-291, Aug. 1999  S. Brin, R Motwani, J. D  Ullman, and S Tsur, D y n amic itemset counting and implication rules for market basket data In Proc. 1997 ACM SIGMOD Intl. Conf. on Management of Data Tucson, AZ, pp. 255-264, May 1997  M. S. Chen  J. Han and P S Yu, Data mining: An overview from a database perspective IEEE Trans. Knowledge Data Engineering Vol. 8 No. 6, pp. 866-883, 1996  D. W. Ch eung, J. Han  V T. Ng and C. Y  Wong  Maintenance of discovered association rules in large databases: an incremental updating technique, In Proc. 12th Intl. Conf. on Data Engineering New Orleans, LA, pp 106-114, Feb. 1996  D. W. Cheu ng, S. D. Lee an d B. Kao A gen eral incr em ental technique for maintaining discovered association rules, In Proc 5th Intl. Conf. on Database Systems for Advanced Applications Melbourne, Australia, pp. 185-194, Apr. 1997  J. Han, J. Pei and Y. Yin Mining frequ ent patterns without candidate generation In Proc 2000 ACM-SIGMOD Intl Conf. on Management of Data Dallas, TX, pp. 1-12, May 2000  T. P. Hong C. Y  Wang, and Y. H Tao A n e w in cremental data mining algorithm using pre-large itemsets Intelligent Data Analysis Vol. 5, No. 2, pp. 111-129, 2001  C. H. Lee M. S. Chen  and C. R. Lin Progressive par tition  miner: An efficient algorithm for mining general temporal association rules IEEE Trans. Knowledge Data Engineering  Vol. 15, No. 4, pp. 1004-1017, 2003  C. H  Lee, C. R  Lin, and M. S. Chen Sliding-window  filtering: An efficient algorithm for incremental mining, In Proc. 10th Intl Conf. on Information and Knowledge Management Atlanta, GA, pp. 263-270, Nov. 2001  association rules dynamically In Proc. Intl. Symposium on Database Applications in Non-Traditional Environments Kyoto, Japan pp. 84-91, Nov. 1999  T. Y. Ng M. L. Wong, an d P. Bao Incremental mining o f  association patterns on compressed data, In Proc. Joint 9th IFSA World Congress and 20th NAFIPS Intl. Conf Vancouver, Canada, pp. 441-446, Jul. 2001   S. Yu, An effective hash-based algorithm for mining association rules, In Proc 7 Proceedings of the 15th International Workshop on Research Issues in Data Engineering: Stream Data Mining and Applications \(RID E-SDMA05 1097-8585/05 $20.00  2005 IEEE 


1995 ACM-SIGMOD Intl. Conf. on Management of Data San Jose, CA, pp. 175-186, May 1995  N. L. Sarda and N. V. Srinivas, An adap tiv e algorithm for  incremental mining of association rules, In Proc. 9th Intl Workshop on Database and Expert Systems Applications  Vienna, Austria, pp. 240-245, Aug. 1998  A. Savasere, E. Omiecinsk i, and S. Nav ath e, An efficient  algorithm for mining association rules in large databases, In Proc. 21st Conf. on Very Large Data Bases Zurich Switzerland, pp. 432-444, Sep. 1995  S. Thomas, S. Bod agala K. Alsabti and S. Rank a, An  efficient algorithm for the incremental updating of association rules in large database, In Proc. 3rd Intl. Conf. on Data Mining and Knowledge Discovery Newport Beach, CA, pp 263-266, Aug. 1997  A. Velos o W. Meira Jr  M B. de Carvalho B Pssas, S  Parthasarathy, and M. J. Zaki, Mining frequent itemsets in evolving databases, In Proc. 2nd SIAM Intl. Conf. on Data Mining Arlington, VA, Apr. 2002 8 Proceedings of the 15th International Workshop on Research Issues in Data Engineering: Stream Data Mining and Applications \(RID E-SDMA05 1097-8585/05 $20.00  2005 IEEE 


CBA and Neural Networks at 1% confidence level while they are all significantly better than C4.5 decision tree. Taking the interpretability of classification model into account, these two adapted CBA algorithm seem to be appropriate choices for credit scoring because they generated much more compact decision lists \(less sequential rules original CBA. They therefore favour the well-known Occam  s Razor theory and are more suitable for decision makers to understand. A deeper insight into the rules structures shows that original CBA and adapted CBA 1 both focus on generating classification rules that predict good clients \(with bad clients as the default class implication, numerous rules with high confidence but low support have lower ranks than they are in original CBA. These rules are finally discarded since they are not fired by any training samples, which are matched by these rules with higher intensity of implications thus making the decision lists generated by adapted CBA 1 more compact. Adapted CBA 2 mainly mines these classification rules for bad clients \(with good clients as the default class compact rule sets Original CBA Adapted CBA1 Adapted CBA2 C45 NN dataset accuracy no. of rules accuracy num. of rules accuracy no. of rules accuracy accuracy 1 Austr 85.65% 130.5 86.52% 26.4 86.96% 12.4 86.52% 85.07 2 Germ 73.30% 134 74.40% 56.5 73.20% 19.7 72.40% 74.90 3 Bene 72.92% 393 72.30% 186 73.51% 51 70.21% 72.63 Table 2. Experiment results Proceedings of the 2005 IEEE International Conference on e-Business Engineering \(ICEBE  05 0-7695-2430-3/05 $20.00  2005 IEEE In addition, decision makers in financial institution certainly pay more attentions to those rules that predict bad clients, which will be extraordinary costly if they are regarded as good ones 5. Conclusion Intensity of implication is proposed in the beginning as an interestingness measure for association rules Another novel interestingness measure called dilated chi-square is designed by us to reveal the statistical interdependence between the antecedents and consequents of association rules We then adapt CBA algorithm, which can be used to build classifiers based on class association rules, by coupling it with intensity of implication and dilated chi-square respectively. More concretely, Intensity of implication \(or dilated chi-square primary criterion to rank class association rules at the first step of the database coverage pruning procedure in CBA algorithm. Experiments on three credit scoring datasets proved that these two adapted algorithms compared with original CBA, classical C4.5 decision tree and neural network, achieve satisfactory performance and generates classifiers much more compact than CBA 6. Acknowledgement The work was partly supported by National Natural Science Foundation of China \(70231010/70321001 7. References 1] Wang, K. and S. Zhou, Growing decision trees on support-less association rules. in KDD'00. 2000. Boston,MA 2] Liu, B., W. Hsu, and Y. Ma, Integrating Classification and Association Rule Mining. in the 4th International Conference on Discovery and Data Mining. 1998. New 


Conference on Discovery and Data Mining. 1998. New York,U.S 3] Dong, G., et al, CAEP:Classification by aggregating emerging patterns. in 2nd International Conference on Discovery Science,\(DS'99 Artificial Intelligence. 1999. Tokyo,Japan: Springer-Verlag 4] Liu, W., J. Han, and J. Pei, CMAR: Accurate and efficient classification based on multiple class-association rules. in ICDM'01. 2001. San Jose, CA 5] Yin, X. and J. Han, CPAR:Classification based on predictive association rules. in 2003 SIAM International Conference on Data Mining \(SDM'03 Fransisco,CA 6] Agrawal, R. and R. Srikant, Fast algorithm for mining association rules. in the 20th International Conference on Very Large Data Bases. 1994. Santiago,Chile 7] Agrawal, R., T. Imielinski, and A. Swami, Mining association rules between sets of items in large databases. in the ACM SIGMOID Conference on Management of Data 1993. Washington,D.C 8] Guillaume, S., F. Guillet, and J. Philippe, Improving the discovery of association rules with intensity of implication Principles of Data Mining and Knowledge Discovery, 1998 1510: p. 318-327 9] Janssens, D., et al, Adapting the CBA-algorithm by means of intensity of implication. in the First International Conference on Fuzzy Information Processing Theories and Applications. 2003. Beijing, China 10] Gras, G. and A. Lahrer, L'implication statistique: une nouvelle methode d'analysis de donnees. Mathematiques Informatique et Sciences Humaines n 20, 1993 11] Suzuki, E. and Y. Kodratoff, Discovery of  surprising exception rules based on intensity of implication. in PKDD'98. 1998. Berlin: Springer 12] Mills, F., Statistical Methods. 1955: Pitman 13] Lan, Y., et al, Dilated Chi-square: A novel interestingness measure to build accurate and compact decision list. in International conference on intelligent information processing. 2004. Beijing,China 14] Quinlan, J.R., C4.5 programs for machine learning 1993: Morgan Kaufmann 15] Witten, I.H. and E. Frank, Data Mining: practical machine learning tools and techniques with Java implementations. 2000: Morgan Kaufmann, San Francisco 16] Fayyad, U.M. and K.B. Irani, Multi-interval discretization of continuous valued attributes for classification learning. in the Thirteenth International Joint Conference on Artificial Intelligence \(IJCAI Chambery,France: Morgan Kaufmann 17] Blake, C.L. and C.J. Merz, UCI repository of machine learning databases http://www.ics.uci.edu/~mlearn/mlrepository.htm]. 1998 Irvine,CA:University of California, Dept. of Information and Computor Science 18] Dietterich, T.G., Approximate statistical tests for comparing supervised classification learning algorithms Neural Computation, 1998. 10\(7 Proceedings of the 2005 IEEE International Conference on e-Business Engineering \(ICEBE  05 0-7695-2430-3/05 $20.00  2005 IEEE pre></body></html 


absolute values. The results can vary on other computers. But it can be guaranteed that performance ratio of the algorithms will remain the same After making the comparisons with sample data, we came to the conclusion that PD algorithm performs significantly better than the other two especially with larger datasets. PD outperforms DCP and PIP regarding running time. On the other hand, since PD reduces the dataset, mining time does not necessary increase as the number of transactions increases and experiments reveals that PD has better scalability than DCP and PIP. So, PD has the ability to handle the large data mine in practical field like market basket analysis and medical report documents mining 5. References 1] R. Agrawal and R. Srikant, "Fast algoritlnns for mining association rules", VLDB'94, pp. 487-499 2] R. J. Bayardo, "Efficiently mining long patterns from databases", SIGMOD'98, pp.85-93 3] J. Pei, J. Han, and R. Mao, "CLOSET: An Efficient Algorithm for Mining Frequent Closed Itemsets \(PDF Proc. 2000 ACM-SIGMOD International Workshop on Data Mining and Knowledge Discovery, Dallas, TX, May 2000 4] Qinghua Zou, Henry Chiu, Wesley Chu, David Johnson, "Using Pattern Decomposition\( PD Finding All Frequent Patterns in Large Datasets", Computer Science Department University of California - Los Angeles 5] J. Han, J. Pei, and Y. Yin, "Mining Frequent Patterns without Candidate Generation \(PDF  SIGMOD International Con! on Management of Data SIGMOD'OOj, Dallas, TX, May 2000 6] S. Orlando, P. Palmerini, and R. Perego, "The DCP algoritlnn for Frequent Set Counting", Technical Report CS2001-7, Dip. di Informatica, Universita di Venezia 2001.Available at http://www.dsi.unive.itl?orlando/TR017.pdf 7] MD. Mamun-Or-Rashid, MD.Rezaul Karim, "Predictive item pruning FP-tree algoritlnn", The Dhaka University  Journal of Science, VOL. 52, NO. 1, October,2003, pp. 3946 8] Park, J. S., Chen, M.-S., and Yu, P. S, "An Effective Hash Based Algoritlnn for Mining Association Rules", Proc ofthe 1995 ACM-SIGMOD Con! on Management of Data 175-186 9] Brin, S., Motwani, R., Ullman, J., and Tsur, S, "Dynamic Itemset Counting and Implication Rules for Market Basket Data", In Proc. of the 1997 ACM-SIGMOD Conf On Management of Data, 255-264 10] Zaki, M. J., Parthasarathy, S., Ogihara, M., and Li, W New Algoritlnns for Fast Discovery of Association Rules In Proc. of the Third Int'l Con! on Knowledge Discovery in Databases and Data Mining, 283-286 11] Lin, D.-I and Kedem, Z. M., "Pincer-Search: A New Algoritlnn for Discovering the Maximum Frequent Set", In Proc. of the Sixth European Conf on Extending DatabaseTechnology, 1998 12] R. Ramakrishnan, Database Management Systems University of Wisconsin, Madison, WI, USA; International Edition 1998 pre></body></html 


tors such as union, di?erence and intersection are de?ned for pairs of classes of the same pattern type Renaming. Similarly to the relational context, we consider a renaming operator ? that takes a class and a renaming function and changes the names of the pattern attributes according to the speci?ed function Projection. The projection operator allows one to reduce the structure and the measures of the input patterns by projecting out some components. The new expression is obtained by projecting the formula de?ning the expression over the remaining attributes [12 Note that no projection is de?ned over the data source since in this case the structure and the measures would have to be recomputed Let c be a class of pattern type pt. Let ls be a non empty list of attributes appearing in pt.Structure and lm a list of attributes appearing in pt.Measure. Then the projection operator is de?ned as follows ls,lm c id s m f p ? c, p = \(pid, s, d,m, f In the previous de?nition, id ing new pids for patterns, ?mlm\(m projection of the measure component and ?sls\(s ned as follows: \(i s usual relational projection; \(ii sls\(s and removing the rest from set elements. The last component ?ls?lm\(f computed in certain cases, when the theory over which the formula is constructed admits projection. This happens for example for the polynomial constraint theory 12 Selection. The selection operator allows one to select the patterns belonging to one class that satisfy a certain predicate, involving any possible pattern component, chosen among the ones presented in Section 5.1.1 Let c be a class of pattern type pt. Let pr be a predicate. Then, the selection operator is de?ned as follows pr\(c p Join. The join operation provides a way to combine patterns belonging to two di?erent classes according to a join predicate and a composition function speci?ed by the user Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE Let c1 and c2 be two classes over two pattern types pt1 and pt2. A join predicate F is any predicate de?ned over a component of patterns in c1 and a component of patterns in c2. A composition function c pattern types pt1 and pt2 is a 4-tuple of functions c cStructureSchema, cDataSchema, cMeasureSchema, cFormula one for each pattern component. For example, function cStructureSchema takes as input two structure values of the right type and returns a new structure value, for a possible new pattern type, generated by the join. Functions for the other pattern components are similarly de?ned. Given two patterns p1 = \(pid1, s1, d1,m1, f1 p2 = \(pid2, s2, d2,m2, f2 p1, p2 ned as the pattern p with the following components Structure : cStructureSchema\(s1, s2 Data : cDataSchema\(d1, d2 Measure : cMeasureSchema\(m1,m2 Formula : cformula\(f1, f2 The join of c1 and c2 with respect to the join predicate F and the composition function c, denoted by c 1   F  c  c 2   i s  n o w  d e  n e d  a s  f o l l o w s    F  c  c 2     c  p 1   p 2   p 1    c 1  p 2    c 2  F   p 1   p 2     t r u e   5.1.3. Cross-over database operators OCD Drill-Through. The drill-through operator allows one to 


Drill-Through. The drill-through operator allows one to navigate from the pattern layer to the raw data layer Thus it takes as input a pattern class and it returns a raw data set. More formally, let c be a class of pattern type pt and let d be an instance of the data schema ds of pt. Then, the drill-through operator is denoted by c c Data-covering. Given a pattern p and a dataset D sometimes it is important to determine whether the pattern represents it or not. In other words, we wish to determine the subset S of D represented by p \(p can also be selected by some query the formula as a query on the dataset. Let p be a pattern, possibly selected by using query language operators, and D a dataset with schema \(a1, ..., an ible with the source schema of p. The data-covering operator, denoted by ?d\(p,D responding to all tuples in D represented by p. More formally d\(p,D t.a1, ..., t.an In the previous expression, t.ai denotes a speci?c component of tuple t belonging to D and p.formula\(t.a1, ..., t.an instantiated by replacing each variable corresponding to a pattern data component with values of the considered tuple t Note that, since the drill-though operator uses the intermediate mapping and the data covering operator uses the formula, the covering ?\(p,D D = ?\(p not be equal to D. This is due to the approximating nature of the pattern formula 5.1.4. Cross-over pattern base operators OCP Pattern-covering. Sometimes it can be useful to have an operator that, given a class of patterns and a dataset, returns all patterns in the class representing that dataset \(a sort of inverse data-covering operation Let c be a pattern class and D a dataset with schema a1, ..., an pattern type. The pattern-covering operator, denoted as ?p\(c,D all patterns in c representing D. More formally p\(c,D t.a1, ..., t.an true Note that: ?p\(c,D p,D 6. Related Work Although signi?cant e?ort has been invested in extending database models to deal with patterns, no coherent approach has been proposed and convincingly implemented for a generic model There exist several standardization e?orts for modeling patterns, like the Predictive Model Markup Language \(PMML  eling approach, the ISO SQL/MM standard [2], which is SQL-based, and the Common Warehouse Model CWM  ing e?ort. Also, the Java Data Mining API \(JDMAPI 3] addresses the need for a language-based management of patterns. Although these approaches try to represent a wide range of data mining result, the theoretical background of these frameworks is not clear. Most importantly, though, they do not provide a generic model capable of handling arbitrary cases of pattern types; on the contrary only a given list of prede?ned pattern types is supported To our knowledge, research has not dealt with the issue of pattern management per se, but, at best, with peripheral proximate problems. For example, the paper by Ganti et. al. [9] deals with the measurement 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


