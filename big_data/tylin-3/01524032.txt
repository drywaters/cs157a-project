A Clickstream-based Collaborative Filtering Recommendation Model for E-Commerce Dong-Ho Kim kimdh@andromeda.rutgers.edu Rutgers University 180 University Ave Newark NJ 07102 1-973-353-1608 Il Im il.im@yonsei.ac.kr Yonsei University 134 Shinchon-dong Seoul Korea 120-749 82-2-2123-5480 Vijayalakshmi Atluri atluri@andromeda.rutgers.edu Rutgers University 180 University Ave Newark NJ 07102 1-973-353-1608 Abstract In recent years clickstream-based collaborative filtering CCF recommendation models have received 
much attention mainly due to their scalability The common CCF recommendation models are Markov modesl sequential association rules association rules and clustering The models have shown the trade-off relationship between precision and recall in performance To address the trade-off relationship some study has combined two or more different models or applied multiorder models The increase of recommendation effectiveness by these models is also at best marginal To increase recall while minimizing the loss of precision and 
therefore to increase overall performance measured by the F value we build a sequentially applied model SAM by applying the individual models in tandem in an order determined through a learning process We evaluated SAM over the individual models with Web usage data and the result is promising 1 Introduction Collaborative Filtering CF is a technology of making use of peers evaluations or behaviors for a personalized recommendation  Depen di n g on dat a t y p e 
in use for CF it can be classified into either user-based CF or item-based CF   Us erbas ed C F employs user data such as user profiles to find the most similar users or a reference group for a user and based on their preferences it recommends items for the user Itembased CF employs item data such as products or Web pages to find item relationships based on which it recommends items for a user User-based CF has been extensively used in research 
and in practice However it has a limitation in scalability 10 6 0 1  T rad itional user-based CF e.g KNearest Neighborhood  does most of recommendation computation in online real time For example from the user-item matrix in Figure 2 its online computational complexity is O mn  in the worst case because it needs to scan all the users once to find similar users and scan all the items to find the items that the similar users have 
selected B ecau s e s o m e ty pical electron ic com m erce EC sites have millions of customers and items this online computation complexity becomes a problem 16  2 0   A ltho ugh so m e statistical m e tho d s such as sampling or clustering can mitigate the online computational complexity these methods reduce recommendation quality 16  F o r instance cluster ing performs much of its computation offline but there are chances that the m ost similar user found in the closest 
cluster is not truly the most similar user 16  T his is illustrated using a simple clustering in Figure 1  Figure 1 A bad clustering example Item-based CF can have heavier computation than user-based CF but the heavy computation is performed in offline batch process The batch process usually builds recommendation models with which online recommendation is made so this item-based CF is called model-based CF T h i s m a k e s i t s on l i n e c om pu t a t i o n 
lighter than user-based CF So the scalability problem can be overcome For this reason item-based CF has received much attention as a way of doing CF In practice Proceedings of the Seventh IEEE International Conference on E-Commerce Technology \(CEC05 1530-1354/05 $20.00  2005 IEEE 


Amazon.com which has tens of millions of customers worldwide and millions of products is using item-based CF F rom Figure 2  its offline computational complexity could be O n 2 m  in the worst case i.e n C 2 m  For an item i it scans all the users and it calculates item similarities for the rest of the items  1,2,..i-1,i+1 n by using m dimensional user space This process is repeated up to n-1  Its online computation complexity is O n in the worst case but on average the complexity is O  constant  because the online computation is just to look at the constant number of items for an item i.e the number of the related items is very small or it recommends only TOP N items Item 1 Item 2  Item n User 1    User 2        User m      selection or purchase of an item by a user Figure 2 A user-item matrix Clickstream-based CF CCF a kind of item-based CF  h a s receiv ed m u ch atten tion as a w ay of doing collaborative filtering recommendation in Web navigation because user data is generally not available IP addresses may be used as implicit user identifications Like other item-based CF CCF needs to adopt prediction models for efficient and effective recommendations of pages or products due to its vast amount of data stream to process it trains the models offline and uses them in online recommendations Unlike other item-based CF recommendations i.e market basket data in EC the sequence of pages is important for its recommendation quality due to the fact that the sequential structure is embedded through the hyperlinks in Web pages Clickstream-based CF recommendation has its importance in EC because it could customize Web sites with relevant pages or recommend relevant products to increase user experience That is from browsing patterns we can know a users product preference and then recommend relevant pages or products     Many studies have worked on CCF recommendation models using browsing patterns and their performance The common models for CCF recommendation are Markov models          sequential association rules      association rules      an d c l u s t eri n g 4][11][30][32 The contribution of this paper is as follows 1 it proposes a hybrid model SAM which applies the individual models in sequence SAM increases recall and the F value while keeping the loss of precision at minimum To the best of our knowledge SAM is the first hybrid model that applies different model types in sequence and 2 we show the performance of the hybrid model SAM mathematically and experimentally In the next section we present our motivation In section 3 we propose a new CCF recommendation model and in the following section we evaluate the proposed model in experiment and present its results And then we discuss related work Finally we make a conclusion with some possible future work 2 Motivation Markov models MM have been well positioned as a CCF recommendation model because of its high precision coming from the consideration of consecutive orders of preceding pages However this high precision comes at the cost of low coverage because the clickstream data is inherently sparse and MM cannot cover the sparse data well Markov models generally have high precision and low coverage In order to address this lower coverage and recall multi-order mixed Markov models have been devised T h e All K-th order Markov model  i s one example This applies the highest order first to predict a next page or product If it cannot predict the page it keeps decreasing the order one by one until it can recommend the next page or product i.e cover a current active clickstream  T h is can in creas e t h e cov erag e and recall of the model but the increase could be still marginal for some applications e.g cross-selling in EC where recall is more important Association rules AR are based on the relationship of co-occurrences of pages or products without considering the sequence of them  T h e cooccurrences may represent diverse multidirectional semantic relationships between pages G irl vs woman similar and male vs female opposite are the examples for all possible relationships refer to    Association rules have been applied for finding frequent product sets in EC The frequent product sets are used for recommending products There is no order consideration This makes AR generally produce low precision but high recall in the prediction Association rules first identify large frequent page or product sets from a training data set using a support threshold i.e minsupport in T h en  f or each active path i.e antecedent in a test set it identifies frequent sets containing the active path From the identified frequent sets it recommends pages or products in the order of their confidence  In order to increase the coverage on the test set with the training set the size of the active path can be decreased one by one recursively until the path is covered by the training set Association Proceedings of the Seventh IEEE International Conference on E-Commerce Technology \(CEC05 1530-1354/05 $20.00  2005 IEEE 


rules performance also varies depending on the threshold values of support and confidence which should be set in advance before training 14  A high thr e sho l d v a lue would result in higher precision and lower recall while a low threshold value would result in lower precision and higher recall There is no good way to a priori select optimal values of support and confidence Therefore AR may require an extensive learning experiment to find the optimal values 10  Sequential association rules SAR stand between MM and AR in that it considers the sequence of pages which is not necessarily contiguous and uses frequent sets for its recommendation as association rules do It is expected that SAR come between MM and AR in precision and recall Clustering alone may not be an appropriate approach for CCF It usually employs different feature sets such as contents or concepts   or l i n k s t r u c t u res 32 of pages or is used as a moderator in terms of computation with other models 12  3 2   T his statistical method is to partition pages or sessions into closely related subsets part of which is used for recommendation  it is possible to create clustering using all data set  Because it does not use all the pages directly but through the controlled clusters i.e the number of clusters and maximum and minimum sizes of clusters are set in advance itmaylose some accuracy C l u s terin g b as ed on th e f eatu re s ets like bit vector visit or non-visit 19  s p e nt tim e o r frequency does not partition pages or sessions well because those feature sets usually cannot capture well structural and semantic relationships between pages The prediction models for CCF show strengths and weaknesses in their performance due to the inherent tradeoff relationship between precision and recall Even the hybrid models 22  3 1  sho w their lim itatio ns in their performance increase especially recall because they mainly focus on increasing precision and apply only one model type at a prediction point Therefore it is always a research issue to find or devise a model addressing precision as well as recall Thus we are presenting a hybrid model SAM combining the individual models at model level to increase recall and the F value while minimizing the loss of precision at minimum 3 Proposed Work OnemaingoalinthisworkistodeviseanewCCF model that provides better recall and the F value over existing models Without using contents or the structure information a way to increase both coverage and recall while keeping the loss of precision at a minimum is to apply the individual models MM SAR and AR in tandem in an order presumably precision order decided through a learning proce ss Conceptual motivation is depicted in Figure 3  Since the prediction models show varying performance depending on a data set and the parameters such as the number of pages or items in recommendation window size minimum confidence and support the learning process is needed to decide the order in applying the individual models In Figure 3 the performance of the proposed model may be represented as the combined area of the three individual models Model h represents a model with the highest precision model l a model with the lowest precision and model m a model with the middle precision Each area i.e the product of precision and recall represents the performance of each model Because the models are applied sequentially and recall has a nondecreasing property we can build SAM that applies the models in tandem in an order In Figure 3  model h represents a model with the highest precision and the lowest coverage model l a model with the lowest precision and the highest recall and model m a model between model h and model l  The precision of SAM comes between the highest precision and the lowest precision of the member models but much more towards the highest precision since SAM is constructed with the individual models usually in their precision order Figure 3 The conceptual representation of the performance of the proposed model First with Figure 4  we can show by a simple calculation how the precision and recall of the proposed SAM position in comparison to the ones of the individual models From Figure 4 let P i be the precision of the individual model i where i  h  m or l  Let the total number of covered records A i be C i  W i If A i  a subset of the whole test set is large enough then the precision P i of each model from the whole data set can be represented as follows P i  C i  C i  W i here i  h  m or l P i  C i  R i 1 Proceedings of the Seventh IEEE International Conference on E-Commerce Technology \(CEC05 1530-1354/05 $20.00  2005 IEEE 


Then the precision P SAM of the proposed SAM is P SAM   i C i   i A i Basically the order of applying the individual models is determined through a learning process However for the sake of explanation Figure 4 assumes SAM constructed with the individual models applied in their precision order which is intuitive If individual models with lower precision are used first there is a chance that the individual model with the highest precision is not used for recommendation which makes the precision of SAM closer to the precision of the model l  Figure 4 The way of building SAM In order to know where the precision P SAM will be positioned compared to the precisions of the individual models lets first compare P SAM with P h  P h  P SAM  C h  A h  i C i   i A i  C h  i A i  A h  i C i  A h  i A i Since A h  A i  0 then we only need to decide the sign of the numerator to determine the order between P SAM and P h  C h  i A i  A h  i C i  C h A m  C h A l C m A h C l A h 2 From 1 C i  P i A i 3 Insert 3 into 2 P h A h A m  P h A h A l P m A m A h P l A l A h  A h A m P h P m A l A h P h P l  Since P h P m 0 and P h P l 0  P h  P SAM This means the precision of the model with the highest precision is always greater than the one of the proposed SAM This implies the decreasing nature of precision in SAM.Inthesameway,ifwecompare P SAM with P l P l  P SAM  C l  i A i  A l  i C i  A l  i A i The numerator is decomposed into A l A h  P l P h  A l A m  P l  P m  Since P l  P h 0and P l  P m 0  P l  P SAM This tells us that the precision of the model with the lowest precision is always less than the precision of the proposed SAM Let us compare P SAM with P m  P m  P SAM  C l  i A i  A l  i C i  A l  i A i In the same way we get the following numerator A l A m  P m P l  A m A h  P h P m  Let A l A m  P m P l  A m A h  P h P m  equal to 0 A l A m  P m P l  A m A h  P h P m   A l  A h  P h P m  P m P l  After transforming the numerator and removing A m from it the sign of the numerator depends on the ratio of two items A l  A h and  P h P m  P m P l  That is if the ratio of the number of records covered by the model l over the number of records covered by the model h is greater less than the ratio of the difference in precision between h and m over the difference in precision between m and l then P m is greater less than P SAM  If the two ratios are equal then P m is equal to P SAM  Proceedings of the Seventh IEEE International Conference on E-Commerce Technology \(CEC05 1530-1354/05 $20.00  2005 IEEE 


Figure 5 Performance representation of SAM For the recall of the proposed hybrid model SAM R SAM   i C i    i  C i  W i  U  R i  C i    i  C i  W i  U   R SAM  R i This tells us the non-decreasing property of recall in SAM In Figure 5 the shaded area represents the overall performance of the proposed hybrid model SAM The more darkly shaded area  P 1 P 2 V 2 V 1  can be included depending on the ratio of A l  A h and  P h P m  P m P l  as explained earlier We need to compare the performance of SAM when the individual models are applied in their precision order like in Figure 4 with the performance of SAM when they are not applied in their precision order for example m  l  and h in that order whereas P h  P m  P l Let P  SAM be the precision of SAM when the individual models are applied in the order of m  l and h  P SAM P  SAM  C SAM  A SAM   C  SAM  A  SAM   C h  C m  C l  A h  A m  A l   C  m  C  h  C  l   A  m  A  l  A  h 9 Since the total number of covered records for both P SAM and P  SAM is the same i.e A h  A m  A l  A  l  A  h  A  l  9 can be written as  C h  C m  C l   C  m  C  h  C  l  10 10 tells us that the sign of P SAM P  SAM depends on the number of the correct records in P SAM and P  SAM  Intuitively the size of C h  C m  C l should be greater than the one of C  m  C  h  C  l  However it is possible that the size of C  m  C  h  C  l be greater than the one of C h  C m  C l i.e P  SAM  P SAM  The reason is the order of applying the individual models determines what part of a training data set that each model uses and the size of the part and therefore it is possible that C i  C  i where i  h  m and l  Thus in order to make sure that we choose SAM with the best performance we need to perform a learning process that measures the performances of all possible SAMs in different orders of the individual models 4 Experimental Evaluation In this section we first present the evaluation metrics we used for measuring recommendation effectiveness and then the results of our experiment based on these metrics 4.1 Performance Evaluation Metrics The performance metrics recall and precision will be used for measuring the prediction quality i.e effectiveness of the models These metrics have been used as the effectiveness measures of the prediction models     To measure the performance of the models in experiment in the context of personalization a session data set is divided into training and test sets The training set is used to generate the models while the test set is used to evaluate the models set with the training set 27  E ach session s in the test set is divided into two parts The first n clicks 1,2 n where0 n  s  in s are treated as an active session and are used for making predictions while the remaining portion  n 1 s  in s is used to evaluate the prediction models Active session window is the portion of a users active clickstream used by a prediction model for recommendation So each model starts to recommend pages with an active session with the highest order i.e a full active session and decreases the order one by one up to a minimum active session window until the active session is covered by the model  in our experiment the average session size of the data set is set as the highest order as suggest in 19  The active session is denoted by as s  and the remaining portion  s n of a session s is by test s  A prediction model takes as s as an input and makes a prediction A set of pages generated by the prediction model is denoted as P  as s  Then precision recall and the F value  can be repres en ted in Figure 6 22  T he F v a l ue is use d f o r c o m p a r ing the o v e r a l l Proceedings of the Seventh IEEE International Conference on E-Commerce Technology \(CEC05 1530-1354/05 $20.00  2005 IEEE 


performance of the models by giving equal weights on precision and recall  Figure 6 Performance metrics 4.2 Experimental Results The performance of SAM was evaluated over varying TOP N values the number of pages recommended by a prediction model Depending on applications TOP N may vary In typical recommendations like in cross-selling in EC the TOP N could be relatively small e.g ten or so For the experiment the publicly available NASA Web access log data NJMC New Jersey Meadowlands Commission http://www.meadowlands.state.nj.us and CIMIC Center for Information Management Integration Connectivity http://cimic.rutgers.edu log data are used A data set is divided into training and test data sets by their dates such that the test data is the session data at later dates than the training data For example the experimental instance of Figure 7 used the NJMC Web server access log between January 01 2004 and February 19 2004 as a training data set and the access log between February 19 2004 and March 4 2004 as a test data set This experimental design reflects the fact that a recommendation for a current active session is based on the past session data The training data set of the instance contains a total of 5,453 session records and its average session length is 7.088 Its test data set contains a total of 939 session records and its average session length is 8.368 which is a little bit longer than the length of the training data probably due to the small size of the test data set As we increase the size of a data set its average session length decreases and vice versa Also as we changed the size of the test set with the size of the training set fixed the performance of SAM over the ones of the individual models is consistent in order The models the individual models plus SAMs in different combinations of the individual models are evaluated also with different parameter values of window size minimum confidence and minimum support etc because these parameters also affect the performance of the models With any given context SAM will be constructed with the individual models after a learning process The performance of SAM is better than at least is equal to the ones of other models in recall and the F measure while SAM keeps the loss of precision at minimum over the model with the highest precision As a part of the learning process the performances of all the other possible SAMs are also shown for comparison in Figure 7  We have tested about hundred trials with different combinations of data set and parameters All the tests support the mathematical proof presented in the section 3 the recall and the F measure of SAM are always greater than the ones of the individual models and its precision loss is minimum when the individual models are applied in their precision order For this instance MSAD applying MM SAR AR and a default model in that order is the best performing SAM The default model is the last model to be used when the other individual models do not cover the active session The default model recommends pages aka consequents or actions  in the order of their frequency from a whole data set Because it doesnt use an antecedent at all its performance is not expected to be as good as the other individual models  T h e preci s i on of MM is the highest while its recall is almost the lowest It shows that SAR takes a position between MM and AR 5 Related Work Gundiz and Ozsu 13 b u ilt cluster s insid e w hich trees are constructed by using sequential association rules to predict Web pages Zuckerman et al bu i l t h y bri d models that combine variants of Markov models based on the time and/or structural sequences of pages Based on these hybrid Markov models they created a hybrid model that selectively chooses a best performing model at a prediction point Nakagawa and Mobasher als o proposed a hybrid model utilizing the site structure and the degree of local hyperlink connectivity The model selectively uses MM or AR based on the hyperlink connectivity measure of a page which affects the individual models precision These hybrid models apply only one model at a prediction point so that their performance increase is at best marginal Also the adoption of the structure information may limit the extension of the models into other domains for example the predictions of a service order 9  e d iting co m m a nd s i n Proceedings of the Seventh IEEE International Conference on E-Commerce Technology \(CEC05 1530-1354/05 $20.00  2005 IEEE 


Microsoft Word an d d i f f e ren t a l arm s i n a t e l eph on e switch 10   Recall All Kth order minConf=0 minSup=0 Win=2 M=MM S=SAR A=AR D=default 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 MM SAR AR DEFAULT MSAD MASD SMAD SAMD AMSD ASMD Models Recall top 2 top 4 top 6 top 8 top 10 top 12 top 14 top 16 top 18 top 20 Precision All Kth order minConf=0 minSup=0 Win=2 M=MM S=SAR A=AR D=default 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 MM SAR AR DEFAULT MSAD MASD SMAD SAMD AMSD ASMD Models Precision top 2 top 4 top 6 top 8 top 10 top 12 top 14 top 16 top 18 top 20 F measure All Kth order minConf=0 minSup=0 Win=2 M=MM S=SAR A=AR D=default 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 MM SAR AR DEFAULT MSAD MASD SMAD SAMD AMSD ASMD Models F value top 2 top 4 top 6 top 8 top 10 top 12 top 14 top 16 top 18 top 20 Figure 7 The performance of the models over top N 6 Conclusion Due to its scalability CCF recommendation has received much attention as a way of doing CF in Web Recommendation Although several prediction models have been adopted for Web recommendation they have shown strengths and weaknesses in their performance Some hybrid models also have limitations in their ability to increase their performance because they primarily focused on precision and used only one model at each prediction point We propose a new hybrid model to increase the CCF recommendation performance especially recall and the F value This is important in EC e.g cross-selling where recall counts more When we evaluated the proposed model over different TOP N values the proposed model shows the highest recall and F value and a good precision comparable to the individual models with higher precision The order of the individual models should be set through a learning process since some parameters i.e minimum support and minimum confidence may affect the performance of the individual models This papers main contribution is that it shows a way of increasing recommendation performance by combining different types of models sequentially without any processing performance issue because of its online realtime reference to lookup tables that are built offline The proposed model is scalable to include other individual models e.g a selective Markov model tog e th er w ith or in place of a regular Markov model 7 Acknowledgments This work is supported in part by NSF grants DUE0226075 DUE-0434998 The first author thanks Nabil Adam for providing the log data of CIMIC and NJMC 8 References 1 A h m ad M  A h m a d W asf i  1 9 9 9  Co llectin g U ser A ccess Patterns for Building User Profiles and Collaborative Filtering ACM IUI 99 Redondo Beach CA USA pages 57-64 2 C or in R  A n de r s on  2 002  P e d r o D o m i ng os  a nd D a nie l S Weld Relational Markov Models and their Application to Adaptive Web Navigation ACM SIGKDD Edmonton Alberta Canada 2002 pages 143 152 3 R akesh A graw al T o m as Im ielin ski a n d A r u n S w am i 1993 Mining Association Rules between Sets of Items in Large Databases Proceedings of the 1993 ACM SIGMOD Conference Washington DC USA May 1993 pages 207216 4 A  B an erjee a n d J G h o s h  2 0 0 1   C lick s tream clu sterin g using weighted longest common subsequences In Proceedings of the Web Mining Workshop at the 1st SIAM Conference on Data Mining Chicago April 2001 pages 33-40 Proceedings of the Seventh IEEE International Conference on E-Commerce Technology \(CEC05 1530-1354/05 $20.00  2005 IEEE 


5 D aniel Billsus a nd Mich ael J Pazzani  1998 Learning Collaborative Information Filters Proc 15th International Conf on Machine Learning 1998 pages 46-54 6 M ao Ch en  A n d rea S  L a P a u g h  Jasw i n d e r P al S i n g h 2002 Predicting Category Accesses for a User in a Structured Information Space ACM SIGIR'02 pages 6572 7 R o b ert Co o l ey  Bam sh ad M o b ash er a n d Jaid eep Srivastava Data Preparation for Mining World Wide Web Browsing Patterns Knowledge and Information Systems Volume 1 No 1 1999 8 A y h a n D e m i r i z  2002  A na l y z i ng Se r v i c e O r d e r D a t a using Association Mining Technical paper Information Technology Verizon Inc.2002 9 M  D e s hpa nde a n d G  K a r y p i s  2 001  S e l e c t i v e M a r k o v models for predicting Web-page accesses First SIAM International Conference on Data Mining SDM2001 2001  M D e s hpa nde a n d G  K a r y p is  2004  I te m B a s e d T opN Recommendation Algorithms ACM Transactions on Information Systems Vol 22 1 January 2004 Pages 143177  Y ong jia n F u K a nw a l pr e e t Sa ndhu a n d M ing Y i Shih 2000 Clustering of Web Users Based on Access Patterns Technical Paper Computer Science Department University of Missouri-Rolla 2000 12 En riq u e F rias-M artin ez an d V ijay K aram ch eti  2 0 0 2  A Prediction Model for User Access Sequences Proc WEBKDD Workshop Web Mining for Usage Patterns and User Profiles 13 S u l e G u n d i z an d M  T am er Ozsu  A W e b P age P red i c t i o n Model Based on Click-Stream Tree Representation of User Behavior SIGKDD03 August 24-27 2003 Washington DC USA 14 W e iy an g L in  S ergio A  A lvarez an d C aro lin a R u i z  2 0 0 0  Collaborative Recommendation via Adaptive Association Rule Mining Technical Paper Dept of Computer Science Worcester Polytechnic Institute 2000  D  L e w i s a nd W  A  G a l e  1 994  A s e que nt i a l a l g or i t h m for training text classifiers In Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval London Springer-Verlag pages 3-12 1 G r e g L i nde n B r e nt Sm ith a n d J e r e m y Y or k  2003  Amazon.com Recommendations Item-to-Item Collaborative Filtering IEEE Internet Computing JanuaryFebruary 2003 pages 76-80 1 I a n T i a n Y i L i  Q i a ng Y a ng  a nd K e W a ng  2 001  Classification Pruning for Web-request Prediction In Poster Proceedings of the 10th World Wide Web Conference WWW10 May 2-4 2001 Hong Kong China  B a m s ha d M oba s h e r  2004  W e b U s a g e M i n i n g a nd Recommendation Practical Handbook of Internet Computing Munindar P Singh ed CRC Press forthcoming in 2004  B a m s ha d M oba s h e r  R obe r C ool e y  a nd J a i d e e p S r i v a s t a v a 2000 Automatic Recommendation Based on Web Usage Mining Communication of the ACM August 2000 43\(8\:142-151  B a m s ha d M oba s h e r  H ong hua D a i T a o L uo Mik i Nakagawa 2001 Effective Recommendation Based on Association Rule Discovery from Web Usage Data WIDM 2001 ACM 2001 pages 9-15  B a m s ha d M oba s h e r  H ong hua D a i T a o L uo Mik i Nakagawa 2002 Using Sequential and Non-Sequential Patterns in Predictive Web Usage Mining Tasks In Proceedings of the IEEE International Conference on Data Mining ICDM2002 Maebashi City Japan December 2002 pages 669-672 22 M i ki Nakagaw a Bam s h a d M o b ash e r  2 0 0 3  A H y b ri d Web Recommendation Model Based on Site Connectivity WebKDD Workshop 2003 Washington USA August 28 2003 pages 59-70 2 A  N a nopoul os  D  K a s t s a r os  Y  M a nol opoul os  2001  Effective Prediction of Web-user Accesses A Data Mining Approach WEBKDD'01 2001 24 J P itk o w an d P  P iro lli 1 9 9 9   M in in g l o n g e st rep eatin g subsequences to Predict WWW Surfing In Proceedings of the 1999 USENIX Annual Technical Conference 1999 pages 139-150 25 C J v a n R ijsb erg e n  1 9 7 9  In f o rm atio n R etriev al Butterworths London second edition 1979 26 Bad ru l S arw ar G e o rge Kary p i s Jo sep h Ko n s tan  an d J o h n Riedl Analysis of Recommendation Algorithms for ECommerce ACM EC00 October 17-20 2000 Minneapolis Minnesota USA 27 Bad ru l S arw ar G e o rge Kary p i s Jo sep h Ko n s tan  an d J o h n Riedl 2001 Item-based Collaborative Filtering Recommendation Algorithms The Proceedings of WWW conference May 1-5 2001 Hong Kong Pages 285-295 2 Z Su Q  Y a ng  H J  Zha ng  2000  A pr e d ic tion s y s te m for multimedia pre-fetching in Internet In Proceedings of the ACM Multimedia Conference 2000 2000 29 Jo o n h e e Y o o  a n d M i ch ael Bieb er 2 0 0 0    T o w a rd s a Relationship Navigation Analysis Proceedings of the 33th Hawaii International Conference on System Sciences IEEE Press Washington D.C January 2000 30 T a k W  Yan  M at t h ew Jaco b s en  H ect o r G a rci a-M o l i n a and Umeshwar Dayal 1996 From User Access Patterns to Dynamic Hypertext Linking Fifth International World Wide Web Conference May 6-10 1996 Paris France pages 1007-1014 3 I  Zuc k e r m a n  D W  A lbr e c h t A E N i c hols o n  1999  Predicting Users Request on the WWW In Proceedings of the International Conference on User Modeling UM99 pages 275-284 3 J i a nha n Z hu J u n H ong  J ohn G  H u g h e s  2002  U s i ng Markov Models for Web Site Link Prediction ACM HT02 June 11-15 2002 College Park Maryland USA pages 169-170 Proceedings of the Seventh IEEE International Conference on E-Commerce Technology \(CEC05 1530-1354/05 $20.00  2005 IEEE 


Figure 5. Similarity curve with system in different states  The experimental results denote that we can find an ideal parameter-set by genetic algorithm. Using the 1991 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005  parameter-set in anomaly detection, we can differentiate normal state of the protected system from anomalous state to the largest extent. Obviously, the membership functions generated by the genetic algorithm is more efficient than those defined with experts  domain knowledge 5.  Conclusions and Future Work The goal of applying genetic algorithm in anomaly detection is to search ideal membership functions. The main advantage is that we can define membership function randomly without experts  domain knowledge. For an IDS genetic algorithm is applied only in system design. The optimized membership functions will be used directly in real-time detection, therefore the performance of IDS will not be influenced although genetic optimization is a time-consuming process Nevertheless, membership functions still need to be tuned constantly because that the normal state of protected system changes with time. We can adapt the normal profile characterized by rule-sets membership functions ultimately, and how to build an adaptive anomaly detection system by genetic algorithm or other intelligent methods will be investigated in future research References 1] Stefan Axelsson. Intrusion detection systems: Asurvey and taxonomy. Technical Report No. 99-15, Dept. of Computer Engineering, Chalmers University of Technology, Sweden, March 2000 2] Debar H., Dacier M., Wepspi A. A Revised Taxonomy for Intrusion Detection Systems. Technical Report Computer Science/ Mathematics. 1999 3] Wenke Lee et. al. Mining audit data to build intrusion detection models. Proc. Int. Conf. Knowledge Discovery and Data Mining \(KDD'98 1998 4] Wenke Lee et. al. Real Time Data Mining based Intrusion Detection. Proceedings of DISCEX II, 2001 5] Susan M. Bridges, Rayford B, Vaughn. Intrusion Detection Via Fuzzy Data Mining. Proc. of 12th Annual Canadian Information Technology Security Symposium, June 19-23, Ottawa, Canada, pages 109-122, 2000 6] Kuok, C., A. Fu, and M. Wong. Mining fuzzy association rules in databases. SIGMOD Record 17\(1 41-6. 1998 7] WengdongWang. Genetic Algorithm Optimization of Membership Functions for Mining Fuzzy Association Rules. International Joint Conference on Information 8] Agrawal R., R.Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th international conference on very large databases 1994 9] Guosizong, Chengang. Method of Soft Computing in Information Science. Northeastern University Press China. 2001 10] 10 Zhutianqing, Wangxianpei, Xiongping. Mining Fuzzy Association Rules and Response in IDS Computer Engineering and Applications. Vol.40\(15 pages 148-150. 2004  1992 pre></body></html 





 0 100 200 300 400 500 600 1 21416181 Time \(epoch counts 0.6 0.9 1.0 a MinMaxLoad WC 0 1000 2000 3000 4000 5000 6000 7000 8000 1 21416181 Time \(epoch counts 0.6 b SS2 Figure 4 Space needed at node R to store answer synopsis S A  non-distributed single-stream case In contrast when SS2 is used to set the precision gradient Figure 4b the space requirement is almost an order of magnitude greater This difference in synopsis size occurs because in SS2 frequency counts are only pruned from synopses at leaf nodes so counts for all items that are locally frequent in one or more local streams reach the root node No pruning power is reserved for the root node and therefore no count in S A is ever discarded irrespective of the  value The same situation occurs if Algorithm 2b is used instead of Algorithm 2a This result underscores the importance of setting  1  in order to limit the size of S A  as discussed in Section 2.1 3.4 Communication Load Figure 5 shows our communication measurements under each of our two metrics for each of our three data sets under each of the ve strategies for setting the precision gradient listed in Table 3 First of all as expected the overhead of SS1 is excessive under both metrics Second by inspecting Figure 5a we see that strategy MinRootLoad does indeed incur the least load on the root node R in all cases as predicted by our analysis of Section 2.1.2 Under this metric MinRootLoad outperforms both simple strategies SS1 and SS2 by a factor of 0 500 1000 1500 2000 2500 3000 3500 4000 INTERNET2 AUCTION BBOARD SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC 247k 10k 296k 132k 20k counts transmitted \(per epoch a Load on root node R 0 500 1000 1500 2000 2500 3000 3500 4000 INTERNET2 AUCTION BBOARD SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC 43k    21k 52k   19k 23k   7k counts transmitted \(per epoch b Maximum load on any link Figure 5 Communication measurements k denotes thousands ve or more in all cases measured However MinRootLoad performs poorly in terms of maximum load on any link as shown in Figure 5b because no early elimination of counts for infrequent items is performed and consequently synopses sent from the grand-children of the root node to the children of the root node tend to be quite large As expected MinMaxLoad NWC performs best under that metric on all data sets For the AUCTION data set even though SS2 outperforms MinMaxLoad WC to be expected because of the high  value our hybrid strategy MinMaxLoad NWC is superior to SS2 by a factor of over two For the I NTERNET 2 and BBOARD data sets the improvement over SS2 is more than a factor of three On the negative side total communication not shown in graphs is somewhat higher under MinMaxLoad WC than under SS2 increase of between 7.5 and 49.5 depending on the data set 4 Related Work Most prior work on identifying frequent items in data streams 6 8 9 19 only considers the single-stream case While we are not aware of any work on maintaining frequency counts for frequent items in a distributed stream setting work by Babcock and Olston does adProceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


dress a related problem In the problem is to monitor continuously changing numerical values which could represent frequency counts in a distributed setting The objective is to maintain a list of the top k aggregated values where each aggregated value represents the sum of a set of individual values each of which is stored on a different node The work of assumes a single-le v e l communication topology and does not consider how to manage synopsis precision in hierarchical communication structures using in-network aggregation which is the main focus of this paper The work most closely related to ours is the recent work of Greenwald and Khanna which addresses the problem of computing approximate quantiles in a general communication topology Their technique can be used to nd frequencies of frequent items to within a congurable error tolerance The work in focuses on providing an asymptotic bound on the maximum load on any link our result adheres to the same asymptotic bound It does not however address how best to congure a precision gradient in order to minimize load which is the particular focus of our work 5 Summary In this paper we studied the problem of nding frequent items in the union of multiple distributed streams The central issue is how best to manage the degree of approximation performed as partial synopses from multiple nodes are combined We characterized this process for hierarchical communication topologies in terms of a precision gradient followed by synopses as they are passed from leaves to the root and combined incrementally We studied the problem of nding the optimal precision gradient under two alternative and incompatible optimization objectives 1 minimizing load on the central node to which answers are delivered and 2 minimizing worst-case load on any communication link We then introduced a heuristic designed to perform well for the second objective in practice when data does not conform to worst-case input characteristics Our experimental results on three real-world data sets showed that our methods of setting the precision gradient are greatly superior to na  ve strategies under both metrics on all data sets studied Acknowledgments We thank Arvind Arasu and Dawn Song for their valuable input and assistance References  R Agra w a l and R Srikant F ast algorithms for mining association rules In VLDB  1994  I Akamai T echnologies Akamai http://www akamai.com   A Ak ella A Bharambe M Reiter  and S Seshan Detecting DDoS attacks on ISP networks In PODS Workshop on Management and Processing of Data Streams  2003  A Arasu and G S Manku Approximate quantiles and frequency counts over sliding windows In PODS  2004  B Babcock and C Olston Distrib uted top-k monitoring In SIGMOD  2003  M Charikar  K  Chen and M F arach-Colton Finding frequent items in data streams In International Colloquium on Automata Languages and Programming  2002  E Cohen and M Strauss Maintaining time-decaying stream aggregates In PODS  2003  G Cormode and S Muthukrishnan What s hot and whats not Tracking frequent items dynamically In PODS  2003  E D Demaine A Lopez-Ortiz and J I Munro Frequency estimation of internet packet streams with limited space In European Symposium on Algorithms  2003  DynaServ er R UBis and R UBBos http://www.cs rice.edu/CS/Systems/DynaServer   eBay Inc eBay  http://www.ebay.com   C Estan and G V a r ghese Ne w directions in traf c measurement and accounting In SIGCOMM  2002  M F ang N Shi v akumar  H  Garcia-Molina R Motwani and J Ulmann Computing iceberg queries efciently In VLDB  1998  P  B Gibbons and Y  Matias Ne w sampling-based summary statistics for improving approximate query answers In SIGMOD  1998  P  B Gibbons and S T irthapura Estimating simple functions on the union of data streams In Symposium on Parallel Algorithms and Architectures  2001  M Greenw ald and S Khanna Po wer conserving computation of order-statistics over sensor networks In PODS  2004  J Han J Pei G Dong and K W ang Ef cient computation of iceberg queries with complex measures In SIGMOD  2001  Internet2 Internet2 Abilene Netw ork http abilene.internet2.edu   R M Karp S Shenk er  and C H P apadimitriou A simple algorithm for nding frequent elements in streams and bags ACM Trans Database Syst  2003  H.-A Kim and B Karp Autograph T o w ard automated distributed worm signature detection In Proceedings of the 13th Usenix Security Symposium  2004  A Manjhi V  Shkapen yuk K Dhamdhere and C Olston Finding recently frequent items in distributed data streams Technical report 2004 http://www cs.cmu.edu/manjhi/freqItems.pdf   G S Manku and R Motw ani Approximate frequenc y counts over data streams In VLDB  2002  Open Source De v elopment Netw ork Inc Slashdot http://slashdot.org  Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


12] S. Fujiwara, J. D. Ullman and R. Motwani, Dynamic Miss-Counting Algorithms: Finding Implications and Similarity Rules with Confidence Pruning. Proceedings of the IEEE ICDE \(San Diego, CA 13] F. Glover  Tabu Search for Nonlinear and Parametric Optimization \(with Links to Genetic Algorithms  Discrete Applied Mathematics 49 \(1-3 14] M. Klemettinen, H. Mannila, P. Ronkainen, H Toivonen, and A. Verkamo, Finding interesting rules from large sets of discovered association rules. Proceedings of the ACM CIKM, International Conference on Information and Knowledge Management \(Kansas City, Missouri 1999 15] C. Ordonez, C. Santana, and L. de Braal, Discovering Interesting Association Rules in Medical Data. Proceedings of the IEEE Advances in Digital Libraries Conference Baltimore, MD 16] W. Perrizo, Peano count tree technology lab notes Technical Report NDSU-CS-TR-01-1, 2001 http://www.cs.ndsu.nodak. edu /~perrizo classes/785/pct.html. January 2003 17] Ron Raymon, An SE-tree based Characterization of the Induction Problem. Proceedings of the ICML, International Conference on Machine Learning \(Washington, D.C 275, 1993 18] N. Shivakumar, and H. Garcia-Molina, Building a Scalable and Accurate Copy Detection Mechanism Proceedings of the International Conference on the Theory and Practice of Digital Libraries, 1996 19] P. Tan, and V. Kumar, Interestingness Measures for Association Patterns: A Perspective, KDD  2000 Workshop on Post-processing in Machine Learning and Data Mining Boston, 2000 20] H.R. Varian, and P. Resnick, Eds. CACM Special Issue on Recommender Systems. Communications of the ACM 40 1997 Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE pre></body></html 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


