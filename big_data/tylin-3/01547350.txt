Clustering Research Using Dynamic Modeling Based on Granular Computi Qun Liu Department of computer and science ChongQing University of posts and telecommuncations ChongQing,400065 CHINA liuqun@cqupt.edu.cn WenBiao Jin Department of computer and science ChongQing University of posts and telecommuncations ChongQing,400065 CHINA jinwb@cqupt.edu.cn SiYuan Wu Department of computer and science ChongQing University of posts and telecommuncations ChongQing,400065 CHINA wusy@cqupt.edu.cn ng YingHua Zhou Department of 
computer and science ChongQing University of posts and telecommuncations ChongQing,400065 CHINA zhouyh\(cqupt.edu.cn Abstract-Clustering techniques is a discovery process in data mining especially used in characterizing customer groups based on purchasing patterns categorizing Web documents and so on Many of the traditional clustering algorithms falter when the dimensionality of the feature space becomes high relativing to the size of the document space So it is important 
to precondition for modeling Secondly we are usually disappointed to their clustering speed when having very large complex data sets and another defect is that they always fit some static model so if the user doesn't select appropriate static-model parameters these algorithms can break down In this paper we introduce a new clustering algorithm to improve the speed of clustering this clustering technique which is based on granular computing and 
hypergraph partition and it is capable of automatically discovering document similarities or associations and this approach considers the internal characteristics of the clusters themselves thus it doesn't depend on a static model Finally we conduct several experiments on real Web data searched by ordinary search engine and received the satisfied results Index terms-Association rule discovery Clustering research Dynamic model Frequent item sets Granular computing Hyper-graph partition algorithm Web 
documents I  INTRODUCTION The World Wide Web is a vast resources of information and services that continues to grow rapidly Powerful search This work is supported in part by ChongQing Municipal Education Commission the contract No is 020502 engines have been developed to aid in locating unfamiliar documents by category contents or keywords Unfortunately queries often return inconsistent results with document retrieved that meet the search keywords but are of no interest 
to the users Clustering is a discovery process in data mining It groups a set of data in a way that maximizes the similarity within clusters and minimizes the similarity between two different clusters These discovered clusters can help explain the characteristics of the underlying data distribution and serve as the foundation for other data mining and analysis techniques but how to select the clustering model and the correctness of 
clustering results will affect the quality of knowledge discovery Granular computing is geared toward representing and processing basic chunks of information-information granules It can be studied in many fields such as image patition decision tree Rough set theory and so on Traditional clustering algorithms either define a distance or similarity among documents or use probabilistic techniques such as Bayesian classification Many of these algorithms however break 
down as the very large complex data sets such as the documents on the World Wide Web There are a number of problems among these traditional algorithms First they need to define many parameters to give the threshold of distance and connection respectively second distance-based schemes generally require to choose the initial value for document clusters and the choice is at random In a very large 0-7803-9017-2/05/$20.00 2512005 IEEE 539 


complex data sets  the randomly chosen means to receive an unsatisfied effect In order to overcome the above defects  in this paper we introduce the granular computing based on binary system and give a data model From the model  we extract the frequent item sets then these frequent item sets are mapped into hyperedges in a hypergraph and at first a coarsening process is used to create a small hypergraph and a good bisection of the small hypergraph is not worse than the bisection directly obtained from the original hypergraph the clustering speed is improved simultaneously At last a multi-level hypergraph partitioning algorithm is used to partition the coarsening hypergraph into k parts two criterions fitness and connectivity are defined so as to prune the bad clusters and bad vertices and a synthesize criterion is defined to select the good partition level In section 2 we describe how to use the binary granular computing to construct the hypergraph model In section 3 we introduce clustering method of hypergraph and give two criterions and a synthesize criterion to filter out of the bad clusters Finally we conduct several experiments on real Web data and present ideas for future research II MODELING FOR DATA SETS A Introducing Granular Computing Information granules are collections of entities as the name itself stipulates the entities are arranged together due to their similarity functional adjacency or alike The process of forming information granules is referred as information granules there are several essential factors that drive all pursuits of information granulation These factors include a need to split the problem into a sequence of more manageable and smaller subtasks and a need to comprehend the problem and provide with a better insight into its essence rather than get buried in all unnecessary details Let S=\(U,A be an information system[l where U={ul,u2,.....,un is a set of all objects A is a set ofattributes BcA is a subset of attributes it can divide U into data blocks these data blocks are defined as granules so we divided U according to attributes and we get the quotient sets U/IND\(A Each quotient set is a granule Then we can mark the number for each element in every granule according to its location in set U In each granule all of the numbers can be described as a binary so the granule can be defined as a binary For example if the granule includes uj the corresponding place i.e the number i  in the binary can be put 1 otherwise if the granule doesn't include ui then the corresponding place in the binary can be put 0 Obviously the length of this binary is equal to the radix of U The association rule discovery methods are often used in the field of data mining[1 so we can find the association rule using granular computing according to its principle Given the granule X and Y if Xrn Y they will do Boolean AND caculation the number of 1 in their intersected result 2 s%\(a given threshold then the association rule X,Y may be adopted For convenience  XrnY may be marked X,Y Obviously the format XrnY may be expanded to intersection of many finite granules Xl1nX2n   nXnn   nY 1nY2n  nyn The combination above can be seen as an association rule and the calculation of intersection between two granules is boolean AND calculation So if we use the method above the speed of data mining on many searched WEB pages will be improved greatly B Modeling The Data Based on Binary Granular Computing At first we can use the ordinary search engines to receive documents for a certain field  the retrieval documents will then be stored into the database Firstly we summary some relative keywords in this field and calculate the number of different keywords in the documents respectively as figure 1 below Then we can give some definitions for our model Definition 1 let S=\(U,A be a information system where U={business capital fund finance  invest is a set of all keywords in a certain field A={docl,doc2 docn is a set of all retrieved documents of this field BcA is a subset of documents it can divide U into different equivalence classes then we get the quotient sets U/IND\(A Definition 2 as shown in figure 1 if the fraction of the number of a keyword and the number of all words in document 260n a given threshold that is the percent of the number of a keyword to the number of all words in document  the corresponding place in the binary will be put l,obviously the length of the binary constructed by each granule is equal to the radix of the set U 540 


business capital fund finance  invest Docl 20 35 10 40  0 Doc2 30 45 0 20  0 Doc3 0 25 0 35  27        Docn 16 12 30 0 50 Fig 1 a Typical Document--Feature Set For example in figure 1 the binary constructed by the granule Doc is described to 1101  0 Definition 3 Let us define the support s it is equal to the number of 1 in the value gotten after two granules had a Boolean AND calculation Such value can also have such calculation with other granules or values received through this method The task of discovering an association rule is to find all rules XnY in which s is greater than a given minimum support threshold this threshold may have to be determined in a specific manner Definition 4 Let us define the frequent item sets the frequent item sets are computed by an association rule algorithm such algorithm only find frequent item sets that have support greater than a specified threshold the frequent item sets capture the relationships among documents of size greater than or equal to 2 Note that the frequent item sets can capture relationship among larger sets of data points  As these definitions shown above the frequent item sets discovery is composed of four steps The first step is to receive the documents and keywords by an ordinary search engine The second step is to discover the granules of this information system The third step is to generate association rules The last step is to discover all frequent sets by an association rule algorithm such as Aprior A hypergraph H=\(V,E consists of a set of vertices V and a set of hyperedges\(E A hypergraph is an extension of a graph in the sense that each hypergraph can connect more than two vertices In our model the set of vertices V corresponds to the set of documents being clustered and each hyperedge eE E corresponds to a set of related documents A key problem in modeling data items as a hypergraph is to determine which related items can be grouped as hyperedges and determine the weights of each hyperedge In our paper hyperedges represent the frequent item sets found by association rule discovery algorithm and the weights of hyperedges represent the support of each frequent item set So a mathematics model is received and can be used for hierarchical clustering based hypergraph III MULTILEVEL HYPERGRAPH BISECTION The flow chat of the hierarchical clustering algorithm based on hypergraph is described as the figure 2 The basic idea of this algorithm is explained below at first the most important problem is modeling data items as a hypergraph the set of vertices of a hypergraph represents the group of related items which will be used to cluster the hyperedges with the weights represent the similarity among items Secondly a multi-level hypergraph partitioning algorithm may be used to partition hypergraph and produce many sub-clusters Thirdly we eliminate bad clusters using the cluster fitness criterion including fitness and connectivity Then the result of clustering can have a optimized treatment at last we may achieve the best clusters A Description of a Clustering Algorithm The clustering algorithm is divided into two steps The first step is to cluster all items as a whole then partition it into many sub-clusters The second step is to select the optimized sub-clusters We have a simple description for the two steps below 541 


coarsening Partition Sparse graph Sparse graph Sparse graph Sub-clusters Fig2 the flow chat of the three-phase algorithm based on hypergraph Step 1 At present the bisection for hypergraph has been solved very well especially using the multi-level hypergraph partitioning algorithm which can partition very large hypergraphs in minutes on personal computers For this task we use HMETIS[4 its basic partitioning processes are composed of four steps 1 initialization clustering all related items as a hypergraph 2 coarsening Some vertices can be merged together to form single vertices we have developed an algorithms for coarsening If a granular's binary value is similar to another granular's and they are on the same hyperedge they can merge together 3 one way to achieve this is to use the partitioning algorithm which can partition the hypergraph into two parts so that the sum of each hyperedge's weight in every part cut by the partitioning is minimized Note that minimizing the sum of each hyperedge's weight in every part partitioned can minimize the separation among data items in the same frequent item sets 4 iteration each of these two parts can be further bisected recursively until each partition is highly connected and the overall hypergraph has been partitioned into k parts.[4 Step 2 Comparing the received sub-clusters by using two criterions-fitness and connectivity when the sum of fitness and the sum of connectivity of each sub-cluster in the same partitioned hierarchy are up to maximum at this time all sub-clusters in the hierarchy are optimization Then we compare the sub-clusters with their fitness and connectivity eliminate bad clusters and bad data items which have worse similarity with others among the sub-clusters.[6 B Optimizing The Cluster Process As we have seen we can receive a general hierarchy i.e a tree of clusters  by using a hierarchical clustering algorithm based on hypergraph The root cluster contains all data items there are different sets of sub-clusters achieved by partition on each level there is a best level which includes the optimizing sub-clusters in this tree of clusters How to choose the best level and its sub-clusters is very important In our paper we use two criterions-fitness and connectivity to filter out the best level Obviously  the best clusters should have two characteristics 1 It must be far away among sub-clusters in other words each sub-cluster should have good fitness 2 A sub-cluster's internal closeness among items should be strong in other words there should be stronger connectivity within a sub-cluster The following defmitions are effective principles  The fitness er_ cWeight\(e fitness\(C ZeC Weight\(e 1 enCI>O Let e be a set of vertices representing a hyperedge and C be a set of vertices representing a partition The fitness function measures the ratio of weights of edges that are within the partition and weights of edges involving any vertex of this partition Thus it can be seen that if the fitness is stronger the independence of this sub-cluster is better So the fitness of a level is the sum of each sub-cluster's fitness which is included in this level Let N be the number of the sub-clusters on this layer the sum is defined as sumf N sumf\(N E fitness\(Ci i=l 2 Ci represents the ith sub-cluster on a level The connectivity of a vertex within the sub-clusters is i{elecC,ve e connectivity\(v C I{e ec V  le e c C 3 542 Construct Optimize 





Proceedings of the Fifth International Conference on Mach ine Learning and Cybernetics, Dalian, 13-16 August 2006  1081 Table 2. The table of experiment result  Generators Closed itemsets Implications  A   AC  AC   B   BE  BE   C   C    D     AC D  DAC   E   BE  EB   AB   ABC E  AB CE   AE   ABC E  AE BC   BC   BC E  CB E   CE   BC E  CE B  6.  Conclusions  The attribute implication rules in formal concept analysis can give the plausible dependencies in data that can be used for data recovery, cl assification, etc. In general the size of the implicational system is huge, even for a small context. And some implication rules are redundant in the sense that it could be retrieved from smaller sets of implication rules. The canonical implication basis is a minimal no-redundant implication set from which all valid implication can be inferred by the Armstrong rules. In this paper, we discuss how to construct the canonical basis in formal context. By the theorem in [5 e k e ys o f  constructing the canonical basis are finding all pseudo-closed itemsets and computing their closures. In order to finding all pseudo-closed, one must find all closed itemsets and their generators. We write out the pseudo-code of algorithms for above computing works. Experimental result show that the algorithms presented in this paper are feasible and efficient for large database Acknowledgements This paper is supported by the Homecoming Fund of North China Electric Power University \(200514001 References    R  Ag ra wal an d R  Sri k a n d. Fa st al gori t h m s for m i ni ng  association rules. Proceeding of the 20th Intíl Conference on Very Large Data Bases, pages 478-499 June 1994, Expanded version in IBM Research Report RJ9839 2  R. Agrawal T  Im ielin sk i, an d A. Swami. Min i n g  association rules between sets of items in large databases. Proc. SIGMOD conf., pp 207-216, May 1993 3  G  Birk ho f f  Lattices th eo ry In Co ll. Pu b. XXV   volume 25. American Mathem atical Society, 1967 Third edition   N. C a s p a r d an d B  M o nja r det  S o m e l a tt i ces o f cl os u r e  system on a finite set. Discrete Mathematics ang Theoretical Computer Sciences, 6:163-190, 2004   5  Dav i d Maier  Th e th eo ty o f  relatio n a l d a ta b a se Computer Science Press, Rockville 1983  6  Gan t er B. W ille R. Form al Co n cep t An alysis Mathematical Foundations[M  Spri nger, 1999  7  N  Pasq u i er Y. Bastid e, an d L. Lakh al D i sco v e r i ng fr  equent closed itemsets for association rules. In LLNCS Springer Verlag, editor, ICDTí99, 1540 398-416,1999 8  G e rd  Stu m m e  A ttr ibu t e exp l o r ation  w ith back gro und implications and exceptions. In:Hans-Herrmann Bock Wolfgang Polasek, editors. Data analysis and information system. Springer, Berlin-Heidlberg-New York, 1996,pp. 457ff  9  W ille R. Reco n s t r u c ting lattice th eory: an app r o a ch based on hierarchies of co ncepts. In: Rival, I ed Ordered Sets Dordrecht-Boston: Reidel, 1982 445-470  W i l d  M. A theory of finite closure s p ace base d on implications. Advances in Mathematics, 108:118-139 1994  


nal published from April 2 1990 through September 28 1990 for the performance comparison between PMIHP and Count Distribution as well a s between MIHP and Apriori The sample contains 21,703 documents and 116,849 unique words We varied the minimum support level from 5 to 1.75 to measure its impact on the miners For PMIHP and Count Distribution the 6-month sample was sequentially distributed to the processing nodes by assigning the articles of 16 or 17 days to each node Figure 4 shows the total execution times of Apriori FPGrowth and MIHP and Figure 5 shows the total execution times of PMIHP and Count Distribution when both algorithms were run on 8 nodes The Apriori and Count Distribution algorithms were not able to run within the memory constraint of 416 Mbytes when the minimum support level is below 2 The memory requirement for the candidate itemsets is the limiting factor for both Apriori and Count Distribution MIHP has much better performance than Apriori because MIHP prunes many candidate itemsets by using the Inverted Hashing and Pruning and processes a limited number of candidate itemsets at a time based on the Multipass approach More performance analysis result of MIHP is available in 12 The FP-Growth algorithm performed well at the higher minimum support levels However its performance deteriorated at the 2 minimum support level The FP-tree becomes too large when the minimum support level is low and as a result the total execution time increases sharply Lower minimum support levels were not attempted As shown in Figure 5 PMIHP performs signiﬁcantly better than Count Distribution and the performance gain increases as the minimum support level decreases It is important to note that the minimum support levels were selected in this test such that the miners would run within the constraint of the main memory and thus eliminates the effect of paging upon the performance Comparing Figures 4 and 5 we can see that the speedup of Count Distribution over Apriori is fairly good When the minimum support level is 2 it is about 6 whereas the speedup of PMIHP over MIHP is about 4 However as we shall see below the speedup of PMIHP is quite good at a lower minimum support level of 0.15 Since the amount of computation increases rapidly as the minimum support level decreases the speedup improvement at low minimum support levels is quite encouraging To evaluate the effect of the number of processing nodes on the performance of PMIHP we used a 8-day sample of the Wall Street Journal starting from October 1 1991 There were 1,427 documents and 31,290 unique words We used a minimum support count of 2 documents i.e minimum support of 0.15 and a stop-word list from Fox 8  There were 12,828 frequent words The minimum support count of two documents was selected based upon the result  Apriori  FP-Growth  MIHP Minimum Support Time \(seconds              0 100000 200000 300000 400000 500000 5.00 4.00 3.00 2.00 1.75 Figure 4 Total execution time to nd all frequent itemsets in 21,703 documents  CD  PMIHP Minimum Support Time \(seconds          0 20000 40000 60000 80000 5.00 4.00 3.00 2.00 1.75 Figure 5 Total execution time to nd all frequent itemsets in 21,703 documents on 8 nodes of our experiments showing that low minimum support levels are required to use the frequent itemsets for document retrieval and ranking We did not stem the words but we monocased the words The database was assigned to the processing nodes sequentially by day For the 2-node case one node was assigned the articles of the rst 4 days and another node was assigned the articles of the last 4 days The assignments for the 4-node and 8-node cases were done in a similar manner These daily collections have a mean of 178 a standard deviation of 22.58 and a median of 174 documents Figure 6 shows the total execution time of PMIHP required to nd frequent 3-itemsets as the number of processing nodes changes and Figure 7 shows the corresponding speedup over the sequential processing We can see that the speedup increases as the number of processing nodes increases and the increasing rate is slightly higher than lin0-7695-2132-0/04/$17.00 \(C Proceedings of the 18th International Parallel and Distributed Processing Symposium \(IPDPSê04 


ear The speedup is 1.65 for the 2-node system indicating some degree of parallelization overhead mainly due to the interprocess communication to exchange the support count information The speedup is 3.76 for the 4-node system but the increasing rate of the speedup is 2.27 as the number of nodes is doubled from 2 to 4 As the number of nodes is doubled from 4 to 8 the increasing rate of the speedup is higher than linear again which indicates that PMIHP is quite scalable Number of Processors Time \(seconds 0 20000 40000 60000 80000 1 2 4 8 Figure 6 Total execution time of PMIHP to nd frequent 3-itemsets in 1,427 documents minsup  0.15 Number of Processors Speedup    0 2 4 6 8 2 4 8 Figure 7 Speedup of PMIHP minsup  0.15 The PMIHP algorithm has two main data mining activities counting the support of local candidate itemsets in the corresponding local database and counting the support of global candidate itemsets in multiple local databases These two activities are interleaved during the mining as the global support counting is invoked when the number of identiﬁed global candidate itemsets exceeds a certain number at each processing node which was set to 20,000 in our experiments To measure this global support counting time we reconﬁgured PMIHP to defer the global support counting of the global candidate itemsets at each node and synchronized the nodes before the start of the global support counting phase Figure 8 shows the global support counting time of the mining process with the logest run time among all the mining processes executed on different processing nodes Moreover we used the wall clock time to measure this global counting time hence it is an upper bound of the actual global support counting time of all the mining processes Comparing Figures 6 and 8 we can see that the 2-node case has a much longer global support counting phase than 4-node and 8-node cases The portion of the global support counting phase for the 2-node case is about 8 of the total execution time but it is about 4 for the 4-node case and about 3 for the 8-node case Thus the impact of the global support counting time on the overall speedup is very small and it is reduced further as the number of processing nodes increases Number of Processors Time \(seconds 0 1000 2000 3000 4000 2 4 8 Figure 8 Global support counting time to nd frequent 3-itemsets Since our processing environment does not provide the statistics for job accounting exact CPU time measurement was not feasible So we measured the average execution of a processing node using the wall clock time Figure 9 shows the average execution time of a node in the 1-node 2-node 4-node and 8-node conﬁgurations We can see that the 2-node case requires signiﬁcantly less average execution time per node than the 1-node case and as the number of processing nodes increases further the average execution time per node deceases more than linearly This result is completely consistent with the observed speedup values and also indicates that increased efﬁciency is behind the performance gain Since the identical PMIHP algorithm was executed on all of our system conﬁgurations the differences in execution time must be associated with some workload differences Figure 10 shows the average number of candidate 0-7695-2132-0/04/$17.00 \(C Proceedings of the 18th International Parallel and Distributed Processing Symposium \(IPDPSê04 


Number of Processors Time \(seconds 0 20000 40000 60000 80000 1 2 4 8 Figure 9 Average execution time per node to nd frequent 3-itemsets 2-itemsets processed by each node in our four different system conﬁgurations Note that the number of candidate 2itemsets for the 1-node case is approximately the same as the average number of candidate 2-itemsets for the 2-node case This result is consistent with the observed total and average execution times for the 1-node and 2-node cases There is signiﬁcant reduction in the average number of candidate 2-itemsets processed for the 4-node and 8-node cases over the 1-node and 2-node cases This result represents the nonuniform distribution of itemsets over the local databases as well as the effective reduction of the candidate itemsets by the Inverted Hashing and Pruning technique Number of Candidate 2-itemsets 0 100000 200000 300000 400000 MIHP 2-node PM IHP 4-node PM IHP 8-node PM IHP Figure 10 Average number of candidate 2itemsets per node Figure 11 shows the average number of candidate 3itemsets processed by each node We included the number of candidate 3-itemsets processed in Apriori to demonstrate the usefulness of the Inverted Hashing and Pruning The number of candidate 2-itemsets for Apriori was about 82 million which is why we did not show that in Figure 10 We can observe the same pattern of reduction in the candidates 3-itemsets as in the candidate 2-itemsets This reduction in the average number of candidate itemsets processed by each processing node may be the most clear explanation for the high increasing rate of the speedup observed as the number of processing nodes increases Number of Candidate 3-itemsets 0 200000 400000 600000 800000 Apriori MIHP 2-node PM IHP 4-node PM IHP 8-node P M IHP Figure 11 Average number of candidate 3itemsets per node We also ran a test with a larger database 8 weeks of the Wall Street Journal published from January 2 1991 through February 22 1991 February 23rd was a Wall Street Journal holiday There were 6,170 documents and 64,191 unique words of which 31,948 were frequent words at the minimum support level of 0.03 i.e 2 out of 6,170 documents The 1-node system required 845,702 seconds to nd 1,554,442 frequent 2-itemsets whereas the 8-node system required 33,183 seconds This performance represents a superliner speedup of 25.5 of the 8-node system over the 1-node system Thus we can conclude that the performance of PMIHP is quite scalable when the database is large and the minimum support level is low which is the case of high workload The 1-node case generated 16,174,357 candidate 2itemsets whereas the 8-node case generated 2,459,629 candidate 2-itemsets per node on the average The total number of candidate 2-itemsets counted by the 8 nodes were 19,677,031 which means that only 21.7 of the candidate 2-itemsets were counted at more than one processing node This implies that the distribution of words across the 8-week sample of the Wall Street Journal is quite skewed In the Count Distribution algorithm all the nodes count the same set of candidate itemsets in each pass over the database regardless of the distribution of items over the local databases On the other hand in our PMIHP algorithm not all candidate itemsets are counted at more than one node when the distribution of items over the local databases is not uniform Obviously the more skewed the data distribution the better the performance of PMIHP Cheung et al 4 propos ed s e v e ral approaches to partition the databas e 0-7695-2132-0/04/$17.00 \(C Proceedings of the 18th International Parallel and Distributed Processing Symposium \(IPDPSê04 


to achieve a high degree of skewness Text documents arranged in a chronological order do appear to have a high degree of skewness and beneﬁt the PMIHP algorithm 4 Conclusions The proposed Parallel Multipass with Inverted Hashing and Pruning PMIHP algorithm is a parallel version of our Multipass with Inverted Hashing and Pruning MIHP algorithm and it is effective for mining frequent itemsets in large text databases The Multipass approach reduces the required memory space at each processor by partitioning the frequent items and pro cessing each partition separately Thus the number of candidate itemsets to be processed is limited at each instance The Inverted Hashing and Pruning is used to prune the local and global candidate itemsets at each processing node and it also allows each processing node to determine the other peer processing nodes to poll in order to collect the local support counts of each global candidate itemset PMIHP distributes the workload to multiple processing nodes to reduce the total mining time without incurring much parallelization overhead The average number of candidate itemsets to be counted at each processing node is much smaller than the case of sequential mining while the time for the synchronization between processing nodes to exchange the count information for the global candidate itemsets is very small compared to the total execution time PMIHP is able to exploit the natural skewed distribution of words in text databases and demonstrates a superlinear speedup as the number of processing nodes increases It has a much better performance than well-known parallel Count Distribution algorithm 2 becaus e the a v erage number of candidate itemsets to be counted at each processing node is much smaller especially when the minimum support level is low Overall the performance of PMIHP is quite scalable even when the size of the text database is large and the minimum support level is low which is the case of high workload References  R  A gra w al and R  S r i kant   Fast Al gori t h ms for M i n i n g A ssociation Rules Proc of the 20th VLDB Conf  1994 pp 487–499  R Agra w a l and J C S hafer  Paral l e l M i n i n g o f A ssoci at i o n Rules IEEE Trans on Knowledge and Data Engineering  Vol 8 No 6 1996 pp 962–969 3 M  S  C hen J Han and P  S  Y u   Dat a Mi ni ng An Overview from a Database Perspective IEEE Trans on Knowledge and Data Engineering  Vol 8 No 6 1996 pp 866–883 4 D  W  C heung S  D L ee and Y  Xi ao  E f f ect of Dat a S k e w ness and Workload Balance in Parallel Data Mining IEEE Trans on Knowledge and Data Engineering  Vol 14 No 3 2002 pp 498–514 5 S  M  C hung and J Y ang  A Par al l e l D i s t r i b ut i v e J oi n A l gorithm for Cube-Connected Multiprocessors IEEE Trans on Parallel and Distributed Systems  Vol 7 No 2 1996 pp 127–137  R  F e l dman and H Hi rsh F i ndi ng Associ at i ons i n Col l ections of Text Machine Learning and Data Mining Methods and Applications  R Michalski I Bratko and M Kubat editors John Wiley and Sons 1998 pp 223–240  R F e l dman I Dagen and H  H i rsh Mi ni ng T e xt Usi n g Keyword Distributions Journal of Intelligent Information Systems  Vol 10 No 3 1998 pp 281–300  C  Fox L e x i cal Anal ysi s and S t opl i s t s   Inforamtion Retrieval Data Structures and Algorithms W.FrakesandR Baeza-Yates editors Prentice Hall 1992 pp 102–130 9 M  G or don and S  Dumai s Usi ng L a t e nt S e mant i c I nde xi ng for Literature Based Discovery Journal of the Amer Soc of Info Science  Vol 49 No 8 1998 pp 674–685  J Han J P e i  and Y  Y i n Mi n i n g F r equent Pat t e r n s w i t hout Candidate Generation Proc of ACM SIGMOD Intêl Conf on Management of Data  2000 pp 1–12  J D Holt and S  M Chung Multipass Algorithms for Mining Association Rules in Text Databases Knowledge and Information Systems  Vol 3 No 2 Springer-Verlag 2001 pp 168–183  J D Hol t and S  M C hung Mi ni ng Associ at i o n R ul es Using Inverted Hashing and Pruning Information Processing Letters  Vol 83 No 4 Elsevier Science 2002 pp 211–220  J D Hol t and S  M C hung Mi ni ng associ at i o n R ul es i n Text Databases Using Multipass with Inverted Hashing and Pruning Proc of the 14th IEEE Intêl Conf on Tools with Artiìcial Intelligence  2002 pp 49–56  J S  Park M S  C hen and P  S  Y u   Usi n g a Hash-B a sed Method with Transaction Trimming for Mining Association Rules IEEE Trans on Knowledge and Data Engineering  Vol 9 No 5 1997 pp 813–825  G  S a l t on Automatic Text Processing The Transformation Analysis and Retrieval of Information by Computer  Addison-Wesley Publishing 1988  E  M V oorhees and D K Harmon edi t o rs The Fifth Text Retrieval Conference  National Institute of Standards and Technology 1997  O R  Z a i a ne and M L  Ant o i ne C l assi f y i n g T e x t D ocuments by Associating Terms with Text Categories Proc of the 13th Australian Database Conf  2002 0-7695-2132-0/04/$17.00 \(C Proceedings of the 18th International Parallel and Distributed Processing Symposium \(IPDPSê04 


  11 could be improved by a simple modification of the feed by adding a small tuning vane to th e feed. Therefore, it can be stated that some improvement can be expected by modification of the feeds, and adaptation of the test antenna in such a way that surrounding Ku-band element are closed   Figure 28 Reflection coefficient of Ku-band stacked patch antenna element in dual-frequency antenna stack  Figure 29 shows the influence of the L-band slots on the return loss of the Ku-band antenna element. To this end, the four connectors of the L-band elements were alternately open and terminated by means of 50 loads. The deviations were measured with respect to the set-up where all connectors were terminated Apparently, the deviations are acceptable  Figure 29 Influence of L-band termination on return loss of Ku-band antenna element, with and without termination Figure 30 and Figure 31 show the isolation between the Lband and Ku-band elements in L-band and Ku-band respectively. To this end the S21-parameters have been measured. These figures reveal that the mutual coupling between the L-band and Ku-band elements is sufficiently small  Figure 30 Measured isolation between L-band and Ku-band antennas in L-band frequencies  Figure 31 Measured isolation between L-band and Ku-band antennas in Ku-band frequencies From these measurements it can be concluded that opportunities should be considered to improve the design of the dual-frequency antenna \(by optimizing the feeds of the Ku-band elements antenna and the measurement set-up \(closure of surrounding Ku-band ports and use of appropriate connectors for the open Ku-band ports 7  M ODIFIED DUAL FREQUENCY ANTENNA  In order to benefit the str ong points of the two separate designs as discussed in section 4, an alternative antenna is proposed that exploits the properties of a \221best of both worlds\222 solution employing ideas from both designs. The modified antenna possesses an aperture fed L-band patch of a similar form to first design, but situated towards the bottom of the stack. Ku band el ements are located within the L-band perforations and para sitic patches are situated above a foam spacer \(see Figure 32 and Figure 33\A measurement campaign is underway to assess the behaviour of this modified test antenna 


  12  Figure 32 Bottom view of dual frequency antenna tile with perforated L-band patc h in lower layer with Kuband patches  Figure 33 Layer stack with perforated L-band patch in lower layer with Ku-band patches  8  B EAM FORMING N ETWORK  A major keystone for the su ccess of phased array antenna onboard aircraft is the capability of steering the main beam in the direction of the geosta tionary satellites. This requires the inclusion of a broadband beam forming network. Beam steering can be realized by adding RF-phase shifters and LNA\222s to the antenna elements of the array. However traditional phase shifters in ge neral have a narrow band, and hence do not yield the re quired broadband capability Alternative technologies for broadband beam forming are switched beam networks \(using Butler matrices innovative designs for RF-compone nts such as phase shifter LNA components in \(M\IC technology, or beam forming by using opti cal ring resonators  The German SME IMST is involved in several projects for development of electronica lly steerable phased array antennas for satellite communication. In the NATALIA project \(New Automotive Track ing Antenna for Low-cost Innovative Applications\ ESA, IMST is investigating the possibility of realizing a compact costeffective solution for a recei ve-only full electronically steerable antenna for cars in Ku-band. This antenna is a planar array composed of approximately 150 patches circularly polarised by using a 90\260 hybrid, and arranged in a hexagonal fashion. Each patc h is equipped with a MMIC corechip containing a phase sh ifting unit, LNA and digital steering logic  In the Netherlands, a consortiu m \(consisting of University of Twente, Lionix BV, National Aerospace Laboratory NLR and Cyner Substrates developing in the national FlySmart project technology for a broadband optical beam forming network. For the steering of the beam of the conformal phased array a squi nt-free, continuously tunable mechanism is proposed that is based on a fully integrated optical beam forming network \(OBFN optical ring resonators \(ORRs as tunable delay elements. A narrowband continuously tunabl e optical TTD device is realized as a recirculating wa veguide coupled to a straight waveguide. This straight wave guide can behave as a bandpass filter with a periodic, bell-shaped tunable group delay response. The maximum group delay occurs at a tunable resonance frequency. A larger delay-bandwidth product can be achieved by cascading multiple ORR sections. A complete OBFN can be obtaine d by grouping several delays and combining elements in one optical circuit. Such an OBFN can be realized on a si ngle-chip. Electrical/Optical E/O O E by means of filter based single-sideband modulation suppressing the carrier lanced coherent optical detection. Further details of the optical beamforming network have been presented in Re The proof-ofconcept has been shown by manufacturing a chip for an 8x1 OBFN. Essential components of the OBFN are the optical modulators, which are used to modulate the light in the ORR system 9  C ONCLUSIONS  For enhanced communicati on on board aircraft, novel antenna systems with broa dband satellite-based capabilities are required. So far, existi ng L-band satellite based systems for communications are used primarily for passenger application \(APC\i nistrative communications AAC and now data are tending to evolve towards broadband dig ital applications \(Voice over IP\any studies are going on worldwide to employ Kuband TV geostationary sate llites for communication with mobile terminals on aircraft The inbound traffic is about 5 times higher than the outbound The inbound traffic requires the availability of a broadband Ku-band antenna in receive mode only. The outbound traffic services can be supplied by the Inmarsat SBB link, whic h requires the installation of an L-band transmit antenna. In order to avoid both the installation of L-band antenna and Ku-band antenna, the concept of a hybrid dual frequency antenna operating L 


  13 band and Ku-band with low aerodynamic profile has been investigated in this paper. Keyaspects of this research are 200  Design and testing dual-fre quency antenna elements operating in both L-band and Ku-band 200  Conformal aspects of Ku-band phased array antennas 200  Beam forming algorithms for planar and conformal phased array antennas Two designs for dual-frequency antenna tiles consisting of 8x8 Ku-band antenna elements and one L-band element The designs have been analysed by means of computer simulations. Both designs show promising performance both in L-band and Ku-band. The design with slotted Lband antenna has a resonant fre quency in receive mode with a bandwidth of about 1 GHz. The Ku-band antenna is a stacked patch configuration where a parasitic element is placed above a lower patch separated by dedicated space filler. The manufactured protot ype antennas indicate that the bandwidth is sufficiently large In order to be able to communicate with geostationary satellites also at high latitudes e.g. during inter-continental flights\stem should have sufficient performance at low elevation angles. The antenna Ku-band system is required to have a small beamwidth \(to discriminate between the satellite signals\gain 30 dB angles. The effects of these requirements on the size and positioning of the antenna on the aircraft fuselage have been investigated. These requirements can be best satisfi ed by installing two planar phased array antennas on both side s of the fuselage with at least 1600 Ku-band elements. Each element has two feed lines, one for each polarization Every feed line has to be connected to the beam formi ng network. This means that the connections cannot be routed to one of the four sides of the antenna. Instead the concept of vertical feed lines \(by means of vias in a sufficiently thick substrate recommended. These vertical f eed lines connect the L-band and Ku-band antenna elements in the upper layer with feed networks in multiple lower laye rs. This vertical feed line system was not available so far due to manufacturing problems The performances of one dua l-frequency antenna design have been investigated by manufacturing two test antennas without vertical feed line syst em. The first antenna contains only a multilayer structure with L-band slots and 8x8 Kuband stacked patches. The performances of the L-band slots and Ku-band stacked patches c ould be measured separately It was concluded that opportun ities should be considered to improve the design of the dual-frequency antenna \(by optimizing the feeds of the Ku-b and elements the dual frequency test-antenna and the measurement set-up More important, however, is the realization of a mechanically stable vertical feed line system, so that the properties of L-band and Ku-band elements can be measured adequately The second test antenna contains only a multilayer structure with 8x8 Ku-band stacked patches and a feed network with 8 combiners, where each comb iner coherently sums 8 antenna elements. In combination with a prototype 8x1 OBFN, a Ku-band phased arra y antenna is obtained of which the beam can be steered in one direction. This second test antenna is used to analyze the broadband properties of the 8x8 Ku-band antenna array and 8x1 OBFN. The measured performances of this antenna are presented in Ref   A CKNOWLEDGMENT  This work was part of the EU 6 th Framework project ANASTASIA., and the FlySmart project, supported by the Dutch Ministry of Economic A ffairs, SenterNovem project numbers ISO53030 The FlySmart project is part of the Eureka PIDEA  project SMART Cyner Substrates is acknowle dged for technical assistance during the fabrication of the prototype antennas 


  14 R EFERENCES  1  P. Jorna, H. Schippers, J. Verpoorte, \223Beam Synthesis for Conformal Array Antennas with Efficient Tapering\224 Proceedings of 5 th European Workshop on Conformal Antennas, Bristol, September 11-12, 2007 2  The Radio Regulations, editi on of 2004, contain the complete texts of the Radio Regulations as adopted by the World Radio-communication Conference \(Geneva WRC-95 tly revised and adopted by the World Radio-communication Conference WRC-97\RadioWRC2000\and the World Radio-communication Conference WRC-03 Resolutions, Recommendations and ITU-R Recommendations incorporat ed by reference 3  RECOMMENDATION ITU-R M.1643, Technical and operational requirements for ai rcraft earth stations of aeronautical mobile-satellite service including those using fixed satellite service network transponders in the band 14-14.5 GHz \(Earth-to-space 4  ETSI EN 302 186 v1.1.1 \(2004-01 Stations and Systems \(SES\onised European Norms for satellite mobile Aircraft Earth Stations AESs\the 11 12/14 GHz frequency bands covering essential requirement s under article 3.2 of the R&TTE directive 5  EUROCAE ED-14E; Environmental Conditions and Test procedures for Airbor ne Equipment, March 2005 6  F. Croq and D. M. Pozar, \223Millimeter wave design of wide-band aperture-coupled stacked microstrip antennas,\224 IEEE Trans. Antennas Propagation, vol. 39 pp. 1770\2261776, Dec. 1991 7  S. D. Targonski, R. B. Waterhouse, D. M. Pozar Design of wide-band aperture stacked patch microstrip antennas ", IEEE Transactions on Antennas and Propagation, vol. 46, no. 9, Sep. 1998, pp. 1245-1251 8  R. B. Waterhouse, "Design of probe-fed stacked patches", IEEE Transactions on Antennas and Propagation, vol. 47, no. 12, Dec. 1999, pp. 1780-1784 9  D.M. Pozar, S. D. Targonski, \223A shared aperture dualband dual-polarised microstrip array\224, IEEE Transactions on Antennas and Propagation,Vol. 49 no. 2,Feb. 2001, pp. 150-157 10  http://www.ansoft.com 11  J-F. Z\374rcher, F.E. Gardiol, \223Broadband patch antennas\224 Artech House, \(1995\N 0-89006-777-5 12  H. Schippers, J. Verpoorte, P. Jorna, A. Hulzinga, A Meijerink, C. G. H. Roeloffzen, L. Zhuang, D. A. I Marpaung, W. van Etten, R. G. Heideman, A. Leinse, A Borreman, M. Hoekman M. Wintels, \223Broadband Conformal Phased array with Optical Beamforming for Airborne Satellite Communication\224, Proc. of the IEEE Aerospace Conference, March 2008, Big Sky, Montana US 13  H. Schippers, J. Verpoorte, P. Jorna, A. Hulzinga, L Zhuang, A. Meijerink, C. G. H. Roeloffzen, D. A. I Marpaung, W. van Etten, R. G. Heideman, A. Leinse M. Wintels, \223Broadband Op tical Beam Forming for Airborne Phased Array An tenna\224, Proc. of the IEEE Aerospace Conference, March 2009, Big Sky, Montana US 


  15  B IOGRAPHIES  Harmen Schippers is senior scientist at the National Aerospace Laboratory NLR. He received his Ph. D. degree in applied mathematics from the University of Technology Delft in 1982. Since 1981 he has been employed at the National Aerospace laboratory NLR. He has research experience in computational methods for aero-eleastics, aeroacoustic and electromagnetic problems. His current research activities are development of technology for integration of smart antennas in aircraft structures, and development of computational tools for installed antenna analysis on aircraft and spacecraft  Jaco Verpoorte has more than 10 years research experience on antennas and propagation Electromagnetic compatibility \(EMC and radar and satellite navigation He is head of the EMC-laboratory of NLR. He is project manager on several projects concerning EMCanalysis and development of advanced airborne antennas    Adriaan Hulzinga received his BEng degree in electronics from the hogeschool Windesheim in Zwolle Since 1996 he has been employed at the National Aerospace laboratory \(NLR as a senior application engineer. He is involved in projects concerning antennas and Electromagnetic compatibility \(EMC  Pieter Jorna received the M.Sc degree in applied mathematics from the University of Twente in 1999 From 1999 to 2005 he was with the Laboratory of Electromagnetic Research at the University of Technology Delft. In 2005 he received the Ph.D. degree for his research on numerical computation of electromagnetic fields in strongly inhomogeneous media Since 2005 he is with the National Aerospace Laboratory NLR\ in the Netherlands as R&D engineer   Andrew Thain is a research engineer in the field of electromagnetic modelling of antennas. He specialises in the use of surface integral methods for the calculation of coupling and radiation patterns and works closely with Airbus on the topic of antenna positioning. He has experience in the field of electromagnetic modelling  Gilles Peres is head of the Electromagnetics group of EADS-IW He has a wide experience in computational EM modelling particularly the use of FDTD, integral and asymptotic techniques for antenna structure interactions. He has contributed with Airbus experts to the certification campaign of the A340/500 and A340/600. Dr Peres holds a PhD thesis from University of Toulouse \(1998\ on impulsive Electromagnetic Propagation effects through plasma   Hans van Gemeren has a BEng degree in electronics. From the beginning of Cyner substrates he is involved in development and production of prototyping and nonconventional Printed Circuit boards Working mainly for design and research centers Cyner got involved in many high tech projects and from this developed a great expertise in the use of different \(RF materials. In the FlySmart project Hans and his colleagues are able to do what they like most: In close cooperation with designers, creatively working on substrate solutions 


  16  


NIMBUS NOAA 8 NOAA 15 NPOESS Preparatory Project Orbital Maneuvering Vehicle Orbcomm Orbiting Carbon Observatory Orbiting Solar Observatory-8 Orbview 2 Orion 1 P78 Pas 4 Phoenix Pioneer P-30 Pioneer Venus Bus/Orbiter Small Probe and Large Probe Pioneer-I 0 Polar QuikSCAT Radarsat Reuven High Energy Solar Spectroscopic Imager REX-II Rosetta Instruments Sage III SAMPEX Satcom C3 Satcom C4 SBS 5 SCATHA Seastar SMART-1 SNOE Solar Dynamics Observatory Solar Maximum Mission Solar Mesosphere Explorer Solar Radiation and Climate Experiment Solar Terrestrial Relations Observatory Space Interferometry Mission Space Test Program Small Secondary Satellite 3 Spitzer Space Telescope SPOT 5A Stardust  Sample Return Capsule STEPO STEPI STEP3 STEP4 STRV ID Submillimeter Wave Astronomy Satellite Surfsat Surveyor Swift Gamma Ray Burst Exporer Synchronous Meterological Satellite-I TACSAT TACSAT 2 TDRS F7 Tellura Terra Terriers Tethered Satellite System Thermosphere Ionosphere Mesosphere Energetics and Dynamics TIMED TIROS-M TIROS-N TOMS-EP Total Ozone Mapping Spectrometer TOPEX TRACE Triana Tropical Rain Measuring Mission TSX-5 UFO I UFO 4 UFO 9 Ulysses Upper Atmosphere Research Satellite Vegetation Canopy Lidar VELA-IV Viking Viking Lander Viking Orbiter Voyager Wilkinson Microwave Anisotropy Probe WIND WIRE XMM X-Ray Timing Explorer XTE XSS-iO XSS-ii APPENDIX B FIELDS COLLECTED Mission Mission Scenario Mission Type Launch Year Launch Vehicle Bus Model Bus Config Bus Diameter Location Expected Life Flight Profile Flight Focus Technical Orbit Currency Total Cost Including Launch Unit Sat Costs Average Sat Cost Production Costs Launch Costs Operations Cost Orbit Weight Wet Weight Dry Weight Max Power EOL Power BOL Power  of Batts 17 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


