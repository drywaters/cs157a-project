The mining of fuzzy gradient in the case of materialized cube Yubao Liu                                                                              Jian Yin Department of Computer Science of                                    Department of Computer Science of Sun Yat-Sen University                                                        Sun Yat-Sen University Guangzhou, Guangdong, China                                          Guangzhou, Guangdong, China liuyubao@zsu.edu.cn  issjyin@zsu.edu.cn Abstract The mining of cube gradient is a new research e data mining community. It is an extension of the traditional association rules mining in the data cube and its purpose is to mine the changes of measures in a multidimensional space. However previous works explore for cube gradients with raw e changes of cube gradients, such as high or low, are not clear and those are not easy to be understood for the decision makers In this paper, the fuzzy gradient mining problem with fuzzy constraint sets is presented. The mining e case of materialized cube is presented. In MFG, the condensed cube technology is used to reduce the searching space of the mining algorithm. The performance tests based on the synthesized data sets show the mining algorithm is effective and scalable with the fuzzy constraint sets 1. Introduction The problem of mining changes of measures in a multidimensional space was first proposed by Imielinski, et al. as a cubegrade problem 1 which can be viewed as a generalization of association rules 2 in data cubes 3 Typically, cubegrades can express the following kinds of questions on the data Q1: How is the average age of buyers of salsa affected by buying soda as well? Example answer: Drops by 10% from 26 to 24 Q2: How is the average amount of milk bought affected by customer age among buyers of cereals? Example answer: Raise by 20% for customers younger than 40 and drops by 5% among customers older than 40 The constrained cube gradient mining 4 represents grade problem. Its purpose is to extract the pairs of gradientprobe cell characteristics associated with big changes in measure from a data cube The previous works explore for cube gradients with raw number, for example, in the above question Q2, the changes of the average amount are 20% and 5 respectively, whereas the semantics of the changes high or low, are not clear and those are not easy to be understood for the decision makers. Fuzzy technology provides a useful method for describing the interface between human conceptual categories and data In this paper, the fuzzy gradient mining problem with the fuzzy constraint sets is presented. The fuzzy constraint sets and the semantics of changes of cube gradient are given in the linguistic formats and those are easy to be understood for the decision makers. Best of our knowledge, there are fewer papers on the problem In the data warehouse environment, in order to enhance the response time of OLAP analysis, there are s that have been computed in advance. The mining algorithm, MFG short for the mining of fuzzy gradient\, for the problem in the case of materialized cube is presented technology is used to duce the searching space of the mining algorithm d cube 5 is a novel and efficient data sic spirit of the condensed cube is to condense a number of cells of a cube into a cell, that is, a single tuple. Thus a condensed cube can reduce dramatically the size of a data cube itself. The performance tests based on the synthesized data sets show the mining algorithm MFG is effective and scalable with the fuzzy constraint sets 2. Preliminary technologies 2.1 Fuzzy set Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


The concept of a fuzzy set extends the notion of a regular crisp set in order to express classes with illdefined boundaries, corresponding in particular to linguistic values such as "tall", "young", "important r than sharp transition between nonmembership and full membership. A degree of membership is associated with every element x of the universal set X. It takes its value in the interval [0,1 i n s t ead o f t h e p a i r 0 1   Such a membership assigning function 265 A X 001 0,1 s  called a membership function and the set defined by it is a fuzzy set The concept of \215 002 cut\216 of a fuzzy set means a subset made of those elements whose membership is over or equal to 002 A 002 x 003 X 265 A x 004\002 A fuzzy predicate expresses the degree to which the arguments satisfy the predicate 2.2 An overview of condensed cube The CUBE BY operator 3 is a multidimensional extension of relational GROUP BY operator. While the semantics of the CUBE BY operator is to partition a relation into groups based on the values of the and then apply aggregations functions to each of such groups the CUBE BY operator computes GROUP BY corresponding to all possible combinations of attributes in the CUBE BY operator. In general, a CUBE BY operator on n attributes computes 2 n GROUP BYs, or cuboids. The grouping attributes are called dimensions and the aggregated attributes are called measures. A tuple with dimension attributes and measure attributes in a data cube is called a cell. While the aggregation is groups of relation table tuples obtained by partitioning the relation table on the cuboid attributes there exist such partitions that contain only one tuple that is named as a single tuple Given a single tuple r\(r\(a 1 r\(a n where r\(a k  denotes the value of dimension a k 1 005 k 005 n\ and its single dimensions SD={a i a j 1 005 i 005 j 005 n\ or its SDSET, the complete set of cells condensed by the single tuple r is denoted as ExpandSet\(r\ and it can be computed by the following Expand operator 1\ Expand\(r, SD\=r\220 such that m\(r\220\\(r r\220\(a k a k r a k 003 SD and r\220\(a k for a k 006 SD, where m\(r\220\and m\(r\enote the measure of r\220 and r respectively 2\ Expand\(r, SDSET\r\220| r\220=Expand\(r, SD i  SD i 003 SDSET For example, given a single tuple r=\(A=2,B=3,C=1,M=60\ and its single dimensions SD={A}, ExpandSet\(r\={\(2, *, *, 60\, \(2, 3, *, 60\, \(2 1, 60\, \(2, 3, 1, 60\}. In general, for a given single tuple r\(r\(a 1 r\(a n d its SD={a i a j 1 005 i 005 j 005 n there are the number of 2 n-j cells in ExpandSet\(r\, in other words, the 2 n-j cells are condensed into the single tuple r. Note that all the cells in ExpandSet\(r\have the same aggregation value m\(r\ because all of them are only aggregated from the same single tuple. According to the Expand principles, we also have two important properties on a single tuple r 1\ The non-* dimension values of the cells in ExpandSet\(r\ are all derived from the single tuple r and rresponding dimensions values of r 2\ All of cells in ExpandSet\(r\share the same non dimension values on SD and the cells that have the ed in the set ExpandSet\(r In a condensed cube, we only need to physically store the single tuple together with an extra field to store the single dimensions information of the single tuple. The cells can be expressed by the single tuple are not stored physically. When needed, these cells can be d principles of the single tuple. The SD fields of these non-single tuples are equal to 007 in a condensed cube. These non-single tuples can be viewed as the general cells in a general data cube since they don\220t condense any cells 3. Problem descriptions 3.1 The similar relationship of cells Given two distinct cells c 1 and c 2 of a data cube D with n dimensions, c 1 is an ancestor of c 2 and c 2 is a descendant of c 1 iff on every dimension attributes either c 1 and c 2 share the same value, or c 1 has value 215*\216, where \215*\216 indicates \215all\216; c 1 is a sibling of c 2 and vice versa, iff c 1 and c 2 have identical values in all dimensions except one dimension in which neither has value *. The single tuple has not descendant cells because each dimension has the non-* value, i.e., the specific value. For simplicity, we sometimes say c 1 is similar to c 2 if c 1 is a descendant, an ancestor or a sibling of c 2  Example 1 Suppose that the data cube D has three dimensions A, B, C and one measure M and there are three cells, c 1 A=4, B=*, C=*, M=90\, c 2 A=4, B=5 C=1, M=70\ and c 3 A=4, B=5, C=2, M=80\. Then the cell c 1 is an ancestor of c 2 and c 3 the cell c 2 is a sibling of c 3  3.2 Fuzzy constraint sets Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


A significance constraint C sig is usually defined as ibutes. A probe constraint C prb is usually defined as conditions on dimension attributes and is used to select a set of user-desired cells. A cell c is significant iff C sig c\rue, and a cell c is a probe cell iff c is significant and C prb c\rue. The complete set of probe cells is denoted as P. The significant cell that may have gradient relationship with a probe cell is a gradient cell. A gradient constraint \(i.e gradient relationship\ has the form C grad c g c p  b g\(c g  c p where g is a gradient function. In this paper, the gradient function form is defined as \215m\(c g c p  where m\(c\ is a measure value for a cell c. A gradient cell c g is interesting with respect to a probe cell c p 003 P iff c g is significant, c g and c p satisfy similar relationship and C grad c g c p true. Considering a quantitative attribute of these constraints, say x, it is possible to define some fuzzy sets for x, with a membership function per fuzzy set, such that each value of x qualifies to be in one or more of these fuzzy set Example 2 Suppose that the given fuzzy constraint sets C prb b A is low, B=*, C=*\, C sig b M is expensive C grad c g c p  b m\(c g c p is huge\, where A, B, C are the dimensions and M is the measure of data cube. The corresponding membership functions in these constraints are defined as following Then the three cells in example 1, that is, c 1 c 2 and c 3 are significant and probe cells. The cell c 1 is ting with the cell c 2 According to the Expand operator, it is known that if a single tuple c is significant, that is C sig c\=true, the cells in ExpandSet\(c\e also significant because they have the same aggregate value with c 3.3 The fuzzy cube gradient mining definition Formally, given a data cube D and the fuzzy constraint sets C sig C prb C grad the fuzzy cube gradient mining is to find all the interesting gradient-probe cell pairs \(c g c p It is worth mentioned that the significance constraint is assumed to be anti-monotonic in our problem. Anti-montonicity is very useful for the pruning of searching cube. Some methods 6 for deriving weaker anti-monotonic constraints from nonanti-monotonic constraints are discussed 4. The algorithms for our problem Essentially, the mining of cube gradient in the case of materialized cube is equivalent to the searching of gradient-probe cell pairs from a given materialized cube. In this section, the algorithm MFG for our problem is presented. In MFG, the condensed cube data is used to store the cells 4.1 The description of MFG algorithm The detailed description of MFG algorithm is shown in figure 1. The inputs of the algorithm are the given ts that include the given constraint conditions itself and the given membership functions of the quantitative attributes of the constraints. The outputs are the interesting gradientprobe cell pairs. In detail, each cell pair has the following format If c g x 1 x 2 203, x t m g1 m g2 203m gq  with S={f g1  f g2 203, f gq  then c p y 1 y 2 203, y t m p1 m p2 203m pq  with P={f y1 f y2 f yt  and S={f p1 f p2 203, f pq  changes m delt m 1 m 2 203m q  with M={f m1 f m2 203 f mq where x 1 y 1 x 2 y 2 203, and x t y t denote the dimension values of c g and c p t denotes the number of the dimensions of the cells, m g1 m p1 m g2 m p2  203m gq m pq denote the measure values of c g and c p q denotes the number of the measures of the cells, f g1 f p1  f g2 f g1 203, f gq f pq denote the membership function values of measures of the cells, f y1 f y2 203, f yt denote the membership function values of dimensions of the probe cell, m 1 m 2 203m q denote the gradient function values, and f m1 f m2 203, f mq denote the membership function values of gradient function values In our study, the probe cells in P are assumed to be stored in the ascending order according to the measures. Thus, if the measure of one probe cell c p can not satisfy the constraint C grad c g c p then all the probe cells following it will not satisfy the gradient function and hence can be pruned from P In the case that the set P is very large, the hash table can be used to fast access P. Although the gradient function is assumed as the form \215m\(c g c p our algorithm is still applicable to the other form gradient functions by modifying the computation part of the gradient function low\(x  0.02x+1 0 005 x 005 50 0   x>50 expensive\(x 0   0<x 005 25 1   x>50 0.04x-1  25<x 005 50 huge\(x 0.1x   0<x 005 10 1  x>10 0  x 005 0 Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


Figure 1. The description of MFG algorithm 4.2 The partial expansion technique In order to produce all the interesting gradient-probe cell pairs, either c p or c g is a single tuple, we should expand the single tuples. While expanding c g or c p we observe that it is not necessary to compute all the cells d into c g or c p In detail, the cells in ExpandSets that are not meaningful \(i.e, similar\e not computed. For example, suppose that c p and c g have four dimensions A, B, C and D, c p 1, 2, 3, 3 c g ons SDs of c p c g are both {A}. The dimension values of the cells of ExpandSet\(c p c g e shown in the two rectangle frames in figure 2 according to the Expand principles Figure 2. The partial expansion of two single tuples We scan and compare the values of dimension A, B C, and D in the two rectangle frames according to the order of dimension lexicography. The value of dimension A in ExpandSet\(c p is equal to 1, whereas value in ExpandSet\(c g s equal to 2. Then the cells in ExpandSet\(c g nd ExpandSet\(c p have different non-* value on dimension A. Thus, there is only one possibly meaningful relationship between the cells in ExpandSet\(c p d ExpandSet\(c g that is, the sibling relationship According to the definition of sibling, the cells in ExpandSet\(c p and ExpandSet\(c g should share the dimensions. Thus, the meaningful dimension values of the cells in ExpandSet\(c p are as follow: A=1, B=*, C=* and D=3 or D=*, that is, ExpandSet\(c p nly contains two cells 1,*,*,3\ \(1,*,*,*\xpandSet\(c p should contain 2 3 8 cells if the single tuple c p takes the complete expansion. Similarly, ExpandSet\(c g should contain two cells \(2,*,*, 3\ and \(2, *, *, *\. The similar gradient-probe cell pairs are \(\(1,*,*,3\, \(2,*,*, 3\d 1,*,*, *\, \(2 The above method is named as partial expansion technique in this paper. The main spirits of partial expansion technique is to recognize the possibly onship between the cells in ExpandSet\(c p and ExpandSet\(c g d then utilize the relationship to determine the meaningful dimension values in ExpanSets and hence reduce the size of ExpandSets. The key of partial expansion is to recognize the possibly meaningful relationship. The possibly meaningful relationship can always be recognized through the different dimension values of the corresponding dimensions. In general, if the dSets are non value, then the meaningful relationship is sibling and if one dimension value is * value and the other is non then the meaningful relationship is ancestor/descendant. The description of the procedure of partial expansion is given in figure 3  Algorithm 1 MFG Input 1\ A condensed cube D, \(2 sig C prb C grad  Output The set of interesting gradient-probe cell pairs \(c g c p  Method 1 P 007 S 007 initialization of P and S sets 2 while i<|D| {// |D| denotes the number of cell of D 3 read\(c 4 if C prb c\rue {  //c is a probe cell 5 for j=1 to |S|   //compared with the cells of S 6   if\(C grad S[j   c    tr ue  S  j  d e no te s the  j t h c e l l 7      if S[j an d  c are n o n s i n g l e t u p l e 8         if S[j  is simila r t o  c o utp ut  S  j    c    9      else call Partial_expansion\(S[j c   10  else break 11 for k=1 to |P| {     // compared with the cells of P 12   if\(C grad c, P[k  t ru e 13      if P[k  an d c are n o n s i n g l e t u p l e 14          if P  is simila r to  c o u tp ut  c  P  k    15      else call Partial_expansion\(c, P[k  16   else break 17   if\(C grad P[k c  t ru e 18     if P[k a n d  c are n o n s i n g l e t u p l e 19         if P s si m i l a r to c o u t p ut P  k  c 20     else call Partial_expansion\(P[k c     21     add c to P 22 else if\(C sig c\rue 23   for m=1 to |P 24     if\(C grad c, P[m   t ru e 25        if P[m a n d c  a r e  nonsingle  tup le  26            if P[m is sim il ar t o c o u t p u t P m  c  27        else call Partial_expansion\(P[m  c  28     else break 29   add c to S 30 i++; }   //process the next cell ExpandSet\(c p A=1,B=2 or *,C=3 or *,D= 3 or ExpandSet\(c g A=2,B=3 or *,C=2 or *,D= 3 or Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


n algorithm 5. The algorithm performance tests In this section, we present our experimental results on the performance \(in terms of time\ of MFG algorithm. All experiments are conducted on a PC platform with an Intel Pentium 001\013 500M CPU, 218M RAM and Windows 2000 OS. All experiments are performed using synthetic \(algorithmically generated datasets. The dataset includes 1,024 tuples following the uniform distribution. The number of dimension is set to 6 and the cardinality of all attributes is set to 1000. The aggregate function used in the cube computation algorithm is SUM function The materialized condensed cube generated from the synthetic dataset by the BU-BST algorithm includes 30,557 tuples whereas the generally materialized cube i.e. generated by the BUC algorithm\ includes 61,114 tuples. So the searching space of the problem is dramatically reduced. The scalability of the algorithm with the probe constraint sets, significant constraint sets and gradient constraint sets are tested and the results show that the algorithm is effective and scalable with the different constraint sets. The runtime of the algorithm are varied from 40s to 600s with different constraint sets. Due to the limit of space, the results are not given here 6. Conclusions In this paper, the problem of fuzzy gradient mining in the case of materialized cube and the mining algorithm MFG are presented. The performance studies of the algorithm based on the synthetic dataset show the algorithm is effective and scalable with different constraint sets. Further enhancing the performance of MFG algorithm is an interesting work such as the study of parallel mining algorithm Acknowledgements The paper is supported by Science Foundation of China No.60205007\, Natural Science Foundation of Guangdong Province \(No.0 31558, No.043 00462 Research Foundation of National Science and gy Plan Project \(No.2004BA721A02 Research Foundation of Science and Technology Plan Project in Guangdong Province \(No.2003C50118 Research Foundation of Science and Technology Plan Project in Guangzhou City \(No.2002Z3-E0017\ and n y No.350416 7. References 1  I m ie l i ns k i  L  K h a c h iy a n   a n d A  A bdul g h a ni  215Cubegrades: Generalization Association Rules\216 Tech. Rep Dept Computer Science Rutgers University, Aug. 2000 2 R  A g raw al T.I m i e lin s k i an d A  S w am i  215M i n in g Association Rules between Sets of Items in Large Databases\216 Proceedings of ACM SIGMOD International Conference on Management of Data \(SIGMOD\22093 ACM Press, 1993 pp.207~216 3  J G r ay  A  Bo s w o r t h A L a y m an  an d H P i r a h e sh  215Dat a l aggregation operator generalizing group, and sub-total\216 Proceedings of International Conference on Data Engineering \(ICDE\22096 IEEE Computer Society Press, 1996, pp. 152~159 4  D o ng J  Ha n J  L a m  J  P e i and K W a ng   215 M ini ng  Multi-Dimensional Constrained Gradients in Data Cubes\216 Proceedings of International Conference on Very Large Data Bases \(VLDB\22001 Morgan Kaufmann, 2001, pp 321~330 5  W a ng  J  Fe ng  H L u a n d X.Y J e f f r e y  215 C onde ns e d  Cube: An Effective Approach to Reducing Data Cube Size\216 Proceedings of International Conference on Data Engineering \(ICDE\22002 IEEE Computer Society Press 2002, pp.155~165 6  H a n, J  P e i G  Dong  a n d K W a ng   215 E f f i c i ent  computation of iceberg cubes with complex measures\216 Proceedings of ACM SIGMOD International Conference on Management of Data \(SIGMOD\22001 ACM Press, 2001 pp.1~12  Algorithm 2 Partital_expansation Input The gradient cell c g the probe cell c p C sig  C prb and C grad Output The interesting gradientp robe cell pairs between c g and c p  Method 1 For each dimension a k 1 005 k 005 n 2 Apply the C prb to determine the dimension values of a k in ExpandSet\(c p  3 If the meaningful relationship is not recognized 4  Compare the dimension values of a k between ExpandSet\(c g and ExpandSet\(c p and recognize the meaningful relationship 5 Else 6 Use the relationship to determine the meaningful values of dimension a k in ExpandSets and if there are no meaningful dimension values that satisfy the possibly meaningful relationship then return 7 Output the interesting gradientp robe cell pairs \(c c\220\here c 003 ExpandSet\(c g  003 ExpandSet\(c p  Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


017 001 001 001 224 m F s 1 1 1 i 004 004 005 001 001 sequence 2 001 001 001 001 or 007 i i i k i g x x x i 003 003 003  s 224 s 016   and   i    i   i parent depth m M item depth n.parent Sequence item p.m p.M  001 022  017 001 004 003 f I I I I  The tree has the property that if  That is we only ever points to the parent of the node so       Each Pre\336xNode is a tuple 003 1 2 y  its measures are not stored along the path of Pre\336xNodes corresponding to s parent since   Let where and or where and  However by the construction of the tree there exists another node where and and pre\336x-node Pre\336xNodes be the sequence corresponding to a pre\336x node for each fringe node Proof Sketch We don\325t generate all possible association rules that can be generated from itemset is not a pre\336x of  Finally the longest sequences are guaranteed to be in the fringe hence all rules will be generated by induction  All association rules can be generated by creating all rules by considering only  Speci\336cally we miss 1 any rules  and 2 any such rules where there is a gap between corresponding to the sequence for some the node for  is in the fringe Hence maximal itemset will be generated from node\(s other than itemset F-itemset maximal itemsets is not in the fringe then by de\336nition all nontrivial all in order to use minimum space while avoiding any re-computations 1 We can construct all itemvectors 224 224       g g       12 Fact 3 in Section 5 13 Since 002   First we introduce an abstract type 320  5 Algorithm k k k i   004 003 003 trivial 007   n m f 002 003 003 p I F m I s s F s n 003 004 n 003 004 003 s 003 004 n s n s s 005 n s 003 004 n m 004 m F y y p 004 We use a SequenceMap M F y 267 267 267 267 267 267 017 021 and  and  Then 11  and 2 it can be used to ef\336ciently generate all association rules Lemma 1 interchangeably The pre\336x tree is built of where represents the pre\336x of where where  Hence we save a lot of space because the tree never duplicates pre\336xes In fact it contains exactly one node per to index the nodes in the Pre\336xTree so we can retrieve them for the following purposes 1 to check in Lemma 1 when generating association rules 13  and 3 to 336nd the the s when we evaluate a by incrementally applying the rule  So to get its measures we need to 336nd its Pre\336xNode 320 by looking up 002 325s of Pre\336xNodes corresponding to subsets of subsequences of a potential itemset for pruning we automatically check two subsets without using the sequence map 12  2 to 336nd the measures  is shown in Figure 2\(a Since each node represents a sequence ordered itemset we can use the terms  is the measure\(s of the itemset evaluated by is an itemset The sequence in reverse represented by any node can be recovered by traversing toward the root To make the link with our itemvector framework clear suppose the itemset represented at a Pre\336xNode is potentially a function of the is in the Pre\336xTree then so are all subsequences by the antimonotonic property of 11 A is a F-itemset for which no superset is interesting has measure above threshold minM easure without alteration to GLIMIT if 1 we output frequent itemsets when they are mined 2 do not care if not In this section we outline the main principles we use in GLIMIT and follow up with an illustrative example We prove space complexity bounds before giving the algorithm in pseudo-code We exploit the following facts itemvectors corresponding to single items to the end of an itemvector This means that given a Pre\336xNode that is in the SequenceMap 14 It is can be considered as a reversed singly linked list fringe  If   is is the empty item so that  subsets are checked before we calculate a new itemvector and 3 have a F M 002 i i 016 001       001 001 001 001 001 001 001 001 001 001 001 001 001 001 001 001 001 001 001 001 001 001  001 001      020  016 016 020 020 022 016 016 001 s 003\004\005 003 004      is its depth of the node and therefore the length of the itemset at that node is the last item in the sequence represented by the node  The Pre\336xTree is designed for ef\336cient storage and not for lookup purposes This is why there are no references to child nodes To facilitate the generation of association rules a set of all nodes that have no children is maintained We call this the  The fringe is useful for because 1 it contains all 320 which represents a sequence of items Equality testing and hash-code calculation is done using the most ef\336cient iteration direction Pre\336xNode can be considered a Sequence 14 and reverse iteration is the most ef\336cient By performing equality comparisons in reverse sequence order we can map any Sequence to a Pre\336xNode by using a hash-table that stores the Pre\336xNode both as the key and the value Hence we can search using any Sequence implementation including a list of items and the hash-table maps Sequences to the Pre\336xNodes that represents them The space required is only that of the hash-table\325s bucket array so it is very space ef\336cient Finally we can avoid the use of both the Pre\336xTree and SequenceMap is not a pre\336x of s   


a  i i i           017 017 017 212 212   017 001          b i j k b i j b i k n n b i j k b i q k k    2 3 4 4 4 3 4 3 3 4 y 1 1 1 1 i i i j a a a k a k a j                                   y 3 3 4 3 2 2 2 4 2 3 4 1 212 y  a a a n a a k k 4 4 3 4 3 4 3 1 3 4 3 2 2 2 2 2 etc and item below 212 1 1 for any j 325s itemset will already have been generated As well as being most space ef\336cient this is required to evaluate nontrivial y 212 004 1 p.depth 1  Conversely while there is still a child to create or test we cannot delete  Fact 6 will always apply in this case too eg we can also delete  Note also that because we need the itemvectors of the single items in memory we have not been able to use Fact 7 yet Similarly Fact 6 is also applied in l m o and p However note that in m o and p we also use Fact 7 to delete  k is the topmost sibling child Hence we can apply Fact 6 in h Note that by Fact 1 we calculate 15  This is because we only ever need at a time If and  then we can have completed all nodes corresponding to all subsets of of a node is created or we 336nd its itemset is not frequent and hence don\325t need to create it the itemvector corresponding to its can be deleted That is we have just created the topmost last immediate child of or when and as per Fact 1 Note we are also making use of Fact 3 320 and by Fact 5 It has no possible children because of the ordering of the sequences The same holds for as  and by Fact 3 We know already that the time complexity  This applies only when we can only ever onto the end We now present an example and the the root we only ever need to keep a single itemvector in memory for any child of only if siblings are in the pre\336x tree Hence we only try to expand nodes which have one or more siblings and for more thorough than Fact 3 pruning using the antimonotonic requirement This is what we call the 324bottom up\325 order of building the Pre\336x Tree 5 When a Pre\336xNode until we generate where may both greater then is not frequent Indeed we can write the result of 325s corresponding itemvector  on the topmost of our algorithm to illustrate some of these facts Suppose we have the items are siblings Once we have created the node for the root we will need to keep its children\325s itemvectors in memory the itemvectors  So if we use the depth 336rst procedure when a Pre\336xNode is created all Pre\336xNodes corresponding to subsets of or the topmost eg directly into the itemvector holding 15 By Fact 1 we cannot apply this to nodes with eg when all itemsets are frequent will correspond to  The reason behind this is that by using the bottom up method and the fact our itemsets are ordered we know that if we have in d we can delete  In l we deleted for two reasons Fact 6 and 7 it is a special case in Fact 6 Finally to better illustrate Fact 3 suppose is not frequent This means that will have no siblings anymore so we do not even need to consider 002  a i i i i  threshold is such that all itemsets are interesting frequent Figure 2 shows the target pre\336x tree and the steps in mining it This example serves to show how we manage the memory while avoiding any re-computations For now consider the frontier list in the 336gure as a list of Pre\336xNodes that have not been completed It should be clear that we use a bottom up and depth 336rst procedure to mine the itemsets as motivated by Facts 2 and 4 We complete all subtrees before moving to the next item In d we calculate p   i   i   i   i n n   n   i   i   i  p.depth p.depth  p.depth  minM easure minSup p.item p.item p.item    i 212 212 2 2 267 y  i i i i i i i i i i 1 i i i i i i i i i i i i j<q<k y k  If we have read in           1 1 y not parent branch i with with we will use  2 It also means we use least space if we perform a depth 336rst search Then for any depth  in f In g the node for it 4 Suppose the items are if is roughly linear in the number of frequent itemsets because we avoid re-computations of itemvectors So the question now is what is the maximum number of itemvectors that we have in memory at any time There are two main factors that in\337uence this First we need to keep the itemvectors for individual items in memory until we we have completed the node for the top-most item Fact 1 and 7 Hence the 324higher\325  we will at most have only one itemvector in memory at a time 3 We only ever check a new sequence by 324joining\325 siblings That is we check  we can delete the itemvector corresponding to the single item is the top-most item cannot have any children because it has no siblings by Fact 3 its itemvector will no longer be needed 6 When a topmost is child        y j>k 020 020 020   001  001 020 001 001 is the top-most unless it is the topmost node as they correspond to single items and are still needed 7 When we create a Pre\336xNode eg 1 1 k             b i j b i j k b i j b j b j k  y k>j p y I k y p p F p p p y y i b a y y p p p i y y y y y y y y sibling i i i i i i i i i i i i i i i  


k Step 10 n Step 13 c Step 2 e Step 4 i Step 8 value Shaded nodes have their corresponding itemvector in memory Dotted nodes have not been mined yet Solid lines are the parts of the tree that have been created b Step 1 f Step 5 h Step 7 j Step 9 m Step 12 o Step 14 p Step 15 a Complete Pre\336xTree d Step 3 g Step 6 l Step 11 Figure 2 Building the Pre\336xTree mining itemsets Example Nodes are labeled with their item 


 then we can additionally bound the space by  Furthermore since the frontier contains all uncompleted  and n n n  This is for the worst case when all itemsets are frequent Clearly a closer bound is if we let 1 2 2 1 1 2 1 2 2 1 itemvectors of space Furthermore 2    n n          b  212  b   The cardinality of both these sets equal to the number of nodes along the path is 942  001 001 001 001 be the largest itemset GLIMIT uses at most 001  007  100  n 1 1 1 1  t\212 n is even the last node is t   t t  b 3 1 t\212 l l l l step 006 n 2 n   n n n minSup 1 1  6 Experiments even or odd b 002 1 and up in the tree we are the more this contributes Secondly we need to keep itemvectors in memory until we complete their respective nodes That is check all their children Fact 6 or if they can\325t have children Fact 5 Now the further we are up in the tree or any subtree for that matter without completing the node the longer the sequence of incomplete nodes is and hence the the more itemvectors we need to keep Considering both these factors leads to the situation in Figure 3 320 that is we are up to the top item and the topmost path from that item so that no node along the path is completed If we have corresponding to the single items children of the root There are a further inclusive to the last coloured node these are the uncompleted nodes 16  Therefore the total space required is just is so that we do not double count the itemvector for  is low and in practice with non-pathological support thresholds we use far fewer than itemvectors If we know that the longest frequent itemset has size 16 When directly into As an aside note we could perform the transpose operation in memory before mining while still remaining within the worst case space complexity However on average and for practical levels of objects Algorithm 1 describes 17 the additional types we use such as items respectively To apply GLIMIT we 336rst transpose the dataset as a preprocessing step 19  We compared GLIMIT to a publicly available implementation of FP-Growth and Apriori We used the algorithms from ARtool 20 as it is written in 17 The pseudo-code in our algorithms is java-like and we assume a garbage collector which simpli\336es it Indentation de\336nes blocks and we ignore type casts 18 http://\336mi.cs.helsinki.\336/data 19 This is cheap especially for sparse matrices 320 precisely what the datasets in question typically are Our data was transposed in 8 and 15 seconds respectively using a naive Java implementation and without using sparse techniques 20 http://www.cs.umb.edu laur/ARtool It was not used via the supplied GUI The underlying classes were invoked directly itemvectors itemvectors along the path from node  We have sketched the proof of Lemma 2 Let  and shows the initialisation and the main loop 320 which calls methods used by We evaluated our algorithm on two publicly available datasets from the FIMI repository 18 320 T10I4D100K and T40I10D100K These datasets have  where the be the number of frequent items Hence we need space linear in the number of frequent items The multiplicative constant  method whereby a list priority queue of states each containing a node that has yet to be completely expanded is maintained The general construct is to retrieve the 336rst state evaluate it for the search criteria expand it create some child nodes and add states corresponding to the child nodes to the frontier Using different criteria and frontier orderings leads to different search techniques Our frontier contains any nodes that have not yet been completed wrapped in is odd it is  5     n y  1  3  5 n 212 3 n 212 1  items the worst case itemvector usage is just the number of coloured nodes in Figure 3 There are  n i  Note that in the even case the next step to that shown will use the same memory the itemvector for node t\212 2 y  1  3  5 n 212 3 n 212 1 n  frontier is no longer needed once we create  and when 3 5 3 5 3 5 5 3 n n 1 1 as we compute it so both need never be in memory at the same time nodes we know from the above that its upper bound is 3 It also describes the 3 step Figure 3 Maximum number of itemvectors Two cases   n   n   n   n 267 267 267 267 n 000 870 minM easure n be the number of frequent items Let b State State    n n n y n n n l n n be the number of items and n l calculateF  check 212 212 212   212 212 212 212  by Fact 6 and we write i eg  this would require more memory The algorithm is a depth 336rst traversal through the Pre\336xTree Any search can be implemented either recursively or using the frontier transactions and a realistic skewed histogram of items They have  n b  


Main Loop to join with  as we are creating    can write result directly into  pref ixT ree.getRoot y 002 003 004     004            1                         002                  boolean localT op F localT op localT op null localT op localT op localT op localT op m State State State State newState       if 001 001 001 001 001 001 001 001 001 001 001 001 001 001 001 001 001 Data-types initialisation main loop and methods  initialise i i i i i i i i  267 267 267 002   state.y state.y M minM easure state.itemvectors.next f f minM easure m p item   Iterator itemvectors boolean top P air newP air List buf f er  state.buf f er.iterator 002 localF rontier.removeF irst be the itemset corresponding to    nd P ref ixN ode M y y M M m m I I I I I I I I I I I I I I       P air p   f rontier.getF irst state.node p.item m 002             I I i i i i i i i i I i K K  Iterator is over   make use of anti-monotonic pruning property if   else   if   void y  hatwecreateif y y  P ref ixN ode node Item item double m  so we don\325t add it again  We need and and and becomes we create if   Fact 5 is not complete we  will add has been completed See and  1 Dataset  have already applied 2 Completed  containing all is the itemvector for in Fact 1 We keep reusing them through  is the itemvector corresponding to and in Fact 1  for the s created to hold the children of to make use of Fact 3 provides the as empty Create initial state objects Reads input one row at a time and annotates the itemvector with the item it corresponds to could also apply   while  is the parent of the new and is set so that is true only for a node that is along the  see end of method  and hence delete   the top child of in this step Fact 6  we are dealing with itemsets of length  Fact 6 or 7 No longer need as this is the last child we can create under  and it is not a single item other than perhaps the topmost  else  need to use additional memory for the child  don\325t need to calculate we know  if   Found an interesting itemset 320 create  there is potential to expand  Let is the itemset represented by a child of would be  This method calculates to look up the to get their values  Then it returns  details depend on  Check whether the itemset to check whether subsets of by Fact 3 exist details omitted Iterators any after              is used to create the may       P air P air   m   m inputF ile state.newP air state.newP air state.newP air p.item p.item pref ixT ree.createChildU nder p.item sequenceM ap.put state.buf f er.size state.newP air p.item P ref ixT ree pref ixT ree pref ixT ree p.y calculateF p.y check p.y p.y calculateF p.m p.M map map map map M   K  Remove y 002 y     006 006 002 002 002 002 002 002 002 002 002      001 001 m F-itemsets topmost branch  006 006 006 001 Itemvector y P ref ixN ode node Itemvector y itemvectors f rontier Iterator itemvectors f rontier.add  null itemvectors f alse null new LinkedList step nextT op State state null null state f rontier Itemvector y null boolean nextT op state.node.isRoot state.top nextT op nextT op state.top state.top state.node P ref ixN ode newN ode newN ode.item newN ode.M  nextT op new LinkedList state state.node node.item  Item item new AnnotatedItemvetorIterator new State state.node newN ode.M newN ode.top double m state.node state.node newN ode.m new State newN ode y f rontier.addF ront newState newState double boolean newN ode top top top newN ode newN ode newN ode node node node node node 002 267 267 267 f rontier.isEmpty 1 1 if  of the pre\336x tree  Algorithm 1 check such as  s else to so that      required subsets of from so  212\005 Input step Output Initialisation could be interesting by exploiting the anti-monotonic property of except  itemvectors 1 0  in transpose format   helps us do this  with its root Initialise     Perform one expansion  is true iff we are processing the top sibling of subtree   if    in the next   initialise  if  for it   add to front of frontier ie in front of if it\325s still present so depth 336rst search Fact 2  by using s corresponding to the 001  P ref ixN ode inputF ile  212 newP air    001 g f minM easure SequenceM ap item buf f er y buf f er buf f er y g state.buf f er.add state.itemvectors.hasN ext state.y I m state.node.getDepth  state.y y y y M M  minM easure  p state.buf f er item p item m F F 002 item 002 item item P ref ixN ode node Item item        calculateF  and corresponds to F use 


The fact that it is also fast when applied to traditional FIM is secondary sum 0 100 000 82  b Runtime and frequent itemsets T40I10D100K longer than 30 minutes for 267  To represent itemvectors for traditional FIM we used bit-vectors 21 so that each bit is set if the corresponding transaction contains the item\(set Therefore  0 001  5 5 step    n minSup 1 1  f  creates the bit-vector 21 We used the Lemma 2 the 336gure clearly shows this is never reached in our experiments Our maximum was approximately 24 over the calls to m    Java like our implementation and it has been available for some time In this section we really only want to show that GLIMIT is quite fast and ef\336cient when compared to existing algorithms on the traditional FIM problem Our contribution is the itemvector framework that allows operations that previously could not be considered and a 337exible and new class of algorithm that uses this framework to ef\336ciently mine data cast into different and useful spaces F m  n n b Colt Figure 4 Results g  Figure 4\(a shows the runtime 22 of FP-Growth GLIMIT and Apriori 23 on T10I4D100K as well   267 267 is larger much of the time and space is wasted GLIMIT uses time and space as needed so it does not waste as many resources making it fast The downside is that the operations on bit-vectors in our experiments of length times the number of items would be so small that the runtime would be unfeasibly large anyhow Furthermore the space a Runtime and frequent itemsets T10I4D100K Inset shows detail for low support  http://dsd.lbl.gov/\367hoschek/colt BitVector implementation 22 Pentium 4 2.4GHz with 1GB RAM running WindowsXP Pro 23 Apriori was not run for extremely low support as it takes as the number of frequent items The analogous graph for T40I10D100K is shown in Figure 4\(b 320 we did not run Apriori as it is too slow These graphs clearly show that when the support threshold is below a small value about 0.29 and 1.2 for the respective datasets FP-Growth is superior to GLIMIT However above this threshold GLIMIT outperforms FP-Growth signi\336cantly Figure 5\(a shows this more explicitly by presenting the runtime ratios for T40I10D100K FP-Growth takes at worst 19 times as long as GLIMIT We think it is clear that GLIMIT is superior above the threshold Furthermore this threshold is very small and practical applications usually mine with much larger thresholds than this GLIMIT scales roughly linearly in the number of frequent itemsets Figure 5\(b demonstrates this experimentally by showing the average time to mine a single frequent itemset The value for GLIMIT is quite stable rising slowly toward the end as there we still need to check itemsets but very few of these turn out to be frequent FP-Growth on the other hand clearly does not scale linearly The reason behind these differences is that FP-Growth 336rst builds an FP-tree This effectively stores the entire Dataset minus infrequent single items in memory The FPtree is also highly cross-referenced so that searches are fast The downside is that this takes signi\336cant time and a lot of space This pays off extremely well when the support threshold is very low as the frequent itemsets can read from the tree very quickly However when  can be time consuming when compared to the search on the FP-tree which is why GLIMIT cannot keep up when is very small Figure 5\(c shows the maximum and average 24 number of itemvectors our algorithm uses as a percentage of the number of items At worst this can be interpreted as the percentage of the dataset in memory Although the worst case space is  1 and  minSup minSup minSup  By the time it gets close to  AN D 


Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 7 Conclusion and Future Work References frontier  pages 487\320499 Morgan Kaufmann 1994  W orkshop on frequent itemset mining implementations 2003 http://\336mi.cs.helsinki.\336/\336mi03  W orkshop on frequent itemset mining implementations 2004 http://\336mi.cs.helsinki.\336/\336mi04  J  Han J  Pei and Y  Y in Mining frequent patterns without candidate generation In Proceedings of 20th International Conference on Very Large Data Bases VLDB VLDB Journal Very Large Data Bases Data Mining and Knowledge Discovery An International Journal Lecture Notes in Computer Science  2004  J W ang and G Karypis Harmon y Ef 336ciently mining the best rules for classi\336cation In The Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD\32504 Symposium on Principles of Database Systems 2 b Average time taken per frequent itemset shown on two scales T10I4D100K is increased and hence the number of frequent items decreases Figure 5\(c also shows that the maximum frontier size is very small Finally we reiterate that we can avoid using the pre\336x tree and sequence map so the only space required are the itemvectors and the minSup SIAM International Conference on Data Mining required drops quite quickly as ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery 2000 ACM SIGMOD Intl Conference on Management of Data Figure 5 Results  8\(3\3204 2000  F  P an G C ong A T ung J Y ang and M Zaki Carpenter Finding closed patterns in long biological datasets In  2121:236 2001  M Steinbach P N T an H Xiong and V  K umar  Generalizing the notion of support In a Runtime ratios T10I4D100K c Number of Itemvectors needed and maximum frontier size T10I4D100K  pages 1\32012 ACM Press May 2000  F  K orn A Labrinidis Y  K otidis and C F aloutsos Quanti\336able data mining using ratio rules  Morgan Kaufmann 2003  J Pei J Han and L Lakshmanan Pushing convertible constraints in frequent itemset mining We showed interesting consequences of viewing transaction data as itemvectors in transactionspace and developed a framework for operating on itemvectors This abstraction gives great 337exibility in the measures used and opens up the potential for useful transformations on the data Our future work will focus on 336nding useful geometric measures and transformations for itemset mining One problem is to 336nd a way to use SVD prior to mining for itemsets larger than  pages 205\320215 2005  We also presented GLIMIT a novel algorithm that uses our framework and signi\336cantly departs from existing algorithms GLIMIT mines itemsets in one pass without candidate generation in linear space and time linear in the number of interesting itemsets Experiments showed that it beats FP-Growth above small support thresholds Most importantly it allows the use of transformations on the data that were previously impossible  That is the space required is truly linear  D Achlioptas Database-friendly random projections In  2001  R Agra w al and R Srikant F ast algorithms for mining association rules In  8:227\320252 May 2004  J Pei J Han and R Mao CLOSET An ef 336cient algorithm for mining frequent closed itemsets In  pages 21\32030 2000  S Shekhar and Y  Huang Disco v ering spatial colocation patterns A summary of results 


mator from sensor 1 also shown 6. CONCLUSIONS This paper derives a Bayesian procedure for track association that can solve a large scale distributed tracking problem where many sensors track many targets. When noninformative prior of the target state is assumed, the single target test becomes a chi-square test and it can be extended to the multiple target case by solving a multidimensional assignment problem. With the noninformative prior assumption, the optimal track fusion algorithm can be a biased one where the regularized estimate has smaller mean square estimation error. A regularized track fusion algorithm was presented which modifies the optimal linear unbiased fusion rule by a less-than-unity scalar. Simulation results indicate the effectiveness of the proposed track association and fusion algorithm through a three-sensor two-target tracking scenario 7. REFERENCES 1] Y. Bar-Shalom and W. D. Blair \(editors Tracking: Applications and Advances, vol. III, Artech House, 2000 2] Y. Bar-Shalom and H. Chen  Multisensor Track-to-Track Association for Tracks with Dependent Errors  Proc. IEEE Conf. on Decision and Control, Atlantis, Bahamas, Dec. 2004 3] Y. Bar-Shalom and X. R. Li, Multitarget-Multisensor Tracking Principles and Techniques, YBS Publishing, 1995 4] Y. Bar-Shalom, X. R. Li and T. Kirubarajan, Estimation with Applications to Tracking and Navigation: Algorithms and Software for Information Extraction, Wiley, 2001 5] S. Blackman, and R. Popoli  Design and Analysis of Modern Tracking Systems  Artech House, 1999 10 15 20 25 30 35 40 45 50 55 60 2 4 6 8 10 12 14 Time N o rm a li z e d e s ti m a ti o n e rr o r s q u a re d Target 1 Sensor 1 Centralized Est Track Fusion 10 15 20 25 30 35 40 45 50 55 60 0 2 


2 4 6 8 10 12 14 Time N o rm a li z e d e s ti m a ti o n e rr o r s q u a re d Target 2 Sensor 1 Centralized Est Track Fusion Fig. 7. Comparison of the NEES for centralized IMM estimator \(configuration \(i estimators \(configuration \(ii sensor 1 also shown 6] H. Chen, T. Kirubarajan, and Y. Bar-Shalom  Performance Limits of Track-to-Track Fusion vs. Centralized Estimation: Theory and Application  IEEE Trans. Aerospace and Electronic Systems 39\(2  400, April 2003 7] H. Chen, K. R. Pattipati, T. Kirubarajan and Y. Bar-Shalom  Data Association with Possibly Unresolved Measurements Using Linear Programming  Proc. 5th ONR/GTRI Workshop on Target Tracking Newport, RI, June 2002 8] Y. Eldar, and A. V. Oppenheim  Covariance Shaping Least-Square Estimation  IEEE Trans. Signal Processing, 51\(3 pp. 686-697 9] Y. Eldar  Minimum Variance in Biased Estimation: Bounds and Asymptotically Optimal Estimators  IEEE Trans. Signal Processing, 52\(7 10] Y. Eldar, A. Ben-Tal, and A. Nemirovski  Linear Minimax Regret Estimation of Deterministic Parameters with Bounded Data Uncertainties  IEEE Trans. Signal Processing, 52\(8 Aug. 2004 11] S. Kay  Conditional Model Order Estimation  IEEE Transactions on Signal Processing, 49\(9 12] X. R. Li, Y. Zhu, J. Wang, and C. Han  Optimal Linear Estimation Fusion  Part I: Unified Fusion Rules  IEEE Trans. Information Theory, 49\(9  2208, Sept. 2003 13] X. R. Li  Optimal Linear Estimation Fusion  Part VII: Dynamic Systems  in Proc. 2003 Int. Conf. Information Fusion, Cairns, Australia, pp. 455-462, July 2003 14] X. D. Lin, Y. Bar-Shalom and T. Kirubarajan  Multisensor Bias Estimation Using Local Tracks without A Priori Association  Proc SPIE Conf. Signal and Data Processing of Small Targets \(Vol 


SPIE Conf. Signal and Data Processing of Small Targets \(Vol 5204 15] R. Popp, K. R. Pattipati, and Y. Bar-Shalom  An M-best Multidimensional Data Association Algorithm for Multisensor Multitarget Tracking  IEEE Trans. Aerospace and Electronic Systems, 37\(1 pp. 22-39, January 2001 pre></body></html 


20 0  50  100  150  200  250  300 Pe rc en ta ge o f a dd iti on al tr af fic Cache size 200 clients using CMIP 200 clients using UIR c Figure 6. The percentage of additional traf?c the cache at every clock tick. A similar scheme has been proposed in [13], which uses fv, a function of the access rate of the data item only, to evaluate the value of each data item i that becomes available to the client on the channel If there exists a data item j in the client  s cache such that fv\(i j replaced with i A prefetch scheme based on the cache locality, called UIR scheme, was proposed in [7]. It assumes that a client has a large chance to access the invalidated cache items in the near future. It proposes to prefetch these data items if it is possible to increase the cache hit ratio. In [6], Cao improves the UIR scheme by reducing some unnecessary prefetches based on the prefetch access ratio \(PAR scheme, the client records how many times a cached data item has been accessed and prefetched, respectively. It then calculates the PAR, which is the number of prefetches divided by the number of accesses, for each data item. If the PAR is less than one, it means that the data item has been accessed a number of times and hence the prefetching is useful. The clients can mark data items as non-prefetching when PAR &gt; b, where b is a system tuning factor. The scheme proposes to change the value of b dynamically according to power consumption. This can make the prefetch scheme adaptable, but no clear methodology as to how and when b should be changed. Yin et al. [19] proposed a power-aware prefetch scheme, called value-based adaptive prefetch \(VAP the number of prefetches based on the current energy level to prolong the system running time. The VAP scheme de?nes a value function which can optimize the prefetch cost to achieve better performance These existing schemes have ignored the following characteristics of a mobile environment: \(1 query some data items frequently, \(2 during a period of time are related to each other, \(3 miss is not a isolated events; a cache miss is often followed by a series of cache misses, \(4 eral requests in one uplink request consumes little additional bandwidth but reduces the number of future uplink requests. In this paper, we addressed these issues using a cache-miss-initiated prefetch scheme, which is based on association rule mining technique. Association rule mining is a widely used technique in ?nding the relationships among data items. The problem of ?nding association rules among items is clearly de?ned by Agrawal et al. in [5]. However in the mobile environment, one cannot apply the existing association rule mining algorithm [4] directly because it is too complex and expensive to use This makes our algorithm different from that of [4] in 


This makes our algorithm different from that of [4] in twofold. First, we are interested in rules with only one data item in the antecedent and several data items in the consequent. Our motivation is to prefetch several data items which are highly related to the cache-miss data item within Proceedings of the 2004 IEEE International Conference on Mobile Data Management \(MDM  04 0-7695-2070-7/04 $20.00  2004 IEEE the cache-miss initiated uplink request. We want to generate rules where the antecedent is one data item, but the cache-missed data item and the consequent is a series of data items, which are highly related to the antecedent. If we have such rules, we can easily ?nd the data items which should also be piggybacked in the uplink request. Second in mobile environment, the client  s computation and power resources are limited. Thus, the rule-mining process should not be too complex and resource expensive. It should not take a long time to mine the rules. It should not have high computation overhead. However, most of the association rule mining algorithms [4, 5] have high computation requirements to generate such rules 5. Conclusions Client-side prefetching technique can be used to improve system performance in mobile environments. However, prefetching also consumes a large amount of system resources such as computation power and energy. Thus, it is very important to only prefetch the right data. In this paper, we proposed a cache-miss-initiated prefetch \(CMIP scheme to help the mobile clients prefetch the right data The CMIP scheme relies on two prefetch sets: the alwaysprefetch set and the miss-prefetch set. Novel association rule based algorithms were proposed to construct these prefetch sets. When a cache miss happens, instead of sending an uplink request to only ask for the cache-missed data item, the client requests several items, which are within the miss-prefetch set, to reduce future cache misses. Detailed experimental results veri?ed that the CMIP scheme can greatly improve the system performance in terms of increased cache hit ratio, reduced uplink requests and negligible additional traf?c References 1] S. Acharya, M. Franklin, and S. Zdonik. Prefetching From a Broadcast Disk. Proc. Int  l Conf. on Data Eng., pages 276  285, Feb. 1996 2] S. Acharya, M. Franklin, and S. Zdonik. Balancing Push and Pull for Data Broadcast. Proc. ACM SIGMOD, pages 183  194, May 1997 3] S. Acharya, R. Alonso, M. Franklin, and S. Zdonik. Broadcast disks: Data Management for Asymmetric Communication Environments. Proc. ACM SIGMOD, pages 199  210 May 1995 4] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In J. B. Bocca, M. Jarke, and C. Zaniolo editors, Proc. 20th Int. Conf. Very Large Data Bases, VLDB pages 487  499. Morgan Kaufmann, 12  15 1994 5] R. Agrawal, Tomasz Imielinski, and Arun Swami. Mining Association Rules Between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207  216, Washington, D.C May 1993 6] G. Cao. Proactive Power-Aware Cache Management for Mobile Computing Systems. IEEE Transactions on Computers, 51\(6  621, June 2002 7] G. Cao. A Scalable Low-Latency Cache Invalidation Strategy for Mobile Environments. IEEE Transactions on Knowledge and Data Engineering, 15\(5 ber/October 2003 \(A preliminary version appeared in ACM MobiCom  00 8] K. Chinen and S. Yamaguchi. An Interactive Prefetching Proxy Server for Improvement of WWW Latency. In Proc INET 97, June 1997 9] E. Cohen and H. Kaplan. Prefetching the means for docu 


9] E. Cohen and H. Kaplan. Prefetching the means for document transfer: A new approach for reducing web latency. In Proceedings of IEEE INFOCOM, pages 854  863, 2000 10] R. Cooley, B. Mobasher, and J. Srivastava. Data preparation for mining world wide web browsing patterns. Knowledge and Information Systems, 1\(1  32, 1999 11] C. R. Cunha, Azer Bestavros, and Mark E. Crovella. Characteristics of WWW Client Based Traces. Technical Report TR-95-010, Boston University, CS Dept, Boston, MA 02215, July 1995 12] D. Duchamp. Prefetching hyperlinks. In USENIX Symposium on Internet Technologies and Systems \(USITS  99 1999 13] V. Grassi. Prefetching Policies for Energy Saving and Latency Reduction in a Wireless Broadcast Data Delivery System. In ACM MSWIM 2000, Boston MA, 2000 14] S. Hameed and N. Vaidya. Ef?cient Algorithms for Scheduling Data Broadcast. ACM/Baltzer Wireless Networks \(WINET  193, May 1999 15] Q. Hu and D. Lee. Cache Algorithms based on Adaptive Invalidation Reports for Mobile Environments. Cluster Computing, pages 39  48, Feb. 1998 16] Z. Jiang and L. Kleinrock. An Adaptive Network Prefetch Scheme. IEEE Journal on Selected Areas in Communications, 16\(3  11, April 1998 17] V. Padmanabhan and J. Mogul. Using Predictive Prefetching to Improve World Wide Web Latency. Computer Communication Review, pages 22  36, July 1996 18] N. Vaidya and S. Hameed. Scheduling Data Broadcast in Asymmetric Communication Environments. ACM/Baltzer Wireless Networks \(WINET  182, May 1999 19] L. Yin, G. Cao, C. Das, and A. Ashraf. Power-Aware Prefetch in Mobile Environments. IEEE International Conference on Distributed Computing Systems \(ICDCS 2002 Proceedings of the 2004 IEEE International Conference on Mobile Data Management \(MDM  04 0-7695-2070-7/04 $20.00  2004 IEEE pre></body></html 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





