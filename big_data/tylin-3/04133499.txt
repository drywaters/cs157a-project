HybridMiner Mining Maximal Frequent Itemsets Using Hybrid Database Representation Approach Shariq Bashir and A Rauf Baig FAST-National University of Computer and Emerging Sciences Islamabad Pakistan shariqadel@yahoo com rauf baig@nu edu.pk Abstract In this paper we present a novel hybrid arraybased layout and vertical bitmap layout database representation approach for mining complete Maximal Frequent Itemset MFI on sparse and large datasets Our work is novel in terms of scalability 
item search order and two horizontal and vertical projection techniques We also present a maximal algorithm using this hybrid database representation approach Dijferent experimental results on real and sparse benchmark datasets show that our approach is better than previous state of art maximal algorithms 1 Introduction Frequent item set mining is 
as association rule mining ARM inductive 
one of the fundamental problems in data mining and has many applications such 
databases and query expansion The association rule mining 
n An itemset X is frequent if it contains at least a transactions where a is the minimum support An itemset X is maximal if it is not 
was first introduced by Agrawal 2 Let T be the transactions of the database and X be the set of items from 1 to are long mining all 
a subset of any other known frequent itemset When the frequent patterns 
frequent itemsets FI is infeasible because of the exponential number of frequent itemsets Thus algorithms for mining Frequent Closed Itemsets FCI 9 as 
now turn to find Maximal Frequent Itemsets MFI Given the set of MFI it is easy to analyze many interesting properties of the dataset such 
a result researchers 
are proposed because FCI is enough to generate association rules However FCI could also be exponentially large as the Fl As 
the longest pattern the overlap of the MFI etc All Fl 
on any part of the MFI to do supervise data mining Lot of recent MFI algorithms 3 5 6 use vertical and horizontal data layout schemes which employ a bottom-up breadth-first search Vertical data layout schemes are very effective when the 
from MFI and a single scan of the database Moreover we can focus 
can be counted for support in 
can be built up 
dataset is dense and small However they are less efficient when the average number of items in transactions is sufficiently less than the total number of items which usually happens in the case of sparse and large datasets Recent studies on enumerating all frequent itemset by using array based layout schemes 7 8 show that these layout schemes are very effective for sparse and large datasets An array based database layout scheme 
projects compressed transactions at each node of search space using pointer adjustment and filtering techniques In this paper we propose a novel database representation approach which uses a hybrid representation of both array based and vertical bitmap layout schemes With array based layout approach we can achieve vertical and horizontal projection and fast frequency counting whereas with vertical bitmap layout approach we can reorder tail elements by ascending frequency order We also present a 
maximal itemset algorithm for hybrid database representation approach which achieves both vertical and horizontal projection Different results on real sparse datasets show that our algorithm is very scalable for sparse and large datasets which does not require any extra memory for projection 2 Preliminaries and Related Work Table 1 A sample experimental transactional dataset Transactions Items 01 a b d 02 c 03 a c e 04 b c 05 ac 


Table 1 shows a sample transactional dataset where first column represents transactions and second column represents items of transactions All items in transactions are sorted according to a lexicographical order 8 We say Ij  Ik if item Ij occurs before item Ik in the ordering Each node of search space is composed of head and tail elements which represent a state in the search space This search space can be traversed by depth first or breadth first search Where heads are the candidate of MFI and tail elements are possible extensions of new heads For example at root node head is empty set and tail elements are a b c d e which generate five possible heads a:bcde b:cde c:de d:e e  2.1 Related Work In last five years lots of algorithms are proposed for efficient mining of maximal itemsets Bayardo in 3 introduced MaxMiner to mine only long patterns maximal frequent itemsets MaxMiner performs breadth-first search and uses look-ahead pruning on search space branches The look-ahead uses superset pruning i.e if the head of a node with its tail is frequent there is no need to further process the node since all descents of the node will be frequent MaxMiner also first introduced the heuristic of reordering items in the tail of a node in the increasing order of their support This technique is known as dynamic reordering DepthProject 1 is a depth first search approach and projects compressed transactions on the current node to speedup the frequency checking costs DepthProject also utilizes the look-ahead pruning and dynamic reordering to delete the infrequent items from tail Mafia 5 proposed parent equivalence pruning PEP and differentiates superset pruning into two classes FHUT and HUTMFI for efficient pruning of non-maximal search space Mafia also uses dynamic reordering to reduce the search space The results show that PEP has the biggest effect of the above pruning methods PEP FHUT and HUTMFI Both DepthProject and Mafia mine a superset of the MFI and require a post-pruning to eliminate nonmaximal patterns Algorithm GenMax 6 integrates pruning with mining and returns the exact MFI by using two strategies First just like transaction database is projected on current node the discovered MFI set can also be projected on the node and thus yields fast superset checking Second GenMax uses Diffset propagation to perform fast frequency computation Experimental results show that GenMax has comparable performance as Mafia 3 Hybrid Array based and Vertical bitmaps layouts Database Representation Approach In this section we propose a novel database representation scheme which is hybrid representation of both array based and vertical bitmaps layouts Both the layout schemes have their own individual advantages and disadvantages Array based layout scheme is a scalable approach and requires a very small memory for projection One of the main disadvantage of array based layout scheme is that it follows a static item search order With static item search order we can't perform dynamic reordering step which dynamically reduces search space at runtime 3 Vertical bitmap layout is not scalable approach but by using vertical bitmap layout scheme we can perform dynamic reordering with ascending frequency order which reduces search space at runtime Table 2 compares different ARM algorithms in terms of scalability and item search order It can be safely conclude that array based approach is better in terms of scalability and database projection projection Whereas vertical bitmaps is better in term of item search order which dramatically reduce the search space at runtime In our hybrid approach we combine the good features of both array based and vertical bitmaps approaches Which not only optimize frequency counting by projection but also reduces search space by ascending frequency order item search order using vertical bitmaps The basis strategy of our mining approach is to traverse search space by depth first search DFS On each node of search space we projects relevant transactions by array based approach H-Struct and reorder tail items by ascending frequency order using vertical bitmaps Table 2 Comparison of ARM algorithms Strategy Memory Item required for search projection order Array Small Header Static approach Tables order Vertical Large vertical Ascending bitmaps bitmaps frequency order Hybrid Small Header Ascending approach Tables frequency order 3.1 Projected Database Representation PDR 


our approach is a H-Mine 8 and OP 7 proposed similar type of filter array based schemes Our approach is different from previous work in three c d _e 3 2 4 1 0000c 111-111 Head Link Each item contains a pointer to Transaction bitmap called bitmap link any node in the search space is projected representation of its parent node if it maintains transactions that contribute to the further construction of descendant nodes Otherwise node is unprojected Figure 2 shows the compressed nodes that share frequency order instead of fix static order for dynamic reordering using vertical layout scheme Third by using hybrid array based and vertical layout schemes we can optimize frequency counting cost with horizontal projection c,4 d,I e,I 3.2 Hybrid Database Approach HDR Figure 3 shows the HDR database representation of Table 1 dataset Header Table Transaction 03 ace 05 ac e,1I Figure 1 Compressed Representation of Table 1 dataset Figure 1 shows the projected database representation of Table 1 dataset with minimum support equal to 2 We any additional memory for projection Second we use ascending uses the following terms and principles to maintain its dataset Header Links Each node of search space contains PDR transactions projected by its parent pointer With using header link we can get a pointer of first transaction in PDR For example item ta in Figure 3 has header link to transaction 01 Header links can be changed when vertical reordering is performed Vertical Links As a pointer of first transaction in PDR using Header Link other transactions in the PDR can be traversed with Vertical Links Vertical links 05 ac d,I1  01 abd e 1 03  a Figure 2 Compressed nodes with head a b a  are bi-directional links and point to previous and next transactions in the PDR For example item a in transaction 03 has previous link to transaction 01 and subsequent link to transaction 05 fa,3 fb,2 Representation 01 abd 03 a 03 ace 05 we get say that a fb 1 01 abd c,2 ce ac senses First more scalable it does not require a prefix of head a b d a c b s 4 c Vertical Link Horizontal Link Figure 3 HDR Representation of Table 1 dataset Hybrid Database Representation HDR approach 


Horizontal Links Horizontal links are also bidirectional links and point to next and last items in a transaction The main difference between vertical and horizontal link is that the former is for items and the later is for transactions Bitmap Links HDR achieves horizontal projection when the ATL Average Transaction Length is very small Normally on the top level search space nodes where ATL is very small horizontal projection optimizes frequency counting cost But on lower nodes PDR shrinks and becomes dense Here transaction bitmaps similar to vertical bitmaps gives a performance better than the horizontal links 3.3 Vertical Projection We can define vertical projection as follows let X be the head of node P and contains T transactions Let Y be an item picked from tail of node P We know that itemset tXUY will contain exactly T or less than T transactions An algorithm with vertical projection will propagate only those transactions to P children that share a prefix X Example 1 Figure 4 shows the process of propagating projected transactions using vertical projection from root node to maximal itemset a c with minimum support of 2 At root node items a b c are locally frequent because their support is greater than or equal to 2 By using this information root node prepares a new child node a with head a and tail b c Where only transactions 01 03 05 are propagated to node a the search then explore new node by depth first search In next recursion node a calculate the frequency support of its tail items b c by only traversing its locally projected transactions 01 03 05 which are less than total 5 transactions At node a item c is only locally frequent among all tail items of node a Then node a prepares a new node a c with head a c and tail  where only transactions 03 05 are propagated to node a c This is due to fact that itemset a c is present in all these transactions After this the search then explore new node a c At node a c tail is empty so itemset a c is new maximal itemset Note that each node of search space modifies its header links of tail items with new links and vertical links in the node's PDR are adjusted by using vertical reordering Vertical Reordering Each parent node performs vertical reordering to project compressed transactions on its children nodes We know that by using header link we can get a pointer of first transaction in the PDR and with vertical links we can traverse other transactions in the PDR Vertical reordering step basically makes PDRs of children nodes A simple way of performing vertical reordering step is to traverse the node's PDR transactions one by one While by using horizontal link change the item header links with next links b,1 c,2 d,1 e,1 e 1 Figure 4 Example of mining maximal itemset a c using HDR approach 3.4 Horizontal Projection We can define horizontal projection as follows let P be the node in search space and X be the head of P and Y be the tail of P To check that tail extensions of a a c 2 0 c c d 0 e I 00 e 


an important consideration and has direct effect a given node same support as possible This heuristic node P are frequent or infrequent we must count the support of all tail AX U Z Z E Y items If we use vertical bitmaps to check the support of all the tail items of was originally presented by Rymon 9 and adopted by many MFI algorithms 3 5 6 The idea behind enumerating the MFI by using HybridMiner approach is to traverse search space in depth first space with head X and tail Y If tail Y is the subset of any known maximal frequent itemset then whole sub tree and sibling is pruned away 5 Lemma3 Let P be the node with head X and tail Y If Y element S has we use vertical bitmaps Figure 5 shows the pseudo code of Vertical and Horizontal Projection Lines from 3 to 8 show the frequency counting process with horizontal links Lines from 10 to 12 show the frequency counting process with bitmap links Figure 5 Pseudo code of Vertical and Horizontal projection 4 HybridMiner Maximal Enumeration Algorithm HybridMiner traverse the search space in we use horizontal projection otherwise manner reduces the search space Lexicographical order we employ in support ofy in support array 6 make y the new header link 7 move previous header link to next oJry 8 i=x 9 else 10 for all tail elements in P tail y e P tail 1 ify is in the bitmap link 12 increment support ofy in support array 13 return support storage HybridMiner PDR Node P IsHUT FI Support 1 HUT  P head UP tail 2 If HUT is MFI 3 Stop generation of children and return 4 VertHorzProjection PDR P 5 use PEP to trim the tail and reorder by increasing support 6 for each item x in P.reorder tail 7 IsHUT  whether x is the first item in the tail 8 HDR x newNode IsHUT Support of x 9 if IsHUT and all extensions are frequent 10 stop search and go back up subtree 1 if P is a leaf and P.head is not in MFI 12 Add C.head to MFI was first used by Bayardo 3 and also used in many other algorithms 5 6 VertHorzProjection node PDR Node P I i  get header link in PDR 2 retrieve vertical links in PDR tx e using i 3 ifP tail less than ATL 4 for each horizontal link ofx ty e horizontal link Of x 5 increment manner Where infrequent branches on search space Ordering the tail elements possible children by increasing support will keep the search space are more costly than vertical bitmaps if ATL is very close to total tail items A simple heuristic that are generated by using lexicographical order Generating children in this are pruned away by tree pruning techniques described in 5 4.1 Search space Pruning Techniques Lemmal Let P be the node of search space with head X and tail Y If XUY is our total support checking cost will be O\(n*m where m is the number of transactions and n is the number of tail elements With horizontal projection using horizontal links we can optimize this O\(n*m cost Example 2 Figure 4 shows PDR transactions of root node In Figure 4 our ATL is 2.2 which is less than half of total tail elements 5 With horizontal projection we can reduce root node frequency counting cost from 25 five transactions  five tail items to 11 We observe that traversing tail items in PDR transactions with horizontal links our algorithm is that if PDR ATL is less than half of total tail elements then a lexicographical order where root element contains empty list Possible children at each level are also frequent but not maximal and can be pruned away 5 Lemma2 Let P be the node of search as head X then S is moved from tail to head We know that transactions\(X c transactions S 5 Figure 6 Pseudo code of HybridMiner Algorithm 4.2 Reordering Tail Elements The order of the tail elements is also as small a maximal frequent itemset then all subsets of tail Y combined with head X 


4.3 Checking MFI To check whether current frequent itemset found is maximal or not has a major impact on running time of any MFI algorithm Current frequent itemset is compared will all previously found MFI and if it is maximal then it is added into MFI storage When the minimal support is very low then MFI checking takes a heavy computation time to optimize this cost GenMax proposed a LMFI approach for MFI checking LMFI is calculated from parent node and propagated to its lower level children Different results show that LMFI has a major impact on the running time of algorithm Figure 6 shows pseudo code of HybridMiner maximal frequent itemset mining algorithm 5 Results and Implementation The source code of HybridMiner is written and complied in Microsoft Visual C 6.0 Experiments are conducted on the Pentium4 2.4 GHz processor with main memory of size 256 MB running windows NT 2000 5.1 Comparing Maximal Mining Algorithms For comparison we have use implementations of In this section we give the performance of our algorithm versus Mafia GenMax Eclat-max and apriori_max on the sparse benchmark datasets downloaded from 11 The main features of the datasets are listed in Table 3 Table 3 Main features of datasets Dataset Items Average Records Length T1OI4D100K 1000 10 100,000 BMS-WebViewl 497 2.5 59,602 BMS-WebView2 3341 5.6 77,512 BMS-POS 1658 7.5 515,597 Kosarak 20,753 8.1 66,283 Retail 16,469 10.3 88,162 The performance measure is the execution time of the algorithms on the sparse datasets with different support threshold Figures from 7\(a to 7\(f show the performance curve of two algorithms As we can see the HybridMiner algorithm outperforms the other algorithms on almost all sparse datasets The performance improvements of HybridMiner over other algorithms are significant at reasonably low support thresholds 1 Mafia 5 Uses vertical bit vector database representation approach for itemset frequency calculation DFS traversal for itemset generation Implementation is available at www.fimi.cs.helsinki.fi/fimiO3/implementatio ns.html 2 GenMax 6 Uses vertical diffset database representation approach itemset frequency calculation backtracking for itemset generation Implementation is available at www.adrem/ua.ac/be/goethals/software 3 Apriori-max 4 Uses tree projection for itemset frequency calculation BFS traversal for candidate itemset generation Implementation is available at http://fuzzy.cs.uni-magdeburg.de/borgelt/soft ware.html 4 Eclat-max 4 Uses vertical bitmap database representation approach with no projection for itemset frequency calculation DFS traversal for itemset generation Implementation is available at http://fuzzy.cs.uni-magdeburg.de/borgelt/soft ware.html 5.2 Results 


paper we present uses ascending frequency order a maximal frequent itemset mining algorithm by using our hybrid approach Different computational experiments show that Hybrid database representation approach is better than the previous techniques in three ways Firstly it is uses static order Thirdly it optimizes frequency counting cost by using horizontal projection 7 References 1 R Agrawal C Aggarwal and V Prasad Depth first generation of long patterns In SIGKDD 2000 2 R Agrawal and R Srikant Fast algorithms for mining association rules In Proceedings of the 20th VLDB Conference Santiago Chile 1994 3 R.J Bayardo Efficiently mining long patterns from databases In SIGMOD 1998 4 C Borgelt Efficient Implementation of Eclat and Apriori In Proc of the IEEE ICDM Workshop on Frequent Itemset Mining Implementations 2003 5 D Burdick M Calimlim and J Gehrke Mafia A maximal frequent itemset algorithm for transactional databases In Proc ofICDE Conf pp 443 452 200 1 6 K Gouda and M J Zaki Efficiently mining maximal frequent itemsets In ICDM pp 163-170 2001 7 J Liu Y Pan K Wang and J Han Mining frequent item sets by opportunistic projection In Proc of KDD Conf 2002 8 J Pei J Han H Lu S Nishio S Tang and D Yang H-Mine Hyper-structure mining of frequent patterns in large databases In Proc of ICDM Conf pp 441.448 2001 9 R Rymon Search through Systematic Set Enumeration In Proc of Third Int'l Conf On Principles of Knowledge Representation and Reasoning pp 539 550 1992 10 T Uno T Asai Y Uchida and H Arimura LCM An Efficient Algorithm for Enumerating Frequent Closed Item Sets In Proc IEEE ICDM'03 2003 11 100.00 10.00 E F 1.00 0.10 100.00 nnI max Eclatmax 0 0n 1 0.00 E 1.00 50 45 40 35 30 27 24 21 18 15 12 9 Support Figure 7\(d T1014Dl00K dataset 10oo000 mtlmbx  lat-maEx 10.00 E F 1.00 0.10 Nbfia 100.00 GenNbx Apriorimx Eclaton array based approach where previous approach H-Mine mx 1000 E 110 100 95 90 85 80 75 70 65 60 55 50 Support Figure 7\(b Kosarak dataset I.ia GenMax Apriori-mx Eclat-m3x 10.00 u E F 1.00 0.10 91 81 60 50 40 35 30 27 24 21 18 15 12 Support Figure 7\(e BMS1 dataset I\\/bf ia GenlMbx Apriori-nx Eclat-nax uu 1600 1400 1200 1000 870 810 750 Support Figure 7\(c BMS-POS dataset 100.00 I\\/bfia Genl\\bx x Apriori-max 10.00  Eclat-max 1 00 0.10 50 45 40 35 30 27 24 21 18 15 12 9 Support Figure 7\(f BMS2 dataset 6 Conclusion In this array based and vertical bitmap schemes We also present a novel hybrid database representation approach using a scalable approach for sparse and real dataset and does not require any extra memory from projection projection Secondly it 41 Ger Apri Ecla 76 58 41 23 05 88 68 40 25 Support Figure 7\(a Retail dataset Genl\\bx Apriori 


so on. Since s  a 4.5  0.5 frequent in this projected database, combining it with the associated pre?x e yields fuzzy frequent pattern  ea  This list array is then processed recursively before working on the next list, i.e., the one for item a in the main set \(Figure 2 left Proceedings of the Fourth International Conference on Machine Learning and Applications  ICMLA  05 0-7695-2495-8/05 $20.00  2005 IEEE census number of sets time/s Relim \(original data Relim \(with deletions Relx \(no insertion Relx \(wmin = 0.4 Relx \(wmin = 0.2 T10I4D100K number of sets time/s Relim 10 0.01 Relx \(no insertion Relx \(wmin = 0.4 Relx \(wmin = 0.2 Table 2. Results on census and T10I4D100K Note that there are now list elements referring to empty transactions. In contrast to exact mining, mining fuzzy frequent patterns requires that we also reassign and copy a list element representing a \(shrunk item. Even though the transaction is empty after the leading item is removed, it has to be maintained as it can be processed further by inserting items. Therefore such a list element is reassigned/copied  as an empty transaction  to the list associated with the only item it contains. An example of this can be seen when the list corresponding to item a is processed \(step 2 a \(shrunk counter for item d is incremented and an empty element is kept in the list associated with item d \(both reassignment and copy In addition, a new element labeled ? is added to the array of transaction lists. This new list is needed when an empty transaction has to be reassigned/copied. Since an empty transaction has no leading item, it cannot be inserted into one of the lists corresponding to the items of the database. It cannot be discarded either, because in later processing items may be inserted into it, and then it has to be considered in the corresponding recursion. Formally, ? can be seen as an additional pseudo-item, which is contained in all transactions, but which is not to be reported as part of a frequent pattern. An example of how this new array element is used can be seen when the list corresponding to item b is processed \(step 4 which are reassigned and copied to the additional lists array element labeled with ?. Even though these transactions are now empty, they have to be considered when processing item d, because this item may be inserted into them 5. Experimental results To evaluate our algorithm, we implemented it in C and ran experiments on a laptop with a 1.8 GHz Intel Pentium Mobile processor and 1 GB main memory using Windows XP Professional SP2. Results obtained with the original program \(exact frequent pattern mining  Relim  those for fuzzy frequent pattern mining  Relx   In all experiments, we updated the weight of a transaction by multiplying it with an insertion cost factor \(if necessary And unless otherwise stated, uniform insertion costs of 0.5 for all items \(it is rather a arbitrary choice merely for illustration weight \(thus allowing exactly one insertion In an initial test, we used the very simple transaction database shown in Table 1, using a minimum support of 30%, to check the basic functionality of the approach When mining this database with exact matching \(i.e. without insertions 


out insertions matching 23 patterns are found. The item set ec, which we used as an example in Section 2, is found with fuzzy matching, but not with exact matching. On the other hand, if the insertion of item e is ruled out by setting c\(e set ec is not found anymore To check the performance on larger data sets, we tested our programs on the data sets census [4] and T10I4D100K 1], with a minimum support of 30% for census and 5 for T10I4D100K. The number of frequent item sets discovered and the corresponding execution time \(in seconds shown in Table 2. If insertions were inhibited, the number of sets reported by Relx coincides with that of Relim, proving the sanity of the implementation. However, as was to be expected, Relx needs more time as it has to invest additional effort into managing empty transactions \(cf. Section 4; an additional factor is the computation of penalized weights which takes place nevertheless Results produced by Relx with different thresholds for the transaction weight \(allowing 0 to 2 insertions  not surprisingly  that with decreasing threshold the number of frequent patterns and the execution time increases Frequent patterns that could not be found before are now discovered. Note, however, that the frequent patterns now have fractional support due to the transaction weighting Note also that the execution times are still bearable, even though the insertions make it necessary to process a much higher number of transactions in the recursion Finally, we ran the program on data from which items had been deleted randomly to check the effectiveness of the proposed algorithm. Here we present only an example of the results. We randomly deleted 4% of item  hours=full-time  and 3% of the item  sex=female  from the census data set to simulate the missing items and then mined with a minimum support of 30%. We ran Relim on both the unmodi?ed and the preprocessed data. With the former 244 frequent itemsets were found, while only 238 of them were detected in the later \(cf. Table 2 is, due to missing occurrences of two items, we lost 6 Proceedings of the Fourth International Conference on Machine Learning and Applications  ICMLA  05 0-7695-2495-8/05 $20.00  2005 IEEE frequent itemsets    country=United-States, hours=fulltime, loss=none, race=White, workclass=Private    country=United-States, hours=full-time, loss=none sex=Male    gain=none, hours=full-time, salary?50K    gain=none, loss=none, sex=female    hours=full-time loss=none, race=White, sex=Male  and  hours=full-time loss=none, salary?50K  When we used Relx with an insertion cost of 0.5 for both items, we could ?nd the complete item sets that were obtained by Relim from the unmodi?ed data. In fact, a superset of the original frequent patterns were reported. However, we have to accept this as an inherent feature of the insertion concept. Still the results are encouraging, and prove that our algorithm is capable of rediscovering frequent itemsets, which are lost with classical approaches due to missing information in the data 6. Conclusions Frequent pattern mining on real-world data with missing information calls for fuzzy mining. Facing this challenge, we introduced a concept of fuzzy frequent patterns based on transaction editing. The algorithm we developed for mining fuzzy frequent patterns relies on deleting items editing transactions, recursive processing, and reassigning transactions. Our algorithm is very simple, works without complicated data structures, and performs reasonably well Some other work in this direction like in [7], [12], perform approximate matching only by counting the number of different items in the two item sets to be compared, and use an Apriori-like algorithm. The algorithm in [12], as re 


use an Apriori-like algorithm. The algorithm in [12], as reported, performed much slower \(about 100 times or even more ber of mismatches from 1 to 2. Compared to this our approach provides two main advantages: \(1 matching is based on a more general scheme  edit operations. It allows the individual treatment of every single item, which enables a better involvement of background knowledge. \(2 many times \(which is the case in Apriori-like algorithms Instead it looks for \(locally projected databases and combines them with the associated pre?x \(which is the frequent pattern found so far the frequent patterns. Thus it is more ef?cient, as can be seen in Table 2. The execution time in the case of allowing two insertions is only about thrice that for one insertion Up to now, we only investigated how to edit an item set by insertion. However, there are also other interesting editing operations. If we take the order of items into account operations like exchanging the order of two items are de?nitely worth to be studied References 1] Synthetic data generation code for associations and sequential patterns. Intelligent Information Systems, IBM Almaden Research Center http://www.almaden.ibm.com/software/quest/Resources index.shtml 2] R. Agrawal, T. Imielienski, and A. Swami. Mining association rules between sets of items in large databases. In Proc Conf. on Management of Data, pages 207  216, New York NY, USA, 1993. ACM Press 3] R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A. Verkamo. Fast discovery of association rules. In U.M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, eds. Advances in Knowledge Discovery and Data Mining, pages 307  328, Cambridge,CA,USA, 1996. AAAI Press / MIT Press 4] C. L. Blake and C. J. Merz. UCI repository of machine learning databases. Dept. of Information and Computer Science, UC Irvine, CA, USA, 1998 http://www.ics.uci.edu  mlearn/MLRepository.html 5] C. Borgelt. Keeping things simple: Finding frequent item sets by recursive elimination. In Proc. Workshop Open Source Data Mining Software \(OSDM  05, Chicago, IL 6] C. Borgelt. Ef?cient implementations of apriori and eclat. In Proc. 1st IEEE ICDMWorkshop on Frequent Itemset Mining Implementations \(FIMI 2003, Melbourne, FL shop Proc. 90, Aachen, Germany, 2003. http://www.ceurws.org/Vol-90 7] Y. Cheng, U. Fayyad, and P. Bradley. Ef?cient discovery of error-tolerant frequent itemsets in high dimensions. In Proc 7th Int. Conf. on Knowledge Discovery and Data Mining KDD  01, San Francisco, CA  203, New York NY, USA, 2001. ACM Press 8] J. Han, H. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In Proc. Conf. on the Management of Data \(SIGMOD  00, Dallas, TX  12, New York NY, USA, 2000. ACM Press 9] C. Kuok, A. Fu, and M. Wong. Mining fuzzy association rules in databases. SIGMOD Record, 27\(1  46, 1998 10] P. Moen. Attribute, Event Sequence, and Event Type Similarity Notions for Data Mining. Report A-2000-1. PhD thesis Department of Computer Science, University of Helsinki Finland, 2000 11] J. Pei, J. Han, H. Lu, S. Nishio, S. Tang, and D. Yang. Hmine: Hyper-structure mining of frequent patterns in large databases. In Proc. IEEE Conf. on Data Mining \(ICDM  01 San Jose, CA  448, Piscataway, NJ, USA, 2001 IEEE Press 12] J. Pei, A. K. H. Tung, and J. Han. Fault-tolerant frequent pattern mining: Problems and challenges. In Proc. ACM 


pattern mining: Problems and challenges. In Proc. ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery \(DMK  01, Santa Babara, CA May 2001, Santa Babara, CA 13] M. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. New algorithms for fast discovery of association rules. In Proc 3rd Int. Conf. on Knowledge Discovery and Data Mining \(KDD  97, Newport Beach, CA  296, Menlo Park, CA, USA, 1997. AAAI Press Proceedings of the Fourth International Conference on Machine Learning and Applications  ICMLA  05 0-7695-2495-8/05 $20.00  2005 IEEE pre></body></html 


Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 7 Conclusion and Future Work References frontier  pages 487\320499 Morgan Kaufmann 1994  W orkshop on frequent itemset mining implementations 2003 http://\336mi.cs.helsinki.\336/\336mi03  W orkshop on frequent itemset mining implementations 2004 http://\336mi.cs.helsinki.\336/\336mi04  J  Han J  Pei and Y  Y in Mining frequent patterns without candidate generation In Proceedings of 20th International Conference on Very Large Data Bases VLDB VLDB Journal Very Large Data Bases Data Mining and Knowledge Discovery An International Journal Lecture Notes in Computer Science  2004  J W ang and G Karypis Harmon y Ef 336ciently mining the best rules for classi\336cation In The Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD\32504 Symposium on Principles of Database Systems 2 b Average time taken per frequent itemset shown on two scales T10I4D100K is increased and hence the number of frequent items decreases Figure 5\(c also shows that the maximum frontier size is very small Finally we reiterate that we can avoid using the pre\336x tree and sequence map so the only space required are the itemvectors and the minSup SIAM International Conference on Data Mining required drops quite quickly as ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery 2000 ACM SIGMOD Intl Conference on Management of Data Figure 5 Results  8\(3\3204 2000  F  P an G C ong A T ung J Y ang and M Zaki Carpenter Finding closed patterns in long biological datasets In  2121:236 2001  M Steinbach P N T an H Xiong and V  K umar  Generalizing the notion of support In a Runtime ratios T10I4D100K c Number of Itemvectors needed and maximum frontier size T10I4D100K  pages 1\32012 ACM Press May 2000  F  K orn A Labrinidis Y  K otidis and C F aloutsos Quanti\336able data mining using ratio rules  Morgan Kaufmann 2003  J Pei J Han and L Lakshmanan Pushing convertible constraints in frequent itemset mining We showed interesting consequences of viewing transaction data as itemvectors in transactionspace and developed a framework for operating on itemvectors This abstraction gives great 337exibility in the measures used and opens up the potential for useful transformations on the data Our future work will focus on 336nding useful geometric measures and transformations for itemset mining One problem is to 336nd a way to use SVD prior to mining for itemsets larger than  pages 205\320215 2005  We also presented GLIMIT a novel algorithm that uses our framework and signi\336cantly departs from existing algorithms GLIMIT mines itemsets in one pass without candidate generation in linear space and time linear in the number of interesting itemsets Experiments showed that it beats FP-Growth above small support thresholds Most importantly it allows the use of transformations on the data that were previously impossible  That is the space required is truly linear  D Achlioptas Database-friendly random projections In  2001  R Agra w al and R Srikant F ast algorithms for mining association rules In  8:227\320252 May 2004  J Pei J Han and R Mao CLOSET An ef 336cient algorithm for mining frequent closed itemsets In  pages 21\32030 2000  S Shekhar and Y  Huang Disco v ering spatial colocation patterns A summary of results 


mator from sensor 1 also shown 6. CONCLUSIONS This paper derives a Bayesian procedure for track association that can solve a large scale distributed tracking problem where many sensors track many targets. When noninformative prior of the target state is assumed, the single target test becomes a chi-square test and it can be extended to the multiple target case by solving a multidimensional assignment problem. With the noninformative prior assumption, the optimal track fusion algorithm can be a biased one where the regularized estimate has smaller mean square estimation error. A regularized track fusion algorithm was presented which modifies the optimal linear unbiased fusion rule by a less-than-unity scalar. Simulation results indicate the effectiveness of the proposed track association and fusion algorithm through a three-sensor two-target tracking scenario 7. REFERENCES 1] Y. Bar-Shalom and W. D. Blair \(editors Tracking: Applications and Advances, vol. III, Artech House, 2000 2] Y. Bar-Shalom and H. Chen  Multisensor Track-to-Track Association for Tracks with Dependent Errors  Proc. IEEE Conf. on Decision and Control, Atlantis, Bahamas, Dec. 2004 3] Y. Bar-Shalom and X. R. Li, Multitarget-Multisensor Tracking Principles and Techniques, YBS Publishing, 1995 4] Y. Bar-Shalom, X. R. Li and T. Kirubarajan, Estimation with Applications to Tracking and Navigation: Algorithms and Software for Information Extraction, Wiley, 2001 5] S. Blackman, and R. Popoli  Design and Analysis of Modern Tracking Systems  Artech House, 1999 10 15 20 25 30 35 40 45 50 55 60 2 4 6 8 10 12 14 Time N o rm a li z e d e s ti m a ti o n e rr o r s q u a re d Target 1 Sensor 1 Centralized Est Track Fusion 10 15 20 25 30 35 40 45 50 55 60 0 2 


2 4 6 8 10 12 14 Time N o rm a li z e d e s ti m a ti o n e rr o r s q u a re d Target 2 Sensor 1 Centralized Est Track Fusion Fig. 7. Comparison of the NEES for centralized IMM estimator \(configuration \(i estimators \(configuration \(ii sensor 1 also shown 6] H. Chen, T. Kirubarajan, and Y. Bar-Shalom  Performance Limits of Track-to-Track Fusion vs. Centralized Estimation: Theory and Application  IEEE Trans. Aerospace and Electronic Systems 39\(2  400, April 2003 7] H. Chen, K. R. Pattipati, T. Kirubarajan and Y. Bar-Shalom  Data Association with Possibly Unresolved Measurements Using Linear Programming  Proc. 5th ONR/GTRI Workshop on Target Tracking Newport, RI, June 2002 8] Y. Eldar, and A. V. Oppenheim  Covariance Shaping Least-Square Estimation  IEEE Trans. Signal Processing, 51\(3 pp. 686-697 9] Y. Eldar  Minimum Variance in Biased Estimation: Bounds and Asymptotically Optimal Estimators  IEEE Trans. Signal Processing, 52\(7 10] Y. Eldar, A. Ben-Tal, and A. Nemirovski  Linear Minimax Regret Estimation of Deterministic Parameters with Bounded Data Uncertainties  IEEE Trans. Signal Processing, 52\(8 Aug. 2004 11] S. Kay  Conditional Model Order Estimation  IEEE Transactions on Signal Processing, 49\(9 12] X. R. Li, Y. Zhu, J. Wang, and C. Han  Optimal Linear Estimation Fusion  Part I: Unified Fusion Rules  IEEE Trans. Information Theory, 49\(9  2208, Sept. 2003 13] X. R. Li  Optimal Linear Estimation Fusion  Part VII: Dynamic Systems  in Proc. 2003 Int. Conf. Information Fusion, Cairns, Australia, pp. 455-462, July 2003 14] X. D. Lin, Y. Bar-Shalom and T. Kirubarajan  Multisensor Bias Estimation Using Local Tracks without A Priori Association  Proc SPIE Conf. Signal and Data Processing of Small Targets \(Vol 


SPIE Conf. Signal and Data Processing of Small Targets \(Vol 5204 15] R. Popp, K. R. Pattipati, and Y. Bar-Shalom  An M-best Multidimensional Data Association Algorithm for Multisensor Multitarget Tracking  IEEE Trans. Aerospace and Electronic Systems, 37\(1 pp. 22-39, January 2001 pre></body></html 


20 0  50  100  150  200  250  300 Pe rc en ta ge o f a dd iti on al tr af fic Cache size 200 clients using CMIP 200 clients using UIR c Figure 6. The percentage of additional traf?c the cache at every clock tick. A similar scheme has been proposed in [13], which uses fv, a function of the access rate of the data item only, to evaluate the value of each data item i that becomes available to the client on the channel If there exists a data item j in the client  s cache such that fv\(i j replaced with i A prefetch scheme based on the cache locality, called UIR scheme, was proposed in [7]. It assumes that a client has a large chance to access the invalidated cache items in the near future. It proposes to prefetch these data items if it is possible to increase the cache hit ratio. In [6], Cao improves the UIR scheme by reducing some unnecessary prefetches based on the prefetch access ratio \(PAR scheme, the client records how many times a cached data item has been accessed and prefetched, respectively. It then calculates the PAR, which is the number of prefetches divided by the number of accesses, for each data item. If the PAR is less than one, it means that the data item has been accessed a number of times and hence the prefetching is useful. The clients can mark data items as non-prefetching when PAR &gt; b, where b is a system tuning factor. The scheme proposes to change the value of b dynamically according to power consumption. This can make the prefetch scheme adaptable, but no clear methodology as to how and when b should be changed. Yin et al. [19] proposed a power-aware prefetch scheme, called value-based adaptive prefetch \(VAP the number of prefetches based on the current energy level to prolong the system running time. The VAP scheme de?nes a value function which can optimize the prefetch cost to achieve better performance These existing schemes have ignored the following characteristics of a mobile environment: \(1 query some data items frequently, \(2 during a period of time are related to each other, \(3 miss is not a isolated events; a cache miss is often followed by a series of cache misses, \(4 eral requests in one uplink request consumes little additional bandwidth but reduces the number of future uplink requests. In this paper, we addressed these issues using a cache-miss-initiated prefetch scheme, which is based on association rule mining technique. Association rule mining is a widely used technique in ?nding the relationships among data items. The problem of ?nding association rules among items is clearly de?ned by Agrawal et al. in [5]. However in the mobile environment, one cannot apply the existing association rule mining algorithm [4] directly because it is too complex and expensive to use This makes our algorithm different from that of [4] in 


This makes our algorithm different from that of [4] in twofold. First, we are interested in rules with only one data item in the antecedent and several data items in the consequent. Our motivation is to prefetch several data items which are highly related to the cache-miss data item within Proceedings of the 2004 IEEE International Conference on Mobile Data Management \(MDM  04 0-7695-2070-7/04 $20.00  2004 IEEE the cache-miss initiated uplink request. We want to generate rules where the antecedent is one data item, but the cache-missed data item and the consequent is a series of data items, which are highly related to the antecedent. If we have such rules, we can easily ?nd the data items which should also be piggybacked in the uplink request. Second in mobile environment, the client  s computation and power resources are limited. Thus, the rule-mining process should not be too complex and resource expensive. It should not take a long time to mine the rules. It should not have high computation overhead. However, most of the association rule mining algorithms [4, 5] have high computation requirements to generate such rules 5. Conclusions Client-side prefetching technique can be used to improve system performance in mobile environments. However, prefetching also consumes a large amount of system resources such as computation power and energy. Thus, it is very important to only prefetch the right data. In this paper, we proposed a cache-miss-initiated prefetch \(CMIP scheme to help the mobile clients prefetch the right data The CMIP scheme relies on two prefetch sets: the alwaysprefetch set and the miss-prefetch set. Novel association rule based algorithms were proposed to construct these prefetch sets. When a cache miss happens, instead of sending an uplink request to only ask for the cache-missed data item, the client requests several items, which are within the miss-prefetch set, to reduce future cache misses. Detailed experimental results veri?ed that the CMIP scheme can greatly improve the system performance in terms of increased cache hit ratio, reduced uplink requests and negligible additional traf?c References 1] S. Acharya, M. Franklin, and S. Zdonik. Prefetching From a Broadcast Disk. Proc. Int  l Conf. on Data Eng., pages 276  285, Feb. 1996 2] S. Acharya, M. Franklin, and S. Zdonik. Balancing Push and Pull for Data Broadcast. Proc. ACM SIGMOD, pages 183  194, May 1997 3] S. Acharya, R. Alonso, M. Franklin, and S. Zdonik. Broadcast disks: Data Management for Asymmetric Communication Environments. Proc. ACM SIGMOD, pages 199  210 May 1995 4] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In J. B. Bocca, M. Jarke, and C. Zaniolo editors, Proc. 20th Int. Conf. Very Large Data Bases, VLDB pages 487  499. Morgan Kaufmann, 12  15 1994 5] R. Agrawal, Tomasz Imielinski, and Arun Swami. Mining Association Rules Between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207  216, Washington, D.C May 1993 6] G. Cao. Proactive Power-Aware Cache Management for Mobile Computing Systems. IEEE Transactions on Computers, 51\(6  621, June 2002 7] G. Cao. A Scalable Low-Latency Cache Invalidation Strategy for Mobile Environments. IEEE Transactions on Knowledge and Data Engineering, 15\(5 ber/October 2003 \(A preliminary version appeared in ACM MobiCom  00 8] K. Chinen and S. Yamaguchi. An Interactive Prefetching Proxy Server for Improvement of WWW Latency. In Proc INET 97, June 1997 9] E. Cohen and H. Kaplan. Prefetching the means for docu 


9] E. Cohen and H. Kaplan. Prefetching the means for document transfer: A new approach for reducing web latency. In Proceedings of IEEE INFOCOM, pages 854  863, 2000 10] R. Cooley, B. Mobasher, and J. Srivastava. Data preparation for mining world wide web browsing patterns. Knowledge and Information Systems, 1\(1  32, 1999 11] C. R. Cunha, Azer Bestavros, and Mark E. Crovella. Characteristics of WWW Client Based Traces. Technical Report TR-95-010, Boston University, CS Dept, Boston, MA 02215, July 1995 12] D. Duchamp. Prefetching hyperlinks. In USENIX Symposium on Internet Technologies and Systems \(USITS  99 1999 13] V. Grassi. Prefetching Policies for Energy Saving and Latency Reduction in a Wireless Broadcast Data Delivery System. In ACM MSWIM 2000, Boston MA, 2000 14] S. Hameed and N. Vaidya. Ef?cient Algorithms for Scheduling Data Broadcast. ACM/Baltzer Wireless Networks \(WINET  193, May 1999 15] Q. Hu and D. Lee. Cache Algorithms based on Adaptive Invalidation Reports for Mobile Environments. Cluster Computing, pages 39  48, Feb. 1998 16] Z. Jiang and L. Kleinrock. An Adaptive Network Prefetch Scheme. IEEE Journal on Selected Areas in Communications, 16\(3  11, April 1998 17] V. Padmanabhan and J. Mogul. Using Predictive Prefetching to Improve World Wide Web Latency. Computer Communication Review, pages 22  36, July 1996 18] N. Vaidya and S. Hameed. Scheduling Data Broadcast in Asymmetric Communication Environments. ACM/Baltzer Wireless Networks \(WINET  182, May 1999 19] L. Yin, G. Cao, C. Das, and A. Ashraf. Power-Aware Prefetch in Mobile Environments. IEEE International Conference on Distributed Computing Systems \(ICDCS 2002 Proceedings of the 2004 IEEE International Conference on Mobile Data Management \(MDM  04 0-7695-2070-7/04 $20.00  2004 IEEE pre></body></html 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





