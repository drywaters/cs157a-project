html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap  A Transaction-based Neighbourhood-driven Approach to Quantifying Interestingness of Association Rules   B. Shekar Quantitative Methods and Information Systems Area Indian Institute of Management Bangalore Bannerghatta Road, Bangalore  560 076 Karnataka, INDIA shek@iimb.ernet.in Rajesh Natarajan Information Technology and Systems Group Indian Institute of Management Lucknow Prabandh Nagar, Lucknow-226 013 Uttar Pradesh, INDIA rajeshn@iiml.ac.in   Abstract  In this paper, we present a data-driven approach for ranking association rules \(ARs The occurrence of unrelated or weakly related item-pairs in an AR is interesting. In the retail market-basket context, items may be related through various relationships arising due to mutual interaction  substitutability  and  complementarity  Item-relatedness is a composite of these relationships. We introduce three relatedness measures for capturing relatedness between item-pairs. These measures use the concept of function embedding to appropriately weigh the relatedness contributions due to complementarity and substitutability between items. We propose an interestingness coefficient by combining the three relatedness measures. We compare this with two objective measures of interestingness and show the intuitiveness of the proposed interestingness coefficient   1. Introduction  An important problem in association rule \(AR mining is the generation of a large number of mined rules Usage of interestingness measures to rank ARs is a solution.  Objective measures [1, 12, 13] quantify the interestingness of a rule in terms of rule structure and the underlying data used in rule generation. Subjective measures [2, 3, 4, 5, 6, 8, 10, 18], in addition incorporate views of the user while evaluating interestingness of patterns Here, we consider the retail market-basket context ARs reveal items that are likely to be purchased together ARs that contain weakly related or unrelated items are interesting. This is because frequent co-occurrence of such items is unexpected. We present a data-driven approach to evaluating interestingness The paper is organized as follows. In the following section, we adopt a functional point of view to classify item interactions and the relationships arising from them It should be noted that we consider only the transactions containing customer purchases. We introduce and intuitively evolve three measures for capturing various aspects of item-relatedness namely, MI \(a modification of confidence example. The three item-relatedness measures are then combined to evolve a total relatedness coefficient \(TR We then develop an interestingness coefficient \(IC the basic relationship - relatedness and interestingness are mutually inverse concepts. We compare IC with two commonly used objective measures of interestingness and bring out its intuitiveness  2. Item Relatedness  Any item possesses many attributes or properties. A subset of these properties implies a function  the purpose for which the item is manufactured. We consider only this primary function. In addition, an item may also possess many secondary functions. We define a simple function as a function that cannot be sub-divided i.e. it is atomic. A compound function is one that is realized by combining one or more simple or compound functions.  A scenario or situation may consist of many such functions. A set of items possessing these functions to varying extents satisfies the scenario either partially or completely. A combination of many related scenarios constitutes a context, for example, a household context, an industrial context, etc Interacting functions give rise to relationships between the items. Any two items can be related through many such relationships. Item-relatedness is a composite of these relationships. Relatedness is influenced by two attributes, strength of a relationship and cardinality of the set of relationships. Relatedness is directly proportional to the strength and also to the cardinality.  Consider two items x and y. When they are brought together in a context, their primary functions may interact in one of the following ways  Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE   2.1. Complementarity  Two items may be termed complementary if they together find application. The primary function of item x may need the primary function of item y when they are used together. This interaction between the two primary functions gives rise to a new compound function f. For example, a knife and a fork can be considered 


example, a knife and a fork can be considered complementary to each other. The attribute of sharpness enables a knife to cut objects. A fork is needed to hold the object. Absence of any one of the items usually inhibits the effective execution of the compound function We identify two shades of complementarity; intrinsic and flexible. The primary functions of two items, say x and y, may be inextricably bound together. Also, x and y may not serve any meaningful function independently The compound function f, implied by the primary functions of items x and y can be realized if and only if the two items function together. We refer to this type of complementarity as intrinsic complementarity. For example, consider the case of a 0.5 mm clutch pencil and 0.5 mm leads. Though these items are sold separately in a retail store they are intrinsically complementary to each other. Neither of them can be used separately for writing on a piece of paper. Consider another situation in which the primary functions of two items not only interact with each other but also combine with primary functions of other items to imply different functions. For example items x and y may interact with item z in one situation to imply function f1. In another scenario, the same two items may interact with item w to imply function f2. Flexible complementarity is the ability of the primary functions of two items to serve multiple functions. In general complementary items tend to be purchased together in purchase transactions  2.2. Substitutability  An item functionally substitutes another item if it serves the other  s function to a significant extent. Thus, a pen can substitute a pencil as far as the function of writing on paper is concerned. Substitutes have similar or closely related primary functions. In general, substitutes belong to the same generic category. Generally, substitution is with respect to one single function in a particular context Although substitutes are not used together, they are strongly related. Two items that substitute each other are likely to possess many similar or closely related attributes.  If two items are substitutes to each other, then they are not likely to be purchased in the same transaction. However, two substitutable items are likely to be bought with identical or similar items that go with them, in separate transactions    2.3. Non-dependence  In case of non-dependence, the primary functions of x and y do not interact with each other to serve any other compound function. Primary functions of x and y however, may separately find application in different domains and different contexts. Since their functions do not interact, the likelihood of being used together in the same or similar contexts is quite low. Non-dependent items are likely to be from different domains, for example, kitchen knife and 0.5 mm lead. A combination of non-dependent items is unlikely to be purchased with a frequency higher than the product of the expected frequencies of the individual items Relatedness between two items increases if complementarity and substitutability increases. A pair of items can take shades of complementarity and substitutability depending on the context. Therefore, on some occasions, we do find substitutes being purchased together in the same transaction. Two items can exhibit different relationships depending on the function demanded and the context. In this paper, we are concerned only with the primary functions of items  3.  Measures for Item Relatedness  We clarify some of the assumptions regarding customer purchasing behaviour that we make in our approach to item relatedness. First, we assume that items are purchased mainly for their primary function. Hence we are concerned mainly with primary functions of items Next, we assume that customers purchase items mainly for self or family consumption over a short period Consequently, item usage patterns get reflected in customer purchasing behaviour. Hence, we draw conclusions about relatedness of items by examining customer transactions Consider a database of retail market-basket transactions. Let I be the set of items available for sale in the retail market. I= {a, b, c  Database D consists of a set of transactions, T. Each transaction t consists of a set of items purchased by a customer on a single buying instance at a retail store. Thus, x?t?x?I; t?I Let x and y be two items occurring in an association rule whose relatedness is of interest to us. This means that  t t xy minsup where |t| is the total number of transactions in the database. Let tx represent the set of transactions that contains item x but not y; ty represent the set consisting of transactions containing item y but not x, and txy represent the set consisting of transactions containing both x and y Each transaction in tx, ty and txy might contain items other than x and y accordingly. The remaining transactions Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE contain neither of the two items. In order to compute relatedness between items x and y, we need to examine txy and neighbourhood items in tx, ty and txy. Other items purchased with items x and y, can also contribute substantially to their relatedness if they are similar. This is because x and y might find applicability with other items in many more contexts than all by themselves. This increases their usefulness and relatedness Let Z be the set of items/item-sets purchased together 


Let Z be the set of items/item-sets purchased together with x and y. In particular, an item-set z? Z if  xy xyz  t t minsup where minsup is user-specified threshold The item-set z and the set Z are called co-occurring neighbour of {x,y} and co-occurring neighbourhood respectively. Co-occurring neighbours are likely to be used together with items {x,y}. Therefore, a relationship exists between co-occurring neighbours and item-pair x,y}. Similarly, let M and N be item-sets that are purchased with items x and y in transaction sets tx and ty respectively. Then, M?N is the set of item-sets that are purchased with each of x and y separately. We call set M?N the non co-occurring neighbourhood of {x,y} and each item w?M?N the non co-occurring neighbour. It should be noted that  neighbourhood  as defined above represents the item-set frequently purchased along with a specific item-pair. A context, on the other hand, is a combination of many related scenarios each of which may refer to a set of items. A context is not specific to any particular item-pair. All neighbours of {x,y} need not occur in a context containing {x,y}. Here again we assume that items comprising sets M and N occur in respective transactions in significant numbers. In addition transaction sets tx and ty are also assumed to be significant  3.1. MI \(Mutual Interaction  A customer is likely to purchase items that are used together in the same transaction. Thus, two items x and y are related if they are purchased together in large numbers.  The more the co-occurrence, greater is the relatedness. Co-occurrence of the two items implies that the function of item x and the function of items y interact with each other in a useful way. This gives rise to a compound function. However, the frequency of item x in transactions containing y need not equal the frequency of item y in transactions containing x. The presence of item x in transactions containing y can be stronger than the presence of item y in transactions containing x. This aspect is not captured if we consider only txy. We try to capture this aspect through mutual interaction measure MI, given by                5.0 yf xyf xf xyf M I 1 where, f\(xy items x and y together i.e. |txy|;  f\(x transactions in which item x occurs, either alone or along with other items which is |tx|+|txy|; f\(y of transactions in which item y occurs which is |ty|+|txy From \(1 confidence of ARs x?y and y?x. In other words, MI gives the predictive ability of the presence of one item given the other. Thus, MI gives us a numerical estimate of the mutual interaction of items x and y in the database. It does not consider the presence of other items that may occur with x, y, or both. MI can take a value in the range 0,1]. When items x and y are never purchased with each other, then MI takes a value of 0. If the two items are intrinsically complementary to each other \(x always occurring with y and vice versa xy f\(x y Suppose x and y are perfectly substitutable with respect to their primary functions. This is reflected in their never being purchased together in transactions. Then, the value of f\(xy be noted that MI is computed by considering the value of f\(xy correlation [11] considers both the presence and absence of items while computing linear dependency between items. A large positive correlation coefficient does not distinguish between presence \(of both the items absence. Either of f\(xy   x  y  3.2. Function Embedding  The concept of function embedding helps us in deciding the relevant relationships to be included while calculating relatedness between items. Consider P={x, a b, c}. We do not consider transactions containing P while evaluating its subsets as frequent/infrequent. While calculating its relatedness we need to assign a larger weight to it as compared to its frequently purchased subsets {x, a}, {x, b} and {x, a, b}. Transaction data decides the relevance of a particular function. Consider subset {x, a, c} of P. This subset does not imply a relevant function since it does not feature in the frequently purchased market baskets. Therefore, while assigning a weight to P we do not consider subset {x, a, c The implied function of a set assigns a weight of 1 to it. In addition, each of its subsets purchased in separate transactions and in significant numbers assigns a weight of 1. Thus, set P is assigned a weight of 4. Sets {x, a}, {x b} and {x, a, b} each contribute a weight of 1 to it Similarly, {x, a, b} is assigned a weight of 3. It should be noted that while calculating the weight of P, {x, a, b  s contribution is 1 \(and not 3 


contribution is 1 \(and not 3 implies one function different from its subsets. The functions of its subsets {x, a} and {x, b} have already been taken into account by their contributions to the weight of P. By assigning a weight of 1 to {x, a, b} we Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE avoid double counting the weights of {x, a} and {x, b Note that the number of non-empty subsets of P is 15  3.3. CI \(Intensity of Complementarity  Two items {x, y} are complementary if they are used together in significant numbers. Other items purchased with {x, y} give an indication of the different relationships that may exist between x and y Complementarity of {x,y} increases if this pair can be used together with other items in different situations. Note that an item-set {a, b} and its subset {a} can both be considered as co-occurring neighbours of {x, y} if and only if both are individually purchased with {x, y} in significant numbers. While considering the significance of the occurrence of set {a} along with {x, y} we do not consider its occurrence in set {a, b}. This is because we consider sets {a} and {a, b} as separate sets that serve different functions Let Z be the set of all co-occurring neighbours of {x y}. Note that each element of Z can be either a single item or a set of items. The intensity of complementarity is given by CI = 0                                               if  |Z|=0 i.e. Z      Zz xyzfzWt xyf      1 Otherwise 2 where, Wt\(z item set z is assigned as follows. Wt\(z the number of subsets of z that occur in Z. Thus, CI measures the intensity of flexible complementarity only and not intrinsic complementarity  3.4. SI \(Intensity of Substitutability  Item-sets in the non co-occurrence neighbourhood of x,y} contribute to this relatedness between them Consider an item w present in the non co-occurrence neighbourhood of {x,y}. Thus, items x and y can substitute each other in the presence of item set w?\(M?N substituting each other with respect to different item sets from their non co-occurring neighbourhood the relatedness of {x, y} increases. This is because items x and y can substitute each other with respect to many functions. Common attributes between x and y in turn imply greater relatedness. We try to capture this aspect through measure SI. SI is given by SI =     0                                               if  \(M?N      NMw wwwWt ??1 Otherwise 3 where       xyfxf xywfxwf w   and       xyfyf xywfywf w    W, ?w ?Sig, where Sig is a user-defined threshold. Wt\(w is the weightage of the item set w. Wt\(w the number of subsets of w?M?N The weight of an item-set gives an indication of the number of useful functions implied by w.  ?W gives the proportion of transactions containing x \(and not y item set w in it. Similarly, ?w gives the proportion of transactions containing y \(and not x ensures that item-set w occurs in a significant portion of tx and ty. |?w- ?w| gives the deviation of this proportionate occurrence of item-set w with items x and y in tx and ty respectively. Intuitively, we can expect substitutes to interact in a similar manner with respect to items they substitute. Therefore, if x and y substitute each other perfectly with respect to w, ?w and ?w can be expected to be nearly equal and  |?w-?w| will take a value close to 0 At this point the deviation is the lowest and we should get highest value for substitutability contribution i.e. weight of the item-set w. Greater the deviation |?w- ?w|, lower the relatedness. Therefore, we subtract |?w- ?w| from 1 to give the contribution of w to relatedness due to substitutability We have used two filters to ensure that substitutability contributions from non co-occurring neighbours are significant. The first filter ensures that the size of tx is significant. This is done by checking if xyx x tt t  


 is greater than a threshold, say, minsup. If it is less, then we can say that none of the non co-occurring neighbours of the set {x,y} will contribute significantly to substitutability. Then, SI can be directly assigned a value of 0. The second filter considers the frequency of non cooccurring neighbours of x in tx. ?w, ?w ?Sig ensures that item-set w occurs in a significant portion of tx and ty of the database  3.5. Total Relatedness \(TR  Total relatedness coefficient TR is the sum of the values of the three relatedness measures. Thus TR= MI + CI +SI                                                              \(4 The rationale for the summation is as follows Relatedness between x and y is a composite of relationships existing between them. Stronger and larger number of relationships implies greater relatedness Therefore, TR includes both the strength of relationships and the number of relationships between x and y. As can be observed from Equation 1, MI reveals the strength of only one relationship i.e. mutual interaction between x and y. Its value varies in the range [0, 1]. CI and SI bring out the intensity of complementarity and substitutability between the two items. Complementarity is due to the interaction between item sets present in {x,y  s cooccurring neighbourhood and {x,y}. On the other hand substitutability is a consequence of the Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE  Table 1. A sample data set Transaction Nos. Transaction Nos. Transaction Nos. Transaction Nos. Transaction Nos. Transaction Nos a,d,f} 3 {a,b} 2 {a,e}   2 {b,c,e,f} 4 {b,e,f} 3 {c,d,e} 4 d,f} 1 {a,b,c} 4 {a,e,f}   3 {b,c,e}    4 {a,b,c,d} 3 {c.d} 5 b,e} 2 {a,b,d} 4 {a,f}   2 {b,d}   3 {b,f}   1  Table 2. Relatedness values for item-pairs occurring in association rules Sr No Pair Z * M?N * MI CI SI TR 1 {a, b} {{c}, {d}, {c, d}} {{e}, {e, f}} 0.4993 1.3077 2.6705 4.4775 2 {e, f} {{a}, {b}, {b, c}} {{a}, {b}} 0.5214 1.4 1.8571 3.7785 3 {b, e} {{c}, {f}, {c, f}} {a} 0.5121 1.4615 0.8954 2.869 4 {b, c} {{a}, {a, d}, {e}, {e, f}} {d} 0.5625 1.4667 0.575 2.6042 5 {c, e} {{b}, {d}, {b, f}} {?} 0.5227 1.3333 0 1.856 Z and M?N are the respective co-occurring and non co-occurring neighbourhoods interaction between {x,y  s non co-occurring neighbourhood and {x,y}. Each item set that interacts with {x, y} contributes to one instance of a relationship The strength of the relationship is revealed by the frequency of occurrence of the item set. Therefore, CI and SI are assigned values depending on the cardinality and strength of relationships between x and y. Consequently values of CI and SI may exceed 1 Table 1 shows every transaction and the number of occurrences in the database. Each transaction, a subset of the set of six items {a, b, c, d, e, f}, represents purchases by a customer during a single buying instance at a mall Though the data set given in Table 1 is small, with respect to both transactions and items, it is sufficient to demonstrate the efficacy of the relatedness measures. Item b that occurs in 30 transactions is the most frequently occurring item. {b, c} is the most frequent 2-itemset with 15 occurrences. The most frequent 3-itemset {b, c, e occurs in 8 transactions. ARs were mined using a minimum support threshold of 15% and a minimum confidence threshold of 50%. This gave rise to ten ARs satisfying the minimum support and minimum confidence constraints. Relatedness measures MI, CI and SI were computed for those item pairs that occurred in at least one AR. The first filter \(checking for the significance of tx and ty second filter \(?w, ?w ? 0.10 computation of SI. The results of the computations are given in Table 2   4. From Relatedness to Interestingness  TR reveals the total relatedness of an item-pair based on information contained in transactions. Relatedness contributions due to substitutability and complementarity components for two unrelated items such as {beer diaper} may not be high. Hence, TR will be assigned a low value. On the other hand, two related items may be expected to have similar or closely related properties Many complementarity and substitutability relationships may arise due to these closely related properties. Hence two related items, such as {bread, butter}, can be expected to have a high TR value. From the relatedness perspective, a rule containing {beer, diaper} would be deemed more interesting than a rule containing {bread butter}. However, in general, an AR contains more than one item-pair. Therefore, we need one consolidated value that represents the AR rather than any one of its itempairs. It can be intuitively argued that the relatedness of a set of items is driven by the least related item-pair in it 6].  Let {a1, a2  an}?{ b1, b2  bm} represent an AR 


AR m+n C2 item pairs can be formed from this AR. Let the least related item-pair among these m+n C2 item-pairs be {x,y}and its total relatedness coefficient be TR\(x,y Since relatedness and interestingness are opposing notions, the interestingness of an AR can be given by   2   yxTR kC I nm C    5 In Equation 5, we have weighted the interestingness coefficient by the number of item-pairs in the rule Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Table 3. Comparison of IC with Conviction \(V Int Sr No Association Rule Support Confidence TR* IC Conviction V Interest Int 1 {c, e}?{b} 0.16 0.6667 1.8560 1.6164 1.2000 1.1100 2 {b, e}?{c} 0.16 0.6154 1.8560 1.6164 1.3520 1.2821 3 {b, c}?{e} 0.16 0.5333 1.8560 1.6164 1.2000 1.2121 4 {c}?{e} 0.24 0.50 1.8560 0.5388 1.12 1.1364 5 {e}?{c} 0.24 0.5454 1.8560 0.5388 1.144 1.1364 6 {b}?{c} 0.30 0.50 2.6042 0.3840 1.04 1.0417 7 {c}?{b} 0.30 0.625 2.6042 0.3840 1.0667 1.04167 8 {e}?{b} 0.26 0.5909 2.869 0.3486 0.9778 0.9848 9 {f}?{e} 0.20 0.588 3.7785 0.2647 1.3600 1.3368 10 {a}?{b} 0.26 0.5652 4.4775 0.2233 0.92 0.9420 TR value of the least related item-pair in a rule is displayed when the rule has more than two items  Consider two ARs: AR1 and AR2. Let the least-related item pair in both rules be the same. If AR1 has more items than AR2, then AR1 can be more interesting than AR2 This is because AR1 brings out more relationships with the most interesting pair. This intuition leads us to weighing the interestingness of an AR by the cardinality of item-pairs in it. In Equation 5, k is the constant of proportionality. The importance given to the cardinality of item-pairs in a rule can be altered by assigning an appropriate value to  k  A value of k&lt;1 decreases the contribution of the cardinality of item pairs to IC. Here, IC has been calculated using k=1. Finally, we note that TR\(x,y of an AR has a relatedness component due to cooccurrence, namely MI. Since {x, y} is selected from an AR, MI will have a value not less than the minimum support. Note that in any AR D yxf xf yxf      since D|?f\(x database Table 3 contains information about the 10 ARs mined from the transaction set given in Table 1. The ARs have been ordered in the descending order of their interestingness using IC. IC assigns identical values 0.5388 the TR coefficient from which IC is derived, considers antecedent and consequent of a rule in a symmetric fashion. MI component of TR considers the average predictive ability of an item in the presence of the other Rule {c,e}?{b} gets the highest IC value of 1.6164 This is due to two factors: the presence of the least related item-pair {c,e}, and the weight accorded to the number of item-pairs in the rule. Item-pair {c,e} has a low TR value of 1.856. In addition, the rule has three item-pairs. We can contrast this to a single item-pair AR, {c}?{e}. It is assigned an IC value of 0.5388. This is despite {c}?{e having the same least related item-pair as in rule c,e}?{b}. Thus, we observe that IC is larger if: \(1 least related item-pair of the rule has a low relatedness value. \(2 Rule {a}?{b} is assigned the lowest IC value of 0.2233. It has the highest TR value. This is because items a and b are effective substitutes of each other. They are not strongly related due to co-occurrence or complementarity. This component of relatedness due to substitutability is ignored by support and confidence From Table 3, it can be seen that the most interesting rule does not have the highest support. In addition, some of the least interesting rules, namely {a}?{b} and f}?{e} have high confidence values of 0.5652 and 0.588 respectively. This is because support and confidence do not consider the other relationships considered by IC, namely substitutability and complementarity. This observation corroborates our viewpoint that all relationships between item-pairs need to be examined to bring out the item-pair  s interestingness Support and confidence may be used as preliminary filters to select significant rules. The selected rules can then be ordered by IC  5. Comparative Analysis 


5. Comparative Analysis  We compare IC with two commonly used objective measures of interestingness present in the data mining literature. The chosen measures are Conviction \(V 15] and Interest \(Int  considered to have an edge over confidence with respect to measuring implication [15]. Int is essentially a measure of departure from independence. Int=1 if the two items Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE are completely independent. However, by being symmetric it measures only co-occurrence and not implication Consider the 2-item rules. V measures the strength of implication. Therefore, in general we can say that higher V values mean antecedent items and consequent items are strongly related. Rules having higher V-values should be less interesting from relatedness viewpoint. Thus, higher values of conviction should be related to lower IC values Consider rule {f}?{e}. This rule is assigned a high Vvalue of 1.3600. Its IC value is also low \(0.2647 example, V and IC seem to be related as expected. We note that item-pair {f,e} has a high TR- value of 3.7785 due to the large contribution from SI However, from Table 3 we see that in general, rules having higher IC values have higher V values. For example, rule {e}?{c} has an IC and V value of 0.5388 and 1.144 respectively. On the other hand, rule {b}?{c which has a lower IC value of 0.3840 also has a lower Vvalue \(1.04 relatedness of {e,c} is more than that of {b,c}. The Vvalues imply that {b,c} is more interesting than {e,c while IC values seem to suggest the reverse. This apparent counter intuitive result can be explained if we examine the TR components for pairs {e,c} and {b,c}. The pair {e c} has a lower TR value as compared to the pair {b, c mainly due to {e, c  s SI value being assigned a value of 0. This results in {e,c} being assigned a higher IC value than {b,c}. Since V ignores components of relatedness due to substitutability and complementarity, interesting rules brought out by V may not give the complete picture Consider the Int values. Brijs, et al. [19] give an  economical  interpretation of Interest. For an AR a}?{b}, Interest &gt; 1 indicates complementarity effects between a and b while Interest &lt; 1 indicates substitutability effects. Items a and b are conditionally independent when Interest = 1. Consider rule {e}?{b}. It has an Int value of 0.9848 and an IC value of 0.3486 From the economic interpretation of Int, items e and b should be substitutes. However, from Table 2 we see that CI value of {e, b} \(1.4615 0.8954 This aspect is not brought out by Int Consider rule {a}?{b}. It has an Int-value of 0.9420 while IC assigns a value of 0.2233 to it. The Int value indicates that a and b are substitutes for each other. This is confirmed by the SI component \(2.6705 coefficient for {a, b}. There also exists some degree of complementarity between items a and b. This again is not brought out by Int. Another observation pertaining to rule c,e}?{b} is as follows. IC assigns the highest value of 1.6164 to it, whereas the values assigned by V and Int are 1.200 and 1.11 respectively. We cannot infer from the V and Int values that the rule {c, e}?{b} is the most interesting one of the ten rules V and Int do not explicitly account for the number of item pairs in a rule or give additional weightage to the most interesting pair in a rule. This might result in counter-intuitive results as shown in the following example. Consider the rule {c,e}?{b} and its sub-rule c}?{b}. We note that the item pair {c,e} in {c,e}?{b is less related than {c,b}. Also, {c,e}?{b} has three item pairs while {c}?{b} has only one. Therefore, according to IC rule {c,e}?{b} \(1.6164 c}?{b} \(0.3840 c}?{b} are 1.20 and 1.0667 respectively. This means that the items in the first rule are more dependent on each other and hence less interesting. Similarly, Int values for c,e}?{b} and {c}?{b} are 1.110 and 1.04167 respectively. This seems to suggest that item c and item b are more independent and hence rule {c}?{b} is more interesting. This seems to be counter-intuitive as c}?{b} is contained in {c,e}?{b}. In addition, it conveys less knowledge about the domain than c,e}?{b All items in the antecedent and consequent are considered while computing V and Int. This can lead to the masking of the contribution of the least related itempair.  V and Int do not consider substitutability and complementarity relationships explicitly. Therefore interestingness rankings based on IC values are more intuitive than those due to V and Int  6. Discussions  Objective measures like support [1, 12], confidence [1 12], conviction [12, 15] and interest [12, 15, 16] have been extensively used in data mining studies. Hilderman and Hamilton [13] survey seventeen interestingness measures from the data mining literature. Tan, Kumar and Srivatsava [12] have described several key properties that can be used to select the right objective measure for a given application. Our approach is different from the traditional approaches [12, 13]. Jaroszewicz and Simovici 17] have proposed a measure that is a generalization of many conditional and unconditional classical measures Omiecinski [7] has proposed three alternative interestingness measures for associations: any-confidence all-confidence and bond. Our work is quite different from these two studies on two counts. We consider classical association rules in a retail-market basket context Secondly, in the studies mentioned, relationships have not explicitly been taken into account Meo [14] has proposed a new model to evaluate 


Meo [14] has proposed a new model to evaluate dependencies in data mining problems. Our study also focuses on dependencies between items. However, we try to discern interestingness of ARs by using relatedness based on relationships between item-pairs. Brijs, et al 19] have introduced a micro-economic integerprogramming model for product selection \(PROFSET Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Our approach considers the simultaneous existence of complementarity and substitutability unlike Brijs, et al 19].  Substitution rules were introduced by Teng and others [20]. According to them, an item-set X is a substitute for Y if X and Y are negatively correlated along with the existence of a negative AR \(X  Y rule set. On the other hand, in our approach, the degree of substitutability \(a component of relatedness pair {x, y} is computed by identifying item-sets that occur in {x, y  s non co-occurring neighbourhood. Other relationships like flexible complementarity and mutual interaction have also been considered in our approach Dong and Li [9] have presented a method of evaluating interestingness of ARs in terms of neighbourhood-based unexpectedness. The neighbourhood of a rule is defined in terms of a distance function on rules.  In our work, we consider both co-occurring and non co-occurring neighbours of items. We also compare our interestingness coefficient \(IC interestingness Conviction \(V Int that IC compares favourably with V and Int in ranking ARs. IC also takes into account aspects of relatedness not accounted for by V and Int, thus making it more intuitively appealing. IC treats the antecedent and consequent of an AR in a symmetric fashion. As a part of our future work, we propose to examine the feasibility of generalizing IC to account for directionality of ARs. In addition, we also propose to apply the suggested framework on real-life datasets  7. References  1] A. A. Freitas. On Rule Interestingness Measures Knowledge-Based Systems, 12, 1999, pp. 309-315 2] A. Silberschatz, and A. Tuzhilin. What makes Patterns Interesting in Knowledge Discovery Systems. IEEE Transactions on Knowledge and Data Engineering, 8\(6 1996, pp. 970-974 3] B. Liu, W. Hsu, L. Mun, and H. Lee. Finding Interesting Patterns Using User Expectations. IEEE Transactions on Knowledge and Data Engineering, 11\(6 832 4] B. Liu, W. Hsu, S. Chen, and Y. Ma. Analyzing the Subjective Interestingness of Association Rules. IEEE Intelligent Systems, 15\(5 5] B. Padmanabhan, and A. Tuzhilin. Unexpectedness as a Measure of Interestingness in Knowledge Discovery Decision Support Systems, 27\(3 6] B. Shekar, and R. Natarajan. A Framework for Evaluating Knowledge-based Interestingness of Association Rules Fuzzy Optimization and Decision Making, 3\(2 pp. 157-185 7] E. R. Omiecinski. Alternative Interest Measures for Mining Associations in Databases. IEEE Transactions on Knowledge and Data Engineering, 15\(1 57-69 8] G. Adomavicius, and A. Tuzhilin. Discovery of Actionable Patterns in Databases: The Action Hierarchy Approach Proceedings of the Third International Conference on Knowledge Discovery and Data Mining\(KDD  1997 AAAI, 1997, pp. 111-114 9] G. Dong, and J. Li. Interestingness of Discovered Association Rules in Terms of Neighborhood-Based Unexpectedness. Proceedings of the Second Pacific-Asia Conference on Knowledge Discovery and Data Mining 1998, pp.72-86 10] J. F. Roddick, and S. Rice. What  s Interesting About Cricket  On Thresholds and Anticipation in Discovered Rules.  SIGKDD Explorations, 3\(1 11] P. Tan, and V. Kumar. Interestingness Measures for Association Patterns: A Perspective. KDD'2000 Workshop on Postprocessing in Machine Learning and Data Mining Boston, MA, August 2000 12] P. Tan, V. Kumar, V. and J. Srivastava. Selecting the Right Interestingness Measure for Association Patterns Information Systems, 29\(4 13] R. J. Hilderman, and H. J. Hamilton. Knowledge Discovery and Interestingness Measures: A Survey, Technical Report Department of Computer Science, University of Regina Canada, 1999 14] R. Meo. Theory of Dependence Values. ACM Transactions on Database Systems, 25\(3 15] S. Brin, R. Motwani, and C. Silverstein. Beyond Market Baskets: Generalizing Association Rules to Correlations Proceedings of the 1997 ACM SIGMOD International Conference on Management of Data, May 1997, pp.265276 16] S. Brin, R. Motwani, J. D. Ullman, and S. Tsur. Dynamic Itemset Counting and Implication Rules for Market Basket Data. Proceedings of the ACM SIGMOD International Conference on Management of Data, May 1997, pp. 255264 17] S. Jaroszewicz, and D. A. Simovici.  A General Measure of Rule Interestingness. Proceedings of the 5th European Conference on Principles of Data Mining and Knowledge Discovery \(PKDD 2001 Freiburg, September 2001, pp. 253-266 18] S. Sahar. Interestingness Via What is Not Interesting Proceedings of the 5th ACM, SIGKDD International Conference on Knowledge Discovery and Data Mining 1999, pp. 332-336 19] T. Brijs, G. Swinnen, K. Vanhoof, and G. Wets. Building and Association Rules Framework to Improve Product Assortment Decisions. Data Mining and knowledge Discovery, 8, 2004, pp. 7-23 


Discovery, 8, 2004, pp. 7-23 20] W. Teng, M. Hsieh, and M. Chen. On the Mining of Substitution Rules for Statistically Dependent Items Proceedings of IEEE International Conference on Data Mining \(ICDM  02 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


14] Kryszkiewicz, M., and Rybinski, H. \(1999  Incomplete Database Issues for Representative Association Rules  ISBN: 3-540-65965-X, pp. 583-591 15] Little, R.J.A., and Rubin, D.B. \(2002 analysis with missing data, Wiley, New York, ISBN 0471183865 16] Omiecinski, E.R. \(2003  Alternative interest measures for mining associations in databases  IEEE TKDE vol. 15, no. 1, pp.57-69 17] Piramuthu, S. \(1998  Evaluating feature selection methods for learning in data mining applications  in the Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, vol. 5, pp. 294-301 18] Pyle, D. \(1999 Morgan Kaufmann Publishers, Inc.ISBN:1-55860-529-0 19] Ragel, A., and Cremilleux, B. \(1999  MVC - A preprocessing Method to deal with missing values   knowledge based system, vol. 12, pp.285-291 20] Ramoni, M., and Sebastiani, P. \(2000  Bayesian Inference with Missing Data Using Bound and Collapse   Journal of Computational and Graphical Statistics, vol. 9, no 4, pp. 779-800 21] Ramoni, M., and Sebastiani, P. \(2001  Robust Bayes Classifiers  AI, vol. 125, no. 1-2, pp. 207-224 22] Ramoni, M., and Sebastiani, P. \(2001  Robust Learning with Missing Data  Machine Learning, vol. 45, no 2 , pp. 147-170 23] Scott, R.E. \(1993 Logic and Practice, SAGE Publications, ISBN: 0803941072 24] Zaki, M.J., and Hsiao, C.J. \(2002  CHARM: An efficient algorithm for closed itemset mining  in the Proceedings of the Second SIAM International Conference on Data Mining Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE pre></body></html 


13: else 14: E|i?1| = E|i?1| ? s The backward process in Algorithm 1, generates level-wise every possible subset starting from the borProceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE der de?ned by Edge without getting into equivalence classes which have been already mined \(Line 10 such subset satis?es the constraint then it can be added to the output \(Line 12 reused later to generate new subsets \(Line 14 have a monotone constraint in conjunction, the backward process is stopped whenever the monotone border B+\(Th\(CM Lines 3 and 8 4.3. Closed Constrained Itemsets Miner The two techniques which have been discussed above are independent. We push monotone constraints working on the dataset, and anti-monotone constraints working on the search space. It  s clear that these two can coexist consistently. In Algorithm 2 we merge them in a Closet-like computation obtaining CCIMiner Algorithm 2 CCIMiner Input: X,D |X , C, Edge,MP5, CAM , CM X is a closed itemset D |X is the conditional dataset C is the set of closed itemsets visited so far Edge set of itemsets to be used in the BackwardMining MP5 solution itemsets discovered so far CAM , CM constraints Output: MP5 1: C = C ?X 2: if  CAM \(X 3: Edge = Edge ?X 4: else 5: if CM \(X 6: MP5 = MP5 ?X 7: for all i ? flist\(D |X 8: I = X ? {i} // new itemset avoid duplicates 9: if  Y ? C | I ? Y ? supp\(I Y then 10: D |I= ? // create conditional fp-tree 11: for all t ? D |X do 12: if CM \(X ? t 13: D |I= D |I ?{t |I  reduction 14: for all items i occurring in D |I do 15: if i /? flist\(D |I 16: D |I= D |I \\i // ?-reduction 17: for all j ? flist\(D |I 18: if supD|I \(j I 19: I = I ? {j} // accumulate closure 20: D |I= D |I \\{j 21: CCIMiner\(I,D |I , C,B,MP5, CAM , CM 22: MP5 = Backward-Mining\(Edge,MP5, CAM , CM For the details about FP-Growth and Closet see [10 16]. Here we want to outline three basic steps 1. the recursion is stopped whenever an itemset is found to violate the anti-monotone constraint CAM Line 2 2  and ? reductions are merged in to the computation by pruning every projected conditional FPTree \(as done in FP-Bonsai [7 Lines 11-16 3. the Backward-Mining has to be performed to retrieve closed itemsets of those equivalence classes which have been cut by CAM \(Line 22 5. Experimental Results The aim of our experimentation is to measure performance bene?ts given by our framework, and to quantify the information gained w.r.t. the other lossy approaches 


approaches All the tests were conducted on a Windows XP PC equipped with a 2.8GHz Pentium IV and 512MB of RAM memory, within the cygwin environment. The datasets used in our tests are those ones of the FIMI repository1, and the constraints were applied on attribute values \(e.g. price gaussian distribution within the range [0, 150000 In order to asses the information loss of the postprocessing approach followed by previous works, in Figure 4\(a lution sets given by two interpretations, i.e. |I2 \\ I1 On both datasets PUMBS and CHESS this di?erence rises up to 105 itemsets, which means about the 30 of the solution space cardinality. It is interesting to observe that the di?erence is larger for medium selective constraints. This seems quite natural since such constraints probably cut a larger number of equivalence classes of frequency In Figure 4\(b built during the mining is reported. On every dataset tested, the number of FP-trees decrease of about four orders of magnitude with the increasing of the selectivity of the constraint. This means that the technique is quite e?ective independently of the dataset Finally, in Figure 4\(c of our algorithm CCIMiner w.r.t. Closet at di?erent selectivity of the constraint. Since the post-processing approach must ?rst compute all closed frequent itemsets, we can consider Closet execution-time as a lowerbound on the post-processing approach performance Recall that CCIMiner exploits both requirements \(satisfying constraints and being closed ing time. This exploitation can give a speed up of about to two orders of magnitude, i.e. from a factor 6 with the dataset CONNECT, to a factor of 500 with the dataset CHESS. Obviously the performance improvements become stronger as the constraint become more selective 1 http://fimi.cs.helsinki.fi/data Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Information loss Number of FP-trees generated Run time performance 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 10 5 10 6 m I 2 I 1  PUMSB@29000 CHESS @ 1200 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 6 10 


10 1 10 2 10 3 10 4 10 5 10 6 10 7 m n u m b e r o f fp t re e s PUMSB @ 29000 CHESS @ 1200 CONNECT@11000 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 m e x e c u ti o n  ti m e  s e c  CCI Miner  \(PUMSB @ 29000 closet         \(PUMSB @ 29000 CCI Miner  \(CHESS @ 1200 closet         \(CHESS @ 1200 CCI Miner  \(CONNECT @ 11000 closet         \(CONNECT @ 11000 a b c Figure 4. Experimental results with CAM ? sum\(X.price 6. Conclusions 


6. Conclusions In this paper we have addressed the problem of mining frequent constrained closed patterns from a qualitative point of view. We have shown how previous works in literature overlooked this problem by using a postprocessing approach which is not lossless, in the sense that the whole set of constrained frequent patterns cannot be derived. Thus we have provided an accurate de?nition of constrained closed itemsets w.r.t the conciseness and losslessness of this constrained representation, and we have deeply characterized the computational problem. Finally we have shown how it is possible to quantitative push deep both requirements \(satisfying constraints and being closed process gaining performance bene?ts with the increasing of the constraint selectivity References 1] R. Agrawal, T. Imielinski, and A. N. Swami. Mining association rules between sets of items in large databases In Proceedings ACM SIGMOD, 1993 2] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules in LargeDatabases. InProceedings of the 20th VLDB, 1994 3] R. J. Bayardo. E?ciently mining long patterns from databases. In Proceedings of ACM SIGMOD, 1998 4] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Adaptive Constraint Pushing in frequent pattern mining. In Proceedings of 7th PKDD, 2003 5] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi ExAMiner: Optimized level-wise frequent pattern mining withmonotone constraints. InProc. of ICDM, 2003 6] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Exante: Anticipated data reduction in constrained pattern mining. In Proceedings of the 7th PKDD, 2003 7] F. Bonchi and B. Goethals. FP-Bonsai: the art of growing and pruning small fp-trees. In Proc. of the Eighth PAKDD, 2004 8] J. Boulicaut and B. Jeudy. Mining free itemsets under constraints. In International Database Engineering and Applications Symposium \(IDEAS 9] C. Bucila, J. Gehrke, D. Kifer, and W. White DualMiner: A dual-pruning algorithm for itemsets with constraints. In Proc. of the 8th ACM SIGKDD, 2002 10] J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In Proceedings of ACM SIGMOD, 2000 11] L.Jia, R. Pei, and D. Pei. Tough constraint-based frequent closed itemsets mining. In Proc.of the ACM Symposium on Applied computing, 2003 12] H. Mannila and H. Toivonen. Multiple uses of frequent sets and condensed representations: Extended abstract In Proceedings of the 2th ACM KDD, page 189, 1996 13] R. T. Ng, L. V. S. Lakshmanan, J. Han, and A. Pang Exploratory mining and pruning optimizations of constrained associations rules. In Proc. of SIGMOD, 1998 14] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering frequent closed itemsets for association rules In Proceedings of 7th ICDT, 1999 15] J.Pei, J.Han,andL.V.S.Lakshmanan.Mining frequent item sets with convertible constraints. In \(ICDE  01 pages 433  442, 2001 16] J. Pei, J. Han, and R. Mao. CLOSET: An e?cient algorithm formining frequent closed itemsets. InACMSIGMODWorkshop on Research Issues in Data Mining and Knowledge Discovery, 2000 17] J. Pei, J. Han, and J. Wang. Closet+: Searching for the best strategies for mining frequent closed itemsets. In SIGKDD  03, August 2003 18] L. D. Raedt and S. Kramer. The levelwise version space algorithm and its application to molecular fragment ?nding. In Proc. IJCAI, 2001 


ment ?nding. In Proc. IJCAI, 2001 19] R. Srikant, Q. Vu, and R. Agrawal. Mining association rules with item constraints. In Proceedings ACM SIGKDD, 1997 20] M. J. Zaki and C.-J. Hsiao. Charm: An e?cient algorithm for closed itemsets mining. In 2nd SIAM International Conference on Data Mining, April 2002 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


4] K. Chang, C. Chong, Bar-Shalom, Joint Probabilistic Data Association in Distributed Sensor Networks IEEE Transactions on Automatic Control, AC-31\(10 octobre 1986 5] Bar-shalom, Multitarget Multisensor Tracking Advanced Application ,:YBS Publishing, 1990 6] Bar-shalom, Multitarget Multisensor Tracking Principles and Techenices ,:YBS Publishing, 1995 7] Hu Wenlong, Mao shiyi, Multisensor Data Association Based on Combinatorial Optimization[J]. Journal of Systems Engineering and Electronics . 1997 NO.1,1~9 8] Pattipati K R, Passive Multisensor Data Association Using a New Relaxation Algorithm.In Multitarget-Multisensor Tracking: Advanced and Applications,Y.Barshalom,Norwood,MA:Aretech,199 0 9] Deb S,et al, An S-Dimentional Assignment Algorithm for Track Initiation ,Proc. Of the IEEE Int. Conf Systems Engineering, Kobe, Japan,Sept 1992 527~530 10] Deb S,et al, A Multisensor-Multitarget Data Association Algorithm for Heterogeneous Sensors[J].IEEE Trans. on AES 1993 ,29 \(2 560~568 12] Bar-shalom,Y.,and Fortmann,T.E  Tracking and Data Association  New York:Academic press,1988 13] J,A,Roecher and G.L.Phillis,Suboptimal Joint Probabilistic Data Association .IEEE Transaction on Aerospace Electronic Systems.1993,29\(2 14] J,A,Roecker,A Class of Near Optimal JPDA Algorithm IEEE Transaction on Aerospace and Electronic Systems,1994,30\(2 15] Han Yanfei, Analysis and Improvement of Multisensor Multitarget Probabbilistic Data Association Filting Algorithm[J].  Journal of Systems Engineering and Electronics, 2002, Vol.24, 36~38  569 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





