html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Dependence  Among Terms in Vector Space Model Ilm  rio R. Silva                                Jo  o Nunes Souza  Karina S. Santos Univ. Federal de Uberl  ndia             Univ. Federal de Uberl  ndia             Univ. Federal  de Uberl  ndia Uberl  ndia  MG, Brazil                  Uberl  ndia  MG, Brazil  Uberl  ndia  MG, Brazil ilmerio@facom.ufu.br                       nunes@facom.ufu.br  karina_santos@comp.ufu.br Abstract The vector space model is a mathematical-based model that represents terms, documents and queries by vectors and provides a ranking. In this model, the subspace of interest is formed by a set of pairwaise orthogonal term vectors, indicating that terms are mutually independent However, this is a simplification that doesn't correspond to the reality. Based on this scenery, we present, in this work, an extension to the vector space model to take into account the correlation between terms. In the proposed model, term vectors are rotated in space geometrically reflecting the dependence semantics among terms. We rotate terms based on a data mining technique called association rules. The retrieval effectiveness of the proposed model is evaluated and the results shows that our model improves in average precision, relative to the standard vector space model, for all collections evaluated, leading to a gain up to 31 1. Introduction In information retrieval \(IR the most popular [15,16], however, it presents disadvantages [5]. The documents are represented by keywords extracted from the documents, and the relationship among them is not considered. This is a simplification of the model that doesn't correspond to the reality Based on this scenery, the main objective of this work is incorporate information of correlation among terms in the collection to the vector space model to improve it retrieval effectiveness. The proposed solution alters the representation of term vectors in the vector space model In this model, orthogonal vectors represent terms since it is not known a priori any correlation among the terms The algorithm proposed in this work has as main foundation the rotation of those vectors in the space, so that its representations reflect the dependence among the terms In the set of resultant vectors, term vectors are not necessarily orthogonal among themselves, the proximity between the vectors is related with the degree of dependence between the respective terms. The closer the term vectors, greater dependence observed among themselves The rotation of term vectors is based on techniques that result in information on the relationship among terms of the collection. We presented the technique association rules of data mining to obtain that information The remaining of this paper is organized as follow. In the next section we discuss some related work. Section three describes foundations of vector space model. In section four we present the association rules in the information retrieval context. The proposed model is described in section five. The experimental results are discussed in section six. Finally, we present some conclusions and future works 2. Related Work Several approaches for the incorporation of correlation among terms had already been presented in the literature Query expansion in the vector space model is suggested in several proposals, among them, [6,10,11,18]. In [18 Voorhees examined the usefulness of lexical query 


Voorhees examined the usefulness of lexical query expansion in the collection TREC. Mandala et al. [10 analyzed the characteristics of different thesaurus types and proposed a method to combine them and to expand queries In [4], Becker and Kuropka expose a model of IR for the comparison of documents that represents topics, terms and documents as vectors. The basis of the space is formed by a set of orthogonal topic vectors, where term vectors are represented. The angle between the term vectors and the weight of the term is calculated using information about the collection, as for instance, a list of radicals of the terms collection An extension to the Vector space model was suggested by Possas et al. in [12,13,14] considering the correlation among the terms, obtained using association rules. In 14], a new model is presented, denominated set-based model, for computing term weights, based on set theory and for ranking documents. For computing those weights the theory of the association rules is used. The proposal Proceedings of the International Database Engineering and Applications Symposium \(IDEAS  04 1098-8068/04 $20.00  2004 IEEE presented by the authors in [13] is similar with [14 Then, in [12], an extension to the set-based model is proposed using information about proximity among the terms of the query in the documents The generalized vector space model \(GVSM another extension of the vector space model, that contemplates the correlation among terms [19,20]. In GVSM, the terms can be non-orthogonal and are represented by smaller components denoted minterms The minterms are vectors, with binary weights, that indicate all co-occurrence possibilities of terms in documents. The basis for GVSM is formed by a set of 2t t is the number of distinct terms in the collection minterms vectors. The term vectors are linear combinations of minterms, reflecting co-occurrence proceeding from minterms Our work differs from the above related works in the following aspects. In none of the cited works, the term vectors are rotated in the space to reflect their correlation as we made in this work.  Moreover, the association rules are used to determine the proximity between term vectors differing from the cited models 3. Vector Space Model The Vector space model was, initially, proposed by Gerard Salton [15,16]. In this model, all relevant objects for a information retrieval system are represented as vectors: terms, documents and queries Each term ki is represented as a t-dimensional vector where t is the number of distinct terms in the collection In the vector space model, the vector ki represents the term ki. The set  {k1, k2,..., kt} forms the canonical basis for space ?t where k1=\(1,0,0,...,0 0,1,0,...,0   kt=\(0,0,0,..,1 and, in consequence, the corresponding terms are considered independent Document and query vectors are represented using the set K of term vectors. These vectors are built as linear combinations of the term vectors. The vector dj associated with the document dj  is defined as dj   t i 1 wi,j ki or dj=\(w1,j , w2,j ,...,  wt,j defined as q   t i 1 wi,q ki or q = \(w1,q ,w2,q ,...,  wt,q 


wi,q ki or q = \(w1,q ,w2,q ,...,  wt,q In the equalities above, wi,j and wi,q are weights of term i in document dj and in query q, respectively. The most efficient definition of term weights for the information retrieval is named tf-idf [3]. This strategy considers the number of times an index term occurs in a document and the number of documents of the collection an index term occurs The vector space model evaluates the degree of similarity of the document dj in relation to the query q as the correlation between the vectors dj and q. Usually, that correlation is quantified by the cosine of the angle among those two vectors. That is sim\(dj, q  ti=1wi,j . wi,q dj|x|q  ti=1 wi,j2  ti=1 wi,q2 After the computation of the similarity degrees, it is possible to order a list of documents \(ranking respective degrees of relevance to the query 4. Association Rules in Information Retrieval In the area of data mining, the association rules serve typically, to represent discovered frequent patterns in the data [1,2,8]. The main function of the rules is to characterize the data, representing regularities One of the purposes of this work is to use the data mining in IR. In general, the literature regarding the data mining works with items and transactions. However, the algorithms used for the discovery of association rules can be adapted also to work with terms and documents identifying the co-occurrence among terms In IR context, X and Y are terms or sets of terms Consider the following example, which defines an association rule in IR. The information that documents whose theme is tourism also discuss about hotels is represented in the association rule \(1 tourism ? hotel support = 2% , confidence = 80%]  \(1 The support and the confidence of a rule are two measures that reflect, respectively, the usefulness and the certainty of the discovered rules. The support is a percentage in relation to all the collection of analyzed documents. In the example above, in 2% of the collection the words  tourism  and  hotel  appear simultaneously in the same document. The confidence is a percentage in relation to an attribute. A confidence of 80% reveals that 80% of the documents that discuss tourism also talk about hotels 4.1. Basic Concepts Let J = {k1,k2,...km} be the set of distinct terms in a collection of documents D. Each document dj of the database is a set of terms such that dj ? J. An association rule is an implication like A?B, where A ? J, B ? J, and A ? B = ?. The rule A?B is valid in a set of documents D with support s, if s is the percentage of documents in D which contains A and B at the same time. The rule A?B has confidence c in the set of documents D if c is the percentage of documents in D having A that also contains Proceedings of the International Database Engineering and Applications Symposium \(IDEAS  04 1098-8068/04 $20.00  2004 IEEE B. Rules that satisfy a minimum support \(min_sup minimum confidence \(min_conf A set of terms is referred as termset. A termset that contains k terms is a k-termset Association rules are discovered in large databases in two steps 1  Find all the sets of terms \(termsets minimum support. These termsets are denominated frequent termsets 2  Generate strong association rules of the frequent termsets Apriori is an algorithm for mining frequent termsets for association rules [2,3,12]. Apriori uses an iterative 


for association rules [2,3,12]. Apriori uses an iterative approach known as search in levels, where k-termsets are used to explore \(k+1 frequent is found. This set is denoted L1. L1 is used to find L2, the set of frequent 2-termsets, which is used to find L3 and so on, until no more frequent k-termset can be found Once generated the frequent termsets of the transactions in the database D, the confidence of these termsets are computed and the strong association rules are generated In the next section, we present how association rules modify the vector space model 5. Vector Space Model Modified by Association Rules The mais foundation of the algorithm proposed in this work is the rotation of term vectors in the space, so that their representations contemplates, geometrically, the semantics of correlation of terms adopted. We used the association rules as a tool for the generation of information about the dependence among the terms The association rules are of the form ki o kj  and cij is the confidence index of the rule, that indicates the degree of dependence of the term ki in relation to the term kj That index is used, in this work, to compute the new angle between the term vectors ki and kj. The confidence was chosen as parameter to determine the proximity of the term vectors, because it reflects the certainty of the association rule. The term vectors are approximated using the association rules created for the respective terms as it follows Definition 5.1 \(Rotation of basis vectors be term vectors, cij the confidence index of the association rule ki o kj. The new angle Tij between ki and kj is given by  Tij = 90 \(1  cij between the vectors ki and kj. In this case, the rotation occur only in the vector ki The reason for it is related to the semantics of the association rule and the confidence. The index cij of the association rule ki o kj determines that, in c% times the term ki appears, the term kj also appears. Therefore, the rotation is made in the vector corresponding to the term of the antecedent of the association rule The vector ki approximates the vector kj, and the new vector is denominated ki  in that the r th element of the vector ki  named ar, is defined as ar = sin\(Tij ar = cos\(Tij ar = 0            ? r z i and r z j In case a term kp presents two or more associated terms, a normalization is made in the new vector kp   The vector space basis K is formed by the term vectors set {k1, k2,..., kt}. After the rotation of the term vectors the new basis for the vector space, denominated K  is obtained from K, substituting the vectors ki for ki  so K  k1  k2  kt  The set K  continues forming the basis of the vector space ?t because their vectors are linearly independent The document and query vectors, dj and q, are represented in the new basis K  as linear combination of terms vectors ki  Document and query vectors are denoted dj  and q  and defined as dj     t i 1 wij ki  and q     t i 1 


i 1 wiq ki   So, document and query vectors, dj  and q   contemplate, now, the dependence semantics among the terms, implicit in basis K   The same function in the computing of the similarity is used in the vector space model modified by dependence among the terms.  Therefore, we have sim\(dj, q  x q    ti,s=1wi,j ki  ws,q ks  dj| x |q  ti=1 wi,j2  ts=1 ws,q2  The normalization of the similarity, or the factors in the denominator of the formula, is made using the original norm of the documents. That strategy was adopted because otherwise, if the normalization uses the document vectors dj  the norm of all the documents should be recalculated, elevating the computational costs of calculation the similarity. Besides, that simplification doesn't change the results significantly In the computation of the similarity between the query and the documents, the main consequence in term vectors rotation is the automatic query expansion. The query is expanded with terms related to their original terms Besides, documents which have query terms and associated query terms occupy a position in the ranking above the documents that only have query terms Proceedings of the International Database Engineering and Applications Symposium \(IDEAS  04 1098-8068/04 $20.00  2004 IEEE Table 1. Characteristics of the reference collections Reference collections Number of distinct terms Number of documents Average number of terms per document Number of queries Average number of terms per query Average relevante documents per query CFC 2105 1239 12,2 64 4,0 39 CACM 8716 1602 46,6 50 12,7 13 CISI 9728 1460 53,6 50 9,4 50 TREC-3 1749555 741855 301,1 50 18,58 106,38 5.1. Algorithm The implementation of the presented model is divided in two phases. The first one is the generation of the information on dependence among the terms, which means the construction of vectors ki  This task is all accomplished in pre-processing phase. The second phase is the development of the proposed model The search algorithm used in the implementation of the vector space model modified by dependence among the terms, described in Figure 1, is similar to the original model. The function value\(ki , i in the position i in the vector of the term ki 1 A 2 associated 


associated 3 4  fij] in the term inverted list do 5 value\(ki , i 2 6 7 wij*wiq *value\(ki , i ki , j 8 9 10 11 12 13 14 15 16 documents dj  retrieved Figure 1. Search algorithm for the Vector space model modified by dependence among the terms In the step \(2 original algorithm. Once determined the identifiers of query terms, the associated terms to each term of the query are added to the list of query terms. This step of the algorithm defines the automatic expansion of the query with the terms related to the query terms The steps \(5 8   t si 1 wi,jki ws,qks of the equation of the internal product among the vectors dj  and q  The step \(5 for i = s. And the loop of step \(6 other cases, when i z s. These steps are necessary because the term vectors are non-orthogonal When analyzing the algorithm, we clearly notice, that the proposed model is an extension to the original vector space model. That is justified because, if none association among the terms exists, the described algorithm is equivalent to the original algorithm 6. Experiments To evaluate the efficiency of the vector space model modified by dependence among the terms, the experiments were made with four reference collections denominated CACM [7], Cystic Fribosis \(CFC  CISI and Third Text Retrieval Conference \(TREC-3  The collection characteristics are shown in the Table 1 The evaluation of the IR system proposed here is related with the effectiveness of the retrieval, in other words, how much precise is the answer set returned by the system for a given query. We used the average recall versus precision curves to compare the effectiveness of the vector space model modified by dependence among terms with the one of the classic vector space model In the computing of the association rules, some parameters can be adjusted during the process of generation of association rules. Min_sup and min_conf are, respectively, support and confidence thresholds. We accomplished experiments and observed that min_sup should contain a low value \(up to 5 general, the frequency of terms in collections is low Besides, in case min_sup is low, association rules involving terms whose frequency is small in the collection of documents, are discarded. On the other hand, min_conf should contain a higher value \(above 40 among the vectors. In case min_conf contains a low value, term vectors that have very low co-occurrence are approximated. This harms the effectiveness of the Proceedings of the International Database Engineering and Applications Symposium \(IDEAS  04 1098-8068/04 $20.00  2004 IEEE 


1098-8068/04 $20.00  2004 IEEE retrieval, because the system will make the expansion of the query with terms not related to query terms As we can see in Figures 2 to 5, the proposed model yields better precision than Vector Space Model regardless of the collection and of the recall level Recall x Precision CACM 0 20 40 60 80 0% 20% 40% 60% 80% 100 VSM MVSM Figure 2. Recall-Precision for CACM Recall x Precision CISI 0 20 40 60 80 0% 20% 40% 60% 80% 100 VSM MVSM Figure 3. Recall-Precision for CISI Recall x Precision CFC 0 20 40 60 80 0% 20% 40% 60% 80% 100 VSM MVSM Figure 4. Recall-Precision for CFC Table 2 presents a summary of the results obtained, in that the averages of precision are exhibited for the two models in all collections and the gains obtained of the model proposed in relation to the original Recall x Precision TREC-3 0 20 40 60 80 0% 20% 40% 60% 80% 100 VSM MVSM Figure 5. Recall-Precision for TREC-3 Table 2. Average Precision Curves and gain provided by the vector space model modified by association rules Average Precision Collection Classic Modified Gain  CACM 30,03 32,08 6,83 CISI 17,64 20,09 13,89 CFC 10,05 13,24 31,74 TREC-3 12,09 14,04 16,13 The results presented for the vector space model modified by association rules are the best ones considering the analysis of the parameters values described. Then, for maximum min_sup from 4% to 5 and for min_conf alternating among 45% to 70%, the variation of the results is minimum in relation to the one presented. When defining the minimum confidence with a value up to 70%, few rules are generated and consequently, the results approach more of the presented 


consequently, the results approach more of the presented for the classic vector space model. The several possibilities of values of the parameters were tested however the collections behave in a similar way in its alteration The experiments showed that the proposed model improves the average precision of the answer set for all collections. Besides, the obtained medium precision was not harmed by the recall increase happened when expanding the queries 7. Conclusions In this paper, we presented an extension to the vector space model to contemplate the dependence among the terms of the collection. In the proposed model, the dependence among the terms is represented geometrically in the vector space The proposed model bases on the rotation of the term vectors, in agreement with the dependence among the terms. This rotation is made based on techniques that generate information on the correlation among terms of Proceedings of the International Database Engineering and Applications Symposium \(IDEAS  04 1098-8068/04 $20.00  2004 IEEE the collection. In this work, we used the association rules, however, other techniques can be used The generation of association rules is a known technique of data mining, which allows finding frequent patterns in large databases. In the context of this paper it is used to find sets of terms that appear simultaneously in the collection of documents. This information is useful to modify the term vectors, so that they reflect the semantics of co-occurrence defined for the association rules The extension to the vector space model we presented contemplates the dependence among terms in a clear, flexible and new way. It is clear because the dependence incorporation among the terms is made step by step and the vector space basis reflects the semantics defined for the adopted technique. The proposed model is flexible because it allows the correlation incorporation among the terms of collection obtained in several ways Finally, the proposal is new because in the literature there is not an extension to the vector space model which modifies the vector space basis how it was done in this work We evaluated the effectiveness of the model proposed with four reference collections. There was an increase in the retrieval model effectiveness in comparison with the classic vector space model for all of the reference collections used As future works, the effectiveness of the proposed model will be compared to the effectiveness of the generalized vector space model. Besides, we will research other methods of obtaining correlation among the terms of a collection of documents. These methods will be incorporated in a geometric way to the model proposed in this paper. We also intended to evaluate the model proposed for larger collections formed by Web documents References 1]  Agrawal, R., Imielinski, T., Swami, A  Mining association rules between sets of items in large databases   Proceedings of the ACM SIGMOD Conference,. Washington DC USA, may 1993, pp. 207-216 2] Agrawal, R., Srikant, R  Fast algorithms for mining association rules  Proceedings of the 20th Int  l Conference on Very Large Databases, Santiago Chile, September 1994 3] Baeza-Yates, R., Ribeiro-Neto, B., Modern information retrieval, ACM/Addison-Wesley, 1999 4] Becker, J., Kuropka, D  Topic-based vector space model  Proceedings of the 6th International Conference on Business Information Systems, Colorado Springs, June 2003 


Business Information Systems, Colorado Springs, June 2003 pp. 7-12 5] Bollmann-Sdorra, P., Raghavan, V. V  On the necessity of term dependence in a query space for weighted retrieval   Journal of the American Society of Information Science Volume 49\(13 6] Buckley, C., Salton, G., Allan, J., Singhal, A  Automatic query expansion using SMART : TREC 3  D. K. Harmon editor, NIST Special Publication 500-225: The Third Text Retrieval conference \(TREC 3 7] CAM-Collection. ftp://ftp.cs.cornell.edu/pub/smart/cacm 8] Han, J., Kamber, M. Data mining Concepts and techniques, San Diego: Academic Press, 2001, pp.335-393 9] Harman, D  Overview of the third Text Retrieval Conference  Proceedings of the third Text Retrieval Conference \(TREC-3 10] Mandala, R. , Tokunaga, T., Tanaka, H. M  Combining multiple evidence from different types of thesaurus for query expansion  Proceedings of the 22th annual international ACM SIGIR conference on Research and development in information retrieval, Berkeley, California, United States August 1999, pp. 191-197 11] Nie, J. Y., Jin, F  Integrating logical operators in query expansion in Vector Space Model  Workshop on Mathematical/Formal Methods in Information Retrieval, 25th ACM-SIGIR, Tampere, Finland, August 2002 12] P  ssas, B, Ziviani, N., Meira-Jr, W  Enhancing the setbased model using proximity information  Proceedings of the 9th International Symposium of String Processing and Information Retrieval, Lisbon, Portugal, September 2002, pp 104-116 13] P  ssas, B, Ziviani, N., Meira-Jr, W., Ribeiro-Neto, B  Modelagem vetorial estendida por regras de associa  o  XVI Simp  sio Brasileiro de Banco de Dados, Rio de Janeiro Brasil, 2001 14] P  ssas, B, Ziviani, N., Meira-Jr, W., Ribeiro-Neto, B  Set-based model: A new approach for information retrieval   Proceedings of the 25th International ACM SIGIR Conference on Research and Development in Information Retrieval Tampere, Finland, August 2002 15] Salton, G. \(ed   experiments in automatic document processing. Englewood Cliffs, NJ: Prentice Hall, 1971 16] Salton, G., Lesk, M. E  Computer evaluation of indexing and text processing  Journal of the ACM, 15\(1 36, Janeiro 1968 17] Shaw, W. M., Wood, R. E, Tiboo, H. R  The cystic fibrosis database: Content and research opportunities  Library and Information Science Research,Volume 13, 1991, pp.347366 18] Voorhees E. M  Query expansion using lexicalsemantic relations  Proceedings of the 17th ACM- SIGIR Conference, 1993, pp. 171-180 19] Wong, S. K.M., Ziarko, W., Raghavan, V. V., Wong, P C.N  On modeling of information retrieval concepts in vector spaces  Proceedings of the ACM Transactions on Database Systems, Volume 12, New York USA, June 1987, pp.299  321 20] Wong, S. K. M., Ziarko W., Wong, P. C. N  Generalized vector space model in information retrieval   Proceedings of the 8th ACM-SIGIR Conference on Research and Development in Information Retrieval. New York USA 1985, pp.18-25 Proceedings of the International Database Engineering and Applications Symposium \(IDEAS  04 1098-8068/04 $20.00  2004 IEEE pre></body></html 


 10 20 30 40 50 60 70 80 90 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 100 200 300 400 500 600 700 800 900 Implication Count 0.06 0.08 0.1 Mean Error Bounded Fringe Unbounded Fringe 10 20 30 40 50 60 70 80 90 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 100 200 300 400 500 600 700 800 900 Implication Count 0.06 0.08 0.1 Mean Error Bounded Fringe Unbounded Fringe  A   100  A   1  000  A   100  A   1  000 1000 2000 3000 4000 5000 6000 7000 8000 9000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 20000 40000 60000 80000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 1000 2000 3000 4000 5000 6000 7000 8000 9000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 20000 40000 60000 80000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe  A   10  000  A   100  000  A   10  000  A   100  000 Figure 4 DataSet One with c  1 Figure 5 DataSet One with c  2 and therefore these itemsets participate in the implication count The number of tuples created by this step is S  50  c  1   2  4   The rest of the steps create itemsets that should not participate in the count We create three different kind of tuples that break one implication condition The relative weight of each kind is 1  3 Generate   A  S   3 pairs  a i  b j  where each a i is different than all itemsets of A created before As in the previous step for each a i create at most c different b j  For each combination  a i  b j  write 50 tuples Then for each a i create eight b  j different than all b j s created before And write the eight tuples  a i  b  j   This step creates   A  S   3 new itemsets of A that should not participate in the implication count because they do not satisfy the minimum top-condence level although they satisfy both the minimum support and the maximum multiplicity constraint The number of tuples created by this step is   A  S   3  50  c  1   2  8   Generate   A  S   3 pairs  a i  b j  where each a i is different than all itemsets of A previously generated and each one appears with u different b j  where c  1  u  c  10 Write 50 such tuples This step creates   A  S   3 new itemsets of A that should not participate in the implication count because they do not satisfy the maximum multiplicity condition The number of tuples created by this step is   A  S   3  50  c  5  5  Generate   A  S   3 pairs  a i  b j  where each a i is different than all itemsets of A previously generated For each pair  a i  b j  write 40 tuples This step creates   A  S   3 new itemsets of A that should not participate in the implication count because they do not satisfy the minimum support requirement The number of tuples created by this step is   A  S   3  40 Shufe the output le This step just demonstrates that the operation of the algorithm is independent to the ordering of the tuples Estimate the implication count using algorithm NIPS/CI with a fringe size of four and also without a bounded fringe Perform one hundred such experiments and calculate the mean and the standard deviation of both estimations The total number of tuples for each experiment can be derived by adding the partial number of tuples created in each step For example for  A   10000 S  5000 and c  4 the average number of tuples for the corresponding experiment was 015 3  108  333 A minimum support of 50 tuples for this case corresponds to only 015  001 of the tuples demonstrating that in the implication count contribute even implications that hold for a very small number of tuples Figures 4,5 and 6 show the results for c  1  2  4 for varying cardinalities  A   The x-axis corresponds to the actual implication count of the dataset as that was imposed by the creation process The y-axis denotes the mean relative error as it is calculated by running one hundred experiments We used the following formula to estimate the mean relative error relative error   Actual S  Measured S  Actual S  Graphs Bounded Fringe express the experimental results for the case of a fringe with size F  4 while graphs Unbounded Fringe demonstrate the result for the case of an arbitrarily large fringe The error bars correspond to the statistical deviation of the mean error as that was computed by one hundred such experiments.The deviation is generally negligible which means that the error of the estimated S is always very close to the mean error We also observe that the difference between the estimation using a bounded fringe of size four and a unbounded one is negligible for a very wide range of implication counts and therefore a size of four for the fringe zone is sufcient to provide very accurate results for most applications 10 20 30 40 50 60 70 80 90 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe Figure 6 Dataset One with c  4 6.2 Real-world datasets  Algorithmic comparison We compare our estimates with the results taken using Distinct Sampling DS which has been sho wn pro vide highlyProceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


accurate estimates for distinct value queries and event reports This algorithm outperforms other estimators that are based on uniform sampling[8 9 e v e n when using much less sample space We also provide comparison with our Implication Lossy Counting ILC algorithm described in Section 5.1 which is based in the Lossy Counting algorithm introduced in For this series of experiments we used a real dataset of eight dimensions which was given to us by an OLAP company whose name we cannot disclose due to our agreement The cardinalities of the dimensions are presented in Table 3 The parameters of the algorithms are presented in Table 5 For NIPS/CI we used 64 concurrent bitmaps with a fringe size of four thus requiring memory enough to hold  2 4  1   64  K  1920 itemsets We expect that the averaging\([5 o v e r these man y bitmaps will result in an error less than 10 We used the exact same sample space for DS The bound parameter t for DS was set to  1920  50  following the suggestion in F o r ILC we used an approximation parameter   0  01 which increases the memory requirements of ILC relative to those of NIPS/CI or DS On the average ILC used more than twice the memory that NIPS/CI and DS used For example for the experiment in Figure 7\(B it used more than 8,000 entries We evaluate the results of the algorithms with respect to the number of tuples the cardinality of the participating dimensions and the implication conditions To simulate a real data stream scenario we tracked the conditional implication counts of A  B  E  F and the unconditional B  E using the aforementioned algorithms The rst workload corresponds to quite large compound cardinality while the second to very moderate cardinalities Table 4 presents the actual aggregates for various instances of the stream for   5 and  1  60 We believe that most workloads fall somewhere in the middle with respect to the complexity of the wanted implications and the size of the returned counts Figure 7\(A depicts the relative error as the stream evolves for workload A using the algorithms DS NIPS/CI and ILC for different implication parameters In Figure a we show the results for minimum support   5 and  1  60 or  1  80 The different  1 are encoded in the parentheses next to identication of the algorithm in the legend of the graph In Figure b we increased the minimum support to   50 We observe that the behavior of DS varies widely while NIPS/CI remains always below the expected 10 error DS actually keeps a sample of the distinct elements seen so far and tries to scale the implication count that holds for that sample to the whole set of distinct elements In most cases the data in the sample is not representative of the implication The situation for DS is exacerbated when the minimum support increases where quite a lot of samples do not participate in the count making the scaling even more errorprone Algorithm ILC in all cases returned very erroneous results although it used much more space than NIPS/CI and DS since it tries to store not the implication counts but the actual implicated itemsets In these workload the implicated itemsets overwhelm its available memory which is actually larger than the amount given to NIPS/CI and DS In gure 7\(B we present the results of the algorithms for workload B The situation is still in favor of NIPS/CI whose relative error remains always close to the expected 10 unlike DS who returns highly skewed errors even though the domain cardinalities are much smaller and therefore keeps in the sample space much more data As expected from the analysis the error guarantees of NIPS/CI are virtually unaffected by changes in the cardinalities or the number of tuples seen so far in the stream ILC returns very erroneous results although now the cardinalities and the implicated items are much smaller compared to those of workload A The reason is not only because it keeps too much information in memory i.e all the implicated itemsets while both NIPS/CI and DS only hold a mantissa for the count but also because the constraint    rel is broken as the number of tuples increases 7 Related Work There are unique challenges in query processing for the data stream model Most challenges are the result of the streams being potentially unbounded in size Therefore the amount of the storage required in order to get an exact size may also grow out of bounds Another equally important issue is the timely query response required although the volumes of data the need to be processed is continually augmented at a very high rate Essentially the amount of computation per data item received should not add a lot of latency to each item Otherwise any such algorithm wont be able to keep up with the data stream In many cases accessing secondary storage such as disks is not even an option In  there is a discussion of what queries can be answered e xactly using bounded memory and queries that must be approximated unless disk access is allowed Sketching techniques\([14 ha v e been introduced to build summaries of data in order to estimate the number F 0 of distinct elements in a dataset In three algorithms that      approximate the F 0 are described with various space and time requirements Distinct is dri v e n by hashing functions similar to those studied in 14 and provides highly accurate results for distinct value queries compared to those taken by uniform sampling by using only a fraction of their sample size In the algorithms Stick y Sampling and Lossy Counting are introduced that estimate frequency counts with application to association rules and iceberg cubes In a framework for performing set expression on continuously updated streams based on sketching techniques is presented In a general framework over multiple granularities is presented for both range-temporal and spatio-temporal aggregations In a framework for identifying hierarchical heavy hitters i.e hierarchical objects like network addresses whose prexes denes a hierarchy with a frequency above a given threshold is described The effect of impications between columns has been emphasized in the system that identies correlated pairs of columns and soft-dependencies and has been proved very useful in query optimization 8 Conclusions We have presented a generalized and parameterized framework that can accurately and efciently estimate implication counts and can be applied to many scenarios To the best of our knowledge this is the rst practical and truly scalable approach to the problem Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


 Dimension Cardinality A 1557 B 2669 C 2 D 2 E 3363 F 131 G 660 H 693 Table 3 Cardinalities Workload A Workload B Tuples A  B  E  G E  B 134,576 608 50 672,771 12,787 125 1,344,591 34,816 152 2,690,181 84,190 165 4,035,475 132,161 182 5,381,203 187,584 188 Table 4 Impl counts w.r.t tuples NIPS/CI bitmaps 64 NIPS/CI K 2 DS sample size 1920 DS bound t 39 ILC  0.01 Table 5 Algorithm Parameters           0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILC \(.8             0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILS \(.8           0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6 ILC \(.8             0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILC \(.8   a   5 b   50 a   5 b   50 A Workload A B Workload B Figure 7 Relative Error vs stream size of online estimation within small errors of complex implication and non-implication counts between attributes of a data stream under severe memory and processing constraints and even in the presence of noise We prove that the complement problem of estimating non-implication counts can be      approximated when the size of the fringe zone is xed appropriately We demonstrate that existing algorithms for estimating frequent itemsets or sampling cannot be applied to the problem since they lose the cumulative effect of small implications In addition through an extensive set of experiments on both synthetic and real data we have shown that NIPS/CI always remains very close to the actual implication count capturing even very small implications whose total contribution is signicant References  R Agra w al A Evmie vski and R Srikant Information sharing across private databases In ACMSIGMOD  2003  N Alon Y  Matias and M Sze gedy  The space comple xity of approximating the frequency moments Journal of Computer and System Sciences  58:137 147 1999  A Arasu B Babcock S Bab u J McAlister  and J W idom Characterizing memory requirement for queries over continuous data streams In Proceedings of the Twenty-rst ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems  2002  B Babcock S Bab u M Datar  R  Motw ani and J W idom Models and Issues in Data Stream Systems In Proceedings of the Twenty-rst ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems  2002  Z Bar Y ossef T  Jayram R K umar  D  S i v akumar  and L T r e visan Counting Distinct Elements in a Data Stream In RANDOM  pages 110 2002  A Belussi and C F aloutsos Estimating the selecti vity of spatial queries using the correlation fractal dimension In VLDB95 Proceedings of 21th International Conference on Very Large Data Bases September 11-15 1995 Zurich Switzerland  pages 299310 1995  J Bunge and M Fitzpatrick Estimating the number of species A r e vie w  Journal of American Statistical Association  88:364373 1993  M Charikar  S  Chaudhuri R Motw ani and V  Narasayya T o w ards estimation error guaranties for distinct values In ACMPODS  pages 268279 2000  S Chaudhuri R Motw ani and V  Narasayya Random sampling for histogram construction How much is enough In ACMSIGMOD  pages 436 447 1998  G Cormode F  K orn S Muthukrishnan and D Sri v astana Finding hierar chical heavy hitters in data streams In VLDB  2003  M Datar  A  Gionis P  Indyk and R Motw ani Maintaing stream statistics over sliding windows In Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms  2002  A Deshpande M Garof alakis and R Rastogi Independence is Good Dependency-Based Histogram Synopses for High-Dimensional Data In ACM SIGMOD  2001  C Estan S Sa v age and G V a r ghese Automatically inferring patterns of resource consumption in network trafc In ACMSIGCOMM  2003  P  Flajolet and N Martin Probabilistic Counting Algorithms for Data Base Applications Journal of Computer and System Sciences  pages 182209 1985  N Friedman L Getoor  D  K oller  and A Pfef fer  Learning probabilistic relational models In IJCAI  pages 13001309 1999  S Ganguly  M  Garof alakis and R Rastogi Processing set e xpressions o v e r continuous update streams In ACM SIGMOD  pages 265276 2003  P  Gibbons Distinct sampling for highly-accurate answers to distinct v alues queries and event reports In VLDB  pages 541550 2001  P  J Haas J F  Naughton S Seshadri and L Stok es Sampling-based estimation of the number of distinct values of an attribute In VLDB  1995  I Ilyas V Markl P  Haas P  Bro wn and A Aboulnaga CORDS Automatic Discovery of Correlations and Soft Functional Dependencies In ACMSIGMOD  2004  J Jung B Krishnamurthy  and M Rabino vich Flash cro wds and denial of service attacks Characterization and implications for cdns and web sites In ACMWWW  2002  J Ki vinen and H Mannila Approximate dependenc y inference from relations Theoritical Computer Science  149:129149 1995  G Manku and R Motw ani Approximate frequenc y counts o v e r data streams In VLDB  2002  V  Poosala P  J Haas Y  E Ioannidis and E J Shekita Impro v e d histograms for selectivity estimation of range predicates In ACMSIGMOD  pages 294305 1996  S Stolfo W  Lee P  Chan W  F an and E Eskin Data mining-based intrusion detectors An overview of the columbia ids project ACMSIGMOD Record  30\(4 2001  H W ang D Zhang and K G Shin Detecting SYN Flooding Attacks In INFOCOM 2002  2002  K Whang B V ander Zander  and H T aylor  A Linear Time Probabilistic Counting Algorithm for Database Applications ACM Transactions on Database Systems  pages 209229 1990  D Zhang D Gunopulos V  Tsotras and B  See ger  T emporal and spatiotemporal aggregations over data streams using multiple time granularities Information Systems  28\(1-2 2003 The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ofcial policies either expressed or implied of the Army Research Laboratory or the U S Government Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


Figure 3a and 3b, is negative since CBA and PART achieved a higher classification rate against this particular dataset A comparison of the knowledge representation produced by our method, PART and CBA has been conducted to evaluate the effectiveness of the set of rules derived. Figure 4 represents the classifiers generated form the hyperheuristic datasets. Analysis of the rules sets indicated that MMAC derives a few more rules than PART and CBA for the majority of the datasets. In particular, the proposed method produced more rules than PART and CBA on 8 and 7 datasets, respectively. A possible reason for extracting more rules is based on the recursive learning phase that MMAC employs to discover more hidden information that most of the associative classification techniques discard, since they only extract the highest confidence rule for each frequent item that survives MinConf Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 6. Conclusions A new approach for multi-class, and multi-label classification has been proposed that has many distinguishing features over traditional and associative classification methods in that it \(1 that contain rules with multiple labels, \(2 evaluation measures for evaluating accuracy rate, \(3 employs a new method of discovering the rules that require only one scan over the training data, \(4 introduces a ranking technique which prunes redundant rules, and ensures only high effective ones are used for classification, and \(5 discovery and rules generation in one phase to conserve less storage and runtime. Performance studies on 19 datasets from Weka data collection and 9 hyperheuristic scheduling runs indicated that our proposed approach is effective, consistent and has a higher classification rate than the-state-of-the-art decision tree rule \(PART and RIPPER algorithms. In further work, we anticipate extending the method to treat continuous data and creating a hyperheuristic approach to learn  on the fly   which low-level heuristic method is the most effective References 1] R. Agrawal, T. Amielinski and A. Swami. Mining association rule between sets of items in large databases In Proceeding of the 1993 ACM SIGMOD International Conference on Management of Data, Washington, DC May 26-28 1993, pp. 207-216 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rule. In Proceeding of the 20th International Conference on Very Large Data Bases, 1994, pp. 487   499 3] M. Boutell, X. Shen, J. Luo and C. Brown. Multi-label semantic scene classification. Technical report 813 Department of Computer Science, University of Rochester Rochester , NY 14627 &amp; Electronic Imaging Products R &amp D, Eastern Kodak Company, September 2003 4] A. Clare and R.D. King. Knowledge discovery in multilabel phenotype data. In L. De Raedt and A. Siebes editors, PKDD01, volume 2168 of Lecture Notes in Artificial Intelligence, Springer - Verlag, 2001,  pp. 42-53 5] P. Cowling and K. Chakhlevitch. Hyperheuristics for Managing a Large Collection of Low Level Heuristics to Schedule Personnel. In Proceeding of 2003 IEEE conference on Evolutionary Computation, Canberra Australia, 8-12 Dec 2003 6] R. Duda, P. Hart, and D. Strok. Pattern classification Wiley, 2001 7] E. Frank and I. Witten. Generating accurate rule sets without global optimisation. In Shavlik, J., ed., Machine Learning: In Proceedings of the Fifteenth International 


Learning: In Proceedings of the Fifteenth International Conference, Madison, Wisconsin. Morgan Kaufmann Publishers, San Francisco, CA, pp. 144-151 8] J. Furnkranz. Separate-and-conquer rule learning Technical Report TR-96-25, Austrian Research Institute for Artificial Intelligence, Vienna, 1996 9] W. Li, J. Han and J. Pei. CMAR: Accurate and efficient classification based on multiple class association rule. In ICDM  01, San Jose, CA, Nov. 2001, pp. 369-376 10 ] T. Joachims. Text categorisation with Support Vector Machines: Learning with many relevant features. In Proceeding Tenth European Conference on Machine Learning, 1998,  pp. 137-142 11] T. S. Lim, W. Y. Loh and Y. S. Shih. A comparison of prediction accuracy, complexity and training time of thirtythree old and new classification algorithms. Machine Learning, 39, 2000 12] B. Liu, W. Hsu and Y. Ma. Integrating Classification and association rule mining. In KDD  98,  New York, NY, Aug 1998 13] J.R. Quinlan. C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann, San Francisco, 1993 14] J.R. Quinlan. Generating production rules from decision trees. In Proceeding of the 10th International Joint Conferences on Artificial Intelligence,  Morgan Kaufmann San Francisco, 1987, pp. 304-307 15] R. Schapire and Y. Singer, "BoosTexter: A boosting-based system for text categorization," Machine Learning, vol. 39 no. 2/3, 2000, pp. 135-168 16] F. Thabtah, P. Cowling and Y. Peng. Comparison of Classification techniques for a personnel scheduling problem. In Proceeding of the 2004 International Business Information Management Conference, Amman, July 2004 17]Y. Yang. An evaluation of statistical approaches to text categorisation. Technical Report CMU-CS-97-127 Carnegie Mellon University, April 1997 18] X. Yin and J. Han. CPAR: Classification based on predictive association rule. In  SDM  2003, San Francisco CA, May 2003 19]CBA:http://www.comp.nus.edu.sg/~dm2/ p_download.html 20] Weka: Data Mining Software in Java http://www.cs.waikato.ac.nz/ml/weka 21] M. J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. New algorithms for fast discovery of association rules. In Proceedings of the 3rd KDD Conference, Aug. 1997 pp.283-286 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


Low Medium 1.152e3 1.153e3 5.509e3 3.741e3 20 20.0 393 0.647 0.9900 0.0092 1.0000 0.0093 R=1000 1e-4 0.2 High 1.153e3 1.154e3 3.180e3 2.698e3 45 45.0 597 0.214 0.9900 0.0071 1.0000 0.0162 2e-4 2e-4 Low 0.4017 0.4024 109.291 71.994 4 4.0 39.7 0.996 0.9960 0.0075 0.9995 0.0012 2e-3 1.0 1e-3 Medium Medium 0.4128 0.4129 17.488 19.216 6 6.0 29.6 0.992 0.9938 0.0018 0.9987 0.0034 R=0.1 0.01 4e-3 High 0.4612 0.4722 4.1029 5.410e3 12 12.0 28.0 0.992 0.9902 0.0045 1.0000 0.0132 0.04 1e-8 Low 0.0250 0.0272 253.917 155.263 3 3.0 38.0 1.002 0.9991 0.0001 0.9994 0.0008 1e-3 1e4 1e-7 High Medium 0.0338 0.0290 17.303 12.837 3 3.0 14.5 0.332 0.9900 0.0058 0.9951 0.0045 R=1e-5 1e-2 1e-6 High 0.0918 0.0557 1.6071 1.225e5 8 8.0 9.9 0.992 0.9906 0.0034 0.9994 0.0142 1e-3 TABLE II SIMULATION RESULTS FOR: TRACKING REGIME PD=1, Q=100, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 2e-4 Low 0.0408 0.0407 2.9255 2.0060 5 5.0 30.3 0.996 0.9933 0.0016 0.9998 0.0020 0.02 1.0 1e-3 Medium Medium 0.0446 0.0446 0.5143 333.138 10 10.0 27.8 0.996 0.9900 0.0042 0.9995 0.0093 R=0.01 0.1 4e-3 High 0.0763 0.1062 0.1580 3.402e3 84 84.0 90.9 0.793 0.9900 0.0099 1.0000 0.0334 0.4 TABLE III SIMULATION RESULTS FOR: TRACKING REGIME PD=0.9, Q=1000, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 0.05 Low 1.166e3 1.167e3 1.073e4 3.357e4 11 12.2 257 0.542 0.9900 0.0089 1.000 0.0073 5e-5 1e-4 0.1 Low Medium 1.167e3 1.167e3 5.509e3 1.481e4 21 23.2 309 0.858 0.9900 0.0067 1.0000 0.0052 R=1000 1e-4 0.2 High 1.168e3 1.170e3 3.180e3 6.979e3 46 50.6 475 0.798 0.9900 0.0089 1.000 0.0022 2e-4 3] T. Fortmann, Y. Bar-Shalom, Y. Scheffe, and S. B. Gelfand  Detection Thresholds for Tracking in Clutter- A Connection Between Estimation and Signal Processing  IEEE Trans Auto. Ctrl., Mar 1985 4] S. B. Gelfand, T. Fortmann, and Y. Bar-Shalom  Adaptive Threshold Detection Optimization for Tracking in Clutter  IEEE Trans. Aero. &amp; Elec. Sys., April 1996 5] Ch. M. Gadzhiev  Testing the Covariance Matrix of a Renovating Sequence Under Operating Control of the Kalman Filter  IEEE Auto. &amp; Remote Ctrl., July 1996 6] L. C. Ludeman, Random Processes: Filtering, Estimation and Detection, Wiley, 2003 7] L. Y. Pao and W. Khawsuk  Determining Track Loss Without Truth Information for Distributed Target Tracking Applications  Proc. Amer. Ctrl. Conf., June 2000 8] L. Y. Pao and R. M. Powers  A Comparison of Several Different Approaches for Target Tracking in Clutter  Proc Amer. Ctrl. Conf., June 2003 9] X. R. Li and Y. Bar-Shalom  Stability Evaluation and Track Life of the PDAF Tracking in Clutter  IEEE Trans. Auto Ctrl., May 1991 10] X. R. Li and Y. Bar-Shalom  Performance Prediction of 


10] X. R. Li and Y. Bar-Shalom  Performance Prediction of Tracking in Clutter with Nearest Neighbor Filters  SPIE Signal and Data Processing of Small Targets, July 1994 11] X. R. Li and Y. Bar-Shalom  Detection Threshold Selection for Tracking Performance Optimization  IEEE Trans. on Aero. &amp; Elect. Sys., July 1994 12] D. Salmond  Mixture Reduction Algorithms for Target Tracking in Clutter  SPIE Signal and Data Processing of Small Targets, Oct. 1990 13] L. Trailovic and L. Y. Pao  Position Error Modeling Using Gaussian Mixture Distributions with Application to Comparison of Tracking Algorithms  Proc. Amer. Ctrl. Conf., June 2003 4323 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





