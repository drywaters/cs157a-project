html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Proceedings  of the Thicd International Conference on Machine Learning and Cybernetics, Shanghai 26-29 August  2004 INVESTIGATION OF DISTRIBUTIVE DATA MINING CALCULATING ARCHITECTURE YING-WU FANGlt, W-ME1 WANG  GUANG-PENG ZHANG  YI WANG  SHAO-LONG WANG   School of Mechanical and Precision Instrument Engineering, W a n  University of Technology, Xi   710048. Chma b e  Telecommunication Engineering Institute, Air Force Engineering University, Xi  an 7  1OO77, China E-MAIL: fywwy123@yahoo.c0m.cn, wyfyw@tom.com I Abstract A distributive calculating architecture is presented to realize data mining emciently. The architecture is a hierarchical computational method from the wnception which stores the information in every sub-node with the ideas of database partition, a united center distribute unit can be responsible for the wllecbing and maintenance of the information in every sub-node, by the scan of database information is distributed to ditferent nodes, this architecture can maintain a global set enumerate tree, the loeal large item-sets can be constructed by using any effective algorithm it mainly solves the problem of highly effective data distribution and data skew, the detailed explaining and theoretical proving of the calcnlating architecture is given and how to solve the data skew problem highly and effectively is discussed aLw in this paper. The partial implementation of this algorithm shows the correctness and feasibility, the calculating architecture can be used for distribute database and most applicable for distribute calculation, which can be used in highly and effectiYely data mining in distributive and parallel environment Keywords architecture 1. Intruduction Data miniug; large item-sets, data skew; calculating Data mining is a new technology which is arisen in the mid 1990s, which is widely used in finance, credit insurance, process control, quality supervising, engineering and scientific data analyzing, target characteristic mining in militaq information system, as well as the mining of the state association rule, etc. data mining is combined product of many disCiplines and technologies I  In 1993, R Agrawal put forward the definition of association rule mining fxstly 12  and also provided AIS method of calculating which is based on several times scans of database. With the fast development of large database L31 huge amount of data have been stored in computers. But the existing database systems do not provide users with the 0-7803-8403-2/04/$20.00 a 0 0 4  IEEE necessary and effective tools to capture all stored information easily. Hence, automatic knowledge discovery techniques have been developed to capture and use the voluminous information hidden in large database  In 1995, the first international academic conference of knowledge finding and data mining was held in Canada Since then because of the advantages of feasibility and understanding easily of the association rule, it has owned the widely concern and deeply research, discovery of association rules is an important class of data mining whose   aim is to capture the co-occurrences of item-sets, the most important thing to do is to find the large item-sets effectively, because this is time-consuming and will finally decide the efficiency of algorithm 16.71 Now the main study is emphasized on how to find the large item-sets with more and more few times. However how to obtain accuracy of the association rule and efficiency became the key problem  In such a prerequisite research work of this paper is put forward the calculating architecture of the distributive data mining in 


calculating architecture of the distributive data mining in details, the purpose of founding is in the hope of providing a united calculating stage and decrease the communication cost among the panel points at most and maintain the good quality of extending, so as to can be used in supercomputer or microcomputer environments, the explaining and theoretical proving of the. architecture are given in details the corresponding algorithm based on this architecture can be found in Ref.[9]. According to the ideas of da!zhase partition, uses the dynamic generation of set enumerate tree dramatically deduced the YOs, at the same time uses controller to assign transactions randomly to resolve the data skew in the database, it is proved that the calculating architecture have very good flexibility and extensibility 2. Distributive architecture 2.1. Calculating architecture analysis In definition, the distributive architecture is a hierarchical computational method, which stores the 4   1656 Proceedings of the Third International Conference on Machine teaming and Cybemetics, Shanghai  26-29 August 2004 information in every sub-node with the ideas of Partition database, a united center distribute unit shall be responsible for the collecting and maintenance of the information in every sub-node, but the concrete algorithm in each part is not fixed, different algorithm shall be used in different parts this architecture can be used in parallel and distributive environment. It mainly solves the problem of highly effective data distribution and data skew, with the thoughts of mnverse cluster and convese Hash, the close item-sets shall be separated. By the scan of database, information shall be distributed to different node, this architecture shall maintain a global set enumerate tree, the local large item-sets can be constructed by using any effective algorithm, the architecture is shown in Fig.1. The following provides definitions of the required concepts PI LTI P2 LTZ Pn LTo nu I I 1 Figure 1. Calculating architecture Define 1. Data partition P ED refers to any a non-vacant subset of transaction database D, it can  t be crossed to any two different data partitions, namely P i n P j = O ,  i#j Define 2. Local support of the item-sets \(support of item-sets in partition P transaction of the item-sets in data partitions P and total transaction in partitions, wbich is written supportdX namely Define 3. Local large item-sets \(large item-sets of partition P non-vacant item-sets of minsupport minsup by users defined, L: is denoted the local large item-sets of the suppoftdX length k, namely L:=[X 1 X J A  II X I1 =KAsup;ortp \( X All the large item-sets in partition P can be written Where K is the length of the longest large item-set in partition R Define 4. The support of item-sets X support \(X of appearance times in transaction database D of the transaction f of item-sets X and total of the transaction in D namely suppor~ CY Defme 5. Large item-sets refers to the non-vacant set of minsup which is over user  s definition, Lk stands for the large item-sets of the length k, so all large item-sets can be written L=ui=,,2 ,..., L i ,  namely L E [ X  1 X d A  II X II =kAsupport \( X Where CI-Cn denote the data transfer channel, PI-Pn 


Where CI-Cn denote the data transfer channel, PI-Pn denote processor\(tightly coupled computer\(loose1y coupled enumerate tree 2.2. Algorithm routine LP= Ui.,,2 ..... *LP The core of distributive calculation architecture is distribute unit DU, the detailed algorithm routine is 1 TRDU create data transfer channel CI to Cn, according to the number of processor or available single board computer which shall be responsible for all kinds of initial work including the relations between C1 and P I C 2  and P2,Cn and Pn; start the initial global set enumerate tree of information control and management unit or other database structure \(GT TDB and make a preparation for read transaction database because TDB can be concentrated or distributive also According to the number of processor can partition database, and according to the number of internal memory of every processor, the way of data process of sub-node can be decided, if the total internal memory is enough, the partition can be put in it, otherwise the generated dynamically method of local set enumerate tree \(LT used 2  to the different initial variable which handle data skew stratagem, transaction in database can be read in order, and distribute every transaction to different processor, so as to realize load balance and solve data skew. During read the database, large item-sets K-is created and is inserted in GT 3 transaction may be put in internal memory, effective algorithm shall be chosen to create local large item-sets 1657 otherwise transactions shall be buffer stored' in local diskette, the final local large item-sets can be created after the reception of all transactions from TRDU 4 communication among every node, meanwhile it must maintain global set enumerate tree\(GT communicate with ICMU in the process of every node is managed or after it is completed, which shall be decided by different realizing strategy. Every node can be transfer to ICMU after obtaining a local large item-sets K-, ICMU merge all the local large item-sets K- and is inserted to GT the method of forecast border for item-sets support of ordinal set enumerate tree shall be instruct every node that whether it is necessary to search for some sets. Every node can also transfer the whole enumerate tree to ICMU after generating the whole local large item-sets, E M U  shall be responsible for the merge all local large item-sets enumerate tree and generating global large item-sets enumerate tree 3. Architecture analysis 3.1. Explaining architecture The concept of hierarchical calculating of the distributive calculating architecture can be explained in Fig.2. Corresponding with every node, data processing layer shall be responsible for the basic work of local large item-sets, which is the practical working layer of distribute architecture. Corresponding with DU of calculating architecture which is produced in the large item-sets of distributive association rule, data controlling layer shall be the controller and coordinator of the whole distributive process, data accessing layer shall be used to store practical data, meanwhile data shall be distributed to every end point node to being processed. Data physical accessing layer is corresponded with database management system, the only function of the architecture can be to provide original transactions stream to data distributive layer effectively 


transactions stream to data distributive layer effectively Proceedings of.the Third Intemational Conference on Machine Leaming and Cybemetics, Shanghai  26-29 August 2004 1658 Data control layer U I I Data access laver 1  Figure 2. Concept of architecture The distributive layer architechlre is shown that different layer which is not the pure clientlserver concept among which data controlling layer is the most important, it shall coordinate with every node and engage data mining finally it needs to collect the result of partition data mining and get large item-sets, then the final association rules can be obtained. Data physical accessing layer can use existing macro-database management system, the effective methods of knowledge discovery in databases and management Both distributives and centralized can provide original transactions stream for data managing layer. The concept of distributive layer shield the concrete method of every layer problems, hence, according to different layers, different effective algorithm can be developed. After data partition local large item-sets may be created respectively, the final global large item-sets shall be created from the join of all of the local large item-sets, namely, the join of local large item-sets is the super-join of the global large item-sets 3.2 Theoretical prove The conclusion is correct, it can be proved based on the following theorem Theorem: Global large item-sets is complete through the distributive calculating architecture This formula can be formalization described, it shall transaction database is D, the min-support minsup is defined by client, P I ,  P2. -., . . .P.  is a partitioning of D namely,D=PIuP2u ... uP.Pj#0\(i=1,2;-,n, andp in Pj=O\(i#i item-set of D, A is also a large item-set in partition at least Prove: The number of transaction of item-set A included in D is N, the large item-sets is L, the number of transaction included in item-sets A in partition Pi is Ni, the large item-sets in Pi is Lj, the total of the transaction in D is ND the toral of transaction in partition Pi is Npi. The counterevidence is used to prove the above theorem, it is supposed that item-set A is a large item-set in D, while in any partition it is not large item-sets It shall be stated as AEL, but AaLi\(i=1,2;-,n can be obtained from the definition of the large item-sets AaL, \(i=l,  2, -., n 3 % &lt; IllillSUp A 3 &lt; dlSUp A "' A &lt NP, NE minsup 3 NI&lt;Npl.minsup A Nz&lt;Nn"insup A *.. A N.&lt;Nm minsup N,+Nz+ .*. +N.&lt; \(Npl+Nn+ .** +Np N &lt; ND,"insup lt;minsup ND AeL It is pmved that the result is contradiction with the above hypothesis of AE L, so the conclusion shall be right Procee- of the Third International Conference on Machine Learning and Cybernetics, Shanghai, 2629 August 2004 item Pointer of father 4. Examples analysis and discussion 4.1. The main data structure which This paper provides a concrete example by algorithm realizing, the generating algorithm of dynamic trees is realized based on the operation system of Ite1486 and Free BSD4.2, the partial realizing of algorithm is shown the 


BSD4.2, the partial realizing of algorithm is shown the accuracy and feasibility of the algorithm. Testing data of algorithm is come from the generating procedure of algorithm given in Ref.[6], data structure used in algorithm mainly includes the follows as 1 1 Client I Transactions I Transactions I Item list I _ _ item-sets right brother I list Pointer of Pointer of left brother Left brother 1 code I code 1 item 2 I Number of I Support of I Pointer of I Item I transaction n a g  of item-set s S list Item-sets I total 3 Number of I Flag of I Item-set I total I ... 1 item-sets I list of all the transaction which are solved by this node item-sets flag'shows that the closely followings is some large item-sets, if it is the 2 large item-sets, then the domain is 2; the total shows the number of large item-sets; item listing is the large item-sets listing, the width of every item-sets is specified by item-sets flag, the quantity is specified by the total domain 4.2. Generating dynamic set enumerate tree Supposing the transaction allocation unit distribute the record to a node P, when P is receiving node, it transfers a record type to a tree node type, then insert this node in a created enumerate tree, all received records can be denoted as the node listing in Tablel Tablel. Nodes list Number of item support item list 5 0 SRIJMZ _ _ 4 0 Q W L 4 0 SUZL 3 0 Q 5 0 RUZKL 3 0 ZKL 6 0 SNMSKL 4 0 NSZK 3 0 SNM 4 o 4 0 RUKL The final tree structure created is shown in Figure3 I 5 I 1 k R W M  3 1 2  I S U Z W  3 I 2  ISNh4t 61llSNM SKL I Figure3. Generating tree structure 4.3. Calculating the support of item-sets From the description of algorithm, the -K large item-sets can be created in the process of obtaining every transaction in node, a candidate large item-sets can be obtained by using the generating algorithm of candidate Spriori item-sets, then the general support of item-sets can be obtained by using created enumerate tree. For example item-sets \(SN found, \(SN the sub-trees of [SUM algorithm, the nodes have covered all the information of super-item-sets, so the support of { S N searching for its right sub-tree, at last the general support of item-sets { S N ]  is 2. Item-sets {RU] ,  when it reaches STUMZ support of { R U sub-tree of {SRUMZ R U is got, then the support of \( R U has been got, stop searching, at last the general support of R U 


4.4. Data skew problem The application of distributive architecture must solve some key problems, as communication among nodes, the asynchronous and synchronous choices, realizing methods efficiency and accuracy, etc. the effective handle methods 1659 Proceedings of the w d  International Conference on Machine Learning and Cybernetics, Shanghai  26-29 August 2004 to these problems have been mentioned in the research of parallel and distributive system, but it can not be applicable in parallel and distributive data mining, in which data skew is a key problem of distributive environment, the high data skew is meant that the similar or close data is collected to solve, which can cause difference in handle data quantity for different node, the balance of lo@ can be destroyed fmally. The whole distributive environment is synchronous or asynchronous; they need to be accorded at last, so the balance of load can be fully used the capacity of every node to reach the advantaged configuration of system. The influences of data skew in data partition of distributive data mining means the similar or close item-sets is gathered in the specific district, when some node solves the data district since some item-sets are extremely concentrated in the district, these local item-sets may be taken as the most probably large item-sets, but it is possible that these item-sets are only gathered in the district, and seldom appear in other districts, so as to these item-sets can not be the large item-sets. If there is much conditions like this can lead to rate decreasing of local item-sets which is become general large item-sets, in definition, it means increasing candidate large item-sets and decreasing the efficiency of generating large item-sets. In order to solve these problems, the method of random distributlon can be accepted, it means every transaction can random choose the node of processor. The algorithm of Partition provides a comparison of random distribution and order distribution which can be seen that random distribution can raise the accuracy of large item-sets of local node greatly, but the system of Partition decides that read transaction and handle transaction is happened orderly, so the random read can improve disc operation greatly. In distributive architecture structure, since the real read of the transaction are indexed in practical database as TRDU order, so the technologies such as block read and buffer memory can be fully used and random distribution can be carried out in TRDU separated from physical read of database, then this problem can be solved. But the effects of random read is decided by its randomness, TRDU can read data according to block, if the performance of data skew is the similar data which pear by the specific rule, and this rule can not he cleared by random stratagem, then random read can not reach the original target. For instance, the similarity of the data shall be showed that similar transaction can be appear every 10 transactions, while random stratagem means the circling distribution among 10 nodes, then every node can get similar transaction 5. Conclusions The research indicates that fully using of the advantages and characteristics of the distributive architecture can be distributed or paralleled after data partition effectively. The highly effective and brief communication among the panel points can decrease transmitted data quantity among the panel points at most meanwhile increase finding efficiency, this calculating architecture has strong flexibility and the quality of extending. The final given algorithm realizing data architecture is proved that the calculating architecture have very good flexibility and extensibility, and it can be used in highly and effectively data mining in distributive and parallel environment References I] Hu. K. and Xia S.W., Data mining based on large data 


I] Hu. K. and Xia S.W., Data mining based on large data warehouse, Journal of software, Vol 9,No.l,pp.53-63,Jan.1998 2] Agrawal. R., Mining association rules between sets of items in large database, Proc. ACM SIGMOD int  l conf. Management of data, Washington DC, pp 207-216.May.1993 3] Alex. B. and Stephen. IS., Data warebouse, Data mining and OLAP, McGraw-Hill Book Co. 1999 4] Chen. M.S. and Philip. S., Data mining: an overview from database perspective, IEEE Transaction on knowledge and data engineering pp.866-883 Aug.1996 Roberto. J., Efficiently mining long pattems from databases, Proceedings of the 1998 ACM-SIGMOD int  l conf. on management of data pp.85-93,1998 6] Duda. R.O., Hart. P.E. and Stork. D.J., Pattem recognition, Wiley, New York, 2001 7] Zaki, M.J., Ogibara. M. and Li. W., New algorithms for fast discovery of association rules, Proceedings of the third Int  l conf. on knowledge discovery in database and data mining, pp.283-286.1997 8] Goulboume. G.,  Coenen. F. and Leng. P., Algorithms for computing association rules using a partial-support tree, Knowledge-Based Systems, Vol 13 pp.141-149,2000 9] Yingwu Fang, Guangpeng Zhang, Dewei Wu and Wang Yi, Research on distributive data mining calculating process-DDCP algorithm, Joiirnal of university of elechonic science and technology, Vo132 No. 1, pp.80-84, Feb.2003 5 1660 pre></body></html 


It should be noted that after the above process the resulting support constraint set may become inconsistent Thus in the next round the value c   m i 1 z i may be larger If that happens the larger value c does not interpret as the privacy condence level Instead it should be interpreted as an indicator for inconsistency of the support constraint set Thus the above privacy deletion procedure should only be carried out one time We should note that even if the condence level is higher that is c   m i 1 z i is small there is still possibility that the condential information specied by  I,s,S  is leaked in theory That is for each transaction database D that satises the constraints S wehave support  I D    s S   However no one may be able to recover this information since it is NP hard to infer this fact Support constraint inference has been extensively studied by Calders in 2 3 It would be interesting to consider conditional privacypreserving synthetic transaction database generations That is we say that no private information is leaked unless some hardness problems are solved efciently This is similar to the methodologies that are used in public key cryptography For example we believe that RSA encryption scheme is secure unless one can factorize large integers In our case we may assume that it is hard on average to efciently solve integer linear programs Based on this assumption we can say that unless integer linear programs could be solved efciently on average no privacy specied in P is leaked by S if the computed condence level c   m i 1 z i is small 5 Conclusions In this paper we discussed the general problems regarding privacy preserving synthetic transaction database generation for benchmark testing purpose In particular we showed that this problem is generally NP hard Approximation algorithms for both synthetic transaction database generation and privacy leakage condence level approximation have been proposed These approximation algorithms include solving a continuous variable linear program According to 6 l i n ear probl ems ha vi ng hundreds of t housands of continuous variables are regularly solved Thus if the support constraint set size is in the order of hundreds of thousands then these approximation algorithms are efcient on regular Pentium-based computers If more constraints are necessary then more powerful computers are needed to generate synthetic transaction databases References 1 R  A g r a w al T  Imilien sk i an d A  S w a mi Min in g association rules between sets of items in large databases In Proc of ACM SIGMOD International Conference on Management of Database  pages 207216 1993  T  C a lders  Axiomatization and Deduction Rules for the Frequency of Itemsets  PhD Thesis Universiteit Antwerpen 2003  T  C a l ders  C omput at i onal compl e x i t y of i t e ms et frequency satisability In Proc 23rd ACM PODS 04  pages 143154 ACM Press 2004  R  F agi n J  Hal pern and N Me gi ddo A l ogi c f or reasoning about probabilities Information and Computation  87 1,2\78128 1990  G Geor gak opoul os  D  K a v v a di as  a nd C  P a padi mitriou Probabilistic satisability J of Complexity  4 111 1988  Li near P r ogrammi ng F r equent l y As k e d Q ues t i ons  http://www-unix.mcs.anl.gov/otc Guide/faq/linear-programming-faq html  T  M ielik  ainen On inverse frequent set mining In Proc of 2nd Workshop on Privacy Preserving Data Mining PPDM  pages 1823 IEEE Computer Society 2003  C  P o t t s  A nal ys i s of a l i n ear programmi ng heuri s t i c for scheduling unrelated parallel machines Discrete Appl Math 10 155164 1985 9 G  R amesh  W  Man iatty  a n d M Zak i Feasib le itemset distributions in data mining theory and application In Proc 22nd ACM PODS  pages 284295 2003  Y  W a ng X W u  a nd Y  Zheng P r i v ac y p res ervi ng data generation for database application performance testing In Proc 1st Int Conf on Trust and Privacy in Digital Business TrustBus 04 together with DEXA  LNCS 3184 pages 142-151 2004 Springer-Verlag  X W u  Y  W u Y  W a ng and Y  Li P ri v a c y a w are mar ket basket data set generation a feasible approach for inverse frequent set mining In Proc 5th SIAM International Conference on Data Mining  April 2005  Z Zheng R  K oha vi  a nd L Mas on R eal w o rl d performance of association rule algorithms In Proc of the ACM-SIGKDD International Conference on Knowledge Discovery and Data Mining  pages 401 406 ACM Press 2001 8 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDM05 1550-4786/05 $20.00  2005 IEEE 


Discovery, 8, 2004, pp. 7-23 20] W. Teng, M. Hsieh, and M. Chen. On the Mining of Substitution Rules for Statistically Dependent Items Proceedings of IEEE International Conference on Data Mining \(ICDM  02 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


14] Kryszkiewicz, M., and Rybinski, H. \(1999  Incomplete Database Issues for Representative Association Rules  ISBN: 3-540-65965-X, pp. 583-591 15] Little, R.J.A., and Rubin, D.B. \(2002 analysis with missing data, Wiley, New York, ISBN 0471183865 16] Omiecinski, E.R. \(2003  Alternative interest measures for mining associations in databases  IEEE TKDE vol. 15, no. 1, pp.57-69 17] Piramuthu, S. \(1998  Evaluating feature selection methods for learning in data mining applications  in the Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, vol. 5, pp. 294-301 18] Pyle, D. \(1999 Morgan Kaufmann Publishers, Inc.ISBN:1-55860-529-0 19] Ragel, A., and Cremilleux, B. \(1999  MVC - A preprocessing Method to deal with missing values   knowledge based system, vol. 12, pp.285-291 20] Ramoni, M., and Sebastiani, P. \(2000  Bayesian Inference with Missing Data Using Bound and Collapse   Journal of Computational and Graphical Statistics, vol. 9, no 4, pp. 779-800 21] Ramoni, M., and Sebastiani, P. \(2001  Robust Bayes Classifiers  AI, vol. 125, no. 1-2, pp. 207-224 22] Ramoni, M., and Sebastiani, P. \(2001  Robust Learning with Missing Data  Machine Learning, vol. 45, no 2 , pp. 147-170 23] Scott, R.E. \(1993 Logic and Practice, SAGE Publications, ISBN: 0803941072 24] Zaki, M.J., and Hsiao, C.J. \(2002  CHARM: An efficient algorithm for closed itemset mining  in the Proceedings of the Second SIAM International Conference on Data Mining Proceedings of the Fourth International Conference on Hybrid Intelligent Systems \(HIS  04 0-7695-2291-2/04 $ 20.00 IEEE pre></body></html 


13: else 14: E|i?1| = E|i?1| ? s The backward process in Algorithm 1, generates level-wise every possible subset starting from the borProceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE der de?ned by Edge without getting into equivalence classes which have been already mined \(Line 10 such subset satis?es the constraint then it can be added to the output \(Line 12 reused later to generate new subsets \(Line 14 have a monotone constraint in conjunction, the backward process is stopped whenever the monotone border B+\(Th\(CM Lines 3 and 8 4.3. Closed Constrained Itemsets Miner The two techniques which have been discussed above are independent. We push monotone constraints working on the dataset, and anti-monotone constraints working on the search space. It  s clear that these two can coexist consistently. In Algorithm 2 we merge them in a Closet-like computation obtaining CCIMiner Algorithm 2 CCIMiner Input: X,D |X , C, Edge,MP5, CAM , CM X is a closed itemset D |X is the conditional dataset C is the set of closed itemsets visited so far Edge set of itemsets to be used in the BackwardMining MP5 solution itemsets discovered so far CAM , CM constraints Output: MP5 1: C = C ?X 2: if  CAM \(X 3: Edge = Edge ?X 4: else 5: if CM \(X 6: MP5 = MP5 ?X 7: for all i ? flist\(D |X 8: I = X ? {i} // new itemset avoid duplicates 9: if  Y ? C | I ? Y ? supp\(I Y then 10: D |I= ? // create conditional fp-tree 11: for all t ? D |X do 12: if CM \(X ? t 13: D |I= D |I ?{t |I  reduction 14: for all items i occurring in D |I do 15: if i /? flist\(D |I 16: D |I= D |I \\i // ?-reduction 17: for all j ? flist\(D |I 18: if supD|I \(j I 19: I = I ? {j} // accumulate closure 20: D |I= D |I \\{j 21: CCIMiner\(I,D |I , C,B,MP5, CAM , CM 22: MP5 = Backward-Mining\(Edge,MP5, CAM , CM For the details about FP-Growth and Closet see [10 16]. Here we want to outline three basic steps 1. the recursion is stopped whenever an itemset is found to violate the anti-monotone constraint CAM Line 2 2  and ? reductions are merged in to the computation by pruning every projected conditional FPTree \(as done in FP-Bonsai [7 Lines 11-16 3. the Backward-Mining has to be performed to retrieve closed itemsets of those equivalence classes which have been cut by CAM \(Line 22 5. Experimental Results The aim of our experimentation is to measure performance bene?ts given by our framework, and to quantify the information gained w.r.t. the other lossy approaches 


approaches All the tests were conducted on a Windows XP PC equipped with a 2.8GHz Pentium IV and 512MB of RAM memory, within the cygwin environment. The datasets used in our tests are those ones of the FIMI repository1, and the constraints were applied on attribute values \(e.g. price gaussian distribution within the range [0, 150000 In order to asses the information loss of the postprocessing approach followed by previous works, in Figure 4\(a lution sets given by two interpretations, i.e. |I2 \\ I1 On both datasets PUMBS and CHESS this di?erence rises up to 105 itemsets, which means about the 30 of the solution space cardinality. It is interesting to observe that the di?erence is larger for medium selective constraints. This seems quite natural since such constraints probably cut a larger number of equivalence classes of frequency In Figure 4\(b built during the mining is reported. On every dataset tested, the number of FP-trees decrease of about four orders of magnitude with the increasing of the selectivity of the constraint. This means that the technique is quite e?ective independently of the dataset Finally, in Figure 4\(c of our algorithm CCIMiner w.r.t. Closet at di?erent selectivity of the constraint. Since the post-processing approach must ?rst compute all closed frequent itemsets, we can consider Closet execution-time as a lowerbound on the post-processing approach performance Recall that CCIMiner exploits both requirements \(satisfying constraints and being closed ing time. This exploitation can give a speed up of about to two orders of magnitude, i.e. from a factor 6 with the dataset CONNECT, to a factor of 500 with the dataset CHESS. Obviously the performance improvements become stronger as the constraint become more selective 1 http://fimi.cs.helsinki.fi/data Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Information loss Number of FP-trees generated Run time performance 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 10 5 10 6 m I 2 I 1  PUMSB@29000 CHESS @ 1200 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 6 10 


10 1 10 2 10 3 10 4 10 5 10 6 10 7 m n u m b e r o f fp t re e s PUMSB @ 29000 CHESS @ 1200 CONNECT@11000 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 m e x e c u ti o n  ti m e  s e c  CCI Miner  \(PUMSB @ 29000 closet         \(PUMSB @ 29000 CCI Miner  \(CHESS @ 1200 closet         \(CHESS @ 1200 CCI Miner  \(CONNECT @ 11000 closet         \(CONNECT @ 11000 a b c Figure 4. Experimental results with CAM ? sum\(X.price 6. Conclusions 


6. Conclusions In this paper we have addressed the problem of mining frequent constrained closed patterns from a qualitative point of view. We have shown how previous works in literature overlooked this problem by using a postprocessing approach which is not lossless, in the sense that the whole set of constrained frequent patterns cannot be derived. Thus we have provided an accurate de?nition of constrained closed itemsets w.r.t the conciseness and losslessness of this constrained representation, and we have deeply characterized the computational problem. Finally we have shown how it is possible to quantitative push deep both requirements \(satisfying constraints and being closed process gaining performance bene?ts with the increasing of the constraint selectivity References 1] R. Agrawal, T. Imielinski, and A. N. Swami. Mining association rules between sets of items in large databases In Proceedings ACM SIGMOD, 1993 2] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules in LargeDatabases. InProceedings of the 20th VLDB, 1994 3] R. J. Bayardo. E?ciently mining long patterns from databases. In Proceedings of ACM SIGMOD, 1998 4] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Adaptive Constraint Pushing in frequent pattern mining. In Proceedings of 7th PKDD, 2003 5] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi ExAMiner: Optimized level-wise frequent pattern mining withmonotone constraints. InProc. of ICDM, 2003 6] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Exante: Anticipated data reduction in constrained pattern mining. In Proceedings of the 7th PKDD, 2003 7] F. Bonchi and B. Goethals. FP-Bonsai: the art of growing and pruning small fp-trees. In Proc. of the Eighth PAKDD, 2004 8] J. Boulicaut and B. Jeudy. Mining free itemsets under constraints. In International Database Engineering and Applications Symposium \(IDEAS 9] C. Bucila, J. Gehrke, D. Kifer, and W. White DualMiner: A dual-pruning algorithm for itemsets with constraints. In Proc. of the 8th ACM SIGKDD, 2002 10] J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In Proceedings of ACM SIGMOD, 2000 11] L.Jia, R. Pei, and D. Pei. Tough constraint-based frequent closed itemsets mining. In Proc.of the ACM Symposium on Applied computing, 2003 12] H. Mannila and H. Toivonen. Multiple uses of frequent sets and condensed representations: Extended abstract In Proceedings of the 2th ACM KDD, page 189, 1996 13] R. T. Ng, L. V. S. Lakshmanan, J. Han, and A. Pang Exploratory mining and pruning optimizations of constrained associations rules. In Proc. of SIGMOD, 1998 14] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering frequent closed itemsets for association rules In Proceedings of 7th ICDT, 1999 15] J.Pei, J.Han,andL.V.S.Lakshmanan.Mining frequent item sets with convertible constraints. In \(ICDE  01 pages 433  442, 2001 16] J. Pei, J. Han, and R. Mao. CLOSET: An e?cient algorithm formining frequent closed itemsets. InACMSIGMODWorkshop on Research Issues in Data Mining and Knowledge Discovery, 2000 17] J. Pei, J. Han, and J. Wang. Closet+: Searching for the best strategies for mining frequent closed itemsets. In SIGKDD  03, August 2003 18] L. D. Raedt and S. Kramer. The levelwise version space algorithm and its application to molecular fragment ?nding. In Proc. IJCAI, 2001 


ment ?nding. In Proc. IJCAI, 2001 19] R. Srikant, Q. Vu, and R. Agrawal. Mining association rules with item constraints. In Proceedings ACM SIGKDD, 1997 20] M. J. Zaki and C.-J. Hsiao. Charm: An e?cient algorithm for closed itemsets mining. In 2nd SIAM International Conference on Data Mining, April 2002 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


4] K. Chang, C. Chong, Bar-Shalom, Joint Probabilistic Data Association in Distributed Sensor Networks IEEE Transactions on Automatic Control, AC-31\(10 octobre 1986 5] Bar-shalom, Multitarget Multisensor Tracking Advanced Application ,:YBS Publishing, 1990 6] Bar-shalom, Multitarget Multisensor Tracking Principles and Techenices ,:YBS Publishing, 1995 7] Hu Wenlong, Mao shiyi, Multisensor Data Association Based on Combinatorial Optimization[J]. Journal of Systems Engineering and Electronics . 1997 NO.1,1~9 8] Pattipati K R, Passive Multisensor Data Association Using a New Relaxation Algorithm.In Multitarget-Multisensor Tracking: Advanced and Applications,Y.Barshalom,Norwood,MA:Aretech,199 0 9] Deb S,et al, An S-Dimentional Assignment Algorithm for Track Initiation ,Proc. Of the IEEE Int. Conf Systems Engineering, Kobe, Japan,Sept 1992 527~530 10] Deb S,et al, A Multisensor-Multitarget Data Association Algorithm for Heterogeneous Sensors[J].IEEE Trans. on AES 1993 ,29 \(2 560~568 12] Bar-shalom,Y.,and Fortmann,T.E  Tracking and Data Association  New York:Academic press,1988 13] J,A,Roecher and G.L.Phillis,Suboptimal Joint Probabilistic Data Association .IEEE Transaction on Aerospace Electronic Systems.1993,29\(2 14] J,A,Roecker,A Class of Near Optimal JPDA Algorithm IEEE Transaction on Aerospace and Electronic Systems,1994,30\(2 15] Han Yanfei, Analysis and Improvement of Multisensor Multitarget Probabbilistic Data Association Filting Algorithm[J].  Journal of Systems Engineering and Electronics, 2002, Vol.24, 36~38  569 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





