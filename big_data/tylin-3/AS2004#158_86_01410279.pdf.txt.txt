html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Mining  Associations by Linear Inequalities Tsay Young  T. Y   Department of Computer Science San Jose State University San Jose, CA 95192, USA tylin@cs.sjsu.edu Abstract The main theorem is: Generalized associations of a relational table can be found by a finite set of linear inequalities within polynomial time. It is derived from the following three results, which were established in ICDM0  02 and are re-developed here. They are \(1 rem: Isomorphic relations have isomorphic patterns. Such an isomorphism classifies relational tables into isomorphic classes. \(2 dexes uniquely exists in each isomorphic class. We take it as the canonical model of the class. \(3 tributes/features can be generated by a generalized procedure of the classical AOG \(attribute oriented generalization 4 established. By isomorphism theorem, we had the final result \(5 Keywords: association, deduction, feature, granules bitmaps 1 Introduction Though there is no formal de?nition of data mining, but its informal version of [?] is rather universal  Drawing useful high level information \(patterns knowledge and etc In this paper we will attempt to analyze such a informal notion critically on one of the core techniques, namely, association rule mining [4]. As a byproduct of the analysis we have the results stated in the abstract Our methodology is very rigorous, but we take the informal style to explain the rigorous method. We use illustrations if formal proofs do not give good insight. The main goal is to get the idea cross 1.1 Basics Terms in Association Mining \(AM In AM, two measures, support and con?dence, are the criteria. It is well-known among researchers, support is the essential one. In other words, high frequency are more important than the con?dence of implications. We call them undirected association rules, associations, or high frequency patterns Association mining is originated from the market basket data [1]. However, in many software systems, the data mining tools are applied to relational tables. For de?nitive, we have the following translations of terms and will use theminterchangeably 1. an item is an attribute value 2. a q-itemset is a subtuple of length q 3. A q-subtuple is a high frequency q-pattern or a qassociation, if its occurrences are greater than or equal to a given threshold 2 Anatomy of Association Mining\(AM In order to fully understand the mathematical mechanics of AM, we need to understand how the information are developed. First we set up a convention  A symbol is a string of  bit and bytes  it has no real world meaning. A symbol is termed a word, if the intended real world meaning participates in the formal reasoning In summary a word is an interpreted symbol and its interpretation is part of reasoning processes. Notations of word and symbol appear to be the same, but their meanings are very different Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 2.1 Information Flows of AM 1. Representation phase: real world  a relational table 


1. Representation phase: real world  a relational table of words. Each symbol \(column names and attribute values so we refer to them as words. The representations are incomplete. The semantics of words are not implemented and relied on human support \(traditional data processing professionals 2. Data Mining Phase: a table of symbols  patterns of symbols. In this phase, table of words is used as a table of symbols, because data mining algorithms do not consult human for the semantics of symbols; they are treated as  bits and bytes  in AM algorithm. Patterns which are algebraic or logic expressions of symbols are derived mathematically. Brie?y, the table of symbols is the only  axioms  and the patterns are the  theorems  3. Interpretation Phase: patterns of symbols  patterns of words. Patterns are discovered as expressions of symbols in the previous phase. In this phase, those individual symbols are regarded as words again\(using the meaning acquired in representation Phase 4. Realizations Phase: patterns of words  real world phenomena. Do patterns of words represent some real world phenomena 2.2 Axiomatic Approach to Data Mining To examine the foundation of each data mining technique we take the axiomatic approach. We require each technique explicitly speci?ed the following items 1. Input data: Any information utilized by data mining algorithms are considered the input. In AM, the input is a table of symbols. However, in clustering techniques the input is the given set of points plus the background knowledge - the geometry of the ambient space of the given points 2. The logic/reasoning system: AM uses mathematical deduction \(Counting is a very simple deduction 3. The output patterns: The model of output patterns needs to be speci?ed. The model of the traditional AM is the set of associations. They are  conjunctions  of input symbols. We will generalize it to the set of algebraic or logic expressions of symbols Such an approach has been called Deductive Data Mining [8 2.3 Interpretations of Patterns In representation phase, each word, to human, does correspond to a real world phenomenon; we express it by saying the meaning of the word is the real world phenomenon Again such meaning is known only to human, not to the system. For systems words are symbols. The patterns are discovered in the form of expressions of symbols. To interpreting them, we convert the expression of symbols to expression of words based on the interpretations established in the representation phase. Any pattern \(expression of symbols is said to be an un-interpreted pattern, if the transformation of the expression of symbols to the expression of words has not been done. A pattern is un-interpretable, if such a transformation cannot be done 2.4 Realization of Patterns A pattern is un-realizable, if the pattern \(= expression of symbol Note that an expression of words is, of course, interpretable but may not be realizable; we refer to 3 Tables of Symbols - Understand the input First, we will examine how the real world is represented 1 Select a set of attributes, called relational schema 2 Represent a set of real world entities by a table of words These words, called attribute values, are meaningful words to human, but their meaning are not implemented in the system In traditional data processing \(TDP ample, the attribute name, COLOR, means exactly what hu 


ample, the attribute name, COLOR, means exactly what human thinks. Therefore its possible values are yellow, blue and etc. More importantly   D B M S  p r o c e s s e s  t h e s e  d a t a  u n d e r  h u m a n  c o m m a n d s  and carries out the human-perceived semantics To stress such view, we call it 3 Computing with words At the same time, it is equally important to stress that 4 the human views or interpretations of those words are not implemented in the system. They are merely symbols in the system In the system, COLOR, yellow, blue, and etc are  bits and bytes  without any meaning; they are pure symbols. Using AI  s terminology [3], those attribute names and values Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE                                                                                                 


                            Table 1. A Relational Table K and its Isomorphic Copy K  column names, and elements in the tables tic primitives. They are primitives, because they are unde?ned terms inside the system, yet the symbols do represent unimplemented 3.1 Isomorphic Tables and Patterns Let us start this section with obvious, but somewhat a surprise observation. Intuitively, data is a table of symbols\(Section 2.2 symbols, the mathematical structure of the table will not be changed. So its patterns, e.g., association rules, will be preserved. Formally, we have the following theorem [9 Theorem 3.1. Isomorphic relations have isomorphic patterns We will illustrate the idea by an example. The following example is adopted from \([6], pp 702 Example 3.2. Suppose a relation consists of two attributes      o f  t y p e  i n t e g e r  a n d  s t r i n g  r e s p e c t i v e l y   T h e  c u r rent instance has eight tuples. These tuples are knowledge representations of entities. Table 1 illustrates the representations and isomorphism. To illustrate Theorem 3.1., let us assume the support is  two tuples  and count the items. It is easy to see we have 1. 1-assoication in K           2. 1-assoication in K                 3. 2-assoication in K             4. 2-assoication in K            a n d          q-association rules \(q=1,2 adding prime  to associations in K become associations in K  this illustrate the theorem   V a l u e     Value =Bit-Vectors =Granules         1 1 0 0 0 1 1 0                  0 0 1 0 1 0 0 1               0 0 0 1 0 0 0 0       V a l u e   B i t V e c t o r s   G r a n u l e s       1 0 0 1 0 0 0 0    


  1 0 0 1 0 0 0 0            0 1 0 0 1 0 1 0                  0 0 1 0 0 1 0 1         Table 2. Words in K and K  have the same bitmaps and granules 3.2 The Canonical Models In this section, we will introduce tables of Bitmaps \(TOB Granules\(TOG GDM illustrate the idea by examples. Let us consider the bitmap indexes for K \(see Table 1 have three bit-vectors. The ?rst, for value 30, is 11000110 because the ?rst, second, sixth, and seventh tuple have   3 0   s e e  T a b l e  2  Using Table 2 as the translation table, Table 1 is transformed into bitmap table, Table 3. It should be obvious that we will have the exact same bitmap table for K   Next, we note that a bit vector can be interpreted as a granule \(subset  For example, the bit vector, 11000110     3 0  r e p r e s e n t s  t h e  s u b s e t             s i m i l a r l y  0 0 1 0 1 0 0 1   o f      4 0  r e p r e s e n t s  t h e  s u b s e t        s in the bitmap case, Table 1 is transformed into granular table, Table 4 N o t e  t h a t    g r a n u l e s  f o r m s  a  p a r t i t i o n   a n d  h e n c e  i n d u c e s an equivalence relation, denoted by   similarly, we have Pawlak called the the pair          k n o w l edge base and note that it is equivalent to table of granule Since knowledge base often means something else, we have called it granular data model\(GDM rize these observations in Theorem 3.3 1. Isomorphic tables have the same canonical model 2. The canonical model has three forms, table of 


2. The canonical model has three forms, table of granules \(TOG TOB model\(GDM and regarded as synonyms Theorem 3.4. It is adequate to do AM in one of the canonical model Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE         b i t   t                                                               Table 3. Tables of Words and Bitmaps        g r a n u l e   g r a n u l e                                                                                                                                                                       Table 4. Tables of Words, and Granules: For each attribute, the collection of granules forms a partition on  This follows immediately from the isomorphism theorem see Theorem 3.1 4 The Theory of Features in GDM The notion of generalizations of features/attributes has not been formally de?ned; we will examine the classical case and reach our de?nition; see Definition4.1 Let us ?rst examine the well accepted case. Then we will take an obvious extension 4.1 Attribute Oriented Generalizations\(AOG Let be the given relational table. In the traditional attribute oriented generalization \(AOG is introduced by recursively de?ning a sequence of named equivalence relations on a given single attribute of  1. A level zero concept is a base concept\(distinct attribute values 2. A level one concept is a named \(or interpreted alence class of base concept          


                                                                       Table 5. A Generalized Table GK with a new named attribute 3. A level two concept is a named \(or interpreted alence class of ?rst level concepts, in general, are the second innermost relation, and etc 4. A n level two concept is a named \(or interpreted e q u i v a l e n c e  c l a s s  o f      level concepts 5. These concept hierarchy groups the base concepts into a nested sequence of named partitions of based concepts. For each named partition \(that is, the partition and each equivalence class has a name is introduced into the given table I n  t h i s  e x a m p l e     a t t r i b u t e  v a l u e s  a r e  t h e  b a s e  c o n c e p t s   A named partition is de?ned: The equivalence class     i s  n a m e d     a n o t h e r  e q u i v a l e n c e  c l a s s        a n d  t h e p a r t i t i o n      T h i s  g e n e r a l i z a t i o n  i n t r o d u c e s  a  n e w  n a m e d a t t r i b u t e     c o l u m n   i n t o  t h e  g i v e n  t a b l e   T a b l e  5   4.2 AOG on GDM In traditional concept hierarchy, all partitions are named To be uniform, we will consider the unnamed case. We will take the following   C o n v e n t i o n   u n n a m e d  p a r t i t i o n  w i l l  b e  r e g a r d e d  a s canonically named, that is, the partition and equivalence classes themselves are their own names T o  i l l u s t r a t e  t h e  i d e a   T h e  n e w l y  n a m e d    will be  unnamed  in its GDM. Let us use GDM          of the partition           a t t r i b u t e  i n d u c e s a new partition on  as follows: From Table 2   s a granule             d e  n e s      a n d    d e  n e s           T h e  n e w  g r a n u l e  i s                            a n d            T h e s e  t w o  n e w granules de?ne a new partition Table 6 The GDM of new             


                      Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE                                                                                                Table 6. A Generalized Table GK with a uninterpreted attribute By the convention, a canonically named partition or equivalence class will be referred to as an un-interpreted attribute or attribute value. Following the same spirit, a TOG, TOB or GDM is called a un-interpreted table or data model 4.3 The Feature Completion on GDM Traditional AOG focuses on one attribute. There are no reasons to stop at considering one attribute here we will consider a concept hierarchy on a set                      o f  a t t r i b u t e s   A s  w e  h a v e  o b s e r v e d in Theorem 3.4., it is adequate to do AM in GDM is      I n  t h i s  c a s e     i s  a  s u b s e t  o f we will denote it bu  Definition 4.1. A generalization over in a GDM is a coarser partition of             where    


             i s  a  n o n e m p t y  s u b s e t  o f   If we let varies through all non-empty subsets of  we have all possible generalizations of in GDM. We w i l l  d e n o t e  t h e  s e t  o f  a l l  g e n e r a l i z a t i o n s  b y     Observe that the intersection of generalizations is still a generalization. For any given ?nite set of generalizations t h e r e  i s  t h e  s m a l l e s t  g e n e r a l i z a t i o n   S o     i s  c l o s e d under meet \(=the intersection the smallest g e n e r a l i z a t i o n    S o     i s  a  l a t t i c e   M o r e  i m p o r t a n t l y  t h e  m e e t  a n d  j o i n  a r e  t h e  m e e t  a n d  j o i n  i n  t h e  l a t t i c e      of partitions on  Let      b e  t h e  s m a l l e s t  s u b l a t t i c e  o f      t h a t  c o n t a i n s and      b e  t h e  s m a l l e s t  s u b l a t t i c e f      t h a t  c o n t a i n s  a l l  c o a r s e n i n g  o f  Now we have Theorem 4.2         Based on this observation, we de?ne Definition 4.3. GDM           i s  c a l l e d  U n i v e r s a l  M o d e l of      i n  t h e  s e n s e  i t  c o n t a i n s  a l l  i t s  g e n e r a l i z a t i o n s     i s  t h e  f e a t u r e  c o m p l e t i o n  o f   4.4 Intuitive Discussions on Features/attributes We often hear such an informal statement  a new feature  a t t r i b u t e     i s  s e l e c t e d   e x t r a c t e d   o r  c o n s t r u c t e d  f r o m  a s u b s e t                      o f  a t t r i b u t e s  i n  t h e  t a b l e     What does such a statement mean First we observe that feature and attribute have been used interchangeably. In the classical data model, an attribute or a feature is a representation of property, characteristic, and etc.; see e.g., [15]. A feature represents a human perception about the data; each perception is represented by a symbol, and has been called attribute and the set of attributes a schema. Based on our convention, they are words in TDP Section 3 L e t  u s  a s s u m e  a  n e w  f e a t u r e    h a s  b e e  s e l e c t e d   e x t r a c t e d   o r  c o n s t r u c t e d   L e t  u s  i n s e r t  i t  i n t o  t h e  t a b l e    T h e  n e w  t a b l e  i s  d e n o t e d  b y           T h e  i n f o r m a l  s t a t e m e n t  p r o b a b l y  m e a n s  i n  t h e  n e w  t a b l e     i s  a n  a t t r i b u t e   A s i t  i s  d e r i v e d  f r o m     i t  i s  f u n c t i o n a l l y  d e p e n d e d  o n   s extraction and construction are informal words, we can use the functional dependency as formal de?nition of feature selection, extraction and constructions. Formally we de?ne 


selection, extraction and constructions. Formally we de?ne Definition 4.4.1   i s  a  f e a t u r e  d e r i v e d   s e l e c t e d   e x t r a c t e d  a n d  c o n s t r u c t e d f e a t u r e   f r o m     i f    i s  f u n c t i o n a l  d e p e n d e n t  o n    i n  t h e n e w  t a b l e          In Theorem 4.2, we have shown that      i s  f e a t u r e completion of By the convention in Section 4.2    i s  u n i n t e r p r e t e d  f e a t u r e  c o m p l e t e i o n  o f   This theorem is rather anti-intuitive. Taking human  s view there are in?nitely many features. But the theorem says there are only ?nitely many features \(as      i s  a   n i t e set the ?nite-ness slip in? Our analysis says it comes in at the representation phase. We represent the universe by ?nite words. However, in phase two, suddenly these words are reduced to symbols. Thus the in?nite world now is encoded by a ?nite set of symbols. In particular, features can only be encoded in a ?nite distinct ways. A common confusing most likely comes from the confusing of data mining and  facts  mining 5 Generalized Associations in GDM As we have observed that it is adequate to conduct AM in the canonical model, such as GDM Main Theorem 5.1. Let          b e  t h e  u n i v e r s a l m o d e l   L e t  g  b e  a  g r a n u l e  i n  a  p a r t i t i o n          h t h a t         T h e n  g  i s  a n  u n i n t e r p r e t e d  g e n e r a l i z e d associations Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Let us de?ne an operation of binary number x and a set S We write S*x to be de?ned by The two equations            i f                       i f       r     Main Theorem 5.2                         be the smallest element in     L e t                b e  t h e  g r a n u l e s  i n     T h e n  t h e union                    is a granule that represents a un-interpreted generalized association rule, if its cardinality             where s is the threshold 


where s is the threshold Remark: The cardinal number of     i s  b o u n d e d  b y the Bell number [2] of       t h e  c a r d i n a l  n u m b e r  o f               The total number of derived attributes is bounded by Bell number  However the complexity of  minimal solutions  is bounded by the combination   We will report the calculation on real world data in future report soon 5.1 Find Generalized Association Rule by Linear Inequalities - an example We will illustrate the idea of the procedure of ?nding generalized association rules in Table 7 by linear inequality \(supp o r t         T h e  a s s o c i a t i o n  c a n  b e  e x p r e s s e d  a s  g r a n u l e s  1. Associations of length one  a   T E N                  b   S J                  c   L A          2. Associations of length two  a    T E N  S J     T E N    S J                  w h e r e   T E N  S J        


                        is in table format, that is equivalent to GDM f o r m a t   T E N    S J  3. No associations of length      Now let us examine the universal model in Table ??. The c o l u m n    i n  T a b l e     i s  t h e  s m a l l e s t  e l e m e n t  i n  t h e  c o m plete relation lattice      S o  e v e r y  e l e m e n t  o f     s a  c o a r s e n i n g  o f     I n  o t h e r  w o r d s   e v e r y  g r a n u l e  i n     i s  a  u n i o n  o f  s o m e  g r a n u l e s  f r o m  t h e  p a r t i t i o n     b y  t h e expression  a granule in     we mean a granule belonging to one of its partitions I n  t h i s  e x a m p l e   t h e  g r a n u l e s  i n   e    T W E N T Y    N Y           S J                     T W E N T Y    L A        T H I R T Y          be the cardinality of The following expression represents the cardinality of granules in      w h i c h  i s  a u n i o n  o f  s o m e  g r a n u l e s  f r o m  t h e  p a r t i t i o n     T W E N T Y  Y        N  J       T W E N T Y        


L A         T H I R T Y    L A           By taking the actual value of the cardinalities of the granules, we have                                                                 We will express the solutions in vector form              I t  i s  a n   integral convex set  in 4dimensional space The  boundary solutions  are 1   0   1   0   0    t h i s  s o l u t i o n  m e a n s         s cardinality b y  i t s e l f  a l r e a d y  m e e t s  t h e  t h r e s h o l d        2 \(0, 0, 1, 1 granules T W E N T Y    L A  a n d  T H I R T Y    to meet the threshold. In other words, we need a generalized concept that covers both the sub-tuple  T W E N T Y   L A    T W E N T Y    L A  a n d  T H I R T Y   L A    T H I R T Y    For this particular case, since L A     T W E N T Y   L A      T H I R T Y   L A   hence LA is the desirable generalized concept 3 \(1, 0, 0, 1 granules      T H I R T Y    L A  Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Table of Granules Table of Symbols                      


                                                       


                                                  S J       


                                                      


                         L A                                        


          L A  Table 7. Table of Granules at left-hand-side is isomorphic to  at right- hand-side: By Theorem  3.1 one can ?nd patterns in either table as a single generalized concept  Internal points  are:[4]\(1, 1, 0, 0 tions; [5]\(0, 1, 1, 0  0, 1, 0, 1  0, 1, 1, 1  1, 1 1, 0  1, 1, 0, 1  1, 0, 1, 1 11]\(1, 1, 1, 1 form and simplify them into disjoint normal forms 1  T E N    S J    T E N    S J 2  T W E N T Y    L A    T H I R T Y   A 3  T W E N T Y      T H I R T Y   A 4  T W E N T Y            L A 5  T E N      T W E N T Y    L A    T E N    T W E N T Y   A   S J    T W E N T Y   A      T H I R T Y    L A     Y 7  T E N      T W E N T Y      T H I R T Y   L A    T E N   L A    S  J   A 8  T W E N T Y      T E N      T W E N T Y    L A    T E N   T W E N T Y      T H I R T Y 9  T W E N T Y    N Y    T E N    S J    T H I R T Y    L A      T W E N T Y    L A  1 0  T W E N T Y    N Y    T W E N T Y    L A    T H I R T Y   A    J 1 1  T W E N T Y          T W E N T Y    L A   T H I R T Y    L A    a l l If the simpli?ed expression is a single clause \(in the original symbols non-generalized the following associations 1   T E N     S J    T E N    S J  2. SJ   J 4   L A    T W E N T Y    L A    T H I R T Y    6 Conclusions Data, patterns, method of derivations, and useful-ness are key ingredients in AM. In this paper, we formalize the current state of AM: Data are a table of symbols. The patterns are the formulas of input symbols that repeat. The method of derivations is the most conservative and reliable one, namely, mathematical deductions. The results are somewhat surprising 1. Patterns are properties of the isomorphic class, not an individual relation - This implies that the notion of patterns may not mature yet and explains why there are so many extracted association rules 2. Un-interpreted attributes \(features can be enumerated 3. Generalized associations can be found by solving integral linear inequalities. Unfortunately, the number is enormous. This signi?es the current notion of data and patterns \(implied by the algorithms 4. Real world modeling may be needed to create a much more meaningful notion of patterns. In the current state of AM, a pattern is simply a repeated data that may have no real world meaning. So we may need to introduce some semantics into the data model [12],[10],[11 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE References 1] R. Agrawal, T. Imielinski, and A. Swami  Mining Association Rules Between Sets of Items in Large Databases  in Proceeding of ACM-SIGMOD international Conference on Management of Data, pp. 207216, Washington, DC, June, 1993 


216, Washington, DC, June, 1993 2] Richard A. Brualdi, Introductory Combinatorics, Prentice Hall, 1992 3] A. Barr and E.A. Feigenbaum, The handbook of Arti?cial Intelligence, Willam Kaufmann 1981 4] Margaret H. Dunham, Data Mining Introduction and Advanced Topics Prentice Hall, 2003, ISBN 0-13088892-3 5] Fayad U. M., Piatetsky-Sjapiro, G. Smyth, P. \(1996 From Data Mining to Knowledge Discovery: An overview. In Fayard, Piatetsky-Sjapiro, Smyth, and Uthurusamy eds., Knowledge Discovery in Databases AAAI/MIT Press, 1996 6] H Gracia-Molina, J. Ullman. &amp; J. Windin, J, Database Systems The Complete Book, Prentice Hall, 2002 7] T. T. Lee  Algebraic Theory of Relational Databases  The Bell System Technical Journal Vol 62, No 10, December, 1983, pp.3159-3204 8] T. Y. Lin  Deductive Data Mining: Mathematical Foundation of Database Mining  in: the Proceedings of 9th International Conference, RSFDGrC 2003 Chongqing, China, May 2003, Lecture Notes on Arti?cial Intelligence LNAI 2639, Springer-Verlag, 403-405 9] T. Y. Lin  Attribute \(Feature  The Theory of Attributes from Data Mining Prospect  in: Proceeding of IEEE international Conference on Data Mining, Maebashi, Japan, Dec 9-12, 2002, pp. pp.282-289 10] T. Y. Lin  Data Mining and Machine Oriented Modeling: A Granular Computing Approach  Journal of Applied Intelligence, Kluwer, Vol. 13, No 2, September/October,2000, pp.113-124 11] T. Y. Lin, N. Zhong, J. Duong, S. Ohsuga  Frameworks for Mining Binary Relations in Data  In: Rough sets and Current Trends in Computing, Lecture Notes on Arti?cial Intelligence 1424, A. Skoworn and L Polkowski \(eds 12] E. Louie,T. Y. Lin  Semantics Oriented Association Rules  In: 2002 World Congress of Computational Intelligence, Honolulu, Hawaii, May 12-17, 2002, 956961 \(paper # 5702 13  The Power and Limit of Neural Networks  Proceedings of the 1996 EngineeringSystems Design and Analysis Conference, Montpellier, France, July 1-4, 1996 Vol. 7, 49-53 14] Morel, Jean-Michel and Sergio Solimini, Variational methods in image segmentation : with seven image processing experiments Boston : Birkhuser, 1995 15] H. Liu and H. Motoda  Feature Transformation and Subset Selection  IEEE Intelligent Systems, Vol. 13 No. 2, March/April, pp.26-28 \(1998 16] Z. Pawlak, Rough sets. Theoretical Aspects of Reasoning about Data, Kluwer Academic Publishers, 1991 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





