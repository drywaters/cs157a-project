Applications of an Web Information Mining Model to Data Mining and Information Retrieval Tasks  Alvaro R Pereira Jr Department of Computer Science Federal University of Minas Gerais Belo Horizonte Brazil alvaro@dcc.ufmg.br Ricardo Baeza-Yates Center for Web Research CS Dept University of Chile  Santiago Chile  ICREA Research Professor Tech Dept Pompeu Fabra Univ Barcelona Spain Abstract 
We have developed a model to mine information in applications involving graph analysis We demonstrate the model characteristics using a Web warehouse where nodes represent Web pages and edges represent hyperlinks In this paper we apply the model to data mining and information retrieval tasks Related to data mining we present views for clustering Web nodes and for nding frequent itemsets for association rule mining Related to information retrieval we present views for performing a simple query for clustering the query results and an attempt to improve the qual 
ity of the ranking The use of the model with these purposes demonstrates its modularity exibility applicability and broadness in graph problems 1 Introduction The Web is a collection of semistructured documents Most of them are HTML documents whose tags give us some meta information about pieces of the document A particular data warehouse whose data is the Web is known as a Web warehouse In this paper we present several applications of a model to mine information in a Web warehouse involving data mining tasks as clustering and association 
rules and information retrieval tasks as ranking and result clustering There are three distinct data types in the Web semistructured multimedia content hyperlink or just link structure and usage data in the form of Web logs W e model these three data types as objects that represent some view of the Web warehouse manipulated by a formal algebra with distinct operators 2 The Model The information mining model used is based on the object oriented paradigm Objects are instances of classes and incorporate sets of elements The classes 
Node and Relation are subclasses of the superclass Super Node  As instances the objects physical pages and physical links represent respectively all the Web pages and links among pages in a given web warehouse crawled The objects are modied by operations performed on their elements An object is always a set of elements A relation node represents an edge of a graph that may be directed In spite that most applications demand graphs 
such that each edge connects two nodes the model supports hypergraphs In this case other attrib utes apart from start and end are required to store the other nodes involved in each hyperedge In this paper we often refer to elements of the classes Node and Relation by just node\(s and relation\(s  respectively The operations are functions that modify the object characteristics generating a new object or returning a value We identify by 
primitive operations the operations that are important to the model and that cannot be implemented by using other existing operations The primitive operators are summarized in Table 1 A view is a query performed by a sequence of operators and objects where the operators are applied to input objects or to objects returned by other operators Dynamic operations are views with generic input objects and parameters normally useful in other views Objects returned by views can be materialized or not depending on time and space costs and on the application 
Materialized objects can be used in any other view as input objects Details of the model can be found in The operators are explained by using examples in the following sections Proceedings of the 16th International Workshop on Database and Expert Systems Applications \(DEXA05 1529-4188/05 $20.00  2005 IEEE 


Table 1 Summary of primitive operators  Operator Parameters Description Select value select only tuples that satisfy a condition top k return only a given number of tuples from the top ranking Project normal chosen attribute\(s are returned relation all the relations in the hypergraph are returned Merge union return all the tuples that occur in any object involved intersection return only the tuples that occur in both objects dierence return the difference of the tuples Join objects like operator Join in relational algebra bags convert a bag to a set adding a new attribute quantity  non directed eliminate the direction of the relations by eliminating one of the relations with values start and end inverted disjoin separate items that are arrays Every array element generates a new tuple Order increasing sort the tuples in increasing order decreasing sort the tuples in decreasing order Add xed a given node is the start or end node Relation cross a relation is inserted among every pair product of nodes same a relation is inserted among every pair attribute of nodes with the same attribute value for a given attribute The operator may be used with every parameter for classes node and relation Link co citation Add new relations according to one of Distance bibliographic the options A new attribute distance coupling is added to store the distance among distance k nodes It is recursive but it is also transitivity possible to perform partial link distance Connected connected identify the connected components Nodes strongly identify the strongly connected comp spanning identify the minimum spanning tree tree It works by deleting edges of the graph A new attribute subgraph is added Singular pagerank Return an object of the class node Value authority with one more attribute according to the Decomp hub Pagerank authority or hub measures Content relation compare a given textual attribute of the Similarity start and end nodes of every relation query calculate the similarity of a given query and a textual attribute Calculus if binary calculate the sum  dierence  multiplication or division  if unary a mathematical  statistical or user dened function is applied It is also used to count the number of tuples Some operations may be applied to non numerical attributes 3 Applications Related to Data Mining 3.1 Clustering The rst view Cluster Nodes  groups nodes according to the presence of a relation  a very simple but useful heuristic for Web graph applications non-labeled graph The application of Cluster Nodes to one object of the class relation results in an object belonging to the class node with the addition of the attribute subgraph  This new attribute stores the cluster number which each node belongs to Figure 1 shows the view Cluster Nodes as a dynamic operator for future use in other views It means that the input of the view is generic that is other objects belonging to the class relation in the place of for example only a physical pages object It also means that some internal attributes may be variables like the attribute quantity of the operator Select in Figure 1 In gures that represent views and dynamic operators we use boxes to represent objects and arrows to represent an operation The object linked from an arrow is an input for the current operation The object which an arrow links is the output of that operation The internal object labels represent the history of operators applied to objects in the view For example an object has a label O RN.CN if the operator Connected Nodes  CN  was applied to the input object of the view and later the operator Relation to Node  RN  was applied The label of an operation represents the operator used and its parameters An arrow with a dashed line do not represent an operation It means that the input object stores a constant value that is used as a parameter in an operator It occurs in Figures 2 3 and 6 Figure 1 The dynamic operator Cluster Nodes The dynamic operator works as follows Initially the operator Connected Nodes is applied to an object of the class relation The strongly connected components are identied by inserting a new attribute subgraph to store the subgraph number which the relation belongs to Next a dynamic operator Relation to Node is used to convert the object from the class relation to the class node The conversion consists of considering every node in the start and end nodes of the input object as a node in the output object The parameter Union means that the object O RN.CN contains all the nodes existing in the attributes start and end from the input object O CN  The parameter subgraph indicates that the at2 Proceedings of the 16th International Workshop on Database and Expert Systems Applications \(DEXA05 1529-4188/05 $20.00  2005 IEEE 


tribute subgraph is kept in the resulting operator O RN.CN  Details on the dynamic operator Relation to Node can be found in Next Join is used with the option bags  Thus object O Jo.RN.CN contains the same elements than object O RN.CN  but without replication of tuples and with a new attribute quantity  to store the number of identical tuples for every distinct tuple in the previous object O RN.CN  Select is used to consider only the clusters with a minimal number of nodes the constant user-dened quant  what might be important in Web warehouse At the end the nodes are ordered by the attribute subgraph  just to keep the elements of each cluster together Note that the object returned contains the list of node numbers and the subgraph number which the nodes belong to Most views that use this operator may need to merge this result with an object of the class node returning the names or other information of the clustered nodes It is possible to use the dynamic operator just presented to cluster nodes in labeled graphs The simplest way might be to perform the operator Select before the operator Cluster Nodes eliminating edges that do not reach a given threshold The second view is a more sophisticated solution for labeled graph partitioning Some algorithms for clustering on labeled graphs use the minimum spanning tree and thresholds to eliminate edges The mean a v erage of all the edges weights or functions involving this statistical measure are often used as threshold Figure 2 shows the view Improved Cluster     Figure 2 The view Improved Cluster The view Improved Cluster is explained in the following Initially the mean average for the attribute weight is calculated and stored as a constant value in the object O meanAverage  By the operator Connected Nodes only the edges belonging to the minimum spanning trees are maintained The object O CN has a new attribute subgraph  that identies the subgraph number which each edge belongs to since various minimum spanning trees can be generated In the next step Select is used to exclude edges that have the weight less than the mean average Now the minimal spanning trees are partitioned The operator Connected Nodes is applied again to rearrange the new subgraphs associating a new subgraph number to every subgraph Relation to Node returns the list of nodes and its subgraph number 3.2 Association Analysis Association analysis is the discovery of association rules showing attribute-value conditions that occur frequently together in a given set of data In Data mining a set of items is referred to as an itemset  If an itemset satises a minimum support 1  then it is a frequent itemset Association rules are mined from large databases by a two-step process nd all frequent itemsets according to a pre-determined minimum support count and generate strong association rules from frequent itemsets satisfying minimum condence The second step is the easiest of the two We present in this section a view based on the Apriori algorithm for mining frequent itemsets for boolean association rules Every interaction is a two-step process consisting of joining and pruning actions The list of candidate k itemsets itemsets with k elements is generated by joining the previous elements of  k  1 itemsets In the prune step the database is scanned to verify which candidate k itemset may be frequent The view for mining frequent itemsets is shown in Figure 3 The basic idea is that items are considered nodes of hypergraphs and itemsets are relations hyperedges Every new interaction uses a graph with edges connecting a greater number of nodes Thus if we are interested in the frequent 4 itemsets we recover the hyperedges that link four nodes The view Frequent Itemsets shown in Figure 3 is explained below Initially id and list attributes tags are projected The Join operator with the option disjoin  is used to separate the elements of the list where each item generates a tuple For example applying Join disjoin  to the tuple  1  a b c   results in the tuples  1 a    1 b  and  1 c   The attributes itemId and item  added by the operator Join are projected to the object O id  nameList  This object is required to recover the item name attribute item  since next objects of the view do not have this information The operator Add Relation is applied with the option same attribute  where every pair of nodes with the same id becomes a relation It works similarly to a cross-product of the object with itself except that Add Relation crosses only tuples with the same value in the attribute itemId  For example if we have the tuples  1 a    1 b    2 c    2 d  and  2 e   the application of the operator Add Relation with the option same attribute results in the non-directed relations a  b  c  d  c  e and d  e  The object O AR.Jo.P r 1 An itemset satises minimum support if the occurrence frequency of the itemset is greater than or equal to the product of the pre-determined minimum support and the total number of transactions in the database 3 Proceedings of the 16th International Workshop on Database and Expert Systems Applications \(DEXA05 1529-4188/05 $20.00  2005 IEEE 


 Figure 3 The view Frequent Itemsets contains for every list considered in the input node object object of the class node relations among every pair of items of every list The object O AR.Jo.P r contains the candidate 2 itemsets The relation attributes the list of nodes linked by hyperedges are projected In the rst interaction only two nodes are linked the graph is not an hypergraph yet Next duplicated edges are eliminated and counted by the operator Join The operator Join has a different function for the Apriori join step Join adds to the object O Jo.P r.AR.Jo.P r the attribute quantity  that is used to verify if the itemset reaches the minimal support min sup  tuplesN um  in the operator Select Operator Calculus is used to count the number of tuples tuplesN um  Operator Join is used to recover the names of the frequent 2 itemsets returned into the object Itemsets bolded in the gure that is the output of the view Frequent Itemsets  A new interaction begins by merging the current frequent itemsets with the object O AR.Jo.P r  just for recovering the list identiers In the rst interaction just presented the operator Add Relation had been applied outside the loop Since it is required for adding more nodes to each relation it must be applied now inside the loop 4 Applications Related to Information Retrieval We present in this section three examples of applications related to information retrieval a dynamic operator for performing a simple query and views for clustering the query results and an attempt to improve the quality of the ranking The two views use the dynamic operator Simple Query  shown in Figure 4 Figure 4 The dynamic operator Simple Query Initially in the right side of Figure 4 a query is performed using the similarity measure TF-IDF  term frequency and inverse document frequency In the left side the Pagerank is calculated by the operator Singular Value Decomposition Object O P hyLinks  with  Pagerank has a new attribute pagerank  By means of the operator Calculus the Pagerank value of top ranking documents are summed to the similarity calculated using TF-IDF The variable maxP ages is used twice In the rst time it is used to calculate the number of pages that are selected for summing the similarity value with the Pagerank value At the end the variable maxP ages is used to limit the number of pages returned 4.1 Clustering Query Results We present a view that aims to improve the presentation of the results of a search engine based on the idea of clustering the results returned It is an attempt to disambiguate the query by grouping the documents returned We hope that each cluster is related to the meaning of the ambiguous query when it is ambiguous Note that we are not debating if this is a good strategy to the disambiguation problem that is not important at this moment We are interested in showing the application of the model to an information retrieval task It is possible to change the view and obtain better results or yet to propose another view that implements another strategy for the same problem The view Cluster Results is shown in Figure 5 Initially the query is processed using the dynamic operator Simple Query The parameter maxP ages has the value 50  Add Relation with the option cross product inserts a relation among every document pair in the top fty documents returned in the query Now we have a graph where each node is connected to every other node The texts of every document pair are compared using the TF-IDF measure The 4 Proceedings of the 16th International Workshop on Database and Expert Systems Applications \(DEXA05 1529-4188/05 $20.00  2005 IEEE 


dynamic operator Improved Cluster is used to partition the graph returning the clusters  Figure 5 The view Cluster Results 4.2 Ranking Improvement Using Related Items Now we attempt to improve the ranking of a search engine by privileging documents that have other terms related to the query terms Again we are not debating if this is a good strategy for improving the ranking We are just presenting an alternative The view Change Ranking is shown in Figure 6 The rst part of the gure the top ve objects aims to identify the items that are related to the query term For simplication we are considering queries with only one term The dynamic operator Frequent Itemset is used and object O items has the list of terms related to the query terms In the other part of the gure que query is processed using the dynamic operator Simple Query The related items are compared to the top50 documents returned in the query using operator Content Similarity The similarities for the query and for the related items are summed The operator Order returns the new ranking Figure 6 The view Change Ranking 5 Conclusions This paper has presented succinctly a model to mine information in graph applications The focus of the paper has been the use of the model on applications related to data mining and information retrieval By dening some views we tried to address important tasks in these two areas as nding frequent itemsets and improving the quality of the ranking in a query The examples demonstrated some important characteristics of the model as modularity  since a view can be converted in a dynamic operator and used in other views exibility  since we can have distinct views to solve the same problem and we can adjust internal parameters for the views broadness  since many new views were created without changing the model and the operators and the applicability of the model for graph problems and problems that can be converted into graph problems Acknowledgments This work was supported by GERINDO Project grant MCT/CNPq/CT-INFO 552.087/02-5 CYTED VII.19 RIBIDI Project P01-029F of the Millennium Scientic Initiative Mideplan Chile and CNPq Grant 14.1636/2004-1   Alvaro R Pereira Jr References  R Agra w a l and S Srikant F ast algorithm for mining association rules In 20th International Conference on Very Large Data Bases VLDB94  pages 487499 Santiago Chile September 1994  R Baeza-Y ates A R Pereira-Jr  and N Zi viani WIM an information mining model for the web extended abstract In First DEXA International Workshop on Data Management in Global Data Repositories GRep05  Copenhagen Denmark August 2005 IEEE Computer Society Press To appear  T  H Cormen C E Leiserson and R L Ri v est Introduction to algorithms  MIT Press/McGraw-Hill San Francisco CA 1990  M Gaertler  Clustering with spectral methods Master s thesis Universitt Konstanz 2002  J Han and M Kamber  Data mining comcepts and techniques  Morgan Kaufmann United States of America 2001  G Salton and C Buckle y  T erm-weighting approaches in automatic text retrieval Information Processing and Management  24\(5 1988  J Sri v asta v a  R  Coole y  M Deshpande and P N T an W e b usage mining discovery and application of interesting patterns from web data ACM SIGKDD Explorations  1\(2 23 january 2000 5 Proceedings of the 16th International Workshop on Database and Expert Systems Applications \(DEXA05 1529-4188/05 $20.00  2005 IEEE 


9. else 10 Null 11. add into   12. endfor 13. add   into 14. endfor 4.2. Phase II: Generating the Quantitative Association Rules After generating the new database   describing the change of each attribute, we try to discovery the frequent itemsets. Much of the data in   are continuous but not discrete. Therefore, we cannot use ordinary method [3 such as Apriori algorithm, to generate frequent itemsets. We should use the algorithm mentioned in [14] to generate the rules. The following issues should be clari?ed before using the algorithm 1 ments of the weather need different partition rules. Following is the detail of the rule Temperature: The partition of temperature difference would be  \(-  , -20 0 0, 5], \(5, 10], \(10, 15], \(15 Humidity: The partition of humidity difference is the same as that of temperature Dew Point: Dew point is related to temperature. Therefore they are using the same partition Wind Speed: The partition of wind-speed difference is as follows:  \(-  , -40 0 0, 5], \(5, 10], \(10, 20], \(20, 40], \(40 Wind Direction: The partition of wind-direction is as follows:  \(0,6], \(6, 12], \(12, 18], \(18, 24], \(24, 30], \(30 36 Pressure: Its partition is:  \(-  , -0.8 0.4  0.2, 0.4], \(0.4, 0.6 0.6, 0.8], \(0.8 Visibility: Its partition is:  \(-  , -4 2, -1  1, 2], \(2, 3], \(3, 4], \(4 Precipitation:Use the same partition as that of pressure 2 


tion should be set in proportion to the appearance probability within a whole year 4.3. Phase III: Generating the Dynamic Interdimension Association Rules After Phase II, we discover some quantitative association rules. At this time we introduce our prede?ned predicates into these rules to generate the dynamic interdimension association rules. Since many of the items consisting of the rules are ranges, we can either still use this range or the mean value of the range to express those items. The rules generated include the associations of all of the weather condition, not only those severe conditions. The ?nal phase of this algorithm needs more interaction between the algorithm and the end-user so that the discovery may get a much better result 5. Experience with a real-life dataset We assessed the correctness and effectiveness of our algorithm by experimenting with a real-life dataset. The data were weather record data of Dallas City from April 2002 to June 2002 [1]. The data had ten attributes which include Proceedings of the 28th Annual International Computer Software and Applications Conference \(COMPSAC04 0730-3157/04 $20.00  2004 IEEE eight quantitative attributes and two categorical attributes There were 1092 records in the dataset 5.1. Experiment Design The datasets getting from Weather Underground and NCDC are raw data.The most distinct weather changes which happen in all spring and the early summer according to Dallas City. Therefore choosing this part of data can clearly re?ect the relation between the meteorological variables and the weather conditions. The ?rst step of our experiment is to rearrange the dataset. The ?rst phase of our algorithm is to build a new database re?ecting the changing tendency of the meteorological variables. After this phase determing the number of partitions for each quantitative attribute and how to partition should be accomplished. In the second phase of our algorithm, we use the algorithm in [14 to generate the frequent itemsets. The algorithm used to generate association rules is different to the traditional algorithm in [3]. In our algorithm we only generate rule whose right-hand-side only contains the DYSW attribute. The ?nal phase will use our prede?ned predicates to generate the dynamic interdimension association rules 5.2. Result Evaluation Evaluating the usefulness of the results from running our algorithm need some special review. However, some rules we get are pretty straightforward. The following rule is generated when we set minsup to 8% and minconf to 20 direction range \(RDIR, 12..18 DPTP, 0..5 


Thunder-Storm [ s = 8%, c = 20 Dallas City is located near Gulf of Mexico which is located in the southeast of the city. When the warm air?ow moves toward northwest, it is moisture-conditioned. After its landing, it will meet some cold air?ow existing in the land. If the warm air?ow is stronger than the cold one, it can still go north and push the cold one back. Along this way, there will be a plenty of precipitation. Now looks back to the geographical position of Dallas City. Its just on the way of the warm air?ow and the distance between the city and the coast is also not very far. That is why the warm air?ow can bring much precipitation to this city from southeast. When warm air?ow comes, the relative humility will increase which will make the dew point increase 6. Conclusions In this paper, we introduce the problem of mining dynamic interdimension association rules for local-scale severe weather prediction. We ?rst transform the original weather record database into a new database expressing the change tendencies of the measurements of the weather Then we use the algorithm for mining quantitative association rules to generate the result rules in quantitative format Finally, we introduce some predicates to generate the ?nal rules In this paper, we focus on the time-related measurements of weather. In the future, we will incorporate the spacerelated measurements for weather prediction. In addition we will improve our model so that it will ?t for a regional even global weather prediction References 1] Daily surface data. US and some non-US format Documentation, 2002 2] R. Agrawal, T. Imielinski, and A. N. Swami. Mining association rules between sets of items in large databases. In Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data, Washington, D.C., 1993 3] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. In Proc. 20th Int. Conf. Very LargeData Bases VLDB, pages 487499. Morgan Kaufmann, 1994 4] P. F. Baldi and S. Brunak. Bioinformatics: The machine learning approach. MIT Press, Cambridge London, 1998 5] C. Elkan. Shared challenges in data mining and computational biology. In Proceedings of the ACM SIGKDD Workshop on Data Mining in Bioinformatics, San Francisco, CA 2001 6] K. A. Emanuel. The role of water in atmospheric dynamics and climate. Meteorology at the Millennium, 2002 7] U. M. Fayyad, S. G. Djorgovski, and N. Weir. Automating the analysis and cataloging of sky surveys. Advances in knowledge discovery and data mining, pages 471493, 1996 


8] M. Gahegan. Data mining and knowledge discovery in the geographical domain 9] J. R. Holton, J. A. Curry, and J. A. Pyle. Encyclopedia of atmospheric sciences. Academic Press, Boston, 2002 10] Z. Kou, W. W. Cohen, and R. F. Murphy. Extracting information from text and images for location proteomics. In Proceedings of the 3nd ACM SIGKDD Workshop on Data Mining in Bioinformatics, Washington, DC, 2003 11] V. Kumar, M. Steinbach, P. Tan, S. Klooster, C. Potter, and A. Torregrosa. Mining scienti?c data: Discovery of patterns in the global climate system. In Proceedings of the Joint Statistical Meetings, Athens, GA, 2001 12] P. Lynch. Weather forecasting from woolly art to solid science. Meteorology at the Millennium, pages 106119, 2002 13] T. N. Palmer. Predicting uncertainty in numerical weather forecasts. Meteorology at the Millennium, pages 313, 2002 14] R. Srikant and R. Agrawal. Mining quantitative association rules in large relational tables. In Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data, pages 112, Montreal, Quebec, Canada, 1996 15] A. J. Stevermer. Recent advances and issues in meteorology Oryx Press, London, 2002 16] B. Zupan, E. T. Keravnou, and N. Lavrac. Intelligent Data Analysis in Medicine and Pharmacology. Kluwer Academic Publishers, 1997 Proceedings of the 28th Annual International Computer Software and Applications Conference \(COMPSAC04 0730-3157/04 $20.00  2004 IEEE 


variable names mapped by the where keyword to the relation in DataSchema and pv are variable names that appear in the StructureSchema At instantiation time pv is assigned values of the Structure component and dv is mapped to the relation appearing in Data component. The de?nition for the pattern well-formed formula is now straightforward De?nition 14 A pattern formula is of the form fp\(dv 2 where fp \(formula predicate variablesmapped by thewhere keyword to the relation appearing in Data component From the previous de?nitions the semantics of the where keyword become evident: we impose that the variables of the formula will take values from speci?c relations when the formula predicate is employed in queries Example 2 Let us consider the following formulas 1. f\(x x 2. f\(g\(x x In the ?rst formula variable x is mapped to R using the where keyword, thus the formula is well formed. Keep in mind that the formula predicate by itself is just the part f\(x is not well-formed since y is not mapped via where to any relation, or otherwise range restricted 5. Querying the Pattern Warehouse We de?ne queries to be posed over the pattern warehouse and not individually over its data- or patternbase components. Through this approach, we are able to sustain queries traversing from the pattern to the data space and vice-versa. At the same time, the consistency of the results is guaranteed by the pattern-data mapping De?nition 15 Let PW the set of all possible Pattern Warehouses. A query is a function with signature PW ? PW. Given a query q and a pattern warehouse pw = \(DB,PB D?B, P?B q\(pw DB?, PB   P?B? = ?[D?1, ..., D?m], [P?C1:PT1]?. We assume that tr, tp\(tr ? R1 ? tp ? PC1 tr, tp Note that, similarly to the relational case, the result of a query is always a pattern warehouse containing just one relation and one pattern class. It is also important to point out that, in practice, even if a query always involve both the data and pattern space, operations over patterns are executed in isolation, locally at the PBMS. The reference to the underlying data is activated only on-demand \(whenever the user speci?cally requests so stored intermediate mappings or the formula approximation 5.1. Query operators In this section we introduce query operators that allow basic queries over the the PW . Assuming that DB denotes the set of all possible database instances and PB the set of all possible pattern bases, we consider the following groups of operators  Database operators: they can be applied locally to the DBMS. op : DB ? DB. We denote the set of database operators with OD  Pattern base operators: they can be applied locally to the PBMS. op : PB ? PB. We denote the set of database operators with OP  Cross-over database operators: they involve evaluation on both the DBMS and the PBMS, the result is a database. op : DB  PB ? DB. We denote the set of database operators with OCD  Cross-over pattern base operators: they involve evaluation on both the DBMS and the PBMS, the 


evaluation on both the DBMS and the PBMS, the result is a pattern base. op : DB  PB ? PB. We denote the set of database operators with OCP In the following, we present examples of the last three classes of operators \(database operators coincide with usual relational operators operators, we introduce some examples of predicates de?ned over patterns Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE 5.1.1. Pattern predicates We identify two main classes of atomic predicates: predicates over patterns and predicates over pattern components. From those atomic predicates we can then construct complex predicates. In the following, we denote pattern components by using the dot notation. For example, the measure component of a pattern p is denoted by p.Measure Predicates over pattern components. They check properties of speci?c pattern components. Let p1 and p2 be two patterns, possibly selected by some queries. The general form of a predicate over pattern components is t1?t2, where t1 and t2 are path expressions that must de?ne components of patterns p1 and p2, of compatible type and ? must be an operator, de?ned for the type of t1 and t2. For example, if t1 and t2 are integer expressions, then ? can be a disequality operator e.g. one of &lt;,&gt cases  If t1 and t2 are pattern data for patterns p1 and p2, then ? ? {=,?}. t1 = t2 is true if and only if x x ?? p1 ? x ?? p2 and t1 ? t2 is true if and only if ?x x ?? p1 ? x ?? p2  If t1 and t2 are pattern formulas for patterns p1 a n d  p 2   t h e n             t 1    t 2  i s  t r u e  i f  a n d o n l y  i f  t 1    t 2  a n d  t 1    t 2  i s  t r u e  i f  a n d  o n l y  i f  t 1 logically implies t2 Predicates over patterns. We consider the following set of predicates  Identity if they have the same PID, i.e. p1.P ID = p2.P ID  Shallow equality \(=s are shallow equal if their corresponding components \(except for the PID component i.e. p1.Structure = p2.Structure, p1.Source p2.Source, p1.Measure = p2.Measure, and p1.formula = p2.formula. Note that, to check the equality for each component pair, the basic equality operator for the speci?c component type is used  Deep equality \(=d deep equal if their corresponding data are identical, i.e., ?x x ?? p1 ? x ?? p2   S u b s u m p t i o n       A  p a t t e r n  p 1  s u b s u m e s  a  p a t t e r n  p 2   p 1    p 2   i f  t h e y  h a v e  t h e  s a m e  s t r u c ture but p2 represents a smaller set of raw data i.e. p1.Structure = p2.Structure, p1.Source p 2  S o u r c e  a n d  p 1  f o r m u l a    p 2  f o r m u l a  Complex predicates. They are de?ned by applying usual logical connectives to atomic predicates. Thus, if F1 and F2 are predicates, then F1 ? F2,F1 ? F2  F1 are predicates. We make a closed world assumption, thus the calculation of  F is always ?nite 5.1.2. Pattern base operators OP In this subsection, we introduce several operators de?ned over patterns. Some of them, like set-based operators, renaming and selection are quite close to their relational counterparts; nevertheless, some others, like join and projection have signi?cant di?erences Set-based operators. Since classes are sets, usual operators such as union, di?erence and intersection are de 


tors such as union, di?erence and intersection are de?ned for pairs of classes of the same pattern type Renaming. Similarly to the relational context, we consider a renaming operator ? that takes a class and a renaming function and changes the names of the pattern attributes according to the speci?ed function Projection. The projection operator allows one to reduce the structure and the measures of the input patterns by projecting out some components. The new expression is obtained by projecting the formula de?ning the expression over the remaining attributes [12 Note that no projection is de?ned over the data source since in this case the structure and the measures would have to be recomputed Let c be a class of pattern type pt. Let ls be a non empty list of attributes appearing in pt.Structure and lm a list of attributes appearing in pt.Measure. Then the projection operator is de?ned as follows ls,lm c id s m f p ? c, p = \(pid, s, d,m, f In the previous de?nition, id ing new pids for patterns, ?mlm\(m projection of the measure component and ?sls\(s ned as follows: \(i s usual relational projection; \(ii sls\(s and removing the rest from set elements. The last component ?ls?lm\(f computed in certain cases, when the theory over which the formula is constructed admits projection. This happens for example for the polynomial constraint theory 12 Selection. The selection operator allows one to select the patterns belonging to one class that satisfy a certain predicate, involving any possible pattern component, chosen among the ones presented in Section 5.1.1 Let c be a class of pattern type pt. Let pr be a predicate. Then, the selection operator is de?ned as follows pr\(c p Join. The join operation provides a way to combine patterns belonging to two di?erent classes according to a join predicate and a composition function speci?ed by the user Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE Let c1 and c2 be two classes over two pattern types pt1 and pt2. A join predicate F is any predicate de?ned over a component of patterns in c1 and a component of patterns in c2. A composition function c pattern types pt1 and pt2 is a 4-tuple of functions c cStructureSchema, cDataSchema, cMeasureSchema, cFormula one for each pattern component. For example, function cStructureSchema takes as input two structure values of the right type and returns a new structure value, for a possible new pattern type, generated by the join. Functions for the other pattern components are similarly de?ned. Given two patterns p1 = \(pid1, s1, d1,m1, f1 p2 = \(pid2, s2, d2,m2, f2 p1, p2 ned as the pattern p with the following components Structure : cStructureSchema\(s1, s2 Data : cDataSchema\(d1, d2 Measure : cMeasureSchema\(m1,m2 Formula : cformula\(f1, f2 The join of c1 and c2 with respect to the join predicate F and the composition function c, denoted by c 1   F  c  c 2   i s  n o w  d e  n e d  a s  f o l l o w s    F  c  c 2     c  p 1   p 2   p 1    c 1  p 2    c 2  F   p 1   p 2     t r u e   5.1.3. Cross-over database operators OCD Drill-Through. The drill-through operator allows one to 


Drill-Through. The drill-through operator allows one to navigate from the pattern layer to the raw data layer Thus it takes as input a pattern class and it returns a raw data set. More formally, let c be a class of pattern type pt and let d be an instance of the data schema ds of pt. Then, the drill-through operator is denoted by c c Data-covering. Given a pattern p and a dataset D sometimes it is important to determine whether the pattern represents it or not. In other words, we wish to determine the subset S of D represented by p \(p can also be selected by some query the formula as a query on the dataset. Let p be a pattern, possibly selected by using query language operators, and D a dataset with schema \(a1, ..., an ible with the source schema of p. The data-covering operator, denoted by ?d\(p,D responding to all tuples in D represented by p. More formally d\(p,D t.a1, ..., t.an In the previous expression, t.ai denotes a speci?c component of tuple t belonging to D and p.formula\(t.a1, ..., t.an instantiated by replacing each variable corresponding to a pattern data component with values of the considered tuple t Note that, since the drill-though operator uses the intermediate mapping and the data covering operator uses the formula, the covering ?\(p,D D = ?\(p not be equal to D. This is due to the approximating nature of the pattern formula 5.1.4. Cross-over pattern base operators OCP Pattern-covering. Sometimes it can be useful to have an operator that, given a class of patterns and a dataset, returns all patterns in the class representing that dataset \(a sort of inverse data-covering operation Let c be a pattern class and D a dataset with schema a1, ..., an pattern type. The pattern-covering operator, denoted as ?p\(c,D all patterns in c representing D. More formally p\(c,D t.a1, ..., t.an true Note that: ?p\(c,D p,D 6. Related Work Although signi?cant e?ort has been invested in extending database models to deal with patterns, no coherent approach has been proposed and convincingly implemented for a generic model There exist several standardization e?orts for modeling patterns, like the Predictive Model Markup Language \(PMML  eling approach, the ISO SQL/MM standard [2], which is SQL-based, and the Common Warehouse Model CWM  ing e?ort. Also, the Java Data Mining API \(JDMAPI 3] addresses the need for a language-based management of patterns. Although these approaches try to represent a wide range of data mining result, the theoretical background of these frameworks is not clear. Most importantly, though, they do not provide a generic model capable of handling arbitrary cases of pattern types; on the contrary only a given list of prede?ned pattern types is supported To our knowledge, research has not dealt with the issue of pattern management per se, but, at best, with peripheral proximate problems. For example, the paper by Ganti et. al. [9] deals with the measurement 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


