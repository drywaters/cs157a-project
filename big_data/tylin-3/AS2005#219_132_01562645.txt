From Intent Reducts for Attribute Implications to Approximate Intent Reducts for Association Rules Zhipeng Xie Department of Computing and Information Technology Fudan University Shanghai 200433 China xiezp@fudan.edu.cn Zongtian Liu School of Computer Engineering and Science Shanghai University Shanghai 200072 China ztliu@mail.shu.edu.cn Abstract Automatic extraction of data dependencies or regularities from historical data is of great importance in practice Concept lattice could serve as a common framework for such tasks To deal with two typical kinds of data dependencies attribute implications and association rules this paper presents the de\223nitions of intent reducts and approximate intent reducts as the theoretical foundation for extracting these two kinds of knowledge Algorithms for calculating them are developed We also given out the methods for extracting attribute implications and association rules The results provide a better understanding of attribute implications association rules and concept lattices 1 Introduction Automatic extraction of data dependencies or regularities from historical data is of great importance in many practical domains and thus has been addressed in a number of 223elds including databases machine learning data mining and rough sets In databases the most studied dependencies are functional dependencies in machine learning they present themselves as decision trees decision lists and so on;in data ing association rules have iled for more than one decade while in rough sets dependencies take the forms of reducts and decision rules Formal concept analysis 5  p ro vi des a uni 223 e d framework for dependency analysis through a fundamental data structure\205concept lattice Concept lattice is constructed upon a binary relation between objects and attributes Such a binary relation is of highest abstraction which builds a bridge towards various data in many different domain For example to investigate the relation between procedures and global variables in legacy code a binary relation could be built up depending on whether one procedure makes access to one global variable 3 De\223nition 1 A formal context K   G M I  consists of two sets G and M andarelation I between G and M The elements of G are called the objects and the elements of M are called the attributes of the context In order to express that an object g is in a relation with an attribute m we write gIm or  g m  001 I and read it as 215the object g has the attribute m 216 De\223nition 2 Fo r a s e t A 002 G of objects we de\223ne A 001   m 001 M  gIm for all g 001 A  the set of attributes common to the objects in A  Correspondingly r a set B of attributes we de\223ne B 001   g 001 G  gIm for all m 001 B  the set of objects which have all attributes in B  In this paper we shall call B 001 the support set of B in K  with another denotation Support  B  B 001  De\223nition 3 A formal concept of the context  G M I  is a pair  A B  with A 002 G  B 002 M  A 001  B and B 001  A  We ca ll A the extent and B the intent of the concept  A B   B  G M I  denotes the set of all concepts of the context  G M I   Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


Assume B is an intent n K  augmenting B with any other attribute will cause the shrink of corresponding support set t it formally if B  B 001\001 and B 003 B 1 then B 001 1 003 B 001  Example 1 The table below illustrates a simple context  G M I  where G   1234567  and M   abcdef g   Due to the fact that  ab  001   235  and  235  001   abceg   it follows that  ab  004   ab  001\001  and hence  ab  is not an intent On the contrary subset  ac  is an intent for  ac  001   1235  and  1235  001   ac   a b c d e f g 1 327 327 2 327 327 327 327 327 3 327 327 327 327 327 4 327 327 5 327 327 327 327 327 6 327 327 327 327 327 7 327 327 De\223nition 4 If  A 1 B 1  and  A 2 B 2  are concepts f a context  A 1 B 1  is called a subconcept of  A 2 B 2  provided that A 1 002 A 2 which is equivalent to B 2 002 B 1  In this case  A 2 B 2  is a superconcept of  A 1 B 1   and we write  A 1 B 1  005  A 2 B 2   The relation 005 is called the hierarchical order or simply order  of the concepts e setofallconceptsof  G M I  together with the order relation  B  G M I   005   is called the concept lattice of the context  G M I   If  A 2 B 2    A 1 B 1  and there does not exist any other concept  A 3 B 3  such that  A 2 B 2    A 3 B 3    A 1 B 1  then  A 2 B 2  is called a parent concept of  A 1 B 1   while  A 1 B 1  is a child concept of  A 2 B 2   Once concept lattice has been c onstructed various kinds of knowledge can be drawn out from it The paper will explore two such kinds of knowledge one is attribute implications while the other association rules The whole paper is organized as follows Next section is devoted to the basics of these two kinds of knowledge Section 3 is devoted to intent reducts for attribute implications while Section 4 devoted to approximate intent reducts for association ules Algorithms for computing approximate intent reducts are developed and methods for tracting attribute implications and association rules are given out 2 Attribute Implications and Association Rules De\223nition 5 Attribute Implications An attribute implication 215 P 006 Q 216 for a context K  G M I  consists of two subsets  P and Q  of the attribute set M where P is called the premise and Q the conclusion 2 When the sets are small we shall omit the brackets i.e we shall write P 006 m instead of P 006 m  Further,we shall write ab instead of  a b   With its simple yntactic form de\223ned above let us now examine when it holds in a context De\223nition 6 respects and holds A subset T 002 M respects an implication P 006 Q if P 002 T or Q 002 T  T respects a set L of implications if T respects every single implication in L  P 006 Q holds in a set  T 1 T 2   of subsets if each of the subsets T i respects the implication P 006 Q  P 006 Q holds in a context  G M I  if it holds in the system of object intents In this case we also say that P 006 Q is an implication of the context  G M I  or equivalently that within the context  G M I   P is a premise of Q  In context  G M I   an object g is represented by a subset of M i.e g 001 002 M  Therefore we shall not distinguish between objects and subs ets of attributes below and use them terchangeably Example 2 In Example 1,Object 6 respects the implication 215 bc 006 d 216 because  d 002 6 001   bcdef   Object 1 also respects this implication but because  bc  002 1 001   ac  On the contrary object 2 does not respect it for  d  002 2 001   abceg  and  bc 002 2 001   abceg   Therefore the implication 215 bc 006 d 216 does not hold in the formal context Two exemplar implications holding in this context are 215 abc 006 eg 216 and 215 a 006 c 216 because all the objects in the context respect them Theorem 1 An implication P 006 Q holds in  G M I  if and only if Q 002 P 001\001  Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


Proof 1 Assume P 006 Q holds in  G M I   P 001   g 001 G  P 002 g 001  denotes the objects with P  and all these objects must have attributes Q  It follows directly that Q 002 P 001\001  2 Assume that Q 002 P 001\001  Q 001 007 P 001 follows directly which means that all the objects with P must be also with Q  For a given context the number of implications holding in it is normally very huge even for small or moderatelysized contexts Luckily enough many of them could be deduced from much smaller sets of rules in the light of some inference mechanism One such inference system of great popularity is the so-called Armstrong axioms If you are interested please refer to some classical books on database theory such as 4 for det ai l e d i nformat i on What attribute implications represent are deterministic However noises and uncertainties are prevailing in realistic domains It is a must to introduce possibility and probability into knowledge In addition to make the discovered knowledge statistically signi\223cant it is also prerequisite that the knowledge should be supported by enough instances or objects Therewithal association rules emer ged De\223nition 7 Association Rules With respect to the support threshold 001 and con\223dence reshold 002 215 P 006 Q 216is an association rule holding in context K  G M I  ifit satis\223es   P b Q  001 t G 327 001 and   P 002 Q  001   P 001  t 002  Example 3 Follow the context in Example 1 once time again and me that 001 0  2 and 002 0  7  Association rule 215 a 006 bce 216 holds because  abce  001    235  3 t  G 327 001 1  4 and  abce  001   a  001   3 4 t 002 0  7 216 f 006 d 216 does not hold because  df  001    6  1  1  4 215 e 006 a 216 does not hold because  ae  001   e  001   3 5 002 0  7  3 Intent Reducts A formal context is normally generated in some speci\223c application domain representing the accumulation of historical empirical data Those data are doomed to respecting the attribute implications existing in that domain Extracting attribute implications from concept lattice is one kind of re-engineering which recovers the underlying set of atibute implications as a process of inductive learning Holding in a given context the rule of form 215 P 006 Q 216 requires that an object with attribute set P should also have the attribute set Q  Therefore the context satis\223es P 001   P b Q  001  3.1 What are intent reducts For any concept  A B   its intent B is maximized with respect to its extent A  Adding any other attribute into B will make the corresponding support set smaller On the contrary Removing an attribute from B will not necessarily lead to the expansion of the corresponding support set Motivated by this fact the de\223nition of intent reducts was presented 6 De\223nition 8 intent reducts An attribute set R is an intent reduct of concept c  A B  if R satis\223es 1 R 001  B 001  A  and 2 n R 1 003 R  R 001 1 013 R 001  B 001  A  In the above de\223nition the condition 1 is thought of as the invariability of support t which means that the concept and ll its intent reducts have the same support t The condition 2 is called the minimality of intent reducts which means that the removal of any attribute from an intent reduct will cause the expansion of the corresponding support t The family of all the intent reducts f concept c is denoted as IR  c   Property 1 If c  A B  is a concept and P 001 007 B 001 then it holds that P 002 B  Proof From P 001 007 B 001  A  it follows that P 001\001 002 B 001\001  A 001  B  Therefore P 002 P 001\001 002 B  Property 2 For any concept c  A B   provided that P 001  B 001  there must be some intent reduct R of c ch that R 002 P  Proof omitted.\(If there is an attribute m 001 P such that  P 212 m   001  P 001  B 001  then this attribute can be removed from P  repeat this process until we can not 223nd such an attribute If P does not contain such an attribute P will become an intent reduct Theorem 2 For any concept c  A B  and attribute subset P 002 B if P f  B 212 B p  004  r holds for any parent concept c p  A p B p  of c then P 001  B 001  A must hold Proof Because P 002 B  it is evident that P 001 007 B 001  A  For the pair  P 001 P 001\001  is complete it is a concept in concept lattice From the fact that P f  B 212 B p  004  r holds for any parent concept c p  A p B p  of c  it follows directly that P 002 B p  Thus P 001 013 B 001  A Otherwise there must be some parent concept c p  A p B p  of c such that P 002 B p  Therefore P 001  B 001  A  Theorem 3 For any concept c  A B  if P 001  B 001  A  then we have P f  B 212 B p  004  r for any parent concept c p  A p B p  of c  Proof Assume that there exists some parent concept c p   A p B p  that satis\223es P f  B 212 B p  r  Following from property 1 we have P 002 B  and thus P 002 B p  Therefore P 001 007 B 001 p  A p 013 A  which contradicts the assumption of P 001  A  So the assumption is wrong and the theorem gets proved Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


According to the theorem 2 and theorem 3 for any given concept  A B   the necessary and suf\223cient conditions for an attribute subset P 002 B to satisfy P 001  B 001 is speci\223ed as follows Theorem 4 For any concept c  A B   an attribute subset P 002 B satis\223es P 001  B 001  A if and only if P f  B 212 B p  004  r holds for each parent concept c p   A p B p  of c  To go one step further provided that P 001  B 001  P is an intent reduct of c  A B  if and only if for any P 1 003 P  there does exist some parent concept c p  A p B p  of c such that P 1 f  B 212 B p  r  In addition it is evident that removing any attribute from an intent reduct will lead to the expanding of the corresponding support set As we discussed before the removal of an attribute from a concept will not necessarily lead to the expansion of the corresponding support set Next we shall put forward the notation of intent core The intent core of an concept c consists of all the indispensable attributes in c Once any such attribute was removed the corresponding support t would be expanded De\223nition 9 intent core For any given concept c   A B   attribute m 001 B is indispensable in c if  B 212 m  001 013 A holds The set of all the indispensable attributes in c is called the intent core of c  denoted as core  c   A concept c may have more than one intent reduct but I must have exactly one intent core Further this intent core is included in each intent reduct The next property describes the relationship between intent reduct and intent core Property 3 For any given concept c  A B   the following properties hold 1 core  c  is included in each intent reduct of c  that is n R 001IR  c   core  c  002 R   2 core  c  001 IR  c   3 core  c   m  there exists some parent concept c p   A p B p  of c ch that B p b m   B   Example 4 Consider concept 8 in the example 1 with intent abceg and extent 235  Attribute subset abc satis\223es  abg  001  235   abceg  001  but it does not be an intent reduct of this concept This is because its true subset ag satis\223es  ag  001  235 as well Note ag is an intent reduct of concept 8 you could verify it yourself On the other hand with the help of theorem 4 we can judge whether the true subset abg of the intent reduct of concept 8 satis\223es the invariability of the support set Concept 8 has three parent concepts with bce  ac and cg as their intents respectively Due to the facts that abg f  abceg 212 bce  abg f ag  a 004  r  that abg f  abceg 212 ac  bg 004  r  and that abg f  abceg 212 cg  ab 004  r wecan draw the n that abg satis\223es  abg  001  abceg  001  From property 3 it can be concluded that the intent core of concept 8 is empty that is to say no one attribute in the intent is indispensable However the intent core of concept 4 is  c   3.2 Minimal Covers of a Family of Sets The previous section de\223ned what are intent reducts and described some properties of them This section is devoted to computing intent reducts based on the minimal cover of a given family set De\223nition 10 minimal cover For a given family F   F 1 F 2 F n  set F is called a minimal cover of F if it satis\223es two conditions 1 n F i 001F  F f F i 004  r   and 2 n S 003 F  016 F i 001F  S f F i  r   The family of all the minimal covers for F is denoted as MC  F   F  F is a minimal cover of F  Example 5 Let F be a family of three sets F   F 1 F 2 F 3  where F 1   a b   F 2   b c   F 3   a b e   Although the set  b c  has common elements with all the three sets in F  it is not a minimal cover because its true subset  b  intersects with each F i  i 1  2  3 as well The family F has exactly two minimal covers  b  and  a c   Theorem 5 Attribute subset P 002 B is an intent reduct of the concept c  A B  if and only if P is a minimal over of the family  B 212 B p   A p B p  is a parent concept of c   Proof It can be easily proved with the help of De\223nition 8 Theorem 4 and De\223nition 10 According to the theorem above the problem of computing all the intent reducts of a given concept has been transformed into the problem of computing all the minimal covers of a speci\223c family To solve the later problem e de\223nition of minimized family is introduced as follows De\223nition 11 minimal set d minimal family Given a family D set D 001D is called a minimal set if there does not exist any other t D 1 001D ch that D 1 003 D The family of all the minimal sets is called the minimal family denoted as D minimal   D 001D|\n D 1 001D  D 1 003 D    In addition we could prove the next two theorems which would serve as the foundation for calculating the family of approximate intent reducts in this paper Theorem 6 If the family F consists of exactly one set F 1  that is F   F 1   then we have MC  F   f  f 001 F 1   Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


Proof It is evident Theorem 7 Assume that Q  MC  F  is the family of minimal covers for a given family F Ifaset F new is newly added to F  and let D   Q b f  Q 001Q f 001 F new   then we have MC  F\b F new   D minimal  Proof 1 Firstly we prove that each t Q 1 001D minimal 002 D has non-empty intersection with each set in F\b F new   Assume that Q 1 is not a minimal cover of F\b F new  that is to say here exist some Q 2 003 Q 1 which intersects with all the sets in F\b F new  That Q 2 intersects with F new means that there exists some element f 1 001 F new f Q 2  Because Q 2 intersects with all the sets in F theremustexist Q 3 002 Q 2 which is a minimal cover of F  Q 3 001Q  MC  F   Therefore Q 3 b f 1 001D  while Q 1 001D minimal and Q 3 b f 1 002 Q 2 b f 1   Q 2 003 Q 1 Thatistosay Q 1 is a minimal set of family D  while there exists in D another set Q 3 b f 1  that is a true subset of Q 1  which contradicts with the de\223nition of minimal t De\223nition11 As a result Q 1 is a minimal cover of F\b F new   2 Due to the fact that each set Q 1 001MC  F\b F new   intersects with each set in F\b F new  theremustexist some Q 2 001Q  MC  F  such that Q 2 002 Q 1 andthere also exists some element f 1 001 Q 1 f F new  Because Q 2 b  f 1  intersects with each set in F\b F new  and Q 1 is a minimal cover of F\b F new   it follows that Q 2 b f 1   Q 1  It holds that Q 1 001D   Q b f  Q 001Q f 001 F new   While each set in D intersects with each set in F\b F new   From that Q 1 is a minimal cover we get to know that Q 1 must be a minimal set in D thatistosay Q 1 001D minimal  Example 6 In the example 5 MC  F   b    a c   Suppose F new   c e   we have MC  F\b F new   b c    b e    a c    a c e  minimal   b c    b e    a c  3.3 Algorithms r calculating e family of minimal covers To compute the family of minimal covers the simplest possible way is a brute-force algorithm which consists of two phases For a given family F   F 1 F 2 F n  to calculate its family of minimal covers as the 223rst step we must 223nd out all the subsets s atisfying the condition 1 in De\223nition 10 and then store these subsets into the variable F COV  After that for each set F 001F COV  if there does not exist any other t H 001F COV such that H 003 F then F is a minimal set in F COV  The second step is to locate all the minimal sets and they will constitute the family of all the minimal covers for F  Function MC F  brute-force version Input a family of sets F   F 1 F n  Output all the minimal covers of F begin F COV  r  for each set F 002 F 1 b F 2 b\267\267\267\b F n do b_ok:=true for each set F i 001F do if F f F i  r then b_ok=false break endif endfor if b_ok THEN F COV  F COV b F   endif endfor return minimal F COV  end Function minimal F COV  begin result r  for each F 001F do if 004 016 F i 001F such that F i 003 F then result:=result b F   endif endfor return result end Based on theorem 6 and theorem 7 we can work out the incremental algorithm for calculating the family of all the minimal covers with the following pseudo-code Function MC F  incremental version Input a family of sets F   F 1 F n  Output all the minimal covers of F begin MIN_COV r  for each F i 001F do MIN_COV:=MC_INC\(MIN_COV F i  endfor return MIN_COV end Function MC_INC\(MIN_COV F i  begin COV r  for each R 001 MIN_COV do temp R f F i  if temp 004  r then COV:=COV b R   else for each r 001 R do Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


COV:=COV b R b r   endfor endif endfor return minimal\(COV end On the basis of the algorithm for computing the family of minimal covers it is a simple task to calculate the family of intent reducts of a given concept c  which can be ful\223lled by the following several lines of codes begin F  r  for each parent  A p B p  of concept c do F  F\b B 212 B p   endfor return MC F  end 3.4 Extracting Attribute Implications Based on Intent Reducts De\223nition 12 true intent reduct An intent reduct R of the given concept c  A B  is called a true intent reduct if R 003 B If c has one true intent reduct then all its other intent reducts are true Concept  P 001 P 001\001  is called the host concept of the attribute implication 215 P 006 Q 216 If this implication holds in the context that is P 001  P b Q  001  it can be inferred that  P 001 P 001\001  P b Q  001   P b Q  001\001   All the holding implications could be partitioned according to their host concepts Thus each concept c corresponds to a set of implications rules  c   R 002 Intent  c  212 R  R is a true tent reduct of concept c   Such an implication set is non-redundant because the premise of any implication in the set is not a subset of the premise of any other implication in the set Let rules  K  002  rules  c   c is a concept in context K   we could also prove that rules  K  is complete with respect to the set of all the implications holding in K This is because any holding implication can be inferred from this implication set by applying Armstrong Axioms 4 However rules  K  is usually redundant To obtain the complete and non-redundant set of implications one possible way is to apply the redundant functional dependency elimination algorithms in 4 t o rules  K   4 Approximate Intent Reducts 4.1 what are approximate intent reducts De\223nition 13 Approximate Intent Reduct For a given concept c  A B  and a real number 002  0 002 005 1  attribute subset R 002 B is called a 002 approximate intent reduct of c if 1  B 001   R 001    A   R 001  t 002  and 2 n R 1 003 R  B 001   R 001 1    A   R 001 1  002  e family of all the 002 approximate intent reducts of concept c is denoted as IR 001  c   Approximate intent reduct is one kind of generalization to intent reduct and tent reduct can be thought of as a special case of approximate int ent reduct Put another way intent reduct corresponds to the 002 approximate intent reduct with 002 1  IR  c  IR 1  c   De\223nition 14  002 cap For a given concept c  A B  and 0 002 005 1  another concept c 1  A 1 B 1  is called a 002 cap of c ifitsatis\223es 1 c 1 is a superconcept of c  that is A 1 007 A  2  A   A 1  002  The set of all the 002 caps of c is denoted as CAP  c 002   According to the above de\223nition it is evident that when 002 1  c 1 is a 002 cap of c if and only if c 1 c  Theorem 8 For any given concept c  A B  and 0  002 005 1  attribute subset P 002 B satis\223es  B 001   P 001  t 002 if and only if  B 212 B 1  f P 004  r holds r each c 1  A 1 B 1  001 CAP  c 002   Proof From P 002 B  it follows that c 2  P 001 P 001\001  is a superconcept of c  1 Firstly Let us prove that if  A   P 001  t 002 then  B 212 B 1  f P 004  r holds for any c 1  A 1 B 1  001 CAP  c 002   Assume that there exists some c i  A i B i  001 CAP  c 002  ng  B 212 B i  f P  r  Due to the fact that both P and B i are subsets of B wehave P 002 B i and then P 001 007 B 001 i  A i  According to  A   A i  002 wehave  A   P 001  002  which is a contradiction As a result the assumption is wrong 2 Next we proceed to prove that if  B 212 B 1  f P 004  r holds for each c 1  A 1 B 1  001 CAP  c 002  then  A   P 001  t 002  For each 002 cap c 1  A 1 B 1  of c  c 2 is de\223nitely not its superconcept that is c t c 1  because P f  B 212 B 1  004  r  It means that concept c 2 itself is not a 002 cap of c otherwise P f  B 212 P 001\001  r  Thus  A   P 001  t 002 comes out as a result according to De\223nition 14 Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


By the theorem above the problem of computing the family of approximate intent reducts can be transformed into the problem of computing the family of minimal covers of a corresponding family Theorem 9 For each concept c  A B  and a subset P 002 B  P is a 002 approximate intent reduct of c if and only if P is a minimal over of the family  B 212 B 1   A 1 B 1  001 CAP  c 002    Proof Omitted Thus the algorithm for calculating all the 002 approximate intent reducts could be easily designed with the help of the function MC_INC c  in section 3 Input a concept c  A B  Output IR 001  c  begin IR 001  r  for each  A i B i  001 CAP  c 002  do F i  B 212 B i  IR 001 MC_INC IR 001  F i  endfor return IR 001  end Next subsection will give out several theorems which could be applied to make improvements on this algorithm 4.2 Improvements Theorem 10 For any given family F  F is a minimal cover of F if and only if F is a minimal over of the minimal family F minimal of F  Proof Let D  F\212F minimal Forany D j 001D there must exist some F i 001F minimal that satis\223es F i 003 D j  1 Firstly let us prove that if F is a minimal cover of F  then F is also a minimal cover of F minimal  Evidently each set in F minimal has non-empty intersection with F for F minimal 002F  Assume that there exists some F s 003 F such that F s intersects with every set in F minimal  It follows directly that for each D 001D there exist F i 001F minimal such that F i 003 D Since F s intersects with F i  F s also intersects with D  Therefore F s intersects with all the sets in F  which contradicts with the fact that F is a minimal cover of F  2 Next we could prove that if F is a minimal cover of F minimal then F is also a minimal cover of F whichis evident and thus omitted With this theorem it is clear that if some child concept c 2 of a 002 cap c 1 is also a 002 cap then c 1 could be removed from CAP  c 002   Such consideration leads to the de\223nition of so called 215 002 restrictor\216 De\223nition 15  002 restrictor For a given concept c   A B  and 0 002 005 1 a 002 cap c 1 of c is call a 002 restrictor of c  if none of its child concepts is 002 cap of c Thesetofall the 002 restrictors of c is denoted as REST R  c 002   When 002 1  concept c 1 is a parent concept of c if and only if c 1 is a 002 r f c  t is evident that REST R  c 002  CAP  c 002  minimal  Consequently following Theorems 9 and 10 we have the theorem below Theorem 11 For each concept c  A B  and a subset P 002 B  P is a 002 approximate intent reduct of c if and only if P is a minimal over of the family  B 212 B 1   A 1 B 1  001 REST R  c 002    Similar to the de\223nition of indispensable attribute the 002 approximate indispensable attribute goes here De\223nition 16 approximate intent core For a given concept c  A B  and 0 002 005 1  attribute m 001 B is 002 approximate indispensable in c  if it satis\223es  B 001    B 212 m  001  002  The set of all the 002 approximate attributes in c is called the 002 approximate intent core denoted as core 001  c   Property 4 Any 002 approximate indispensable attribute in c must be indispensable in c  Put it another way core 001  c  002 core  c   Property 5 For any concept c  A B   1 core 001  c  is included in each 002 approximate intent reduct of c  2 core 001  c  001 IR 001  c   3 core 001  c   m  there exists some 002 restrictor c 1   A 1 B 1  of c ch that B 1 b m   B   By using the notion of approximate intent core we have the following theorem Theorem 12 For a given concept c  A B  let CNS  c   B 212 B 1   A 1 B 1  001 REST R  c 002    B 212 B 1  f core  c  r  it holds that IR 001  c   core  c  b R  R 001MC  CNS  c    Example 7 Consider the concept lattice in Example 1 With 002 0  7  what are the 002 restrictors of the concept 8 Firstly the 002 caps should be the superconcepts of 8 There are 1 2 3 4 5 and 6 satisfying this constraint Secondly e 002 caps should have enough objects Therefore 4 5 and 6 get excluded That is concept 8 has only three 002 caps 1 2 and 3 Finally due to the fact that 1 is the parent concept of both 2 and 3 1 is not a 002 restrictor of c  As a result concept 8 has only two 002 restrictors concept 2 and concept 3 Next what are the 002 approximate intent reducts of concept 8 According to Theorem 11 the problem can be transformed to what are the minimal covers of the family Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


 acg abeg   In virtue of the algorithms in section 3.3 we get IR 001 8   a bc ce g   Finally we get to know that the approximate intent core of concept 8 consists of two attributes a and g  by applying Property 5 4.3 Extracting Association Rules On the basis of approximate intent reducts extracting association rules is quite a simple job which can be done as follows Concept c  A B  is frequent if  A t G 327 001 For each frequent concept c  A B   let us 223rstly calculate all its 002 approximate intent reducts IR 001  c   And then for each R 001IR 001  c  anassociationrule R 006 B 212 R would be derived The correctness of such extracted association rules is easy to prove Since c is frequent and R 002 B wehave   R b  B 212 R  001    B 001    A    G 327 001  Because R is a 002 approximate intent reduct of c wehave   R 002  B 212 R  001   R 001   B 001 R 001 t 002  In addition each association rule can be mapped to exactly one concept For each association rule 215 P 006 Q 216 holding in the given context concept  P b Q  001   P b Q  001\001  is also called the host concept Such a mapping is helpful in the proof of the completeness of the method for tracting association rules Before we dive into the problem let us consider the following inference rules for association rules Property 6 Given support threshold 001 and con\223dence reshold 002  if association rule P 006 Q holds then P 006 Q 1 must hold for any Q 1 002 Q  Property 7 Given support threshold 001 and con\223dence reshold 002  if association rule P 006 Q b W holds then P b W 006 Q must hold For any association rule S 006 T that holds let c  A   S b T  001 B  A 001  S b T  001\001  be the host concept There must exist some 002 approximate intent reduct R of c such that R 002 S  Therefore association rule R 006 B 212 R is extracted by the method at the beginning of this subsection Let W  S 212 R Wehave B 212 R  B 212 S  b  S 212 R   B 212 S  b W From R 006 B 212 R 017 R 006  B 212 S  b W  we could infer R b W 006  B 212 S  017 S 006 B 212 S by Property 7 Due to the fact that T 002 B  S 006 T could also be inferred from S 006 B 212 S  5 Conclusions This paper presents the de\223nitions of intent reducts and approximate intent reducts serving as the theoretical foundation for extracting attribute implications and association rules The properies of them are studied and algorithms are developed for their computation We also give out the methods for extracting attribute implications or association rules The resulting set of rules are shown to be complete with respect to some inference rules Futher research work will focus on removing redundancy from the resulting rule sets Acknowledgments This work was funded in part by the Science  Technology Commission of Shanghai Municipality under grant number 03ZR14014 by the project sponsored by SRF for ROCS SEM and by the National Natural Science Fund of China No 60275022 References 1 R  A g r a w l T  I m ielin sk i an d A  S w a m i  M in in g a sso ciation rules between sets of items in large databases In Proc ACM SIGMOD International Conference on Management of Data Washington D.C May 1993 pp 207-216 2 B  G an ter  R  W ille F o rm al Co n cep t A n a ly sis Math ematical Foundations Springer-Verlag Berlin 1999  C  Li ndi g G S n el t i ng As s e s s i ng modul ar s t ruct ure of legacy code based on mathematical concept analysis In Proc International Conference on Software Engineering Boston USA May 1997 pp 349-359  D Maier  T he Theory of R e lational Databas es  C omputer Science Press Rockville 1983 5 R  W ille Restru c tu rin g lattice th eo ry  a n a p p r o ach based on hierarchies of concepts In I Rival Eds Ordered Sets Reidel Dordrecht 1982 pp 445-470 6 Z  X ie Z Liu  Co n cep t l attice an d asso ciatio n r u l e d iscovery Computer Research and Development 2000 37\(12\1415-1421 in Chinese  Z Xie Z Liu Intent reduct of concept lattice node and its computing Computer Enginnering 2001 27\(3 911 in Chinese Proceedings of the 2005 The Fifth International Confer ence on Computer and Information Technology \(CIT\22205 0-7695-2432-X/05 $20.00 \251 2005  IEEE 


Observe that I2 is a superset of I1: it contains all closed itemsets which are under the CAM border \(as I1 plus those itemsets which arise in equivalence classes which are cut by the CAM border \(such as for instance ce and cde in Figure 3\(a Proposition 3 Cl\(FThD\(Cfreq ? CAM FThD\(Cfreq CAM Let us move to the dual problem. In Figure 3\(b show the usual equivalence classes and how they are cut by CM ? sum\(X.prices are upward closed, we have no problems with classes which are cut: the maximal element of the equivalence class will be in the alive part of the class. In other words when we have a CM constraint, the two interpretations I1 and I2 correspond Proposition 4 Cl\(FThD\(Cfreq ? CM FThD\(Cfreq CM The unique problem that we have with this condensed representation, is that, when reconstructing FThD\(Cfreq[D,?] ? CM care of testing itemsets which are subsets of elements in Cl\(FThD\(Cfreq ? CM not to produce itemsets which are below the monotone border B+\(Th\(CM not need to access the transaction dataset D anymore Since we mine maximal itemsets of the equivalence classes it is impossible to avoid this problem, unless we store, together with our condensed representation the border B+\(Th\(CM closed itemset. This could be an alternative. However since closed itemsets provide a much more meaningful set of association rules, we consider a good tradeo? among performance, conciseness and meaningfulness the use of Cl\(FThD\(Cfreq?CM resentation Finally, if we use free sets instead of closed, we only shift the problem leading to a symmetric situation. Using free sets interpretations I1 and I2 coincide when dealing with anti-monotone constraints because minimal elements are not cut o? by the constraint \(e.g. de in Fig. 3\(a constraints \(e.g. no free solution itemsets in Fig. 3\(b Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 4. Algorithms In this Section we study algorithms for the computation of MP5. We ?rst discuss separately how monotone and anti-monotone constraints can be pushed in the computation, then we show how they can be exploited together by introducing the CCIMiner algorithm 4.1. Pushing Monotone Constraints Pushing CAM constraints deep into the frequent itemset mining algorithm \(attacking the problem FThD\(Cfreq[D,?] ? CAM  since they behave exactly as Cfreq . The case is di?erent for CM constraints, since they behave exactly the opposite of frequency. Indeed, CAM constraints can be used to e?ectively prune the search space to a small downward closed collection, while the upward closed collection of the search space satisfying the CM constraints cannot be exploited at the same time. This tradeo? holding on the search space of the computational problem FThD\(Cfreq[D,?] ? CM extensively studied [18, 9, 4], but all these studies have failed to ?nd the real synergy of these two opposite types of constraints, until the recent proposal of ExAnte [6]. In that work it has been shown that a real synergy of the two opposites exists and can be exploited by reasoning on both the itemset search space and the transactions input database 


set search space and the transactions input database together According to this approach each transaction can be analyzed to understand whether it can support any solution itemset, and if it is not the case, it can be pruned In this way we prune the dataset, and we get the fruitful side e?ect to lower the support of many useless itemsets, that in this way will be pruned because of the frequency constraint, strongly reducing the search space. Such approach is performed with two successive reductions  reduction \(based on monotonicity reduction \(based on anti-monotonicity to  reduction we can delete transactions which do not satisfy CM , in fact no subset of such transactions satis?es CM and therefore such transactions cannot support any solution itemsets. After such reduction, a singleton item may happen to become infrequent in the pruned dataset, an thus it can be deleted by the ?reductions. Of course, these two step can be repeated until a ?xed point is reached, i.e. no more pruning is possible. This simple yet very e?ective idea has been generalized in an Apriori-like breadth-?rst computation in ExAMiner [5], and in a FP-growth [10] based depth-?rst computation in FP-Bonsai [7 Since in general depth-?rst approaches are much more e?cient when mining closed itemsets, and since FP-Bonsai has proven to be more e?cient than ExAMiner, we decide here to use a FP-growth based depth?rst strategy for the mining problem MP5. Thus we combine Closet [16], which is the FP-growth based algorithm for mining closed frequent itemset, with FPBonsai, which is the FP-growth based algorithm for mining frequent itemset with CM constraints 4.2. Pushing Anti-monotone Constraints Anti-monotone constraints CAM can be easily pushed in a Closet computation by using them in the exact same way as the frequency constraint, exploiting the downward closure property of antimonotone constraints. During the computation, as soon as a closed itemset X s.t  CAM \(X ered, we can prune X and all its supersets by halting the depth ?rst visit. But whenever, such closed itemset X s.t  CAM \(X e.g. bcd in Figure 3\(a some itemsets Y ? X belonging to the same equivalence class and satisfying the constraint may exist \(e.g bd and cd in Figure 3\(a ery such X in a separate list, named Edge, and after the mining we can reconstruct such itemsets Y by means of a simple top-down process, named Backward-Mining, described in Algorithm 1 Algorithm 1 Backward-Mining Input: Edge, C, CAM , CM C is the set of frequent closed itemsets CAM is the antimonotone constraint CM is a monotone constraint \(if present Output: MP5 1: MP5 = C split Edge by cardinality 2: k = 0 3: for all c ? Edge s.t. CM \(c 4: E|c| = E|c| ? {c 5: if \(k &lt; |c 6: k=c generate and test subsets 7: for \(i = k; i &gt; 1; i 8: for all c ? E|i| s.t. CM \(c 9: for all \(i? 1 10: if  Y ?MP5 | s ? Y 11: if CAM \(s 12: MP5 = MP5 ? s 13: else 


13: else 14: E|i?1| = E|i?1| ? s The backward process in Algorithm 1, generates level-wise every possible subset starting from the borProceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE der de?ned by Edge without getting into equivalence classes which have been already mined \(Line 10 such subset satis?es the constraint then it can be added to the output \(Line 12 reused later to generate new subsets \(Line 14 have a monotone constraint in conjunction, the backward process is stopped whenever the monotone border B+\(Th\(CM Lines 3 and 8 4.3. Closed Constrained Itemsets Miner The two techniques which have been discussed above are independent. We push monotone constraints working on the dataset, and anti-monotone constraints working on the search space. It  s clear that these two can coexist consistently. In Algorithm 2 we merge them in a Closet-like computation obtaining CCIMiner Algorithm 2 CCIMiner Input: X,D |X , C, Edge,MP5, CAM , CM X is a closed itemset D |X is the conditional dataset C is the set of closed itemsets visited so far Edge set of itemsets to be used in the BackwardMining MP5 solution itemsets discovered so far CAM , CM constraints Output: MP5 1: C = C ?X 2: if  CAM \(X 3: Edge = Edge ?X 4: else 5: if CM \(X 6: MP5 = MP5 ?X 7: for all i ? flist\(D |X 8: I = X ? {i} // new itemset avoid duplicates 9: if  Y ? C | I ? Y ? supp\(I Y then 10: D |I= ? // create conditional fp-tree 11: for all t ? D |X do 12: if CM \(X ? t 13: D |I= D |I ?{t |I  reduction 14: for all items i occurring in D |I do 15: if i /? flist\(D |I 16: D |I= D |I \\i // ?-reduction 17: for all j ? flist\(D |I 18: if supD|I \(j I 19: I = I ? {j} // accumulate closure 20: D |I= D |I \\{j 21: CCIMiner\(I,D |I , C,B,MP5, CAM , CM 22: MP5 = Backward-Mining\(Edge,MP5, CAM , CM For the details about FP-Growth and Closet see [10 16]. Here we want to outline three basic steps 1. the recursion is stopped whenever an itemset is found to violate the anti-monotone constraint CAM Line 2 2  and ? reductions are merged in to the computation by pruning every projected conditional FPTree \(as done in FP-Bonsai [7 Lines 11-16 3. the Backward-Mining has to be performed to retrieve closed itemsets of those equivalence classes which have been cut by CAM \(Line 22 5. Experimental Results The aim of our experimentation is to measure performance bene?ts given by our framework, and to quantify the information gained w.r.t. the other lossy approaches 


approaches All the tests were conducted on a Windows XP PC equipped with a 2.8GHz Pentium IV and 512MB of RAM memory, within the cygwin environment. The datasets used in our tests are those ones of the FIMI repository1, and the constraints were applied on attribute values \(e.g. price gaussian distribution within the range [0, 150000 In order to asses the information loss of the postprocessing approach followed by previous works, in Figure 4\(a lution sets given by two interpretations, i.e. |I2 \\ I1 On both datasets PUMBS and CHESS this di?erence rises up to 105 itemsets, which means about the 30 of the solution space cardinality. It is interesting to observe that the di?erence is larger for medium selective constraints. This seems quite natural since such constraints probably cut a larger number of equivalence classes of frequency In Figure 4\(b built during the mining is reported. On every dataset tested, the number of FP-trees decrease of about four orders of magnitude with the increasing of the selectivity of the constraint. This means that the technique is quite e?ective independently of the dataset Finally, in Figure 4\(c of our algorithm CCIMiner w.r.t. Closet at di?erent selectivity of the constraint. Since the post-processing approach must ?rst compute all closed frequent itemsets, we can consider Closet execution-time as a lowerbound on the post-processing approach performance Recall that CCIMiner exploits both requirements \(satisfying constraints and being closed ing time. This exploitation can give a speed up of about to two orders of magnitude, i.e. from a factor 6 with the dataset CONNECT, to a factor of 500 with the dataset CHESS. Obviously the performance improvements become stronger as the constraint become more selective 1 http://fimi.cs.helsinki.fi/data Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Information loss Number of FP-trees generated Run time performance 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 10 5 10 6 m I 2 I 1  PUMSB@29000 CHESS @ 1200 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 6 10 


10 1 10 2 10 3 10 4 10 5 10 6 10 7 m n u m b e r o f fp t re e s PUMSB @ 29000 CHESS @ 1200 CONNECT@11000 0 2 4 6 8 10 12 14 x 10 5 10 0 10 1 10 2 10 3 10 4 m e x e c u ti o n  ti m e  s e c  CCI Miner  \(PUMSB @ 29000 closet         \(PUMSB @ 29000 CCI Miner  \(CHESS @ 1200 closet         \(CHESS @ 1200 CCI Miner  \(CONNECT @ 11000 closet         \(CONNECT @ 11000 a b c Figure 4. Experimental results with CAM ? sum\(X.price 6. Conclusions 


6. Conclusions In this paper we have addressed the problem of mining frequent constrained closed patterns from a qualitative point of view. We have shown how previous works in literature overlooked this problem by using a postprocessing approach which is not lossless, in the sense that the whole set of constrained frequent patterns cannot be derived. Thus we have provided an accurate de?nition of constrained closed itemsets w.r.t the conciseness and losslessness of this constrained representation, and we have deeply characterized the computational problem. Finally we have shown how it is possible to quantitative push deep both requirements \(satisfying constraints and being closed process gaining performance bene?ts with the increasing of the constraint selectivity References 1] R. Agrawal, T. Imielinski, and A. N. Swami. Mining association rules between sets of items in large databases In Proceedings ACM SIGMOD, 1993 2] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules in LargeDatabases. InProceedings of the 20th VLDB, 1994 3] R. J. Bayardo. E?ciently mining long patterns from databases. In Proceedings of ACM SIGMOD, 1998 4] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Adaptive Constraint Pushing in frequent pattern mining. In Proceedings of 7th PKDD, 2003 5] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi ExAMiner: Optimized level-wise frequent pattern mining withmonotone constraints. InProc. of ICDM, 2003 6] F. Bonchi, F. Giannotti, A.Mazzanti, andD. Pedreschi Exante: Anticipated data reduction in constrained pattern mining. In Proceedings of the 7th PKDD, 2003 7] F. Bonchi and B. Goethals. FP-Bonsai: the art of growing and pruning small fp-trees. In Proc. of the Eighth PAKDD, 2004 8] J. Boulicaut and B. Jeudy. Mining free itemsets under constraints. In International Database Engineering and Applications Symposium \(IDEAS 9] C. Bucila, J. Gehrke, D. Kifer, and W. White DualMiner: A dual-pruning algorithm for itemsets with constraints. In Proc. of the 8th ACM SIGKDD, 2002 10] J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In Proceedings of ACM SIGMOD, 2000 11] L.Jia, R. Pei, and D. Pei. Tough constraint-based frequent closed itemsets mining. In Proc.of the ACM Symposium on Applied computing, 2003 12] H. Mannila and H. Toivonen. Multiple uses of frequent sets and condensed representations: Extended abstract In Proceedings of the 2th ACM KDD, page 189, 1996 13] R. T. Ng, L. V. S. Lakshmanan, J. Han, and A. Pang Exploratory mining and pruning optimizations of constrained associations rules. In Proc. of SIGMOD, 1998 14] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering frequent closed itemsets for association rules In Proceedings of 7th ICDT, 1999 15] J.Pei, J.Han,andL.V.S.Lakshmanan.Mining frequent item sets with convertible constraints. In \(ICDE  01 pages 433  442, 2001 16] J. Pei, J. Han, and R. Mao. CLOSET: An e?cient algorithm formining frequent closed itemsets. InACMSIGMODWorkshop on Research Issues in Data Mining and Knowledge Discovery, 2000 17] J. Pei, J. Han, and J. Wang. Closet+: Searching for the best strategies for mining frequent closed itemsets. In SIGKDD  03, August 2003 18] L. D. Raedt and S. Kramer. The levelwise version space algorithm and its application to molecular fragment ?nding. In Proc. IJCAI, 2001 


ment ?nding. In Proc. IJCAI, 2001 19] R. Srikant, Q. Vu, and R. Agrawal. Mining association rules with item constraints. In Proceedings ACM SIGKDD, 1997 20] M. J. Zaki and C.-J. Hsiao. Charm: An e?cient algorithm for closed itemsets mining. In 2nd SIAM International Conference on Data Mining, April 2002 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


4] K. Chang, C. Chong, Bar-Shalom, Joint Probabilistic Data Association in Distributed Sensor Networks IEEE Transactions on Automatic Control, AC-31\(10 octobre 1986 5] Bar-shalom, Multitarget Multisensor Tracking Advanced Application ,:YBS Publishing, 1990 6] Bar-shalom, Multitarget Multisensor Tracking Principles and Techenices ,:YBS Publishing, 1995 7] Hu Wenlong, Mao shiyi, Multisensor Data Association Based on Combinatorial Optimization[J]. Journal of Systems Engineering and Electronics . 1997 NO.1,1~9 8] Pattipati K R, Passive Multisensor Data Association Using a New Relaxation Algorithm.In Multitarget-Multisensor Tracking: Advanced and Applications,Y.Barshalom,Norwood,MA:Aretech,199 0 9] Deb S,et al, An S-Dimentional Assignment Algorithm for Track Initiation ,Proc. Of the IEEE Int. Conf Systems Engineering, Kobe, Japan,Sept 1992 527~530 10] Deb S,et al, A Multisensor-Multitarget Data Association Algorithm for Heterogeneous Sensors[J].IEEE Trans. on AES 1993 ,29 \(2 560~568 12] Bar-shalom,Y.,and Fortmann,T.E  Tracking and Data Association  New York:Academic press,1988 13] J,A,Roecher and G.L.Phillis,Suboptimal Joint Probabilistic Data Association .IEEE Transaction on Aerospace Electronic Systems.1993,29\(2 14] J,A,Roecker,A Class of Near Optimal JPDA Algorithm IEEE Transaction on Aerospace and Electronic Systems,1994,30\(2 15] Han Yanfei, Analysis and Improvement of Multisensor Multitarget Probabbilistic Data Association Filting Algorithm[J].  Journal of Systems Engineering and Electronics, 2002, Vol.24, 36~38  569 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





