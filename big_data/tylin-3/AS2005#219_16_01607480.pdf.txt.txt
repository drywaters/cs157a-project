html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Fuzzy  Frequent Pattern Discovering Based on Recursive Elimination Xiaomeng Wang, Christian Borgelt and Rudolf Kruse Department of Knowledge Processing and Language Engineering School of Computer Science, Otto-von-Guericke-University of Magdeburg Universita  tsplatz 2, 39106 Magdeburg, Germany Email: {xwang,borgelt,kruse}@iws.cs.uni-magdeburg.de Abstract Real life transaction data often miss some occurrences of items that are actually present. As a consequence some potentially interesting frequent patterns cannot be discovered, since with exact matching the number of supporting transactions may be smaller than the user-speci?ed minimum. In order to allow approximate matching during the mining process, we propose an approach based on transaction editing. Our recursive algorithm relies on a step by step elimination of items from the transaction database together with a recursive processing of transaction subsets This algorithm works without complicated data structures and allows us to ?nd fuzzy frequent patterns easily 1. Introduction In many applications of frequent pattern mining \(also known as frequent item set mining actions do not contain all items that are actually present An example is the analysis of alarm sequences in telecommunication networks, where each alarm can be treated as an item. Unfortunately, the alarms often get delayed, lost or repeated due to noise, transmission errors, failing links etc. If alarms do not get through or are delayed, they can be missing from the transaction \(time window ated items \(alarms this case, the support of some item sets, which could be frequent if the items did not get lost, may be smaller than the user-speci?ed minimum. This leads to a possible loss of potentially interesting frequent patterns To cope with such missing information, we introduce the notion of a  fuzzy  frequent pattern. In contrast to other work on fuzzy association rules \(e.g. [9 fuzzy approach is used to handle quantitative items, our term  fuzzy  denotes a pattern that may not be found exactly in all supporting transactions, but only approximately some related work see [7], [12 based on transaction editing and a step by step elimination of items together with a recursive processing of transaction subsets. Due to its simple data structure, transaction editing is straightforward, thus allowing effective fuzzy mining 2. Fuzzy frequent patterns Let us brie?y recall the problem of frequent pattern mining: let I = {i1, ..., in} be the set of items in a database D consisting of transactions T = \(tid,X action identi?er and X ? I. A transaction T = \(tid,X said to contain an item set Y if Y ? X . The support of Y , denoted as s\(Y item set Y . Given a transaction database D and a support threshold smin, an item set Y is called frequent if and only if s\(Y action T contributes to the support of an item set Y either with 1 if T contains all items in Y , or with 0 if not Motivated by the problem stated in Section 1, we de?ne a  fuzzy frequent pattern  by allowing approximate matching instead of the exact matching reviewed above. Preceding this, however, we introduce two additional notions 1. Edit costs: The distance between two item sets can be de?ned as the costs of the cheapest sequence of edit operations needed to transform one item set into the other [10 Here we consider only insertions, since they are very easy to implement with our algorithm \(see below items can have different insertion costs. For example, in telecommunication networks different alarms can have a different probability of getting lost: usually alarms originating in lower levels of the module hierarchy get lost more 


nating in lower levels of the module hierarchy get lost more easily than alarms originating in higher levels. Therefore the former can be associated with lower insertion costs than the latter. The insertion of a certain item may also be completely inhibited by assigning a very high insertion cost 2. Transaction weight: Each transaction T in the original 1Note that deletions are implicit in the mining process anyway. Only replacements are an additional case we do not consider here Proceedings of the Fourth International Conference on Machine Learning and Applications  ICMLA  05 0-7695-2495-8/05 $20.00  2005 IEEE database is associated with a weight w\(T of each transaction is 1. After each insertion of an item i into a transaction, its weight is  penalized  with the cost c\(i sociated with the insertion of this item. Formally, this can be described by a combination function. The new weight of a transaction T after editing is w{i}\(T w\(T i where f is a function that combines the weight w\(T fore editing and the insertion cost c\(i variety of combination functions that may be used, for instance, any t-norm. For simplicity, we use multiplication i.e., w{i}\(T T  c\(i trary choice. Note, however, that in this case lower values for the cost c\(i the weight more. Note also that the above de?nition can easily be extended to the insertion of multiple items as w{i1,...,im}\(T t  mk=1 c\(ik is w /0\(T T How many insertions into a transaction are allowed can be limited by a user-speci?ed lower bound wmin for the transaction weight. If the weight of a transaction falls below this threshold, it is not considered in further mining steps and thus no insertions can be done on it anymore De?nition 1 Given a user-speci?ed lower bound wmin for the transaction weight, a transaction T = \(tid,X contains an item set Y ? I if wY\\\(X?Y T case T contributes to the support of Y with wY\\\(X?Y T De?nition 2 Given a database D = {T1, . . . ,Tr} of transactions Tk = \(k,Xk an item set Y ? I is a fuzzy frequent pattern if s  Y where s  Y Xk?Y Tk The basic idea of our approach is to try to  complete  transactions by inserting items during the mining process Thus we allow for a certain number of mismatches, by which we account for possibly missing items. That is a transaction still contributes to the support of a pattern though only to a reduced degree, if it contains only part of the items in the set. As an example consider the transaction database in Table 1 on the right, in particular, the 2nd transaction \(ecd cb ecb cd we want to determine the support of the item set  ec  the second and the eighth transaction contribute to the support with a weight of 1 each. However, the 5th transaction \(cb and the 9th \(cd  ec  if we insert item e into them. Due to this insertion, they should not contribute with full weight, though, but only to some degree. Therefore these two transactions are counted with penalized weights for the support of item set  ec   In the following, to ease understanding our work, we ?rst present an algorithm called recursive elimination with exact matching, then introduce our algorithm for fuzzy frequent pattern mining which extended the former. Here we focus on implementation aspects, since the fuzzy version strongly relies on the data structures used in the implementation 1 a d f 2 c d e 3 b d 4 a b c d 5 b c 6 a b d 7 b d e 


7 b d e 8 b c e g 9 c d f 10 a b d g 1 f 2 e 3 a 4 c 5 b 7 d 8 a d e c d b d a c b d c b a b d e b d e c b c d a b d Table 1. Transaction database \(left quencies \(middle database with items in transactions sorted ascendingly w.r.t. their frequency \(right 3. Recursive elimination Methods for mining frequent item sets have been studied extensively. Among the best-known algorithms are Apriori [2, 3], Eclat [13, 6], and FP-growth [8]. Here we consider recursive elimination [5] \(Relim for short data structures very similar to those of H-Mine [11], even though it was developed independently and ?nds the frequent item sets in a different order. Inspired by the FPgrowth algorithm, but working without a pre?x tree representation, Relim processes the transactions directly, organizing them merely into singly linked lists 3.1. Preprocessing and data representation Recursive elimination preprocesses the transaction database similar to several other algorithms for frequent item set mining: in an initial scan it determines the frequencies of the items \(support of single element item sets All infrequent items  that is, all items that appear in fewer transactions than a user-speci?ed minimum number  are discarded from the transactions, since they can obviously never be part of a frequent pattern. In addition, the items in each transaction are sorted in ascending order w.r.t. their frequencies in the database. Although the algorithm does not require this speci?c order, experiments showed that it leads to much shorter execution times than a random order. This preprocessing is demonstrated in Table 1, the left of which shows an example transaction database. The frequencies of the items in this database, sorted ascendingly are shown in the table in the middle. If we are given a user speci?ed minimum support of 3 transactions, items f and g can be discarded. After doing so and sorting the items in each transaction ascendingly w.r.t. their frequencies we obtain the reduced database shown in Table 1 on the right Relim uses very simple data structures: each transaction is represented as an array of item identi?ers \(integer numProceedings of the Fourth International Conference on Machine Learning and Applications  ICMLA  05 0-7695-2495-8/05 $20.00  2005 IEEE bers transaction lists, with one list for each item. These lists are stored in a simple array, each element of which contains a support counter and a pointer to the head of the list. The list elements consist only of a successor pointer and a pointer to or rather into, see below are inserted one by one into this structure by using their leading item \(the ?rst item ing item is removed from the transaction, i.e., the pointer in 


ing item is removed from the transaction, i.e., the pointer in the transaction list element points to the second item. Note that this does not lose any information as the ?rst item is implicitly represented by the list the transaction is in As an illustration, Figure 1 shows, at the very top, how the \(reduced rst list, corresponding to item e, contains the 2nd, 7th, and 8th transaction, with item e removed. The counter in an array element states the number of transactions starting with the corresponding item \(3 for item e is not always equal to the length of the associated list, although this is the case for the representation of the initial database. Differences result from \(shrunk contain no other items thus do not appear in the list 3.2. Recursive processing Recursive elimination works as follows: The array of lists that represents a \(reduced  disassembled  by traversing it from left to right, processing the transactions in a list in a recursive call to ?nd all frequent patterns that contain the item the list corresponds to. After a list has been processed recursively, its elements are either reassigned to the remaining lists or discarded \(depending on the transactions they represent on. Since all reassignments are made to lists that lie to the right of the currently processed one, the list array will ?nally be empty \(will contain only empty lists Before a transaction list is processed, however, its support counter is checked, and if it exceeds the user-speci?ed minimum support, a frequent pattern is reported, consisting of the item associated with the list and a possible pre?x associated with the whole list array \(see below One transaction list is processed as follows: for each list element the leading item of its \(shrunk trieved and used as an index into the list array; then the element is added at the head of the corresponding list. In such a reassignment, the leading item is removed from the transaction, which is implemented as a simple pointer increment In addition, a copy of the list element \(with the leading item of the transaction already removed by the pointer increment second array of transaction lists. \(Note that only the list element is copied, not the transaction. Both list elements, the reassigned one and the copy, refer to the same transaction initial databasee a c b d3 4 2 1 0 c d b d c b d c b d b d b d b d d 1  e a c b d 0 4 4 2 0 d c b d b d b d b d b d d d pre?x e e a c b d 0 0 2 1 0 b 


b d d 2  e a c b d 0 0 5 4 1 b d b d b d d d d d pre?x a e a c b d 0 0 1 2 1 b d d d 3  e a c b d 0 0 0 7 3 d d d d d pre?x c e a c b d 0 0 0 3 2 d 4  e a c b d 0 0 0 0 8 pre?x b e a c b d 0 0 0 0 5 Figure 1. Procedure of the recursive elimination with the modi?cation of the transaction lists \(left transaction lists for the recursion \(right Since the elements of a transaction list all share an item given by the list index set of transactions that contain a speci?c item \(also called a projection of a transaction database w.r.t. a speci?c item and represents them as a set of transaction lists. This set of transaction lists is then processed recursively, noting the item associated with the list it was generated from as a common pre?x of all frequent item sets found in the recursion After the recursion the next transaction list is reassigned copied, and processed in a recursive call and so on The process is illustrated for the root level of the recursion in Figure 1, which shows the transaction list representation of the initial database at the very top. In the ?rst step all item sets containing the item e are found by processing the leftmost list. The elements of this list are reassigned to the lists to the right \(grey list elements serted into a second list array \(shown on the right second list array is then processed recursively, before proceeding to the next list, i.e., the one for item a Proceedings of the Fourth International Conference on Machine Learning and Applications  ICMLA  05 0-7695-2495-8/05 $20.00  2005 IEEE A list element representing a \(shrunk only one item is neither reassigned nor copied, because the transaction is empty after the leading item is removed. For such elements only the counter in the lists array element is incremented. Such a situation occurs, for example, when the list corresponding to the item a is processed. The ?rst list element refers to a \(shrunk only item d and thus only the counter for item d \(grey 


only item d and thus only the counter for item d \(grey incremented. For the same reason only one of the ?ve elements in the list for item c is reassigned/copied in step 3 After four steps all transaction lists have been processed and the lists array has become empty. Note that the list for the last element \(referring to item d because there are no items left that could be in a transaction and thus all transactions are represented in the counter 4. Fuzzy mining For fuzzy frequent pattern mining we extend Relim with the two notions introduced in Section 2  edit costs and transaction weights. To store a transaction weight we add a component to each list element described in Section 3.2 The list array that represents a \(reduced database is processed in basically the same way as before The most important modi?cation lies in the construction of the subset of transaction lists that represents the projected transaction database w.r.t. a speci?c item. Figure 2 shows how the extended algorithm \(called Relx the root level of the recursion. We consider the same transaction database as in Figure 1. In this example we use the same cost factor c\(i action weight is updated after an insertion according to w? =w  0.5. Furthermore, we assumewmin = 0 and smin = 3 Before a transaction list is processed, its support counter in the array element is checked. This support counter is now even less an indicator of the number of the elements of the transaction list, but the sum of the weights of list elements, several of which may differ from the initial weight of 1 \(cf. Figure 2 found by processing the leftmost list. The support counter actually used is the number stored in the list corresponding to e plus the sum of the weights of transactions in other lists, into which the insertion of e is possible. Since s  e 4+ 2+ 1  0.5 frequent. Reassigning the elements of this list to the lists on the right is the same as before \(cf. Figure 1 left the copies inserted into a second list array are different now compare Figure 1 right and Figure 2 right only the elements of the leftmost list with a weight of 1 grey list elements, they all contain item e ments of the lists corresponding to a, c, and b with a penalized weight of 0.5 \(white list elements, 0.5 ? wmin we virtually insert item e into the corresponding transacinitial databasee a c b d ?3 4 2 1 0 0 c d b d c b d c b d b d b d b d d 1  e a c b d 0 4 4 2 0 0 d c b d b d b d b d b d d d pre?x e e a c b d 0 2 3 3/2 0 0 d 


d c b d b d b d b d b d d d 2  e a c b d 0 0 5 4 1 0 b d b d b d d d d d pre?x a e a c b d 0 0 3 3 1 0 b b d b d d d d d d 3  e a c b d 0 0 0 7 3 0 d d d d d pre?x c e a c b d 0 0 0 5 5/2 0 d d d d d 4  e a c b d 0 0 0 0 8 2  8 pre?x b e a c b d 0 0 0 0 13/2 2  8 Figure 2. Procedure of the recursive eliminationwith the insertion ofmissing items. Since insertions of all items are possible, the subset right main set \(left differ due to the transaction weighting tions. Obviously with such operations we have more transactions to process in the recursion and hence a \(considerably Due to the insertion, in the projected transaction database w.r.t. item e \(i.e. the list array resulting from the copy step the support counter of the list corresponding to a is 4  0.5 2 and that corresponding to c is \(2  1 2  0.5 so on. Since s  a 4.5  0.5 


so on. Since s  a 4.5  0.5 frequent in this projected database, combining it with the associated pre?x e yields fuzzy frequent pattern  ea  This list array is then processed recursively before working on the next list, i.e., the one for item a in the main set \(Figure 2 left Proceedings of the Fourth International Conference on Machine Learning and Applications  ICMLA  05 0-7695-2495-8/05 $20.00  2005 IEEE census number of sets time/s Relim \(original data Relim \(with deletions Relx \(no insertion Relx \(wmin = 0.4 Relx \(wmin = 0.2 T10I4D100K number of sets time/s Relim 10 0.01 Relx \(no insertion Relx \(wmin = 0.4 Relx \(wmin = 0.2 Table 2. Results on census and T10I4D100K Note that there are now list elements referring to empty transactions. In contrast to exact mining, mining fuzzy frequent patterns requires that we also reassign and copy a list element representing a \(shrunk item. Even though the transaction is empty after the leading item is removed, it has to be maintained as it can be processed further by inserting items. Therefore such a list element is reassigned/copied  as an empty transaction  to the list associated with the only item it contains. An example of this can be seen when the list corresponding to item a is processed \(step 2 a \(shrunk counter for item d is incremented and an empty element is kept in the list associated with item d \(both reassignment and copy In addition, a new element labeled ? is added to the array of transaction lists. This new list is needed when an empty transaction has to be reassigned/copied. Since an empty transaction has no leading item, it cannot be inserted into one of the lists corresponding to the items of the database. It cannot be discarded either, because in later processing items may be inserted into it, and then it has to be considered in the corresponding recursion. Formally, ? can be seen as an additional pseudo-item, which is contained in all transactions, but which is not to be reported as part of a frequent pattern. An example of how this new array element is used can be seen when the list corresponding to item b is processed \(step 4 which are reassigned and copied to the additional lists array element labeled with ?. Even though these transactions are now empty, they have to be considered when processing item d, because this item may be inserted into them 5. Experimental results To evaluate our algorithm, we implemented it in C and ran experiments on a laptop with a 1.8 GHz Intel Pentium Mobile processor and 1 GB main memory using Windows XP Professional SP2. Results obtained with the original program \(exact frequent pattern mining  Relim  those for fuzzy frequent pattern mining  Relx   In all experiments, we updated the weight of a transaction by multiplying it with an insertion cost factor \(if necessary And unless otherwise stated, uniform insertion costs of 0.5 for all items \(it is rather a arbitrary choice merely for illustration weight \(thus allowing exactly one insertion In an initial test, we used the very simple transaction database shown in Table 1, using a minimum support of 30%, to check the basic functionality of the approach When mining this database with exact matching \(i.e. without insertions 


out insertions matching 23 patterns are found. The item set ec, which we used as an example in Section 2, is found with fuzzy matching, but not with exact matching. On the other hand, if the insertion of item e is ruled out by setting c\(e set ec is not found anymore To check the performance on larger data sets, we tested our programs on the data sets census [4] and T10I4D100K 1], with a minimum support of 30% for census and 5 for T10I4D100K. The number of frequent item sets discovered and the corresponding execution time \(in seconds shown in Table 2. If insertions were inhibited, the number of sets reported by Relx coincides with that of Relim, proving the sanity of the implementation. However, as was to be expected, Relx needs more time as it has to invest additional effort into managing empty transactions \(cf. Section 4; an additional factor is the computation of penalized weights which takes place nevertheless Results produced by Relx with different thresholds for the transaction weight \(allowing 0 to 2 insertions  not surprisingly  that with decreasing threshold the number of frequent patterns and the execution time increases Frequent patterns that could not be found before are now discovered. Note, however, that the frequent patterns now have fractional support due to the transaction weighting Note also that the execution times are still bearable, even though the insertions make it necessary to process a much higher number of transactions in the recursion Finally, we ran the program on data from which items had been deleted randomly to check the effectiveness of the proposed algorithm. Here we present only an example of the results. We randomly deleted 4% of item  hours=full-time  and 3% of the item  sex=female  from the census data set to simulate the missing items and then mined with a minimum support of 30%. We ran Relim on both the unmodi?ed and the preprocessed data. With the former 244 frequent itemsets were found, while only 238 of them were detected in the later \(cf. Table 2 is, due to missing occurrences of two items, we lost 6 Proceedings of the Fourth International Conference on Machine Learning and Applications  ICMLA  05 0-7695-2495-8/05 $20.00  2005 IEEE frequent itemsets    country=United-States, hours=fulltime, loss=none, race=White, workclass=Private    country=United-States, hours=full-time, loss=none sex=Male    gain=none, hours=full-time, salary?50K    gain=none, loss=none, sex=female    hours=full-time loss=none, race=White, sex=Male  and  hours=full-time loss=none, salary?50K  When we used Relx with an insertion cost of 0.5 for both items, we could ?nd the complete item sets that were obtained by Relim from the unmodi?ed data. In fact, a superset of the original frequent patterns were reported. However, we have to accept this as an inherent feature of the insertion concept. Still the results are encouraging, and prove that our algorithm is capable of rediscovering frequent itemsets, which are lost with classical approaches due to missing information in the data 6. Conclusions Frequent pattern mining on real-world data with missing information calls for fuzzy mining. Facing this challenge, we introduced a concept of fuzzy frequent patterns based on transaction editing. The algorithm we developed for mining fuzzy frequent patterns relies on deleting items editing transactions, recursive processing, and reassigning transactions. Our algorithm is very simple, works without complicated data structures, and performs reasonably well Some other work in this direction like in [7], [12], perform approximate matching only by counting the number of different items in the two item sets to be compared, and use an Apriori-like algorithm. The algorithm in [12], as re 


use an Apriori-like algorithm. The algorithm in [12], as reported, performed much slower \(about 100 times or even more ber of mismatches from 1 to 2. Compared to this our approach provides two main advantages: \(1 matching is based on a more general scheme  edit operations. It allows the individual treatment of every single item, which enables a better involvement of background knowledge. \(2 many times \(which is the case in Apriori-like algorithms Instead it looks for \(locally projected databases and combines them with the associated pre?x \(which is the frequent pattern found so far the frequent patterns. Thus it is more ef?cient, as can be seen in Table 2. The execution time in the case of allowing two insertions is only about thrice that for one insertion Up to now, we only investigated how to edit an item set by insertion. However, there are also other interesting editing operations. If we take the order of items into account operations like exchanging the order of two items are de?nitely worth to be studied References 1] Synthetic data generation code for associations and sequential patterns. Intelligent Information Systems, IBM Almaden Research Center http://www.almaden.ibm.com/software/quest/Resources index.shtml 2] R. Agrawal, T. Imielienski, and A. Swami. Mining association rules between sets of items in large databases. In Proc Conf. on Management of Data, pages 207  216, New York NY, USA, 1993. ACM Press 3] R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A. Verkamo. Fast discovery of association rules. In U.M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, eds. Advances in Knowledge Discovery and Data Mining, pages 307  328, Cambridge,CA,USA, 1996. AAAI Press / MIT Press 4] C. L. Blake and C. J. Merz. UCI repository of machine learning databases. Dept. of Information and Computer Science, UC Irvine, CA, USA, 1998 http://www.ics.uci.edu  mlearn/MLRepository.html 5] C. Borgelt. Keeping things simple: Finding frequent item sets by recursive elimination. In Proc. Workshop Open Source Data Mining Software \(OSDM  05, Chicago, IL 6] C. Borgelt. Ef?cient implementations of apriori and eclat. In Proc. 1st IEEE ICDMWorkshop on Frequent Itemset Mining Implementations \(FIMI 2003, Melbourne, FL shop Proc. 90, Aachen, Germany, 2003. http://www.ceurws.org/Vol-90 7] Y. Cheng, U. Fayyad, and P. Bradley. Ef?cient discovery of error-tolerant frequent itemsets in high dimensions. In Proc 7th Int. Conf. on Knowledge Discovery and Data Mining KDD  01, San Francisco, CA  203, New York NY, USA, 2001. ACM Press 8] J. Han, H. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In Proc. Conf. on the Management of Data \(SIGMOD  00, Dallas, TX  12, New York NY, USA, 2000. ACM Press 9] C. Kuok, A. Fu, and M. Wong. Mining fuzzy association rules in databases. SIGMOD Record, 27\(1  46, 1998 10] P. Moen. Attribute, Event Sequence, and Event Type Similarity Notions for Data Mining. Report A-2000-1. PhD thesis Department of Computer Science, University of Helsinki Finland, 2000 11] J. Pei, J. Han, H. Lu, S. Nishio, S. Tang, and D. Yang. Hmine: Hyper-structure mining of frequent patterns in large databases. In Proc. IEEE Conf. on Data Mining \(ICDM  01 San Jose, CA  448, Piscataway, NJ, USA, 2001 IEEE Press 12] J. Pei, A. K. H. Tung, and J. Han. Fault-tolerant frequent pattern mining: Problems and challenges. In Proc. ACM 


pattern mining: Problems and challenges. In Proc. ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery \(DMK  01, Santa Babara, CA May 2001, Santa Babara, CA 13] M. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. New algorithms for fast discovery of association rules. In Proc 3rd Int. Conf. on Knowledge Discovery and Data Mining \(KDD  97, Newport Beach, CA  296, Menlo Park, CA, USA, 1997. AAAI Press Proceedings of the Fourth International Conference on Machine Learning and Applications  ICMLA  05 0-7695-2495-8/05 $20.00  2005 IEEE pre></body></html 


Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 7 Conclusion and Future Work References frontier  pages 487\320499 Morgan Kaufmann 1994  W orkshop on frequent itemset mining implementations 2003 http://\336mi.cs.helsinki.\336/\336mi03  W orkshop on frequent itemset mining implementations 2004 http://\336mi.cs.helsinki.\336/\336mi04  J  Han J  Pei and Y  Y in Mining frequent patterns without candidate generation In Proceedings of 20th International Conference on Very Large Data Bases VLDB VLDB Journal Very Large Data Bases Data Mining and Knowledge Discovery An International Journal Lecture Notes in Computer Science  2004  J W ang and G Karypis Harmon y Ef 336ciently mining the best rules for classi\336cation In The Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD\32504 Symposium on Principles of Database Systems 2 b Average time taken per frequent itemset shown on two scales T10I4D100K is increased and hence the number of frequent items decreases Figure 5\(c also shows that the maximum frontier size is very small Finally we reiterate that we can avoid using the pre\336x tree and sequence map so the only space required are the itemvectors and the minSup SIAM International Conference on Data Mining required drops quite quickly as ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery 2000 ACM SIGMOD Intl Conference on Management of Data Figure 5 Results  8\(3\3204 2000  F  P an G C ong A T ung J Y ang and M Zaki Carpenter Finding closed patterns in long biological datasets In  2121:236 2001  M Steinbach P N T an H Xiong and V  K umar  Generalizing the notion of support In a Runtime ratios T10I4D100K c Number of Itemvectors needed and maximum frontier size T10I4D100K  pages 1\32012 ACM Press May 2000  F  K orn A Labrinidis Y  K otidis and C F aloutsos Quanti\336able data mining using ratio rules  Morgan Kaufmann 2003  J Pei J Han and L Lakshmanan Pushing convertible constraints in frequent itemset mining We showed interesting consequences of viewing transaction data as itemvectors in transactionspace and developed a framework for operating on itemvectors This abstraction gives great 337exibility in the measures used and opens up the potential for useful transformations on the data Our future work will focus on 336nding useful geometric measures and transformations for itemset mining One problem is to 336nd a way to use SVD prior to mining for itemsets larger than  pages 205\320215 2005  We also presented GLIMIT a novel algorithm that uses our framework and signi\336cantly departs from existing algorithms GLIMIT mines itemsets in one pass without candidate generation in linear space and time linear in the number of interesting itemsets Experiments showed that it beats FP-Growth above small support thresholds Most importantly it allows the use of transformations on the data that were previously impossible  That is the space required is truly linear  D Achlioptas Database-friendly random projections In  2001  R Agra w al and R Srikant F ast algorithms for mining association rules In  8:227\320252 May 2004  J Pei J Han and R Mao CLOSET An ef 336cient algorithm for mining frequent closed itemsets In  pages 21\32030 2000  S Shekhar and Y  Huang Disco v ering spatial colocation patterns A summary of results 


mator from sensor 1 also shown 6. CONCLUSIONS This paper derives a Bayesian procedure for track association that can solve a large scale distributed tracking problem where many sensors track many targets. When noninformative prior of the target state is assumed, the single target test becomes a chi-square test and it can be extended to the multiple target case by solving a multidimensional assignment problem. With the noninformative prior assumption, the optimal track fusion algorithm can be a biased one where the regularized estimate has smaller mean square estimation error. A regularized track fusion algorithm was presented which modifies the optimal linear unbiased fusion rule by a less-than-unity scalar. Simulation results indicate the effectiveness of the proposed track association and fusion algorithm through a three-sensor two-target tracking scenario 7. REFERENCES 1] Y. Bar-Shalom and W. D. Blair \(editors Tracking: Applications and Advances, vol. III, Artech House, 2000 2] Y. Bar-Shalom and H. Chen  Multisensor Track-to-Track Association for Tracks with Dependent Errors  Proc. IEEE Conf. on Decision and Control, Atlantis, Bahamas, Dec. 2004 3] Y. Bar-Shalom and X. R. Li, Multitarget-Multisensor Tracking Principles and Techniques, YBS Publishing, 1995 4] Y. Bar-Shalom, X. R. Li and T. Kirubarajan, Estimation with Applications to Tracking and Navigation: Algorithms and Software for Information Extraction, Wiley, 2001 5] S. Blackman, and R. Popoli  Design and Analysis of Modern Tracking Systems  Artech House, 1999 10 15 20 25 30 35 40 45 50 55 60 2 4 6 8 10 12 14 Time N o rm a li z e d e s ti m a ti o n e rr o r s q u a re d Target 1 Sensor 1 Centralized Est Track Fusion 10 15 20 25 30 35 40 45 50 55 60 0 2 


2 4 6 8 10 12 14 Time N o rm a li z e d e s ti m a ti o n e rr o r s q u a re d Target 2 Sensor 1 Centralized Est Track Fusion Fig. 7. Comparison of the NEES for centralized IMM estimator \(configuration \(i estimators \(configuration \(ii sensor 1 also shown 6] H. Chen, T. Kirubarajan, and Y. Bar-Shalom  Performance Limits of Track-to-Track Fusion vs. Centralized Estimation: Theory and Application  IEEE Trans. Aerospace and Electronic Systems 39\(2  400, April 2003 7] H. Chen, K. R. Pattipati, T. Kirubarajan and Y. Bar-Shalom  Data Association with Possibly Unresolved Measurements Using Linear Programming  Proc. 5th ONR/GTRI Workshop on Target Tracking Newport, RI, June 2002 8] Y. Eldar, and A. V. Oppenheim  Covariance Shaping Least-Square Estimation  IEEE Trans. Signal Processing, 51\(3 pp. 686-697 9] Y. Eldar  Minimum Variance in Biased Estimation: Bounds and Asymptotically Optimal Estimators  IEEE Trans. Signal Processing, 52\(7 10] Y. Eldar, A. Ben-Tal, and A. Nemirovski  Linear Minimax Regret Estimation of Deterministic Parameters with Bounded Data Uncertainties  IEEE Trans. Signal Processing, 52\(8 Aug. 2004 11] S. Kay  Conditional Model Order Estimation  IEEE Transactions on Signal Processing, 49\(9 12] X. R. Li, Y. Zhu, J. Wang, and C. Han  Optimal Linear Estimation Fusion  Part I: Unified Fusion Rules  IEEE Trans. Information Theory, 49\(9  2208, Sept. 2003 13] X. R. Li  Optimal Linear Estimation Fusion  Part VII: Dynamic Systems  in Proc. 2003 Int. Conf. Information Fusion, Cairns, Australia, pp. 455-462, July 2003 14] X. D. Lin, Y. Bar-Shalom and T. Kirubarajan  Multisensor Bias Estimation Using Local Tracks without A Priori Association  Proc SPIE Conf. Signal and Data Processing of Small Targets \(Vol 


SPIE Conf. Signal and Data Processing of Small Targets \(Vol 5204 15] R. Popp, K. R. Pattipati, and Y. Bar-Shalom  An M-best Multidimensional Data Association Algorithm for Multisensor Multitarget Tracking  IEEE Trans. Aerospace and Electronic Systems, 37\(1 pp. 22-39, January 2001 pre></body></html 


20 0  50  100  150  200  250  300 Pe rc en ta ge o f a dd iti on al tr af fic Cache size 200 clients using CMIP 200 clients using UIR c Figure 6. The percentage of additional traf?c the cache at every clock tick. A similar scheme has been proposed in [13], which uses fv, a function of the access rate of the data item only, to evaluate the value of each data item i that becomes available to the client on the channel If there exists a data item j in the client  s cache such that fv\(i j replaced with i A prefetch scheme based on the cache locality, called UIR scheme, was proposed in [7]. It assumes that a client has a large chance to access the invalidated cache items in the near future. It proposes to prefetch these data items if it is possible to increase the cache hit ratio. In [6], Cao improves the UIR scheme by reducing some unnecessary prefetches based on the prefetch access ratio \(PAR scheme, the client records how many times a cached data item has been accessed and prefetched, respectively. It then calculates the PAR, which is the number of prefetches divided by the number of accesses, for each data item. If the PAR is less than one, it means that the data item has been accessed a number of times and hence the prefetching is useful. The clients can mark data items as non-prefetching when PAR &gt; b, where b is a system tuning factor. The scheme proposes to change the value of b dynamically according to power consumption. This can make the prefetch scheme adaptable, but no clear methodology as to how and when b should be changed. Yin et al. [19] proposed a power-aware prefetch scheme, called value-based adaptive prefetch \(VAP the number of prefetches based on the current energy level to prolong the system running time. The VAP scheme de?nes a value function which can optimize the prefetch cost to achieve better performance These existing schemes have ignored the following characteristics of a mobile environment: \(1 query some data items frequently, \(2 during a period of time are related to each other, \(3 miss is not a isolated events; a cache miss is often followed by a series of cache misses, \(4 eral requests in one uplink request consumes little additional bandwidth but reduces the number of future uplink requests. In this paper, we addressed these issues using a cache-miss-initiated prefetch scheme, which is based on association rule mining technique. Association rule mining is a widely used technique in ?nding the relationships among data items. The problem of ?nding association rules among items is clearly de?ned by Agrawal et al. in [5]. However in the mobile environment, one cannot apply the existing association rule mining algorithm [4] directly because it is too complex and expensive to use This makes our algorithm different from that of [4] in 


This makes our algorithm different from that of [4] in twofold. First, we are interested in rules with only one data item in the antecedent and several data items in the consequent. Our motivation is to prefetch several data items which are highly related to the cache-miss data item within Proceedings of the 2004 IEEE International Conference on Mobile Data Management \(MDM  04 0-7695-2070-7/04 $20.00  2004 IEEE the cache-miss initiated uplink request. We want to generate rules where the antecedent is one data item, but the cache-missed data item and the consequent is a series of data items, which are highly related to the antecedent. If we have such rules, we can easily ?nd the data items which should also be piggybacked in the uplink request. Second in mobile environment, the client  s computation and power resources are limited. Thus, the rule-mining process should not be too complex and resource expensive. It should not take a long time to mine the rules. It should not have high computation overhead. However, most of the association rule mining algorithms [4, 5] have high computation requirements to generate such rules 5. Conclusions Client-side prefetching technique can be used to improve system performance in mobile environments. However, prefetching also consumes a large amount of system resources such as computation power and energy. Thus, it is very important to only prefetch the right data. In this paper, we proposed a cache-miss-initiated prefetch \(CMIP scheme to help the mobile clients prefetch the right data The CMIP scheme relies on two prefetch sets: the alwaysprefetch set and the miss-prefetch set. Novel association rule based algorithms were proposed to construct these prefetch sets. When a cache miss happens, instead of sending an uplink request to only ask for the cache-missed data item, the client requests several items, which are within the miss-prefetch set, to reduce future cache misses. Detailed experimental results veri?ed that the CMIP scheme can greatly improve the system performance in terms of increased cache hit ratio, reduced uplink requests and negligible additional traf?c References 1] S. Acharya, M. Franklin, and S. Zdonik. Prefetching From a Broadcast Disk. Proc. Int  l Conf. on Data Eng., pages 276  285, Feb. 1996 2] S. Acharya, M. Franklin, and S. Zdonik. Balancing Push and Pull for Data Broadcast. Proc. ACM SIGMOD, pages 183  194, May 1997 3] S. Acharya, R. Alonso, M. Franklin, and S. Zdonik. Broadcast disks: Data Management for Asymmetric Communication Environments. Proc. ACM SIGMOD, pages 199  210 May 1995 4] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In J. B. Bocca, M. Jarke, and C. Zaniolo editors, Proc. 20th Int. Conf. Very Large Data Bases, VLDB pages 487  499. Morgan Kaufmann, 12  15 1994 5] R. Agrawal, Tomasz Imielinski, and Arun Swami. Mining Association Rules Between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207  216, Washington, D.C May 1993 6] G. Cao. Proactive Power-Aware Cache Management for Mobile Computing Systems. IEEE Transactions on Computers, 51\(6  621, June 2002 7] G. Cao. A Scalable Low-Latency Cache Invalidation Strategy for Mobile Environments. IEEE Transactions on Knowledge and Data Engineering, 15\(5 ber/October 2003 \(A preliminary version appeared in ACM MobiCom  00 8] K. Chinen and S. Yamaguchi. An Interactive Prefetching Proxy Server for Improvement of WWW Latency. In Proc INET 97, June 1997 9] E. Cohen and H. Kaplan. Prefetching the means for docu 


9] E. Cohen and H. Kaplan. Prefetching the means for document transfer: A new approach for reducing web latency. In Proceedings of IEEE INFOCOM, pages 854  863, 2000 10] R. Cooley, B. Mobasher, and J. Srivastava. Data preparation for mining world wide web browsing patterns. Knowledge and Information Systems, 1\(1  32, 1999 11] C. R. Cunha, Azer Bestavros, and Mark E. Crovella. Characteristics of WWW Client Based Traces. Technical Report TR-95-010, Boston University, CS Dept, Boston, MA 02215, July 1995 12] D. Duchamp. Prefetching hyperlinks. In USENIX Symposium on Internet Technologies and Systems \(USITS  99 1999 13] V. Grassi. Prefetching Policies for Energy Saving and Latency Reduction in a Wireless Broadcast Data Delivery System. In ACM MSWIM 2000, Boston MA, 2000 14] S. Hameed and N. Vaidya. Ef?cient Algorithms for Scheduling Data Broadcast. ACM/Baltzer Wireless Networks \(WINET  193, May 1999 15] Q. Hu and D. Lee. Cache Algorithms based on Adaptive Invalidation Reports for Mobile Environments. Cluster Computing, pages 39  48, Feb. 1998 16] Z. Jiang and L. Kleinrock. An Adaptive Network Prefetch Scheme. IEEE Journal on Selected Areas in Communications, 16\(3  11, April 1998 17] V. Padmanabhan and J. Mogul. Using Predictive Prefetching to Improve World Wide Web Latency. Computer Communication Review, pages 22  36, July 1996 18] N. Vaidya and S. Hameed. Scheduling Data Broadcast in Asymmetric Communication Environments. ACM/Baltzer Wireless Networks \(WINET  182, May 1999 19] L. Yin, G. Cao, C. Das, and A. Ashraf. Power-Aware Prefetch in Mobile Environments. IEEE International Conference on Distributed Computing Systems \(ICDCS 2002 Proceedings of the 2004 IEEE International Conference on Mobile Data Management \(MDM  04 0-7695-2070-7/04 $20.00  2004 IEEE pre></body></html 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





