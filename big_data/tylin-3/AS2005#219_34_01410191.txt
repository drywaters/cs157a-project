Finding Recently Frequent Items in Distributed Data Streams Amit Manjhi  Vladislav Shkapenyuk Kedar Dhamdhere  Christopher Olston Carnegie Mellon University  manjhi vshkap kedar olston  cs.cmu.edu Abstract We consider the problem of maintaining frequency counts for items occurring frequently in the union of multiple distributed data streams Na  ve methods of combining approximate frequency counts from multi 
ple nodes tend to result in excessively large data structures that are costly to transfer among nodes To minimize communication requirements the degree of precision maintained by each node while counting item frequencies must be managed carefully We introduce the concept of a precision gradient for managing precision when nodes are arranged in a hierarchical communication structure We then study the optimization problem of how to set the precision gradient so as to minimize communication and provide optimal solutions that minimize worst-case communication load over all possible 
inputs W e then intr oduce a variant designed to perform well in practice with input data that does not conform to worst-case characteristics We verify the effectiveness of our approach empirically using real-world data and show that our methods incur substantially less communication than na  ve approaches while providing the same error guarantees on answers 1 Introduction The problem of identifying frequently occurring items in continuous data streams has attracted signiìcant attention recently 4 9 12 14 19 Potential applications include identifying large network ows answering iceber g queries 13 computing 
iceberg cubes and nding frequent itemsets and association rules Ho we v e r  nearly all prior w ork on identifying frequent items in data streams and estimating their occurrence frequencies falls short of meeting the needs of the many real-world applications that exhibit one or both of the following two properties 1 Distributed streams Streams originate from multiple distributed sources Data from all sources needs to be aggregated to arrive at the nal result as in the distributed streams model of   Supported by an ITR grant from the NSF 
 Supported by NSF ITR grants CCR-0085982 and CCR-0122581 2 Time sensitivity Recent data is more important than older data We brieîy describe two real-world applications exhibiting the properties just mentioned 1 Monitoring usage in large-scale distributed systems Web content providers using the services of a Content Delivery Network CDN like Akamai may wish to monitor recent access frequencies of content served e.g HTML pages/images to keep tabs on current hot spots The CDN may serve requests from any 
of a number of cache nodes Akamai currently has over 10,000 such nodes typically requests are served by the cache node closest to the end-user making the request in order to minimize latency Hence keeping tabs on overall access frequencies requires distributed monitoring across many CDN cache nodes 2 Detecting malicious activities in networked systems a Detecting worms Previously unknown Internet worms can be detected by discovering that a large number of recent trafìc ows contain the same bit string Distrib uted monitoring can reduce detection time b Detecting DDoS attacks 
Early detection of Distributed Denial of Service DDoS attacks is an important topic in network security While a DDoS attack typically targets a single victim node or organization there is generally no common path that all packets take In fact even packets sent to the same destination and originating from within the same organization may follow different routes due to so-called hot potato routing This property mak es it v ery dif cult to detect distributed denial of service attacks effectively by only considering the trafìc passing through any single monitoring point and motivates a distributed monitoring ap 
proach Furthermore techniques that weigh recent data more than past data may help in early detection of attacks 1.1 Problem Variants Both applications outlined above require algorithms for identifying recently frequent items in the union of many distributed streams and estimating the corresponding occurrence frequencies In general we can Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


classify applications of frequent item identiìcation into four categories in terms of whether they require a time-sensitivity and b distributed monitoring capability We brieîy describe each problem variant 1 Finding frequent items in a single stream A single node sees an ordered stream of possibly repeating items The goal is to maintain frequency counts of items whose frequency currently exceeds a user-supplied fraction of the size of the overall stream seen so far 2 Finding recently frequent items in a single stream In this variant recent occurrences of items in the stream are considered more important than older occurrences of items At any given time a numeric weight is associated with each item occurrence in the stream that is a function of the amount of time that has elapsed since the appearance of the item in the stream A commonly-used weighting scheme is exponential decay  in which weights are assigned according to a negative-exponential function of elapsed time The goal is to identify items whose cumulative weighted frequency currently exceeds a user-supplied fraction of the total across all items and provide an estimate of the cumulative weighted frequencies of any such items 3 Finding frequent items in the union of distributed streams In this variant there are m ordered streams S 1 S 2 S m  each produced at a different node in a distributed environment and consisting of a sequence of item occurrences The goal is the same as in Variant 1 except that item frequencies are computed over the union of streams S 1 S 2 S m  instead of over a single stream 4 Finding recently frequent items in the union of distributed streams This variant represents the natural combination of Variants 2 and 3 Of these four variants only Variant 1 has been studied in prior work Some work conducted concurrently with our own 4 also addresses problems quite similar to Variants 2 and 3 but there are signiìcant differences with our work see Section 4 for further discussion Algorithms for time-insensitive frequent item identiìcation over a single stream include those presented in 9,19 It is straightforw ard to e xtend these algorithms to handle Variant 2 although the effect on the space bounds and error guarantees of the resulting algorithms in some cases is nonobvious Variants 3 and 4 present a larger challenge As we will show simple adaptations of existing frequent item identiìcation algorithms to work in a distributed setting incur excessive communication In this paper we present a new framework for distributed frequent item identiìcation that minimizes communication requirements Before outlining our approach we rst provide a formal problem statement that uniìes the four variants listed above 1.2 Uniﬁed Problem Statement Our problem statement extends that of There are m  1 ordered data streams S 1 S 2 S m  Each stream S i consists of a sequence of item occurrences with time-stamps  o i 1 t i 1    o i 2 t i 2   etc Each item occurrence o ij is drawn from a xed universe U of items i.e  i j o ij  U  Arbitrary repetition of item occurrences in streams is allowed Each stream S i is monitored by a corresponding monitor node M i of which there are m  Monitored frequency counts for high frequency items are to be supplied to a central root node R  which may or may not be the same as one of the monitor nodes Let S be the sequence-preserving union of streams S 1 S 2 S m  Further let c  u  be the frequency of occurrence of item u in S up to the current time weighted by recency of occurrence in an exponentially decaying fashion Mathematically c  u    o i t i  S,o i  u   t now  t i  T  where t now denotes the current time and  and T are user-supplied parameters The parameter   0  1 controls the aggressiveness of exponential weighting As a special case setting  1 causes all item occurrences to be weighted equally regardless of age as in Variants 1 and 3 of Section 1.1 The parameter T 0 controls the frequency with which answers are reported and also the granularity of time-sensitivity A time period of T time units is referred to as an epoch  The objective is to supply at the end of every epoch i.e every T time units an estimate  c  u  of c  u  for items occurring in S whose true time-weighted frequency c  u  exceeds a support threshold T  T is deìned as the product of a user-supplied support parameter s  0  1  and the sum of the weighted item occurrences seen so far on all input streams N  u  U c  u   i.e T  s  N  The amount of allowable inaccuracy in the frequency estimates  c  u  is governed by a usersupplied parameter   It is required that 0    s usually   s  Each time an answer is produced it must adhere to the following guarantees 1 All items whose true time-weighted frequency exceeds s  N are output 2 No item whose true time-weighted frequency is less than  s     N is output 3 Each estimate  c  u  supplied in the answer satisìes max  0 c  u     N   c  u   c  u   Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


 R   1 1\nopses level 0 \(Root Output    synopsis level 1   M 1 level  \(l-1 Leaves   M 2 M d   M m  Input streams S 1 S 2 S d S m   2 1 synopses   l-1 1\synopses  Figure 1 Hierarchical communication structure A useful data structure for storing intermediate answers is an     synopsis of item frequencies over a stream or union of several streams An     synopsis S consists of a possibly empty set of time-weighted frequency estimates each denoted S  c  u   where each S  c  u  estimate satisìes max  0 c  u    S  n S   c  u   c  u   S  n denotes the total time-weighted frequency of all items in the synopsis  S  n   u  U c  u   The salient property of an     synopsis is that items with weighted frequency below  S  n need not be stored resulting in a reduced-size representation In the extended technical report version of this paper we sho w h o w to e xtend tw o frequenc y counting algorithms that produce   1 synopses to produce     synopses for any   0  1  to achieve Variant 2 of Section 1.1 In particular we show how to do so for lossy counting  and the algorithm presented in both and 19 which we refer to as majority  counting  We analyze the correctness and space requirements of the resulting algorithms We show that the worst-case size of time-sensitive synopses is bounded by a timeindependent constant 1.3 Overview of Approach There are two obvious simple strategies for adapting single-stream frequency counting algorithms to a distributed setting to achieve Variants 3 and 4 of Section 1.1 and both have serious drawbacks SS1 Simple Strategy 1 Periodically at the end of every epoch each monitor node M i sends to the root node R the exact frequency counts of all items occurring in S i over the last T time units Node R then combines the counts received from the monitor nodes with possibly time-decayed counts maintained over prior epochs and outputs items whose overall weighted counts exceed the support threshold T  SS2 Each monitor node M i maintains an   1 synopsis S i over the recent portion of its local stream S i  Intuitively the   1 synopsis is a reduced summary of item frequencies that does not include items whose frequency in S i is small Periodically at the end of every epoch each M i sends its local synopsis S i to node R  Upon receiving all local synopses node R combines them into a single uniìed   1 synopsis containing estimated item frequencies for the union of the contents of all input streams in the most recent epoch This synopsis is then combined additively with an     synopsis containing estimated weighted counts from previous epochs after multiplying those synopsis counts by   to generate a new     synopsis valid for the current epoch Lastly items whose estimated timedecayed counts exceed the support threshold T after taking into account the error tolerance in this synopsis are output 1 Clearly strategy SS1 is likely to incur excessive communication because frequency counts for all items including rare ones must be transmitted over the network Furthermore the root node R must process a large number of incoming counts While strategy SS2 alleviates load on the root node to some extent in the presence of a large number of monitor nodes and rapid incoming streams the root node may still represent a signiìcant bottleneck To further reduce the load on the root node nodes can be arranged in a hierarchical communication structure see Figure 1 in which synopses are combined additively at intermediate nodes as they make their way to the root In this setting SS2 compresses data by dropping small counts as much as possible at each leaf node without violating the  error bound Consequently no further compression can be performed as synopses are combined on their way to the root or at the root node itself making it impossible to eliminate counts for items whose frequency exceeds  fraction of one or more individual streams but does not exceed  fraction in the union of the streams whose synopses are combined at a non-leaf node Hence if input streams have different distributions of item occurrences counts for items of small frequency may reach the root node unnecessarily under strategy SS2 There are thus two main disadvantages of using SS2 1 High communication load on root node R  2 High space requirement on R  Suppose that instead of applying maximal synopsis compression at the leaf nodes some compression capability is reserved until synopses of multiple incoming streams are combined at non-leaf nodes If that is done more aggressive compression can be performed by nonleaf nodes by taking into account the distributions of item frequencies over a larger set of input streams As a 1 Note that in both strategies time-sensitivity is only introduced at node R  It is not possible to introduce time-sensitivity in data before it is sent to R  since all item frequencies in the most recent epoch have weight 1 in our formulation Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


result the synopses reaching the root and the synopsis maintained over previous epochs at the root will likely be signiìcantly smaller than in SS2 On the other hand the synopses passed from the leaf nodes to their parents may be larger than in SS2 which is an undesirable sideeffect Indeed to avoid excessive communication load on any particular node or link the amount of compression performed by each node while creating or combining synopses must be managed carefully In hierarchicallystructured monitoring environments we can conìgure the amount of compression performed and consequently the amount of error introduced at each level so that synopses follow a precision gradient as they ow from leaves to the root It turns out that worst-case communication load on any link is minimized by using a gradual precision gradient rather than either deferring the introduction of error entirely until data reaches the root as in SS1 or introducing the maximum allowable error at the leaf nodes as in SS2 Still the best gradual precision gradient to use is not obvious In Section 2 of this paper we study the problem of how best to set the precision gradient formally We rst show how use of a gradual precision gradient alleviates storage requirements at the root node R  Then we derive optimal settings of the precision gradient under two objectives a minimize load on the root node R  and b minimize maximum load on any single communication link under worst-case input behavior We then introduce a variant that aims to achieve low load on all links in practice when input data may not exhibit worstcase characteristics by exploiting a small sample of the expected input data obtained in advance In Section 3 we conìrm our analytical ndings of Section 2 through extensive experimental evaluation on three real-world data sets Our experiments demonstrate that na  ve methods of nding frequent items in distributed streams SS1 and SS2 can incur high communication and storage costs compared with our methods Related work is discussed in Section 4 and we summarize the paper in Section 5 2 Finding Frequent Items in Distributed Streams In this section we show how to maintain approximate time-sensitive frequency counts for frequent items in a distributed setting and study how to set the precision gradient so as to minimize communication Recall that in our scenario m monitor nodes M 1 M 2 M m relay data periodically once every T time units to a central root node R  Data may be relayed through a hierarchy of nodes interposed between the monitor nodes and the central root node as illustrated in Figure 1 Let l  2 denote the number of levels in the hierarchy We number the levels from root to leaf with the root node R of the communication hierarchy representing level 0  its children representing level 1  etc and the monitor nodes M 1 M m representing level  l  1 Let d  2 denote the fanout of all non-leaf nodes in the hierarchy i.e the number of child nodes relaying data to each internal node 2 In this hierarchical communication structure we associate with each non-root level 1  i   l  1 of the communication hierarchy an error tolerance  i For correctness it must be ensured that    1     l  1  0  which gives rise to a precision gradient along the communication hierarchy For now we assume that all nodes at the same level in the hierarchy use the same error tolerance Any values of  1  l  1 satisfying the above constraints can be used and the guarantees of Section 1.2 will hold The manner in which the precision gradient i.e  1  l  1 values is set determines the size of the synopsis that must be stored persistently at R  as well as the amount of communication that must be performed during frequency counting For now let us assume that some precision gradient has been decided upon We return to the issue of how best to set the precision gradient in Section 2.1 Given a precision gradient our procedure for computing time-sensitive frequency counts for items occurring frequently in S  S 1  S 2    S m is as follows Recall that time is divided into equal epochs of length T  During each epoch each monitor node M i invokes a single-stream approximate frequency counting algorithm e.g 9 19 using error parameter  l  1 to generate an   l  1  1 synopsis for the portion of stream S i seen so far during the current epoch Each monitor node then sends its   l  1  1 synopsis to its parent in the communication hierarchy which combines the d   l  1  1 synopses it receives from its d children into a single   l  2  1 synopsis using either Algorithm 1a shown below based on lossy counting or Algorithm 1b shown below based on majority  counting 9 The same process is repeated until each of R s children combines the d   2  1 synopses they receive into an   1  1 synopsis which is then sent to R  The root node R maintains at all times a single     synopsis S A  from which the answer is derived When at the end of each epoch R receives d   1  1 synopses from its children R updates S A using either Algorithm 2a based on lossy counting or Algorithm 2b based on majority  counting Then R generates the new answer to be output for the current epoch by nding items in S A whose approximate count in S A exceeds  s    S A  n   2 For simplicity we assume all internal nodes of the communication hierarchy have the same fanout Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


 Algorithm 1 Combine synopses from children executed by nodes other than leaves and root Inputs d   i 1  1 synopses S 1  S 2    S d Output single   i  1 synopsis S Algorithm 1a 1 Set S  n  d  j 1 S j  n 2 For each u  d  j 1 S j  set S  c  u  d  j 1 S j  c  u  3 For each u S  set S  c  u  S  c  u     i   i 1  S  n Algorithm 1b 1 For each S j S 1  S 2  S d  and for each u S j  a If S  c  u  exists set S  c  u  S  c  u  S j  c  u   Else create S  c  u   set S  c  u  S j  c  u  b If S  1  i   i 1  let u   argmin u S S  c  u    For each u S  set S  c  u  S  c  u  S  c  u   if S  c  u   0  eliminate count S  c  u  2 Set S  n  d  j 1 S j  n 2.1 Setting the Precision Gradient Our approach is rst to set  1 based on space considerations at node R using worst-case analysis and then set the remaining error tolerance values  2  l  1 so as to minimize communication The value of  1 determines the maximum size of the synopsis S A that must be stored by node R at all times If Algorithm 2b is used by the root node the size of S A is at most 1    1 counts at all times Otherwise if Algorithm 2a is used analysis of the maximum size of S A is similar to the analysis of and our o w n analysis in of time-sensiti v e lossy counting o v e r a single-stream yielding the following results If no timesensitivity is employed   1  the size of S A is at most ln     1  S A  n     1 counts formula adapted from for  1  the size is at most 1    1   3+ln 2  k    k     1 counts where    log 1  1  2    1  012 1 and k denotes the maximum number of item occurrences on any input stream during any single epoch As long as stream rates remain steady using  1   the synopsis S A does not grow with time after reaching a steady-state size In contrast when  1   as in strategy SS2 the space requirement increases with time as we demonstrate empirically in Section 3.3 Our approach is to set  1 such that the worst-case size of S A under the maximum possible stream rate k  is below any space constraint at R  Given a value for  1 such that  1   the remaining error tolerance values  2  l  1 making up the precision gradient determine the communication load incurred We illustrate the effect of the precision gradient Algorithm 2 Update the answer synopsis executed at the root node R  Input d   1  1 synopses S 1  S d  S A Output new answer     synopsis S A Algorithm 2a 1 Set S A  n   S A  n  d j 1 S j  n 2 For each u S A  set S A  c  u   S A  c  u  3 For each u  d  j 1 S j  set S A  c  u  S A  c  u  d j 1 S j  c  u  4 For each u S A  set S A  c  u  S A  c  u       1    d j 1 S j  n Algorithm 2b 1 Set S A  n   S A  n  d j 1 S j  n 2 For each u S A  set S A  c  u   S A  c  u  3 For each S j S 1  S 2  S d  and for each u S j  a If S A  c  u  exists set S A  c  u  S A  c  u  S j  c  u   Else create S A  c  u   set S A  c  u  S j  c  u  b If S A  1    1  let u   argmin u S A S A  c  u    For each u  S A  set S A  c  u  S A  c  u  S A  c  u   if S A  c  u   0  eliminate count S A  c  u  R I 2 I 1 M 4 M 3 M 2 M 1 S 1 S 2 S 3 S 4   2 1   1 1 Input streams Monitor nodes Root node Output     Figure 2 Example topology on communication using the following rather contrived but simple example that highlights the effect clearly our experimental results presented later in Section 3 are conducted over real-world data 2.1.1 Motivating Example Figure 2 shows the communication topology we use for our example We assume Algorithm 1a is used at the intermediate nodes Suppose the overall user-speciìed error tolerance  0  05  and for simplicity assume  1   0  05  Suppose that during one epoch 100 items occur on each of S 1 S 2 S 3 and S 4  drawn from a universe of 27 distinct items For ease of comprehension we partition the 27 distinct items into three categories A B and C Category A contains one item and categories B and C each contain 13 The frequency of occurrence in each input stream of items in each category is given in the shaded region of Table 2 The Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


Table 1 Communication loads in example scenario  Load on Maximum load on any Maximum  2 root node R link excluding load on links to R any link 0 2 27 27 0.03 2 14 14 0.05 54 14 27 Table 2 Link loads in example scenario M 1  I 1 and M 2  I 1  I 1  R  M 3  I 2 M 4  I 2 I 2  R  2 category frequency cat freq cat freq estimate est est 0 A 9 A 9 A 8 B 6 B 1 C 1 C 6 0.03 A 6 A 6 A 8 B 3 C 3 0.05 A 4 A 4 A 8 B 1 C 1 B 1 C 1 single item in category A occurs nine times in each of S 1 S 2 S 3 and S 4  Each item in category B occurs six times each in S 1 and S 3 but only once each in S 2 and S 4  The opposite is true for items in category C each occurs once in each of S 1 and S 3 but six times in each of S 2 and S 4  Table 1 summarizes the effects of varying  2  which determines the amount of error introduced at level 2 nodes M 1 M 4  assuming lossy counting with perepoch batch processing is used to produce the initial synopses at the leaf nodes Three measures of communication load are reported 1 load on the root node R  2 maximum load on any link excluding links to R  and 3 maximum load on any link In all cases communication load is measured in terms of the number of frequency counts transmitted during the epoch Setting  2 0  05 corresponds to simple strategy SS2 outlined in Section 1.3 We do not report measurements for SS1 in which  1 0 and  2 0  since communication load is higher than under any of our three example strategies under all three metrics To understand how these numbers come about consider Table 2 which shows for each setting of  2  the frequency estimate for items of each category sent along each link In the case in which  2 0  the estimated counts sent from leaf nodes M 1 M 4 to nodes I 1 and I 2 shown with shaded background are exact All other values in Table 2 are underestimates We focus on the case in which  2 0  03 to illustrate how these underestimates are computed At each leaf node when  2 0  03 application of the lossy counting algorithm leads to undercounting of each itemês frequency by  2  100  0  03  100  3  Hence estimated counts transmitted in synopses from the leaf nodes M 1 M 4 to nodes I 1 and I 2 are less than their actual counts by 3  some counts fall below zero and are eliminated Once these synopses are received at nodes I 1 and I 2  Algorithm 1a is invoked in which synopsis counts received from leaf nodes are rst combined additively and then decremented by   1   2   200  0  02  200  4  For the single item in Category A leaf nodes M 1 and M 2 each supply a count of 6 to node I 1  for a combined count of 12  which is then decremented by 4 for a nal estimated count of 8 to be sent to node R  Items in Categories B and C each have combined counts of 3 at I 1  which fall below zero when decremented by 4 and thus are not transmitted to R  From Table 1 we observe a tradeoff between communication load on the root node R and load on links not connected to R  Furthermore in this particular case although not always true in general of our three example strategies the strategy of using a gradual precision gradient   2 0  03  is best with respect to all three metrics To see why consider that if error tolerances are made large for levels of the communication hierarchy close to the leaves in the most extreme case by setting  l  1    as in SS2 some locally-infrequent items are eliminated early thereby reducing communication near the leaves However an undesirable side-effect arises in the presence of items just frequent enough at one or more leaf nodes to survive elimination locally but not frequent enough overall to exceed the error threshold as with items in categories B and C in our example Counts for such items may avoid being eliminated until very late or worse may never be eliminated thus resulting in increased communication near the root Hence there is a tradeoff between high communication among non-root nodes and heavy load on the root node R  The best way to set the precision gradient depends on the application scenario For some applications the most important criterion may be to minimize load on the root node R where the answers are generated which may need to devote the majority of its resources to other critical tasks for the application even if that means increased load on the nodes responsible for monitoring streams and merging synopses For other applications it is most important to minimize the maximum load on any link to ensure that large volumes of input data can be handled without overloading network resources Next we study the optimization problem of how best to select the precision gradient and synopsis-merging algorithm to use at each node in order to achieve one of two objectives 1 minimize communication load on the root node R  or 2 minimize worst-case communication load on the most heavily-loaded link in the hierarchy Communication load is measured in terms of Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


the number of frequency counts transmitted during one epoch We study each optimization objective in turn in Sections 2.1.2 and 2.1.3 and provide optimal algorithm choices and settings for the error tolerances  2  l  1 making up the precision gradient Then since realworld data is unlikely to exhibit worst-case behavior in Section 2.1.4 we propose a variant that seeks to achieve low load on the most heavily-loaded link under nonworst-case inputs for which estimated data distributions are available 2.1.2 Minimizing Total Load on the Root Node Using Algorithm 1a at all applicable nodes and setting  i 0 for all 2  i  l  1  whereby all decrementing and elimination of synopsis counts is performed by children of root node R  minimizes communication load on the root node R under any input streams We term this strategy MinRootLoad Lemma 1 Given a value for  1  for any input streams no values of  2  l  1 satisfying  1   2     l  1 and no choice of synopsis-merging algorithm results in lower total communication load on node R than the values  2   3     l  1 0 and Algorithm 1a assuming buffer space at each node is sufcient to store all inputs arriving during one epoch Proof Consider node X  an arbitrary child of the root node R  Let S X denote the union of all streams arriving at the monitor nodes belonging to the subtree rooted at X during one epoch Since an   1  1 synopsis is sent from X to R  for any setting of  2  l  1  counts for all items v with frequency c  v    1  S X  are sent over the link from X to R here  S X  denotes the number of item occurrences in S X  Using  2   3     l  1 0 and Algorithm 1a at X it is easy to see that an item u will be sent over the link from X to R only if c  u    1  S X   Therefore this setting of  2  l  1 along with the use of Algorithm 1a results in the smallest possible number of counts sent over the link from X to R  Since this property holds for any child X of R  strategy MinRootLoad minimizes the total communication load on R  for any input streams  2.1.3 Minimizing Worst-Case Maximum Load on Any Link In this section we show how to set  2  l  1 and how to select a synopsis-merging algorithm to use at each node so as to minimize the maximum load on any communication link in the worst case over all possible input streams We provide a two step solution First we show that for any precision gradient  2  l  1  use of Algorithm 1a at each node minimizes the load on every link provided buffer space at each node is sufìcient to store all inputs arriving during one epoch Then we derive the optimal precision gradient when Algorithm 1a is used at each node We begin with the issue of selecting a synopsismerging algorithm Observation 1 If presented with identical inputs Algorithm 1b produces output S and Algorithm 1a produces output S   then S  n  S   n and for all items u S  S  c  u  S   c  u   Observation 2 Consider two sets of inputs to one of Algorithm 1a or Algorithm 1b Let input 1  S 1  S 2  S d   and input 2  S  1  S  2  S  d  where for all j  1  j  d  S j  n  S  j  n and for all items u S  j  S j  c  u  S  j  c  u   Let input 1 lead to output S  whereas input 2 lead to output S   Then S  n  S   n and for all items u S  S  c  u  S   c  u   Lemma 2 At any node X use of Algorithm 1a results in no higher communication on any link than use of Algorithm 1b Proof Follows from Observation 1 and multiple invocations of Observation 2  Lemma 3 Given a choice between Algorithms 1a and 1b under any precision gradient use of Algorithm 1a at each node minimizes the maximum load on any link Proof Follows from Lemma 2  It is trivial to extend this result to include leaf nodes replacing Algorithm 1a with the original lossy counting algorithm Next we show how to set  2  l  1 assuming Algorithm 1a is used at each node and the lossy counting algorithm is used to generate the local synopsis at each monitor node We also assume the buffer each monitor node uses for lossy counting is large enough to store frequency counts of all items arriving on the input stream during any one epoch As we later conìrm in Section 3 this assumption poses no problem in practice particularly if the epoch duration is small For our worst-case analysis we extend the set of possible inputs in two minor ways 1 The occurrence frequency of an item arriving on an input stream can be a positive real number 2 Associated with each item u is a weight w u  0  1  In an epoch at most one item occurrence per input stream can be an occurrence of an item of weight less than 1 The cost of transmitting the count of item u with weight w u is w u  In a synopsis S  n   w u  c  u   Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


As will become clear later both of these enhancements allow load on a link to be expressed as a continuous function which in turn simpliìes our worst-case analysis Neither enhancement alters the worst-case input signiìcantly First during an epoch at most one item occurrence per input stream can have non-integral weight Second any input with real-valued item frequencies can be transformed into an input with nearly integral frequencies that yields identical results by multiplying each frequency by a large number and dividing all answers produced by the same number For notational ease we transform the problem of setting  2  l  1 to that of setting  2   l  1  where for all 2  i  l  2   i   i   i 1 and  l  1   l  1  It is required that  i  0 for all 2  i  l  1  and that  l  1 i 2  i   1   i denotes the precision margin at level i  i.e the difference between the error tolerances at level i and level i 1  Let the vector     2   3   l  1  Let I denote the contents of all input streams S 1 S m during a single epoch Let I denote the set of all possible instances of I  Given an input I  a communication hierarchy T deìned by degree d and number of levels l  and a setting of the precision gradient   let w represent the maximum load on any link in the communication hierarchy w  I T    max k  links     load  k   Worst-case load W is deìned as W  T    max I I  w  I T    Given a communication hierarchy T  the objective is to set  such that the worst-case load W  T   is minimized We rst show that it is sufìcient to consider a speciìc subset of all instances of the general problem for worstcase analysis Then we nd precision gradient values  values that cause the worst-case load under any of these instances to be minimal There exists a subset I wc of the set of all input instances I such that for all instances I IäI wc  there exists an instance I  I wc such that for any T    w  I   T    w  I T    Hence I wc denotes the set of worst-case inputs  Instance I is a member of I wc if and only if it satisìes each of the following three properties P1 For any two input streams S i and S j  there is no item occurrence common to both S i and S j  P2  For any input stream S i  all items occurring in S i occur with equal frequency P3  For any two input streams S i and S j  both the number of item occurrences and the number of distinct items in S i and S j are equal Lemma 4 For xed T and   given any input instance I  it is possible to nd an input instance I  I wc such that w  I   T    w  I T    Proof Our proof of Lemma 4 is rather involved and is provided in  From Lemma 4 we know it is sufìcient to consider the set I wc for worst-case communication load Hence we can rewrite our expression for W  T   as W  T    max I I wc  w  I T    Property P3 of I wc implies that the total number of item occurrences at any leaf node is the same Let n denote this number   S i   n for all 1  i  m  Let tc  j  denote the total number of item occurrences arriving on streams monitored by at the leaf nodes of a subtree rooted at a node at level j  It is easy to see that tc  j  d  l  1  j   n  where l is the number of levels in the communication hierarchy and d is the fanout of all non-leaf nodes The next lemma shows that worst-case inputs induce a high degree of symmetry on the resulting synopses Lemma 5 For any input instance I I wc  the following two properties hold for the d j   j  1 synopses relayed by the d j levelj nodes to their parents 1 No item is present in more than one synopsis 2 The estimated frequency counts corresponding to any two items even if present in two different synopses have the same value Proof See  Due to the high degree of symmetry formalized in Lemma 5 the count for each item is eliminated due to being decremented and falling below zero at the same level of the communication hierarchy Let us call this level x  If all counts are dropped at the leaf level then x  l  1  If all counts are retained through the entire process and are sent to the root node R level 0  then x 0  Otherwise all counts are dropped at some intermediate level 1  x  l  2  The most heavily loaded link\(s are the ones leading to level x  To see why consider that no data is transmitted on subsequent links and previous links have lower load since data is spread more thinly in any communication hierarchy T  the number of links between levels decreases monotonically as data moves from leaves to the root When synopses are combined at nodes of level i using Algorithm 1 the frequency count estimate of each item is decremented by the quantity tc  i    i let Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


 1   1   l  1 i 2  i  Hence the true frequency count of any item occurring on some input stream must be C  l  1 j  x 1  tc  j    j    where  is a small quantity 3  The number of items present in each input stream is thus n  C 4  Since synopses for d l  1  x input streams are transmitted through a node at level x  the load on the most heavily loaded link\(s is L  x  d l  2  x  n C  Clearly the maximum value of L  x  is achieved when   0  The expression for L  x  can be simpliìed to L  x  1  l  1 j  x 1  j  d x  j 1  Now our expression for the worst-case load on any link can be reduced to W  T    max x 0  1 l  2  L  x   We desire to minimize W  T   subject to the constraints  2   l  1  0 and  l  1 j 2  j   1 It is easy to show that this minimum is achieved when L 0  L 1    L  l  2  Solving for  2   l  1  we obtain  i   1  d  1  l  2   d  1 d  2  i  l  2 and  l  1   1  d  l  2   d  1 d  Translating to error tolerances we set  i   1   l  1  i    d  1 d  l  2   d  1 d for all 2  i  l  1  This setting of  2  l  1 minimizes worst-case communication load on any link We term this strategy MinMaxLoad WC Under this strategy the maximum possible load on any link is L wc   l  2   d  1 d d   1 counts per epoch Lastly we note that MinMaxLoad WC remains the optimal precision gradient even if nodes of the same level can have different  values Informally since with worst-case inputs all incoming streams have identical characteristics maximum link load cannot be improved by using non-uniform  values for nodes at a given level we omit a formal proof for brevity 2.1.4 Good Precision Gradients for Non-Worst-Case Inputs Real data is unlikely to exhibit worst-case characteristics Consequently strategies that are optimal in the worst case may not always perform well in practice In terms of minimizing the maximum communication load on any link the worst-case inputs are ones in which the set of items occurring on each input stream are disjoint When this situation arises a gradual precision gradient is best to use as shown in Section 2.1.3 Using a gradual precision gradient some of the pruning of frequency 3 Recall that we allow the frequency of an item to be a real number 4 More precisely each stream contains  n C  items of weight 1 each and one item of weight  n C  n C   Note that each input stream contains at most one item with weight less than 1 as stipulated earlier counts is delayed until a better estimate of the overall distribution is available closer to the root thereby enabling more effective pruning In the opposite extreme when all input streams contain identical distributions of item occurrences there is no beneìt to delaying pruning and performing maximal pruning at the leaf nodes as in strategy SS2 is most effective at minimizing communication In fact it is easy to show that SS2 is the optimal strategy for minimizing the maximum load on any link when all input streams are comprised of identical distributions we omit a formal proof Note however that SS2 still incurs a high space requirement on the root node R since it sets  1    We posit that most real-world data falls somewhere between these two extremes To determine where exactly a data set lies with regard to the two extremes we estimate the commonality between input streams S 1 S m by inspecting an epoch worth of data from each stream We compute a commonality parameter   0  1 as   1 m   m i 1 G i L i  where G i and L i are deìned over stream S i as follows The quantity G i is deìned as the number of distinct items occurring in S i that occur at least   S i  times in S i and also at least   S  times in S  S 1  S 2  S m  where  S  denotes the number of item occurrences in S during the epoch of measurement The quantity L i is deìned as the number of distinct items occurring in S i that occur at least   S i  times in S i  Hence commonality parameter  measures the fraction of items frequent enough in one input stream to be included in a leaf-level synopsis by strategy SS 2 that are also at least as frequent globally in the union of all input streams A natural hybrid strategy is to use a linear combination of MinMaxLoad WC and SS2 weighted by   The strategy is as follows set  i 1       1   l  1  i    d  1 d  l  2   d  1 d        for 2  i   l  2  and  l  1 1       1  d  l  2   d  1 d         We term this hybrid strategy MinMaxLoad NWC for non-worstcase Commonality parameter  1 implies that locally frequent items are also globally frequent and SS2 modiìed to use  1   is a good choice Conversely  0 indicates that MinMaxLoad WC is a good choice For 0  1  a weighted mixture of the two strategies is best 2.1.5 Summary The precision gradient strategies we have introduced are summarized in Table 3 Sample precision gradients are illustrated in Figure 3 3 Experimental Evaluation In this section we evaluate the performance of our newly-proposed strategies for setting the precision graProceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


Table 3 Summary of precision gradient settings studied  Strategy Description Section Introduced Simple Strategy 1 SS1 Transmits raw data to root node R 1.3 Simple Strategy 2 SS2 Reduces data maximally at leaves 1.3 MinRootLoad Minimizes total load on root in all cases 2.1.2 MinMaxLoad WC Minimizes worst-case maximum load on any link 2.1.3 MinMaxLoad NWC Achieves low load on heaviest-loaded link under non-worst-case inputs 2.1.4 0 0.0002 0.0004 0.0006 0.0008 0.001 43210 Tree level \(i SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC Error tolerance  i input            leaf root Figure 3 Precision gradients for  0  001   0  5  dient using the two na  ve strategies suggested in Section 1 as baselines We begin in Section 3.1 by describing the real-world data and simulated distributed monitoring environment we used Then in Section 3.2 we analyze the data using our model of Section 2.1.4 to derive appropriate parameters for our MinMaxLoad NWC strategy that is geared toward performing in the presence of non-worst-case data We report our measurements of space utilization on node R in Section 3.3 and provide measurements of communication load in Section 3.4 3.1 Data Sets As described in Section 1 our motivating applications include detecting DDoS attacks and monitoring hot spots in large-scale distributed systems For the rst type of application we used trafìc logs from Internet2 and sought to identify hosts recei ving lar ge numbers of packets recently For the second type we sought to identify frequently-issued SQL queries in two dynamic Web application benchmarks conìgured to execute in a distributed fashion The I NTERNET 2 traf c traces were obtained by collecting anonymized netîow data from nine core routers of the Abilene network Data were collected for one full day of router operation and were broken into 288 ve-minute epochs To simulate a larger number of nodes we divided the data from each router in a random fashion We simulated an environment with 216 network nodes which also serve as monitor nodes For the web applications we used Java Servlet versions of two publicly available dynamic Web application benchmarks RUBiS and R UBBoS 10 R U BiS is modeled after eBay an online auction site and RUBBoS is modeled after slashdot an online bulletin-board so we refer them as AUCTION and BBOARD  respectively We used the suggested conìguration parameters for each application and ran each benchmark for 40 hours on a single node.We then partitioned the database requests into 216 groups in a roundrobin fashion honoring user session boundaries We simulated a distributed execution of each benchmark with 216 nodes each executing one group of database requests and also serving as a monitor node For all data sets we simulated an environment with 216 monitoring nodes  m  216  and a communication hierarchy of fanout six  d 6  Consequently our simulated communication hierarchy consisted of four levels including the root node  l 4  We set s 0  01   0  1  s  and  1 0  9    Our simulated monitor nodes used lossy counting in batch mode whereby frequency estimates were reduced only at the end of each epoch in all cases less than 64KB of buffer space was used to create synopses over local streams The epoch duration T was set to 5 minutes for the I NTERNET 2 data set and 15 minutes for the other two data sets 3.2 Data Characteristics Using samples of each of our three data sets we estimated the commonality parameter  for each data set Recall that we use  to parameterize our strategy MinMaxLoad NWC presented in Section 2.1.4 We obtained  values of 0.675 0.839 and 0.571 for the I NTER NET 2 AUCTION and BBOARD data sets respectively Hence the AUCTION data set exhibited the most commonality among all three data sets Results presented in Section 3.4 show that AUCTION indeed has the most commonality 3.3 Space Requirement on Root Node Figure 4 plots space utilization at the root node R as a function of time in units of epochs using Algorithm 2a to generate the synopsis for different values of the decay parameter   using two different strategies for the precision gradient The plots shown are for the I N TERNET 2 data set The y-axis of each graph plots the current number of counts stored in the     synopsis S A maintained by the root node R  Figure 4a plots synopsis size under our MinMaxLoad WC strategy under three different values of   0.6 0.9 and 1 As predicted by our analysis in when  1 the size of S A remains roughly constant after reaching steady-state whereas when  1 synopsis size increases logarithmically with time similar results were obtained for the Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


 0 100 200 300 400 500 600 1 21416181 Time \(epoch counts 0.6 0.9 1.0 a MinMaxLoad WC 0 1000 2000 3000 4000 5000 6000 7000 8000 1 21416181 Time \(epoch counts 0.6 b SS2 Figure 4 Space needed at node R to store answer synopsis S A  non-distributed single-stream case In contrast when SS2 is used to set the precision gradient Figure 4b the space requirement is almost an order of magnitude greater This difference in synopsis size occurs because in SS2 frequency counts are only pruned from synopses at leaf nodes so counts for all items that are locally frequent in one or more local streams reach the root node No pruning power is reserved for the root node and therefore no count in S A is ever discarded irrespective of the  value The same situation occurs if Algorithm 2b is used instead of Algorithm 2a This result underscores the importance of setting  1  in order to limit the size of S A  as discussed in Section 2.1 3.4 Communication Load Figure 5 shows our communication measurements under each of our two metrics for each of our three data sets under each of the ve strategies for setting the precision gradient listed in Table 3 First of all as expected the overhead of SS1 is excessive under both metrics Second by inspecting Figure 5a we see that strategy MinRootLoad does indeed incur the least load on the root node R in all cases as predicted by our analysis of Section 2.1.2 Under this metric MinRootLoad outperforms both simple strategies SS1 and SS2 by a factor of 0 500 1000 1500 2000 2500 3000 3500 4000 INTERNET2 AUCTION BBOARD SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC 247k 10k 296k 132k 20k counts transmitted \(per epoch a Load on root node R 0 500 1000 1500 2000 2500 3000 3500 4000 INTERNET2 AUCTION BBOARD SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC 43k    21k 52k   19k 23k   7k counts transmitted \(per epoch b Maximum load on any link Figure 5 Communication measurements k denotes thousands ve or more in all cases measured However MinRootLoad performs poorly in terms of maximum load on any link as shown in Figure 5b because no early elimination of counts for infrequent items is performed and consequently synopses sent from the grand-children of the root node to the children of the root node tend to be quite large As expected MinMaxLoad NWC performs best under that metric on all data sets For the AUCTION data set even though SS2 outperforms MinMaxLoad WC to be expected because of the high  value our hybrid strategy MinMaxLoad NWC is superior to SS2 by a factor of over two For the I NTERNET 2 and BBOARD data sets the improvement over SS2 is more than a factor of three On the negative side total communication not shown in graphs is somewhat higher under MinMaxLoad WC than under SS2 increase of between 7.5 and 49.5 depending on the data set 4 Related Work Most prior work on identifying frequent items in data streams 6 8 9 19 only considers the single-stream case While we are not aware of any work on maintaining frequency counts for frequent items in a distributed stream setting work by Babcock and Olston does adProceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


dress a related problem In the problem is to monitor continuously changing numerical values which could represent frequency counts in a distributed setting The objective is to maintain a list of the top k aggregated values where each aggregated value represents the sum of a set of individual values each of which is stored on a different node The work of assumes a single-le v e l communication topology and does not consider how to manage synopsis precision in hierarchical communication structures using in-network aggregation which is the main focus of this paper The work most closely related to ours is the recent work of Greenwald and Khanna which addresses the problem of computing approximate quantiles in a general communication topology Their technique can be used to nd frequencies of frequent items to within a conìgurable error tolerance The work in focuses on providing an asymptotic bound on the maximum load on any link our result adheres to the same asymptotic bound It does not however address how best to conìgure a precision gradient in order to minimize load which is the particular focus of our work 5 Summary In this paper we studied the problem of nding frequent items in the union of multiple distributed streams The central issue is how best to manage the degree of approximation performed as partial synopses from multiple nodes are combined We characterized this process for hierarchical communication topologies in terms of a precision gradient followed by synopses as they are passed from leaves to the root and combined incrementally We studied the problem of nding the optimal precision gradient under two alternative and incompatible optimization objectives 1 minimizing load on the central node to which answers are delivered and 2 minimizing worst-case load on any communication link We then introduced a heuristic designed to perform well for the second objective in practice when data does not conform to worst-case input characteristics Our experimental results on three real-world data sets showed that our methods of setting the precision gradient are greatly superior to na  ve strategies under both metrics on all data sets studied Acknowledgments We thank Arvind Arasu and Dawn Song for their valuable input and assistance References  R Agra w a l and R Srikant F ast algorithms for mining association rules In VLDB  1994  I Akamai T echnologies Akamai http://www akamai.com   A Ak ella A Bharambe M Reiter  and S Seshan Detecting DDoS attacks on ISP networks In PODS Workshop on Management and Processing of Data Streams  2003  A Arasu and G S Manku Approximate quantiles and frequency counts over sliding windows In PODS  2004  B Babcock and C Olston Distrib uted top-k monitoring In SIGMOD  2003  M Charikar  K  Chen and M F arach-Colton Finding frequent items in data streams In International Colloquium on Automata Languages and Programming  2002  E Cohen and M Strauss Maintaining time-decaying stream aggregates In PODS  2003  G Cormode and S Muthukrishnan What s hot and whatês not Tracking frequent items dynamically In PODS  2003  E D Demaine A Lopez-Ortiz and J I Munro Frequency estimation of internet packet streams with limited space In European Symposium on Algorithms  2003  DynaServ er R UBis and R UBBos http://www.cs rice.edu/CS/Systems/DynaServer   eBay Inc eBay  http://www.ebay.com   C Estan and G V a r ghese Ne w directions in traf c measurement and accounting In SIGCOMM  2002  M F ang N Shi v akumar  H  Garcia-Molina R Motwani and J Ulmann Computing iceberg queries efìciently In VLDB  1998  P  B Gibbons and Y  Matias Ne w sampling-based summary statistics for improving approximate query answers In SIGMOD  1998  P  B Gibbons and S T irthapura Estimating simple functions on the union of data streams In Symposium on Parallel Algorithms and Architectures  2001  M Greenw ald and S Khanna Po wer conserving computation of order-statistics over sensor networks In PODS  2004  J Han J Pei G Dong and K W ang Ef cient computation of iceberg queries with complex measures In SIGMOD  2001  Internet2 Internet2 Abilene Netw ork http abilene.internet2.edu   R M Karp S Shenk er  and C H P apadimitriou A simple algorithm for nding frequent elements in streams and bags ACM Trans Database Syst  2003  H.-A Kim and B Karp Autograph T o w ard automated distributed worm signature detection In Proceedings of the 13th Usenix Security Symposium  2004  A Manjhi V  Shkapen yuk K Dhamdhere and C Olston Finding recently frequent items in distributed data streams Technical report 2004 http://www cs.cmu.edu/òmanjhi/freqItems.pdf   G S Manku and R Motw ani Approximate frequenc y counts over data streams In VLDB  2002  Open Source De v elopment Netw ork Inc Slashdot http://slashdot.org  Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


12] S. Fujiwara, J. D. Ullman and R. Motwani, Dynamic Miss-Counting Algorithms: Finding Implications and Similarity Rules with Confidence Pruning. Proceedings of the IEEE ICDE \(San Diego, CA 13] F. Glover  Tabu Search for Nonlinear and Parametric Optimization \(with Links to Genetic Algorithms  Discrete Applied Mathematics 49 \(1-3 14] M. Klemettinen, H. Mannila, P. Ronkainen, H Toivonen, and A. Verkamo, Finding interesting rules from large sets of discovered association rules. Proceedings of the ACM CIKM, International Conference on Information and Knowledge Management \(Kansas City, Missouri 1999 15] C. Ordonez, C. Santana, and L. de Braal, Discovering Interesting Association Rules in Medical Data. Proceedings of the IEEE Advances in Digital Libraries Conference Baltimore, MD 16] W. Perrizo, Peano count tree technology lab notes Technical Report NDSU-CS-TR-01-1, 2001 http://www.cs.ndsu.nodak. edu /~perrizo classes/785/pct.html. January 2003 17] Ron Raymon, An SE-tree based Characterization of the Induction Problem. Proceedings of the ICML, International Conference on Machine Learning \(Washington, D.C 275, 1993 18] N. Shivakumar, and H. Garcia-Molina, Building a Scalable and Accurate Copy Detection Mechanism Proceedings of the International Conference on the Theory and Practice of Digital Libraries, 1996 19] P. Tan, and V. Kumar, Interestingness Measures for Association Patterns: A Perspective, KDD  2000 Workshop on Post-processing in Machine Learning and Data Mining Boston, 2000 20] H.R. Varian, and P. Resnick, Eds. CACM Special Issue on Recommender Systems. Communications of the ACM 40 1997 Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE pre></body></html 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


