Map-based BitSet Association Rule 
 Mining of Remote Sensing Images Huang Hongyu, Huang Duanqiong, Ch en ChongCheng and Fan Minghui Key Laboratory of Data Mining and Information Sharing of Ministry of Education, Spatial  Information Research Center, Fuzhou University, Fuzhou, 350002, China  Abstract This paper presents MBSA algorithm, which uses TreeMap class and a compressed BitSet class in Java. Boolean values are stored in the Bitset. MBSA algorithm scans the transaction database once and further database scans can be 
replaced by Bitset logical AND operation, which efficiently improves the computation. MBSA algorithm was applied to crop remote sensing image for association rules mining. Interesting association rules discovered from remote sensing image composed of Red, Green and Blue bands and crop yield is useful for improving crop production  Keywords  Association rule mining, data mining remotely sensed imagery  I  INTRODUCTION Association rule mining is one of important tasks in the area of data mining. The initial application of association rule 
mining was on market basket n a s s o ciatio n ru le i s a relationship of the form X=>Y, where X and Y are sets of items. X is called the antecedent and Y the consequence. An example of the rule can be, \223customers who purchase an item X are very likely to purchase another item Y at the same time\224 There are two primary quality measures for each rule, support and confidence. The rule X=>Y has support s% in the transaction set D if s% of transactions in D contains X and Y The rule has confidence c% if c% of transactions in D that contain X also contain Y. The goal of association rule mining is to find all the rules that exceed user specified support and confidence thresholds  The following are characteristics of association rule mining on remotely sensed imagery 
 In remotely sensed imagery, each pixel can be considered as a transaction   Each transaction \(pixel\has the same number of items which is the number of bands  Special types of association rules: we are particularly interested in the kinds of rules where the left side of the rule is a combination of band intervals while the right side of the rule is a particular interval of yield band. For example: B11 ^ B23 ^ B34 => B41, Where B11={Band1 0-63}, B23={Band2, 128-191}, B34={Band3, 192-255 B41={Yield, 0-63 Based on these characteristics, we propose a new approach which uses bitset to organize remote sensing image and TreeMap structure to represent bitset in a data-mining-ready 
way. Efficiency is an important issue for a mining algorithm The performance improvement is an essential concern for a particular algorithm because mining association rules may require multiple scan of database. By using bitset and Treemap association rule mining algorithm only scans transaction database once with fast computation  This paper is organized as follows. Section 2, a modified Apriori MBSA algorithm is described in detail. Experiment results and performance analysis are given in Section 3 Finally the MBSA algorithm is applied to crop remote sensing image for association rules mining   II  MBSA ALGORITHM 
Given a user specified minimum support \(minsup\d confidence \(minconf\he task is to find association rules where support and confidence are at least minsup and minconf This can be broken into two steps  1. Find all itemsets with support above minsup \(find all frequent item sets 2. For each frequent item set, derive all rules that are supported by that set which have at least minconf. That is: given a frequent item set, X, find all antecedent subsets, A of X, such that rule, R: A => \(X-A\ has at least minconf  Various improved algorithms have been proposed to discover correlations in large items    Mos t of th e m  are variations of the Apriori Algorithm. Overall performance of 
association rules mining is largely determined by the first step Generally speaking, these algorithms first construct a candidate set of large itemsets based on some heuristics, and then discover the subset that indeed contains large itemsets This process can be done iteratively in the sense that the large itemsets discovered in one iteration will be used as the basis to generate the candidate set for the next iteration  We propose a new approach, which uses bitset to organize remote sensing image and TreeMap structure to represent BitSet in a data-mining-ready way. By using bitset and Treemap, association rule mining algorithm only scans 
transaction database once with fast computation TreeMap keeps the elements in order at all times. It lets you access the elements in your collection by key, or sequentially by key. A modified Apriori algorithm is given below  Algorithm: MBSA algorithm Input: Database D; minimum support threshold min_sup minimum confidence threshold min_conf  749  0-7803-9050-4/05/$20.00 \2512005 IEEE 749 


Output: The complete set of frequent itemsets For all items BitSet generation For\( i=1;i<=rowcount;i++\*rowcount represents the transaction number of database For\( j=1;j<=N;j++\N represents attribute number of database if the row i of column j is 1 set value of the row i of bit[j eq u a l to 1   else  set value of the row i of bit[j eq u a l to 0   end frequent set generation Input :frequent set L k bitset  Output: L k+1 Begin 1  for  i=1 to L k 1 2   for j= i+1 to L k  3  Join\(L k i],L k j    Note 1  4  if  bitset\( i,j\ bitcount min_sup 5  L k+1 L k+1 bitset \(i,j  Note 2  6   7   complete frequent set Generate pattern from L 1 to L k End Note 1  Using the join method of bitset class in java Note 2  Using put method of the TreeMap class to insert bitset i,j\ to L k to generate L k+1  III  EXPERIMENT RESULTS AND PERFORMACE ANALYSIS In this section, we compare our work with the Apriroi algorithm. The experiments are performed on a Pentium-4 1.6 GHz desktop computer with 256 megabytes main memory running Windows 2000. We set our algorithm to find all the frequent itemsets, not just those of interest \(e.g., containing Yield\ for fairness. We got identical rules by running the Apriori and MBSA algorithms. The images used were aerial Tiff images with size up to 200\327200 pixels, along with a synchronized yield band. The data are available on [8 In  ou r performance study, each dataset has four bands {Blue, Green Red, Yield From the Fig.1 we can see that the running time under the Apriori algorithm increases markedly as the number of transactions increases. However, using MBSA algorithm the efficiency is improved The following are the advantages of the MBSA algorithm   It avoids scanning over the entire transaction database each time in generating frequent k-itemsets  It only needs little memory. Because it uses the BitSet class to store frequent itemsets                  Fig.1 Run-time comparison between MBSA and Apriori with different numbers of  transactions IV  MINING OF RMOTE SENSING IMAGE USING MBSA ALOGITHM A  Remote Sensing Image Data Source We use a remote sensing image scene and a yield map as data mining source. These are both digital photos taken from the same field. The image scene contains red, blue and green bands. It represents the reflectance levels of each pixel of the scene. The yield map contains red, blue and green bands. Now we have five attributes in our database. They are: pixel-number red \(band1\ue \(band2\reen \(band3\d yield \(band4 The problem of mining association rules from this imagery data is to discover the associations between band1, band2 band3 and band4. This will help farmers to understand what combination of spectral bands will have a high crop yield B  Attribute Partitioning for Numeric Data The MBSA algorithm can only handle categorical market basket data. When we have quantitative attributes in database we cannot directly apply techniques of association rules mining. In market basket database, a specific transaction consists of a set of items. It means these items were involved in this transaction. So we can use a Boolean value TURE or FALSE to express the relationship between item and transaction. To deal with quantitative data, several approaches were proposed. The most common method simply partitioned the attribute values into intervals  We separate the attribute value into uneven, discrete depth according to the histogram for each band of image. We then have a set of <attribute interval> pairs. The \223item\224 <attribute, interval1> would be \2231\224 if the attribute has value fall within the interval in the original tuple, and \2230\224 otherwise. Table 1 shows the result of the partition of four bands        60 Apriori MBAR Run Time \(Sec 0 1  2     3    4   5  Number of transaction K  150 120 90 30 0  750  0-7803-9050-4/05/$20.00 \2512005 IEEE 750 


 TABLE 1 PARTITION OF FOUR ATTRIBUTEVALUES 0  26  51,25  Red  R-low  R-mid  R-high  0  86,17  171,2  Green  G-low  G-mid  G-high  0  41  81,25  Blue  B-low  B-mid  B-high  0  101,1  181,2  Yield  Yl o w  Ym i d  Yh i gh  C  Deriving Association Rule using MBSA Algorithm From the domain knowledge, we know that Red, Green and Blue refer to reflectance data and Yield refer to yield data. The association rules the user likes to mine are in the form of: red green ^blue =>yield. Now we know exactly that red, green and blue are Antecedent and yield is Consequent. Assume the minsup = 10% and minconf = 40%, We derive following association rules TABLE 2  ASSOCIATION RULES         The rule, R-low, G-high=>Y-high, Sup=19.1 Conf=70.2%, which means \223Red reflectance at most 25 and Green reflectance at least 171 implies Yield will be at least 181\224. Such rules are useful to farmers, agribusiness community and agricultural management agencies. If low yield is discovered early in the growing season, the producer can apply additional inputs \(e.g., water and nitrogen\ alter the Red and Green and produce higher yield potential. For the agribusiness community, wide area yield estimation can improve future price prediction V  CONCLUSION In this paper, we define a new data mining problem---mining association rules from imagery data and its application in precision agriculture Since the efficiency of a mining algorithm is a very important issue in data mining, we propose the MBSA algorithm, which uses TreeMap class and a compressed BitSet class in Java to improve computation efficiency. It scans the transaction database once and further database scans can be replaced by Bitset logical AND operations, which efficiently improves the computation. MBSA algorithm was applied to crop remote sensing image for association rules mining Interesting association rules discovered from remote sensing image composed of Red, Green and Blue bands and crop yield is useful for improving crop production   REFERNCES 1  Agrawal R, Srikant R. Fast Algorithms for Mining Association Rules[C  In   P r o ceed i n g o f t h e 20 th international Conference on Very Large Databased, Santiago, Chile, 1994:487-499 2  Agrawal R., Tomasz Imielinski, Arum Swami. Mining association rules between sets of items in large databases[C I n  Proc. Of the ACM SIGMOD Conference on Management of Data. Washington, D. C. May 1993 207 216 3  Ramakrishnan Strikant, Rakesh Agrawal. Mining Generalized Association Rules[C  P r oc e e d ing  of the 2 1 st V L DB Conference Zurich, Swizerland, 1995 4  Tung Anthony K.H, Hongjun Lu, Jiawei Han, Ling Feng Efficient Mining of intertransaction Association Rules [J  I E EE  Transactions on knowledge and data engineering, 2003, 15\(1 5  Park J S, Chen M, Yu P S.Using a Hash-based Method with Transaction Trimming for Ming Association Rules [J  I EEE  Trans on Knowledge and Data Engineering .1997,19\(5 6  Brin S, Motwani R, Ulman J et al. Dynamic Itemset Counting and Implication Rules for Market Basket Data[C A C M SIGMOD Conf. Management of Data, 1997 7  Han J., Pei J., and Y.Yin.Mining Frequent patterns without candidate generation[C I n  Pr o c 20 00 A C MSI G MO D I n t  Conf. Management of Data \(SIGMOD 00\, p.p. 2000.1 12 8  TM image Datasets website, available at http://midas10.cs.ndsu.nodak.edu/data/image accessed Dec 2003 9  Zaki M, Parthassrathy S, Ogihara M .New Algorithms for Fast Discovery of Association Rules [J In  7 th International workshop Research Issues in Data Engineering, 1997  ID Antecedent Consequent Sup Conf 1 R-low Y-high 29.5% 73.6%\037 205 \205 205 \205 \205 20 R-low, G-high Y-high 20.0 72.2 205 \205 205 \205 \205 35 R-low,G-high,B-mid Y-high 19.1 70.1%\037  751  0-7803-9050-4/05/$20.00 \2512005 IEEE 751 


becomes [A:3, B:4, C:4, D:4 After the scan of all the transactions, the cQomplete CT-tree for the example transaction database TDB is shown in Fig. 1 And the frequency count list becomes [A:5, B:8, C:9, D:6 as shown in the head part of Table II in frequency-descending order Having built a CT-tree, the body part of the compact transaction database is constructed as follows. For every node v whose count value is greater than 0 in the CT-tree, a unique transaction t is created in the body part of CTDB. The count value associated with the node is recorded as the occurrence count of t, and the sequence of items labelling the path from the root to v is sorted in frequency-descending order and recorded as the item list of t. For example, no transaction is created for node A or B in the leftmost path because their count values are 0. Whereas transactions [3 C B A] and [1 C B D A] are created for nodes C and D, respectively, as shown in the first two rows in the body part of Table II B. Algorithm description Having shown the above example, we now define CT-tree as follows Definition 3.1: The Compact Transaction Tree \(CT-tree of a transaction database TDB is a tree where each tree node V \(except the root of the tree, which is labeled as "ROOT is a 2-tuple \(v, v denoted by v : v, in the tree is an item in TDB and v, is the number of occurrences in TDB of a unique transaction consisting of all the items in the branch of the tree from the root to node V The algorithm for generating a CT-tree from a transaction database and for generating a compact transaction database from a CT-tree is described as follows Method: Compact Transaction Database Generator Input: Original transaction database TDB Output: Compact transaction database CTDB 1: root[CTtree  ROOT 2: list[item][count] +- null 3: for each transaction Tn in TDB do 4: To +- sort items of Tn in lexicographic order 5: insert\(T0, CTtree 6: end for 7: if CTtree is not empty then 8: list +- sort list[item][count] in count descending order 9: for each item i in list[item] do 10: CTDB write i 11: CTDB &lt; write count[list[i 12: end for 13: startNode child[root[CTtree 14: write\(startNode, CTDB 15: else 16: output "The original transaction database is empty 17: end if procedure insert\(T, CTtree 1: thisNode +- root[CTtree 2: for each item i in transaction T do 3: if i is not in list[item] then 4: list[item] +- add i 5: end if 6: list[count[i]] *- list[count[i]] + 1 7: nextNode +- child[thisNode 8: while nextNode =# null and item[nextNode] # i do 9: nextNode +- sibling[nextNode 10: end while 11: if nextNode = null then 12: item[newNode] -- i 13: if i is the last item in T then 14: count[newNode] - 1 15: else 16: count[newNode] 0 17: end if 18: parent[newNode  thisNode 19: sibling[newNode] &lt;- child[thisNode 


19: sibling[newNode] &lt;- child[thisNode 20: child[newNode] null 21: child[thisNode] - newNode 22: thisNode +- newNode 23: else 24: if item i is the last item in T then 25: count[thisNode 26: else 27: thisNode +- nextNode 28: end if 29: end if 30: end for procedure write\(node, CTDB 1: if count[node] 78 0 then 2: count[newTrans] +- count[node 3: nextNode *- node 4: while nextNode 5&amp; root[CTtree] do 654 5: newTrans insert item[nextNode 6: nextNode parent[nextNode 7: end while 8: if newTrans is not empty then 9: newTrans +- sort newTrans in list order 10: CTDB +- write newTrans 11: end if 12: end if 13: if child[node] =$ null then 14: write\(child[node], CTDB 15: end if 16: if sibling[node] 7&amp; null then 17: write\(sibling[node], CTDB 18: end if In the first two steps in the above method, the root of an empty CT-tree and a 2-dimension array list are initialized. All items in the original transaction database TDB will be stored in this list along with their support counts after constructing the CT-tree. From step 3 to step 6, a complete CT-tree is built with one database scan, where each transaction T in TDB is sorted and inserted into the CT-tree by calling the procedure insert\(T, CT-tree Then, the list is sorted in frequency descending order and written as the head part of the compact transaction database CTDB, as shown in step 8 to step 12. After calling the procedure write\(startNode, CTDB unique transaction newTrans is written into the body of CTDB for each node whose count value is not equal to zero in the CT-tree. The occurrence count of newTrans is the same as the count value \(step 2 of write the sequence of items labelling the path from the node to the root \(step 4 to step 7 of write order \(step 9 of write database is generated IV. CT-Apriori ALGORITHM The Apriori algorithm is one of the most popular algorithms for mining frequent patterns and association rules [4]. It introduces a method to generate candidate itemsets Ck in the pass k of a transaction database using only frequent itemset Fk-l in the previous pass. The idea rests on the fact that any subset of a frequent itemset must be frequent as well. Hence Ck can be generated by joining two itemsets in Fk-l and pruning those that contain any subset that is not frequent In order to explore the transaction information stored in a compact transaction database efficiently, we modify the Apriori algorithm and the pseudocode for our new method CT-Apriori, is shown as follows. We use the notation X[i] to represent the ith item in X. The k-prefix of an itemset X is the k-itemset {X[1],X[2],...,X[k Algorithm: CT-Apriori algorithm Input: CTDB \(Compact transaction database minimum support threshold Output: F \(Frequent itemsets in CTDB 


1: F1 - {{i} i E items in the head of CTDB 2: for each X,YEF1, and X&lt;Y do 3: C2 +- C2 U {XUY 4: end for 5: k +- 2 6: while Ck # 0 do 7: for each transaction T in the body of CTDB do 8: for each candidate itemsets X E Ck do 9: if X C T then 10: count[X] +- count[X] + count[T 11: end if 12: end for 13: end for 14: Fk +- {X support[X] &gt; min-sup 15: for each X,YEFk, X[i]=Y[i] for 1&lt;i&lt;k and X[k]&lt;Y[k] do 16: L +- X U {Y[k 17: ifVJ c L, IJI = k: JE Fk then 18: Ck+l + Ck+l U L 19: end if 20: end for 21: k + k+ 1 22: end while 23: return F = UkFk There are two essential differences between this method and the Apriori algorithm 1 database in the Apriori algorithm by reading the head part of the compact transaction database and inserting the frequent 1-itemsets into F1. Then candidate 2itemset C2 is generated from F1 directly, as shown in step 1 - 4 in the above algorithm 2 candidate k-itemsets, the original database is scanned during which each transaction can add at most one count to a candidate k-itemset. In contrast, in CT-Apriori, as shown in step 10, these counts are incremented by the occurrence count of that transaction stored in the body of the compact transaction database, which is, in most of the time, greater than 1 V. EXPERIMENTAL STUDIES In this section, we report our experimental results on the generation of compact transaction databases as well as the performance of CT-Apriori using the compact transaction databases in comparison with the classic Apriori algorithm using traditional transaction databases A. Environment of experiments All the experiments are performed on a double-processor server, which has 2 Intel Xeon 2.4G CPU and 2G main memory, running on Linux with kernel version 2.4.26. All the programs are written in Sun Java 1.4.2. The algorithms are tested on two types of data sets: synthetic data, which mimic market basket data, and anonymous web data, which belong to the domain of web log databases. To evaluate the performance of the algorithms over a large range of data characteristics 655 TABLE HI PARAMETERS USED IN THE SYNTHETIC DATA GENERATION PROGRAM Parameter [ Meaning ID- Total number of transactions TTT Average size of transactions I Average size of maximal potentially frequent itemsets ILI- Number of maximal potentially frequent itemsets N Total number of items TABLE IV PARAMETERS SETTINGS OF SYNTHETIC DATA SETS we have tested the programs on various data sets and only the results on some typical data sets are reported here. Moreover these two algorithms generate exactly the same set of frequent patterns for the same input parameters The synthetic data sets that we used in our experiments were generated using the procedure described in [4]. These 


were generated using the procedure described in [4]. These transactions mimic the actual transactions in a retail environment. The transaction generator takes the parameters shown in Table Ill Each synthetic data set is named after these parameters For example, the data set T1O.15.D20K uses the parameters ITI = 10, III = 5, and IDI = 20000. For all the experiments we generate data sets by setting N = 1000 and ILI = 2000 since these are the standard parameters used in [4]. We chose 4 values for ITI: 5, 10, 15 and 20. We also chose 4 values for Ill: 3, 5, 10 and 15. And the number of transactions are set to 100,000 and 200,000. Table IV summarizes the data set parameter settings We report experimental results on two realworld data sets. One of them was obtained from http://kdd.ics.uci.edu/databases/msweb/msweb.html. It was created by sampling and processing the web logs of Microsoft The data records the use of www.microsoft.com by 38000 anonymous, randomly-selected users. For each user, the data lists all the areas of the web site that user visited in a one week time frame. The data set contains 32711 instances transactions items area of the www.microsoft.com web site The other data set was first used in [9] to discovery interesting association rules from Livelink 1 web log data. This data set is not publicly available for proprietary reasons. The log files contain Livelink access data for a period of two months April and May 2002 data describe more than 3,000,000 requests made to a Livelink server from around 5,000 users. Each request corresponds to an entry in the log files. The detail of data preprocessing ILivelink is a web-based product of Open Text Corporation which transformed the raw log data into the data that can be used for learning association rules, was described in [9 The resulting session file used in our experiment was derived from the 10-minute time-out session identification method. The total number of sessions \(transactions data set is 30,586 and the total number of objects 2 \(items 38,679 B. Generation of compact databases To evaluate the effectiveness of compact transaction databases, we compared the compact transaction database with the original database in terms of the size of the databases and the number of transactions in the databases. The compression results are summarized in Table V As the experimental data show, the proposed approach guarantees a good compression in the size of the original transaction database with an average rate of 16.2%, and an excellent compression in the number of transactions with an average rate of 28.0 In the best case, a compression down to 63.1% of the size of the original transaction database and 34.3% of the number of transactions can be achieved in the Microsoft web data. Moreover, as can be seen, much higher compression rates are achieved in real-world data sets, which indicates that the compact transaction database provides more effective data compression in real-world applications C. Evaluation of efficiency To assess the efficiency of our proposed approach, we performed several experiments to compare the relative performance of the Apriori and CT-Apriori algorithms. Fig. 2 and Fig. 3 illustrate the corresponding execution times for the two algorithms on two different types of databases with various support thresholds from 2% down to 0.25 From these performance curves, it can be easily observed that CT-Apriori performs better in all situations. As the support threshold decreases, the performance difference between the two algorithms becomes prominent in almost all the cases showing that the smaller the support threshold is, the more advantageous CT-Apriori is over Apriori. The performance gaps between these two methods are even more substantial on the T15.110.D200K and Microsoft data sets, as shown in 


on the T15.110.D200K and Microsoft data sets, as shown in Fig. 2 and Fig. 3 respectively It is easy to see why this is the case. First, Apriori needs one complete database scan to find candidate 1-itemsets, while CT-Apriori can generate them from the head part of compact transaction database. Even though it takes time to construct a compact transaction database, the resultant compact transaction database can be used multiple times for mining patterns with different support thresholds. Second, when the support threshold gets lower, these two algorithms have to scan databases more times to discover the complete set of frequent patterns. For instance, the Apriori algorithm requires 18 passes 2An object could be a document \(such as a PDF file a task description, a news group message, a picture and so on [9 656 L Transaction Database |T| [II IIDI T5.13.DlOOk 5 3 lOOk TIO.15.DlOOk 10 5 100k T20.I1O.DlOOk 20 10 100k TIO.15.D200k 10 5 200k T15.I10.D200k 15 10 200k T20.115.D200k 20 15 200k TABLE V GENERATION OF COMPACT TRANSACTION DATABASES I F Ir c I F0 WV** Suppo,l Fig. 2. Execution times on synthetic databases 657 Transaction i _ Size of Databases Number of Transactions Databases 11 Original \(Kb Kb   Compression Ratio T5.I3.DIOOK 2,583 2,238 13.4 100,000 67,859 32.1 TlO.I5.DIOOK 4,541 4,349 4.2 100,000 83,095 16.9 T20.IO.DIOOK 8,451 8,227 3.7 100,000 89,023 11.0 TlO.15.D200K 9,000 8,644 4.0 200,000 166,161 16.9 T15.IIO.D200K 13,358 11,108 16.8 200,000 142,863 28.6 T20.Il5.D200K 17,913 14,155 21.0 200,000 151,306 24.3 Average Compression Ratio 10.4 Average Compression Ratio 21.6 Microsoft Web Data 545 344 [ 36.9 if 32,711 11,233 65.7 LiveLink Web Data 3,275 2,262 [ 30.9 30,586 21,921 28.3 Average Compression Ratio 33.9 Average Compression Ratio 47.0 Compression Ratio B 16.2% if 28.0 EO CT-Apriori I-- - Apriorid 500 2001002Ca~~~~~. 0.5 05 .3 Minimum Support T10.15.D200K 25 I F 00 1 I F 1 2 1.5 0.75 0.5 0.330 i F Fig. 3. Execution times on real-world databases over the database T15.I10.D200K when the support threshold is set to 0.25 As shown in the above section, the number of transactions in a compact transaction database is always smaller than that in its corresponding original database, which results in time saving in each scan of the database. The time saved 


time saving in each scan of the database. The time saved in each individual scan by CT-Apriori collectively results in a significant saving in the total amount of I/O time of the algorithm VI. RELATED WORK Data compression is an effective method for reducing storage space and saving network bandwidth. A large number of compression schemes have been developed based on character encoding or on detection of repetitive strings, and comprehensive surveys of compression methods and schemes are given in [6], [10], [16 There are two fundamentally different types of data compression: lossless and lossy. As we have mentioned at the beginning of our experimental evaluations, the set of frequent patterns generated from an original transaction database and its corresponding compact transaction database are identical with the same input parameters, therefore, the compact transaction database approach proposed in this paper is lossless The major difference of our approach from others is that our main purpose of compression is to reduce the 1/0 time when mining patterns from a transaction database. Our compact transaction database can be further compressed by any existing lossless data compression technique for storage and network transmission purposes Mining frequent patterns is a fundamental step in data mining and considerable research effort has been devoted to this problem since its initial formulation. A number of data compression strategies and data structures, such as prefix-tree or trie     optimize the candidate generation and the support counting process in frequent patterns mining The concept of prefix-tree is based on the set enumeration tree framework [14] to enable itemsets to be located quickly 0 A:5 B:3 C:2 B:5 C:3 D:2 C:4 D: 1 D:2 D: 1 Fig. 4. Prefix-tree for the database TDB in Table I Fig. 4 illustrates the prefix-tree for the example transaction database in Table I. The root node of the tree corresponds to the empty itemset. Each other node in the tree represents an itemset consisting of the node element and all the elements on nodes in the path \(prefix path \(0 B:3 C:3 D:3 C, D} with support of 3 It can be seen that the set of paths from the root to the different nodes of the tree represent all possible subsets of items that could be present in any transaction. Compression is achieved by building the tree in such a way that if an itemset shares a prefix with an itemset already in the tree, the new itemset will share a prefix of the branch representing that itemset. Further compression can also be achieved by storing only frequent items in the tree The FP-growth method proposed in [8] uses another compact data structure, FP-tree \(Frequent Pattern tree the conditional databases. FP-tree is a combination of prefixtree structure and node-links, as shown in Fig. 5 All frequent items and their support counts are found by the first scan of database, and- are then inserted into the header table of FP-tree in frequency descending order. To facilitate tree traversal, the entry for an item in the header table also contains the head of a list that links all the corresponding 658 nul e~\(C:9 B:1 B:7 D:2 D:1  D:3 A:3 X,A N-I ..f 4\(A:1 Fig. 5. FP-tree for the database TDB in TableI 


Fig. 5. FP-tree for the database TDB in TableI nodes of theFP-tree In the next scan, the set of sorted \(frequency descending order prefix-tree as a branch. The root node of the tree is labeled with "null", every other node in theFP-tree additionally stores a counter which keeps track of the number of itemsets that share that node. When the frequent items are sorted in their frequency descending order, there are better chances that more prefixes can be shared, thus theFP-tree representation of the database can be kept as small as possible The compact transaction database and CT-tree data structure introduced in the previous sections are very different from above approaches. First of all, the prefix-tree andFP-tree data structure are constructed in the main memory to optimize the frequent pattern mining process, whereas CT-tree is designed to generate compact transaction database and store it to disk for efficient frequent pattern mining and othermining process in which compact database can save storage space and reduce mining time In addition, the counter associated with each node in the prefix-tree and FP-tree stores the number of transaction containing the itemset represented by the path from the root to the node. However, each path from every node of the CT-tree to the root represents a unique transaction, and the associated counter records the number of occurrences of this transaction in the original transaction database Moreover, for a given transaction database, the number of nodes and node-links in FP-tree will change with different minimum support threshold specified by the user. But there is only one unchanged CT-tree for every transaction database And the FP-tree structure can be constructed from a compact transaction database more efficiently in only one database scan, since the head part of a compact transaction database lists all items in frequency descending order, and the body part stores all ordered transactions associated with their occurrence counts VII. CONCLUSIONS We have proposed an innovative approach to generating compact transaction databases for efficient frequent pattern mining. The effectiveness and efficiency of our approach are verified by the experimental results on both synthetic and real-world data sets. It can not only reduce the number of transactions in the original databases and save storage space but also greatly reduce theI/O time required by database scans and improve the efficiency of the mining process We have assumed in this paper that the CT-tree data structure will fit into main memory. However, this assumption will not apply for very large databases. In that case, we plan to partition original databases into several small parts until the corresponding CT-tree can be fit in the available memory. This work is currently in progress VIII. ACKNOWLEDGMENTS This research is supported by Communications and Information Technology Ontario \(CITO Engineering Research Council of Canada \(NSERC REFERENCES 1] R. Agarwal, C. Aggarwal, and V. V. V. Prasad. Depth first generation of long patterns. In Proceedings ofACM-SIGKDD International Conference on Knowledge Discovery and Data Mining, 2000 2] R. Agarwal, C. Aggarwal, and V. V. V. Prasad. A tree projection algorithm for generation of frequent itemsets. In Journal of Parallel and Distributed Computing \(Special Issue on High Performance Data Mining 3] R. Agarwal, T. Imielinski, and A. Swami. Mining association rules between sets of items in large databases. In Proceedings ACM SIGMOD International Conference on Management of Data, pages 207-216, Washington, D.C., USA, May 1993 4] R. Agarwal and R. Strikant. Fast algorithms for mining association rules In Proceedings of20th International Conference on Very Large Data Bases pages 487-499, Santiago, Chile, September 1994 5] R. J. Bayardo. Efficiently mining long patterns from databases. In Proceedings of the International ACM SIGMOD Conference, pages 85 


93, May 1998 6] T. Bell, I. H. Witten and J. G. Cleary. Modelling for Text Compression In ACM Computing Surveys, 21, 4 \(December 1989 7] S. Brin, R. Motwani, J. Ullman, and S. Tsur. Dynamic itemset counting and implication rules for market basket data. In Proceedings of the International ACM SIGMOD Conference, pages 255-264, Tucson, Arizona USA, May 1997 8] J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In Proceedings ofACM-SIGMOD International Conference on Management of Data, pages 1-12, Dallas, TX, May 2000 9] X. Huang, A. An, N. Cercone, and G. Promhouse. Discovery of interesting association rules from livelink web log data. In Proceedings of IEEE International Conference on Data Mining, Maebashi City, Japan 2002 10] D. A. Lelewer and D. S. Hirschberg. Data Compression. In ACM Computing Surveys, 19, 3 \(September 1987 11] J. Liu, Y. Pan, K. Wang, and J. Han. Mining frequent itemsets by opportunistic projection. In Proceedings of ACM-SIGKDD International Conference on Knowledge Discovery and Data Mining, Edmonton Canada, July 2002 12] H. Mannila, H. Toivonen, and A. I. Verkamo. Efficient algorithms for discovering association rules. In AAAI Workshop on Knowledge Discovery in Databases, pages 181-192, July 1994 13] J. S. Park, M. S. Chen, and P. S. Yu. An effective hash-based algorithm for mining association rules. In Proceedings ofACM-SIGMOD International Conference on Management of Data, San Jose, CA, May 1995 14] R. Rymon. Search through systematic set enumeration. In Proceedings of 3rd International Conference on Principles of Knowledge Representation and Reasoning, pages 539-550, 1992 15] A. Savasere, E. Omiecinski, and S. Navathe. An efficient algorithm for mining association rules in large databases. In Proceedings of the 21st International Conference on Very Large Data Bases, Zurich, Switzerland September 1995 16] J. A. Storer. Data Compression: Methods and Theory. In Computer Science Press, New York, NY, 1988 659 Support Count Ite ID Nod% If C 9 B 8 -D 6 A 5 pre></body></html 


lossless frequent patterns representations with borders. In practice, they are also much more concise than representations based on closed itemsets and approximate representations. In this paper, we offered three methods of deriving an upper bound on the length of sets in such representations. We proved that the upper bound on the length of a longest generalized disjunction-free set depends logarithmically on the number of records in the database. The obtained result is of high importance as it guarantees that any generalized disjunction-free set representation for all patterns \(both frequent and infrequent scans, where n is the number of records in the database irrespectively how strong or weak correlations among items in the database are and irrespectively of the lengths of records and number of distinct items The modifications of the basic estimation take into account the support threshold of the representation to be found or, additionally, the information on the length of longest sets of the representation already calculated for a higher support threshold. Though these estimations are more accurate than the basic one, they are still quite rough. Further improvements of the quality of estimating the length of longest itemsets in generalized disjunctionfree representations is subject to further research References 1] R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, A.I Verkamo  Fast Discovery of Association Rules   Advances in Knowledge Discovery and Data Mining AAAI, CA, 1996 2] J. Baptiste, J.-F. Boulicaut  Constraint-Based Discovery and Inductive Queries: Application to Association Rule Mining  Pattern Detection and Discovery, Springer London, UK, September 2002, pp. 110-124 3] E. Baralis, S. Chiusano, P. Garza  On Support Thresholds in Associative Classification  SAC, ACM, Taipei Taiwan, March 2004, pp. 553-558 4] Y. Bastide, N. Pasquier, R. Taouil, G. Stumme, L. Lakhal  Mining Minimal Non-redundant Association Rules Using Frequent Closed Itemsets  Computational Logic, 2000 pp. 972  986 5] J.-F. Boulicaut, A. Bykowski, C. Rigotti  Approximation of Frequency Queries by Means of Free-Sets  PKDD Springer, Lyon, France, September 2000, pp. 75-85 6] A. Bykowski, C. Rigotti  A Condensed Representation to Find Frequent Patterns  PODS, ACM, Santa Barbara USA, May 2001, pp. 267-273 7] T. Calders, Axiomatization and Deduction Rules for the Frequency of Itemsets, Ph.D. Thesis, University of Antwerp, 2003 8] T. Calders, B. Goethals  Mining All Non-derivable Frequent Itemsets  PKDD, Springer, Helsinki, Finland August 2002, pp. 74?85 9] T. Calders, B. Goethals  Minimal k-free Representations of Frequent Sets  ECML/PKDD, Springer, CavtatDubrovnik, Croatia, September 2003, pp. 71-82 10] J. Han, M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann Publishers, 2000 11] S.K. Harms, J. Deogun, J. Saquer, T. Tadesse  Discovering Representative Episodal Association Rules from Event Sequences Using Frequent Closed Episode Sets and Event Constraints  ICDM, IEEE Computer Society, San Jose, California, USA, November-December 2001, pp. 603  606 12] M. Kryszkiewicz  Closed Set Based Discovery of Representative Association Rules  IDA, Springer Cascais, Portugal, September 2001, pp. 350-359 13] M. Kryszkiewicz  Concise Representation of Frequent Patterns based on Disjunction  free Generators  ICDM IEEE Computer Society, San Jose, California, USA 


IEEE Computer Society, San Jose, California, USA November-December 2001, pp. 305  312 14] M. Kryszkiewicz M  Inferring Knowledge from Frequent Patterns  Soft-Ware, Springer, Belfast, Northern Ireland April 2002, pp. 247  262 15] M. Kryszkiewicz  Concise Representations of Association Rules  Pattern Detection and Discovery, Springer London, UK, September 2002, pp. 92-109 16] M. Kryszkiewicz, Concise Representation of Frequent Patterns and Association Rules, Habilitation Thesis Publishing House of Warsaw University of Technology 2002 Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE 17] M. Kryszkiewicz  Reducing Infrequent Borders of Downward Complete Representations of Frequent Patterns  Proc. of The First Symposium on Databases Data Warehousing and Knowledge Discovery, Scientific Publishers OWN, Baden-Baden, Germany, July, 2003, pp 29-42 18] M. Kryszkiewicz  Closed Set Based Discovery of Maximal Covering Rules  International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems Vol. 11. World Scientific Publishing Company, Singapore September 2003, pp. 15-29 19] M. Kryszkiewicz  Reducing Borders of k-Disjunction Free Representations of Frequent Patterns  SAC, ACM Taipei, Taiwan, March 2004, pp. 559  563 20] M. Kryszkiewicz  Reducing Main Components of k-Disjunction Free Representations of Frequent Patterns   Proc. of IPMU  2004 \(in print 21] M. Kryszkiewicz, M. Gajek  Concise Representation of Frequent Patterns based on Generalized Disjunction  Free Generators  PAKDD, Springer, Taipei, Taiwan, May 2002, pp. 159-171 22] M. Kryszkiewicz, M. Gajek  Why to Apply Generalized Disjunction-Free Generators Representation of Frequent Patterns  ISMIS, Springer, Lyon, France, June 2002, pp 383  392 23] M. Kryszkiewicz, H. Rybi?ski, M. Gajek  Dataless Transitions between Concise Representations of Frequent Patterns  Journal of Intelligent Information Systems JIIS Netherlands, 2004, pp. 41-70 24] N. Pasquier, Data mining: Algorithmes d  extraction et de R  duction des R  gles d  association dans les Bases de Donn  es, Th  se de Doctorat, Universit  Blaise Pascal   Clermont  Ferrand II, 2000 25] N. Pasquier, Y. Bastide, R. Taouil, L. Lakhal  Efficient Mining of Association Rules Using Closed Itemset Lattices  Journal of Information Systems, Vol. 24, No. 1 1999, pp. 25  46 26] N. Pasquier, Y. Bastide, R. Taouil, L. Lakhal  Discovering Frequent Closed Itemsets for Association Rules  ICDT, Springer, Jerusalem, Israel, January 1999 pp. 398  416 27] J. Pei, G. Dong, W. Zou, J. Han  On Computing Condensed Frequent Pattern Bases  ICDM, IEEE Computer Society, Maebashi City, Japan, December 2002 pp. 378-385 28] V. Phan-Luong V  Representative Bases of Association Rules  ICDM, IEEE Computer Society, San Jose California, USA, November-December 2001, pp. 639-640 29] A. Savinov  Mining Dependence Rules by Finding Largest Itemset Support Quota  SAC, ACM, Taipei Taiwan, March 2004, pp. 525-529 30] M.J. Zaki, C.J. Hsiao  CHARM: An Efficient Algorithm for Closed Itemset Mining  SIAM, Arlington, 2002 


for Closed Itemset Mining  SIAM, Arlington, 2002 Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


                                                  S J       


                                                      


                         L A                                        


          L A  Table 7. Table of Granules at left-hand-side is isomorphic to  at right- hand-side: By Theorem  3.1 one can ?nd patterns in either table as a single generalized concept  Internal points  are:[4]\(1, 1, 0, 0 tions; [5]\(0, 1, 1, 0  0, 1, 0, 1  0, 1, 1, 1  1, 1 1, 0  1, 1, 0, 1  1, 0, 1, 1 11]\(1, 1, 1, 1 form and simplify them into disjoint normal forms 1  T E N    S J    T E N    S J 2  T W E N T Y    L A    T H I R T Y   A 3  T W E N T Y      T H I R T Y   A 4  T W E N T Y            L A 5  T E N      T W E N T Y    L A    T E N    T W E N T Y   A   S J    T W E N T Y   A      T H I R T Y    L A     Y 7  T E N      T W E N T Y      T H I R T Y   L A    T E N   L A    S  J   A 8  T W E N T Y      T E N      T W E N T Y    L A    T E N   T W E N T Y      T H I R T Y 9  T W E N T Y    N Y    T E N    S J    T H I R T Y    L A      T W E N T Y    L A  1 0  T W E N T Y    N Y    T W E N T Y    L A    T H I R T Y   A    J 1 1  T W E N T Y          T W E N T Y    L A   T H I R T Y    L A    a l l If the simpli?ed expression is a single clause \(in the original symbols non-generalized the following associations 1   T E N     S J    T E N    S J  2. SJ   J 4   L A    T W E N T Y    L A    T H I R T Y    6 Conclusions Data, patterns, method of derivations, and useful-ness are key ingredients in AM. In this paper, we formalize the current state of AM: Data are a table of symbols. The patterns are the formulas of input symbols that repeat. The method of derivations is the most conservative and reliable one, namely, mathematical deductions. The results are somewhat surprising 1. Patterns are properties of the isomorphic class, not an individual relation - This implies that the notion of patterns may not mature yet and explains why there are so many extracted association rules 2. Un-interpreted attributes \(features can be enumerated 3. Generalized associations can be found by solving integral linear inequalities. Unfortunately, the number is enormous. This signi?es the current notion of data and patterns \(implied by the algorithms 4. Real world modeling may be needed to create a much more meaningful notion of patterns. In the current state of AM, a pattern is simply a repeated data that may have no real world meaning. So we may need to introduce some semantics into the data model [12],[10],[11 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE References 1] R. Agrawal, T. Imielinski, and A. Swami  Mining Association Rules Between Sets of Items in Large Databases  in Proceeding of ACM-SIGMOD international Conference on Management of Data, pp. 207216, Washington, DC, June, 1993 


216, Washington, DC, June, 1993 2] Richard A. Brualdi, Introductory Combinatorics, Prentice Hall, 1992 3] A. Barr and E.A. Feigenbaum, The handbook of Arti?cial Intelligence, Willam Kaufmann 1981 4] Margaret H. Dunham, Data Mining Introduction and Advanced Topics Prentice Hall, 2003, ISBN 0-13088892-3 5] Fayad U. M., Piatetsky-Sjapiro, G. Smyth, P. \(1996 From Data Mining to Knowledge Discovery: An overview. In Fayard, Piatetsky-Sjapiro, Smyth, and Uthurusamy eds., Knowledge Discovery in Databases AAAI/MIT Press, 1996 6] H Gracia-Molina, J. Ullman. &amp; J. Windin, J, Database Systems The Complete Book, Prentice Hall, 2002 7] T. T. Lee  Algebraic Theory of Relational Databases  The Bell System Technical Journal Vol 62, No 10, December, 1983, pp.3159-3204 8] T. Y. Lin  Deductive Data Mining: Mathematical Foundation of Database Mining  in: the Proceedings of 9th International Conference, RSFDGrC 2003 Chongqing, China, May 2003, Lecture Notes on Arti?cial Intelligence LNAI 2639, Springer-Verlag, 403-405 9] T. Y. Lin  Attribute \(Feature  The Theory of Attributes from Data Mining Prospect  in: Proceeding of IEEE international Conference on Data Mining, Maebashi, Japan, Dec 9-12, 2002, pp. pp.282-289 10] T. Y. Lin  Data Mining and Machine Oriented Modeling: A Granular Computing Approach  Journal of Applied Intelligence, Kluwer, Vol. 13, No 2, September/October,2000, pp.113-124 11] T. Y. Lin, N. Zhong, J. Duong, S. Ohsuga  Frameworks for Mining Binary Relations in Data  In: Rough sets and Current Trends in Computing, Lecture Notes on Arti?cial Intelligence 1424, A. Skoworn and L Polkowski \(eds 12] E. Louie,T. Y. Lin  Semantics Oriented Association Rules  In: 2002 World Congress of Computational Intelligence, Honolulu, Hawaii, May 12-17, 2002, 956961 \(paper # 5702 13  The Power and Limit of Neural Networks  Proceedings of the 1996 EngineeringSystems Design and Analysis Conference, Montpellier, France, July 1-4, 1996 Vol. 7, 49-53 14] Morel, Jean-Michel and Sergio Solimini, Variational methods in image segmentation : with seven image processing experiments Boston : Birkhuser, 1995 15] H. Liu and H. Motoda  Feature Transformation and Subset Selection  IEEE Intelligent Systems, Vol. 13 No. 2, March/April, pp.26-28 \(1998 16] Z. Pawlak, Rough sets. Theoretical Aspects of Reasoning about Data, Kluwer Academic Publishers, 1991 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





