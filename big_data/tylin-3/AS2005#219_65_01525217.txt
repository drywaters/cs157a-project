 Polyanalyst Application for Forest Data Mining C.Kiran Mai VNRVJ Institute of Engg. & Technology Hyderabad, India Email:ckiranmai@rediffmail.com IV Murali Krishna, Member IEEE JNT University, Hyderabad, India  Email:ivm@ieee.org A.Venugopal Reddy Osmania University, Hyderabad, India Email:avgreddy694@yahoo.co.in  Abstract  Extraction of implicit knowledge and spatial 
relationship is one of the principal applications of data mining Polyanalyst is a user friendly software package for spatial data mining. It has package modules to handle association rules classification and prediction and cluster analysis. In this study data mining is applied to forest data to determine the factors responsible for deforestation. Four major factors \226 roads, villages human population and cattle are taken into consideration to assess the extent of deforestation. Satellite images of forest area along with collateral data are the basic input for this project. The 
linear regression module is used for prediction. Identification of attributes which are to be included in the exploration and targeted for prediction is felicitated through Polyanalyst\222s multiparametric stepwise linear regression modules. The predicted vs real graph shows the points in the actual data set, being explored along with where these data points would have been predicted to fall by the model produced. The study is an initial attempt to assess the scope of Polyanalyst for forest data mining   I. INTRODUCTION The use of computing technology has helped researchers to 
collect large volumes of data. The developments in related technologies helped the IT experts to create appropriate data processing tools with potential for application. Some typical examples are Remote Sensing for digital image processing GPS satellites for vehicle tracking and genome data for bioinformatics development. Such collections of data, whether their origin is business organization or scientific experiment have recently spurred a tremendous interest in the areas of knowledge discovery and data mining. Data mining is integration of several techniques from various disciplines like databases, statistics, machine learning, computing, pattern 
recognition, neural networks, data visualization, information retrieval, image and signal processing, and spatial data analysis. Statistical tools are becoming more useful and powerful with the support from domain computer professionals. Spatial data mining, i.e., discovery of interesting, implicit knowledge in spatial databases is an important task for understanding and use of spatial data and knowledge bases. Statistical analysis has been the main method used for spatial data until now. Data mining is a young field with many issues that still need to be researched in depth Already many data mining products particularly domain 
specific data mining application software are available in the market. As a very young discipline spatial data mining has a relatively short history and is constantly evolving. With wide applications of Satellite and Remote sensing technologies and automatic data collection tools, tremendous amounts of spatial and non-spatial data have been collected and stored in large databases. The extraction and comprehension of knowledge implied by the huge amount of spatial data though highly desirable, pose great challenges to currently available GIS technologies. This situation demands new technologies for knowledge discovery in large spatial databases, or spatial data 
mining, i.e., extraction of implicit knowledge and spatial relationships stored in spatial databases  II. DATA MINING FUNCTIONS Data mining methods are classified by the function they perform or according to the class of application they can be used in. Some of the main techniques used in data mining are associations, classifications, and sequential or temporal patterns A. Association Association analysis is the discovery of association rules showing attribute-value conditions that occur frequently in a given set of data. For example, consider issues of forest management. Deforestation is one of the major causes of 
concern in forest management. Population growth is identified as one of the major causes of deforestation. This growth increases rate of deforestation in several ways. It could be conversion of forest land into human settlements, urban area growth, conversion of forest lands to agriculture lands through shifting cultivation or forest plantation zones on a lower scale  756  0-7803-9050-4/05/$20.00 \2512005 IEEE 756 


 turning the closed forest areas into open forests with reduced canopy. In all these tasks, the attribute-value can be built which can bring out the patterns of forest area reduction B. Classification and Prediction It is a process of finding the set of functions that help predict the class of objects whose class label is unknown Here, predictions should be viewed as applicable for both, data value prediction and class label prediction. But conventionally prediction is defined in such a manner that it implies data value prediction only. As such, classification is a process of finding a set of functions to delineate data into a class of labels. This delineation is similar to data abstraction in object oriented programming techniques. Classification and prediction may need to be preceded by relevan ce analysis which attempts to identify attributes that do not contribute to the classification or prediction process C. Cluster Analysis The objects are clustered or grouped based on the principle of maximizing the inter class similarity. Each cluster that is formed can be viewed as a class of objects from which rules can be derived. In addition to these, outlier analysis and related trend patterns, are other important data mining functions which could also contribute to the study and analysis of patterns of data. In this study, all functions of data mining are not applied Regression and statistical data analysis functions have been adopted for analysis.  Algorithms for spatial data mining along with relevance of association principles are discussed in [1   d [3   III. FOREST DATAMINING In this study data mining is applied to forest data to determine the major factors for deforestation. Four major factors namely roads, villages, human population and cattle are taken into consideration to asses the extent of deforestation The data set contains input data of pixels representing a digitized satellite image of a portion of forest. The training data set contains 300 pixels each representing six numeric values consisting of multi-spectral reflectance values in six bands of electromagnetic spectrum \226 blue, green, red, near infra red, infra red\(1\ and infra red\(2\. Roads, villages and area of degradation are the attributes in the output The various forms of input data is taken into consideration and the behavior of the two outputs is studied with the help of the Polyanalyst software. i.e., the dependency of the two outputs on the given inputs are found. This analysis helps to predict the major cause of deforestation next time when the values of all the pixels are given. The software gets trained to predict weather the deforestation was due to village or due to road There is no one easy answer as there are many causes at the root of deforestation. One is overpopulation in cities and developing countries like India. The increase in population forced some peasants landless eventually leading to occupying remote forest areas for shifting cultivation. This is illustrated through the association rules defined below Let X = Forest area 1. Last year area\(X, Agriculture Present area\(X, Fallow  that area \(X, Shifting cultivation\, confidence = 70 Note that this is an association between more than one attribute. Adopting the terminology used in multi-dimensional databases, where each attribute is treated as a dimension, the above rule can be referred to as a multidimensional rule. This rule helps to build association by considering the forest area X which was under agriculture last year and now under fallow which  leads to conclude that the area X is under threat of shifting cultivation. This is a major cause of deforestation Similarly the second rule is formulated 2. Area \(X, closed forest present area \(X, open forest  that area \(X, Forest degradation Another devastating force behind deforestation is a cattle grazing. With the international growth of fast food chains this seems to be an evident factor in the clearing of trees. This cattle population along with human population in a village has definite impact on villages which is to be incorporated into an association rule  IV. PROBLEM FORMULATION A. Study objective  Deforestation increase is due to rapid growth of population To minimize the   deforestation, a decision maker can use the data mining tool efficiently to reach the goal. In this study an experimental attempt is made for analyzing issues related to deforestation using the data mining tool Polyanalyst   B. Approach The overall objective is to make an analytical solution that a decision maker can easily use and get results. The development approach consists of a series of steps Preprocess data so that the most relevant result can be generated quickly Analyze pattern and co-occurrence of identified concept Develop an automated solution  1\ Preprocessing Data A decision maker first analyses the historical data of the past forest obtained from digital satellite image. Each pixel of this image is represented by six numeric values consisting of multi-spectral reflectance values of the ground objects   757  0-7803-9050-4/05/$20.00 \2512005 IEEE 757 


 2\ Problem Solving Using data mining techniques we can analyze generate reports, graphs and charts of historical data. Summary Statistics, Future Expectation and Extract many more up to date results C. Overview of Application Using Polyanalyst in the project Linear regression algorithm and classify algorithm are applied project to determine the cause of deforestation. Past data is taken into consideration and the behavior of the output is studied. Roads and villages are considered as the major factor for deforestation. The cattle and human population data has significant effect. The dataset contains pixels which are part of a satellite image of about 2Mb size representing forest and non forest regions. Each pixel is represented by six numeric values consisting of the multi-spectral reflectance values of six bands of the electromagnetic spectrum  V. REPORTS A. Summary Statistics   The Summary Statistics exploration engine provides basic statistics about the data, including means, standard deviations, and frequencies. In addition, the Summary statistics report includes frequency charts for each category, string and yes/no variable. The details of  statistics are given in [4  T hi s  sample data set containing 10 attributes and 300 records has mean standard deviation, range etc. The values are as given in Table 1 Table 1: The sample dataset for 10 attributes and 300 records   B. Linear Regression  Linear Regression is a standard method of statistical prediction. In the case of dependence of y on x linear Regression provides an excellent model of phenomena that are actually linear, but produces models of nonlinear functions that are accurate at part of the function and quite poor at predicting other parts. Polyanalyst\222s multi-parametric stepwise linear regression can work with any number of attributes. When Explore | Linear Regression is selected, it prompts to select which attributes to include in the exploration and which attribute to target for prediction purposes 1\ Target Attributes The target attribute of a Linear Regression exploration must be of a numerical data type.  The expression it produces is a continuous function, and requires a data type that can represent continuous variation The statistical data shows the digital values along with attribute information. The LR_output1 and LR_output2 values are computed from Regression relationship. Given the above data the mean values are computed to derive the following equation LR_output1  = a 1 b 1 x 1 b 2 x 2 1 a and b are multiple regression coefficients. These are solved by least squares method LR_output2  =  f \( X 1 X 2 A \              \(2 X 1 and X 2 are the forest areas at different times and A is the area of forest degradation. This relation predicts the deforestation scope and extent based on the current changes in the forest area which is obtained from satellite data. Linear regression is used here to model continuous valued functions and also to predict categorical labels. This prediction need to be compared with output derived from classify algorithm which is being attempted now in a separate study which is outside the scope of this paper. The accuracy estimates are made comparing predicted data values 2\ The Output The Linear Regression report begins with the prediction rule.  The rule is a linear expression, with the values of categorical, yes/no, and string a ttributes represented by \223if\224 statements.  After the prediction rule, a variety of measures of accuracy and statistical significance are included. The linear regression graphs are shown in Figure 1 and Figure 2 L inear regression graphs  Figure1. LR_output1   758  0-7803-9050-4/05/$20.00 \2512005 IEEE 758 


 The outputs correspond to the predicted values as obtained from equations \(1\d \(2\he Predicted vs. Real graph shows the data points in the actual dataset being explored along with where these data points would have been predicted to fall by the model produced.  This allows to get a quick look at the accuracy and predictive power of the model.  The real location of the data point is shown along the x-axis, while the predicted location is shown along the y-axis.  If the predictive rule was perfect, all the points would lie along the central diagonal \226 the more accurate the predictive rule, the closer the points will be to the diagonal  Figure 2. LR_output2   Figure 3. Predicted and real vs. counter The Predicted and Real vs. Counter graph \(Figure 3\lows for seeing how closely the Polyanalyst prediction follows the actual value of the attribute over the range of the dataset for LR_output2 The Classify algorithm can be used to solve a very common problem in data mining \226 splitting a dataset into two groups. The results of classify algorithm are not shown here Classify produces both a scoring rule and a threshold, finding not only a way to score records but also the point at which most accurate classification is achieved. Find Laws, PolyNet Predictor, and Linear Regression are all choices for the development of the classification rule, and which to choose depends on the data available. The Classify algorithm can solve any problem in which separating a dataset into two groups based on their attributes is necessary  VI. CONCLUSIONS  The regression relation predicts the deforestation scope and extent based on the current changes in the forest area which is obtained from satellite data. The prediction need to be compared with output derived from classify algorithm. The classify algorithm helps in splitting the data into two groups The study though preliminary in nature it demonstrated utility of Polyanalyst for spatial data mining  REFERENCES  r i e ge l H P  a nd S a nd er J   19 98  \223Algori t h m s for C ha r a c t e ri za t i o n   and Trend Detection in Spatial Databases\224, Proc. 4th Int. Conf. on Knowledge Discovery and Data Mining, New York City, NY, pp. 44-50 2 Es te r M F r o m m e l t  A K r ieg e l H  P a n d S a n d e r J 20 0 0 223 S pa ti al D a t a  Mining: Database Primitives, Algorithms and Efficient DBMS Support\224 Data Mining and Knowledge Discovery, an International Journal, Kluwer Academic Publishers, Vol. 4, No. 2/3 3 E s t er M   K r i e ge l H P  a nd S a nder J  19 97 223Spa ti a l Dat a  M i ni n g: A  Database Approach\224, Proc. 5th Int. Symp. On Large Spatial Databases Berlin, Germany, pp. 47-66 4 K i r a nm a i  C  2 0 05  D a ta M i ni ng a p pl ica t io ns  226 U np u bl i s he d r e po r t   759  0-7803-9050-4/05/$20.00 \2512005 IEEE 759 


Nalional Ccntcr for Toxicological Kcscarch  131 Every record has 250 attributes and four pairs of training-test scts randomly gcncratcd from tlic original datasct The numbcr ofrccords in a training and CSI pair arc 750 snd 250    Testing Set Tcst sct I Tcsr sct I Tcst se I Tcst set I AVERAGE I Sets related attributes Train I Train 2 Train 3 Train 4 Tablc I Thc riiinlbcrof highly rclatcd attribulcs for thc four Learning paradigm LEFKA Logistic Keipssion YO of Correct Classification 222 of Correct Ctassification 71 59 71 62 67 58 72 hl 70 60 training SCE Thc prc-aiialysis of data idcntrfics a subsct ofattributcs for cvcry oiic of thc lour training scl that arc highly rclatcd to cnch other Thc nuinbcr of highly rclatcd attributes is diffcrcrlt from oilc training sct to the next and thcy arc shown iii Tablc I Thc idcntificd rclatcd attributcs rcplacc tlic 250 attributes in cvcry training-tcst pain The rcsulls of applying the LEFRA paradigm on lhcsc four pairs of training-tcst scls arc dcpictcd iii Tablc 2 In addition thc prediction capability of the LEFRA is cnniparcd with thc prcdiction capability of thc logistic rcgrcssion This comparison is donc by applying thc logistic rcgrcssioii on hc samc pairs of training and tcsting scts The rcsulls arc also shown iii Tablc 2 4 CONCLUSION AND FUTURE RESEARCH Thc rcsults iii Tablc I rcvcal that he LEFKA paradigm is a powcrful prcdiction tool Thc rcsults ofprcdiction arc much higlwr than prcdiction by logistic rcgrcssion approach 5 REFERENCES J Han and M Kambcr 223Data Mining Uonccots and Tcchniqiics\224 Morgan Kaufinan Publishcr 2001 pp 225-236 S Chaudlitiri and U Dayal 223An overview ofdata warchousing and OLAP tcchnology ACM SICMOD Rccord 25 1997 pp. 65-74 R Hashcmi L LcBlanc aird T Kobayashi 223The Prcdiction of Vcsscl Accidcnts Using Formal Conccpt Analysis\224 Thc Cilnadian Transportation Kcscarch Forum 38th Annunl Confcrcncc Toronto Ontario Canada May 2003 Vol I pp 478-492 R Willc Rcstructuring latticc Ihcory An approach based on hicmrchics ofconccpls in Ordcrcd Scls I Rivali cd Rcidcl Dordccht I\222ublishcr Boston 1982,445-470 553 


5 K Agrawal T Imiclinski and A Swami, \224Mining association rulcs bctwccn sct of itcnis in largc databascs\224. procccdings of 1993 ACM-SICMOD lntcrnational Corifcrciicc on Managciiicnt of Data SIGMOD\22293 Washington DC May 1993 pp 207-2 16 6 K Agrmval and R Srikant, \223Fast algorirhms for mining association rulcs\224 procccdings of I994 Intcmational Confcrcncc un Very Largc Dalabascs VLDB\22294 Santiago Chilc Scpt H Mannila H Toivonon and A I Vcrkanio 224Efficient algorithms for discovcring association rulcs\224 procccdings or AA AI 22294 Workshop Knowlcdgc Discovcry in Databases 8 S Rainaswainy S Mahajan, and A Silbcrschatz 223On thc discovcry of inlcrcsting pattcms in association rulcs\224, procccdiiigs of I998 Intcrnational Confcrcncc on Vcry Largc Databascs VLDB\2229X Ncw York August 1998 pp 361-379 9 R Hashcini C lppcrson A Tylcr and J Young 223Knowlcdgc Discovcry Prom Sparsc Phamiacokinctic Data\224, Procccdings of Thc 2000 ACM lntcmational Symposium on Applicd Coniputing SAC\221OO lanicc Carroll Erncsto Damiani, Hishom Haddad Daw Opcnhciiii Editors Como Italy, March 2000 pp 75  79 Databasc Mining\224 Thc Intcrnational Joumal of Smart Enginccring Systcm Dcsign No 4 1994 pp 487-499 7 KDD\22294 Scatlc WA Juty 1994 pp 181-192 IO U tlashcmi F Choobinch W Slikkcr and M Paulc 224A Rough-Fuzzy Ctassificr for pp.107-1 14 2002 I I R Hashcnii B Pcarcc K Arani W Hinson, and M Paulc 224A Fusion of Rough Scts Moditicd Rough Scts and Gcnclic Algorithms Ibr Hybrid Diagnoslic Systcms\224 A book chaptcr Rough Scts and Data Mining Analysis of lmprccisc Data Lin T.Y and Ccrcanc N t!dilors Kluwcr Academic Publishers January 1997 pp 149-1 76 12 A Savascrc E Omiccinski, and S Navalhc 223Mining for strong ncgativc associations in a largc databasc of customcr transactions Procccdings of 1998 lntcmational Confcrcncc on Data Enginccring \(ICDE\22298 Fcbmary 1998 Orlando Florida pp 494-502 13 J Young W Tong H Fang Q Xic U Pcarcc K Hashcmi R Bcgcr M Chccscman J Chcn Y Chang and R Kodcll 224 Building an Organ-Spccific Carcinogcnic Databasc for SAK Analyscs\224 Thc Intcmational Journal of Toxicology and Environmcntal Hcalth, Scpt 2003 Acccptcd for publication 554 


Table 1 Gene families in the third and fourth sets. The major functional domains of each cluster are listed by PSSM ID Cluster Family PSSM ID Gene Count 3-15 TCL1/MTCP1 family 2366 2 3-16 DNA binding transcriptional represson 16791 2 3-17 GTP-binding cell division protein 7746 3 3-18 Fork head domain family 7530 2 3-19 Cyclin-dependent kinase inhibitor 6654 2 3-20 beta-catenin binding protein 16750 2 3-21 YEATS family 5124 2 3-22 Exostosin family 17261 2 3-23 Trefoil \(P-type\in 7445 3 3-24 AF-4 proto-oncoprotein 16405 2 3-26 Tropomyosin 16679 2 4-01 Zinc finger protein 2688, 6135, 8207, 9155 2 4-02 CBL proto-oncogene family 2774, 3268, 3269 3 4-03 DEAD/DEAH box helicase 16682, 16683 8 4-03-1 helicase 16683, 7500 1 4-04 ABC transporter 16577, 4292 2 4-05-1 Myosin tail 16998 3 4-05-2 Ezrin/radixin/moesin family 16819, 936 2 4-06 PHD-finger family 16785 4 4-07-1 protein-protein interaction 16720 1 4-07-2 SH2/SH3 signals transduction protein 7409 7 4-07-3 N/A 9144, 9187 1 4-07-4 DNA binding 9179 4 4-07-5 N/A 16755, 16793 1 4-07-6 FGF receptor with tyrosine kinase activity 15108 16 4-07-7 transcription factor 9355 1 4-08-1 Ras association protein 1344 2 4-08-2 N/A 16777, 4277 1 4-08-3 N/A 16743 1 4-09 ETS family transcription factors 745 2 4-10 Clathrin family 7707 2 4-11-1 Helix-loop-hel ix DNA-binding 16579 4 4-11-2 MYC family 16579, 9268 2 4-12 Homeobox family 9043 3 4-13 RRM motif family protein 16608 5 4-14 Ras family 16606 8 4-25 bZIP transcription factor 5934 2 Proceedings of the IEEE Fifth International Symposium on Multimedia Software Engineering \(ISMSE03 0-7695-2031-6/03 $17.00  2003 IEEE 


Table 3 Categories of hypothetically cancerous genes from the NCI databank. The accuracy of each category is a ratio of the number of genes predicted by the 92 rules to that of selected genes Category Count 1 Count 2 Count 3 Accuracy immunology 1696 304 217 71.38 signal transduction 266 151 138 91.39 cell cycle 265 146 123 84.24 cell signaling 348 148 118 79.73 tumor suppressor/oncogenes 226 120 113 94.17 development 269 107 103 96.26 transcription 243 70 63 90.00 gene regulation 156 54 49 90.74 miscellaneous 195 49 38 77.55 angiogenesis 147 38 26 68.42 pharmacology 100 19 19 100.00 DNA replication 55 14 14 100.00 DNA damage 88 15 12 80.00 apoptosis 47 26 10 38.46 metastasis 119 9 3 33.33 behavior 23 2 2 100.00 metabolism 39 1 1 100.00 DNA adducts 19 1 0 0.00 1 The number of genes from the National Cancer Institute 2 The number of selected genes containing at least one of the 154 functional domains from the training data 3 The number of genes recognized as cancer-related genes by the 92 rules          0 50 100 150 200 250 300 350 i m m u n o l o g y s i g n a l t r a n s d u c t i o n c e l l c y c l e c e l l s i g n a l i n g t u m o r s u p p r e s s o r  o n c o g e n e s d e v e lo p m e n t t r a n s c r i p t i o n g e n e r e g u la t io n m i s c e l l a n e o u s a n g i o g e n e s i s p h a r m a c o l o g y D N A r e p l i c a t i o n D N A d a m a g e a p o p t o s i s m e t a s t a s i s b e h a v i o r m e t a b o l i s m D N A a d d u c t s gene count genes to be examined genes recognized by the 92 rules Figure 6 A comparison of 18 categories having genes predicted as cancer-related genes Proceedings of the IEEE Fifth International Symposium on Multimedia Software Engineering \(ISMSE03 0-7695-2031-6/03 $17.00  2003 IEEE 


 1   1   l  1 i 2  i  Hence the true frequency count of any item occurring on some input stream must be C  l  1 j  x 1  tc  j    j    where  is a small quantity 3  The number of items present in each input stream is thus n  C 4  Since synopses for d l  1  x input streams are transmitted through a node at level x  the load on the most heavily loaded link\(s is L  x  d l  2  x  n C  Clearly the maximum value of L  x  is achieved when   0  The expression for L  x  can be simplied to L  x  1  l  1 j  x 1  j  d x  j 1  Now our expression for the worst-case load on any link can be reduced to W  T    max x 0  1 l  2  L  x   We desire to minimize W  T   subject to the constraints  2   l  1  0 and  l  1 j 2  j   1 It is easy to show that this minimum is achieved when L 0  L 1    L  l  2  Solving for  2   l  1  we obtain  i   1  d  1  l  2   d  1 d  2  i  l  2 and  l  1   1  d  l  2   d  1 d  Translating to error tolerances we set  i   1   l  1  i    d  1 d  l  2   d  1 d for all 2  i  l  1  This setting of  2  l  1 minimizes worst-case communication load on any link We term this strategy MinMaxLoad WC Under this strategy the maximum possible load on any link is L wc   l  2   d  1 d d   1 counts per epoch Lastly we note that MinMaxLoad WC remains the optimal precision gradient even if nodes of the same level can have different  values Informally since with worst-case inputs all incoming streams have identical characteristics maximum link load cannot be improved by using non-uniform  values for nodes at a given level we omit a formal proof for brevity 2.1.4 Good Precision Gradients for Non-Worst-Case Inputs Real data is unlikely to exhibit worst-case characteristics Consequently strategies that are optimal in the worst case may not always perform well in practice In terms of minimizing the maximum communication load on any link the worst-case inputs are ones in which the set of items occurring on each input stream are disjoint When this situation arises a gradual precision gradient is best to use as shown in Section 2.1.3 Using a gradual precision gradient some of the pruning of frequency 3 Recall that we allow the frequency of an item to be a real number 4 More precisely each stream contains  n C  items of weight 1 each and one item of weight  n C  n C   Note that each input stream contains at most one item with weight less than 1 as stipulated earlier counts is delayed until a better estimate of the overall distribution is available closer to the root thereby enabling more effective pruning In the opposite extreme when all input streams contain identical distributions of item occurrences there is no benet to delaying pruning and performing maximal pruning at the leaf nodes as in strategy SS2 is most effective at minimizing communication In fact it is easy to show that SS2 is the optimal strategy for minimizing the maximum load on any link when all input streams are comprised of identical distributions we omit a formal proof Note however that SS2 still incurs a high space requirement on the root node R since it sets  1    We posit that most real-world data falls somewhere between these two extremes To determine where exactly a data set lies with regard to the two extremes we estimate the commonality between input streams S 1 S m by inspecting an epoch worth of data from each stream We compute a commonality parameter   0  1 as   1 m   m i 1 G i L i  where G i and L i are dened over stream S i as follows The quantity G i is dened as the number of distinct items occurring in S i that occur at least   S i  times in S i and also at least   S  times in S  S 1  S 2  S m  where  S  denotes the number of item occurrences in S during the epoch of measurement The quantity L i is dened as the number of distinct items occurring in S i that occur at least   S i  times in S i  Hence commonality parameter  measures the fraction of items frequent enough in one input stream to be included in a leaf-level synopsis by strategy SS 2 that are also at least as frequent globally in the union of all input streams A natural hybrid strategy is to use a linear combination of MinMaxLoad WC and SS2 weighted by   The strategy is as follows set  i 1       1   l  1  i    d  1 d  l  2   d  1 d        for 2  i   l  2  and  l  1 1       1  d  l  2   d  1 d         We term this hybrid strategy MinMaxLoad NWC for non-worstcase Commonality parameter  1 implies that locally frequent items are also globally frequent and SS2 modied to use  1   is a good choice Conversely  0 indicates that MinMaxLoad WC is a good choice For 0  1  a weighted mixture of the two strategies is best 2.1.5 Summary The precision gradient strategies we have introduced are summarized in Table 3 Sample precision gradients are illustrated in Figure 3 3 Experimental Evaluation In this section we evaluate the performance of our newly-proposed strategies for setting the precision graProceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


Table 3 Summary of precision gradient settings studied  Strategy Description Section Introduced Simple Strategy 1 SS1 Transmits raw data to root node R 1.3 Simple Strategy 2 SS2 Reduces data maximally at leaves 1.3 MinRootLoad Minimizes total load on root in all cases 2.1.2 MinMaxLoad WC Minimizes worst-case maximum load on any link 2.1.3 MinMaxLoad NWC Achieves low load on heaviest-loaded link under non-worst-case inputs 2.1.4 0 0.0002 0.0004 0.0006 0.0008 0.001 43210 Tree level \(i SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC Error tolerance  i input            leaf root Figure 3 Precision gradients for  0  001   0  5  dient using the two na  ve strategies suggested in Section 1 as baselines We begin in Section 3.1 by describing the real-world data and simulated distributed monitoring environment we used Then in Section 3.2 we analyze the data using our model of Section 2.1.4 to derive appropriate parameters for our MinMaxLoad NWC strategy that is geared toward performing in the presence of non-worst-case data We report our measurements of space utilization on node R in Section 3.3 and provide measurements of communication load in Section 3.4 3.1 Data Sets As described in Section 1 our motivating applications include detecting DDoS attacks and monitoring hot spots in large-scale distributed systems For the rst type of application we used trafc logs from Internet2 and sought to identify hosts recei ving lar ge numbers of packets recently For the second type we sought to identify frequently-issued SQL queries in two dynamic Web application benchmarks congured to execute in a distributed fashion The I NTERNET 2 traf c traces were obtained by collecting anonymized netow data from nine core routers of the Abilene network Data were collected for one full day of router operation and were broken into 288 ve-minute epochs To simulate a larger number of nodes we divided the data from each router in a random fashion We simulated an environment with 216 network nodes which also serve as monitor nodes For the web applications we used Java Servlet versions of two publicly available dynamic Web application benchmarks RUBiS and R UBBoS 10 R U BiS is modeled after eBay an online auction site and RUBBoS is modeled after slashdot an online bulletin-board so we refer them as AUCTION and BBOARD  respectively We used the suggested conguration parameters for each application and ran each benchmark for 40 hours on a single node.We then partitioned the database requests into 216 groups in a roundrobin fashion honoring user session boundaries We simulated a distributed execution of each benchmark with 216 nodes each executing one group of database requests and also serving as a monitor node For all data sets we simulated an environment with 216 monitoring nodes  m  216  and a communication hierarchy of fanout six  d 6  Consequently our simulated communication hierarchy consisted of four levels including the root node  l 4  We set s 0  01   0  1  s  and  1 0  9    Our simulated monitor nodes used lossy counting in batch mode whereby frequency estimates were reduced only at the end of each epoch in all cases less than 64KB of buffer space was used to create synopses over local streams The epoch duration T was set to 5 minutes for the I NTERNET 2 data set and 15 minutes for the other two data sets 3.2 Data Characteristics Using samples of each of our three data sets we estimated the commonality parameter  for each data set Recall that we use  to parameterize our strategy MinMaxLoad NWC presented in Section 2.1.4 We obtained  values of 0.675 0.839 and 0.571 for the I NTER NET 2 AUCTION and BBOARD data sets respectively Hence the AUCTION data set exhibited the most commonality among all three data sets Results presented in Section 3.4 show that AUCTION indeed has the most commonality 3.3 Space Requirement on Root Node Figure 4 plots space utilization at the root node R as a function of time in units of epochs using Algorithm 2a to generate the synopsis for different values of the decay parameter   using two different strategies for the precision gradient The plots shown are for the I N TERNET 2 data set The y-axis of each graph plots the current number of counts stored in the     synopsis S A maintained by the root node R  Figure 4a plots synopsis size under our MinMaxLoad WC strategy under three different values of   0.6 0.9 and 1 As predicted by our analysis in when  1 the size of S A remains roughly constant after reaching steady-state whereas when  1 synopsis size increases logarithmically with time similar results were obtained for the Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


 0 100 200 300 400 500 600 1 21416181 Time \(epoch counts 0.6 0.9 1.0 a MinMaxLoad WC 0 1000 2000 3000 4000 5000 6000 7000 8000 1 21416181 Time \(epoch counts 0.6 b SS2 Figure 4 Space needed at node R to store answer synopsis S A  non-distributed single-stream case In contrast when SS2 is used to set the precision gradient Figure 4b the space requirement is almost an order of magnitude greater This difference in synopsis size occurs because in SS2 frequency counts are only pruned from synopses at leaf nodes so counts for all items that are locally frequent in one or more local streams reach the root node No pruning power is reserved for the root node and therefore no count in S A is ever discarded irrespective of the  value The same situation occurs if Algorithm 2b is used instead of Algorithm 2a This result underscores the importance of setting  1  in order to limit the size of S A  as discussed in Section 2.1 3.4 Communication Load Figure 5 shows our communication measurements under each of our two metrics for each of our three data sets under each of the ve strategies for setting the precision gradient listed in Table 3 First of all as expected the overhead of SS1 is excessive under both metrics Second by inspecting Figure 5a we see that strategy MinRootLoad does indeed incur the least load on the root node R in all cases as predicted by our analysis of Section 2.1.2 Under this metric MinRootLoad outperforms both simple strategies SS1 and SS2 by a factor of 0 500 1000 1500 2000 2500 3000 3500 4000 INTERNET2 AUCTION BBOARD SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC 247k 10k 296k 132k 20k counts transmitted \(per epoch a Load on root node R 0 500 1000 1500 2000 2500 3000 3500 4000 INTERNET2 AUCTION BBOARD SS1 SS2 MinRootLoad MinMaxLoad_WC MinMaxLoad_NWC 43k    21k 52k   19k 23k   7k counts transmitted \(per epoch b Maximum load on any link Figure 5 Communication measurements k denotes thousands ve or more in all cases measured However MinRootLoad performs poorly in terms of maximum load on any link as shown in Figure 5b because no early elimination of counts for infrequent items is performed and consequently synopses sent from the grand-children of the root node to the children of the root node tend to be quite large As expected MinMaxLoad NWC performs best under that metric on all data sets For the AUCTION data set even though SS2 outperforms MinMaxLoad WC to be expected because of the high  value our hybrid strategy MinMaxLoad NWC is superior to SS2 by a factor of over two For the I NTERNET 2 and BBOARD data sets the improvement over SS2 is more than a factor of three On the negative side total communication not shown in graphs is somewhat higher under MinMaxLoad WC than under SS2 increase of between 7.5 and 49.5 depending on the data set 4 Related Work Most prior work on identifying frequent items in data streams 6 8 9 19 only considers the single-stream case While we are not aware of any work on maintaining frequency counts for frequent items in a distributed stream setting work by Babcock and Olston does adProceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


dress a related problem In the problem is to monitor continuously changing numerical values which could represent frequency counts in a distributed setting The objective is to maintain a list of the top k aggregated values where each aggregated value represents the sum of a set of individual values each of which is stored on a different node The work of assumes a single-le v e l communication topology and does not consider how to manage synopsis precision in hierarchical communication structures using in-network aggregation which is the main focus of this paper The work most closely related to ours is the recent work of Greenwald and Khanna which addresses the problem of computing approximate quantiles in a general communication topology Their technique can be used to nd frequencies of frequent items to within a congurable error tolerance The work in focuses on providing an asymptotic bound on the maximum load on any link our result adheres to the same asymptotic bound It does not however address how best to congure a precision gradient in order to minimize load which is the particular focus of our work 5 Summary In this paper we studied the problem of nding frequent items in the union of multiple distributed streams The central issue is how best to manage the degree of approximation performed as partial synopses from multiple nodes are combined We characterized this process for hierarchical communication topologies in terms of a precision gradient followed by synopses as they are passed from leaves to the root and combined incrementally We studied the problem of nding the optimal precision gradient under two alternative and incompatible optimization objectives 1 minimizing load on the central node to which answers are delivered and 2 minimizing worst-case load on any communication link We then introduced a heuristic designed to perform well for the second objective in practice when data does not conform to worst-case input characteristics Our experimental results on three real-world data sets showed that our methods of setting the precision gradient are greatly superior to na  ve strategies under both metrics on all data sets studied Acknowledgments We thank Arvind Arasu and Dawn Song for their valuable input and assistance References  R Agra w a l and R Srikant F ast algorithms for mining association rules In VLDB  1994  I Akamai T echnologies Akamai http://www akamai.com   A Ak ella A Bharambe M Reiter  and S Seshan Detecting DDoS attacks on ISP networks In PODS Workshop on Management and Processing of Data Streams  2003  A Arasu and G S Manku Approximate quantiles and frequency counts over sliding windows In PODS  2004  B Babcock and C Olston Distrib uted top-k monitoring In SIGMOD  2003  M Charikar  K  Chen and M F arach-Colton Finding frequent items in data streams In International Colloquium on Automata Languages and Programming  2002  E Cohen and M Strauss Maintaining time-decaying stream aggregates In PODS  2003  G Cormode and S Muthukrishnan What s hot and whats not Tracking frequent items dynamically In PODS  2003  E D Demaine A Lopez-Ortiz and J I Munro Frequency estimation of internet packet streams with limited space In European Symposium on Algorithms  2003  DynaServ er R UBis and R UBBos http://www.cs rice.edu/CS/Systems/DynaServer   eBay Inc eBay  http://www.ebay.com   C Estan and G V a r ghese Ne w directions in traf c measurement and accounting In SIGCOMM  2002  M F ang N Shi v akumar  H  Garcia-Molina R Motwani and J Ulmann Computing iceberg queries efciently In VLDB  1998  P  B Gibbons and Y  Matias Ne w sampling-based summary statistics for improving approximate query answers In SIGMOD  1998  P  B Gibbons and S T irthapura Estimating simple functions on the union of data streams In Symposium on Parallel Algorithms and Architectures  2001  M Greenw ald and S Khanna Po wer conserving computation of order-statistics over sensor networks In PODS  2004  J Han J Pei G Dong and K W ang Ef cient computation of iceberg queries with complex measures In SIGMOD  2001  Internet2 Internet2 Abilene Netw ork http abilene.internet2.edu   R M Karp S Shenk er  and C H P apadimitriou A simple algorithm for nding frequent elements in streams and bags ACM Trans Database Syst  2003  H.-A Kim and B Karp Autograph T o w ard automated distributed worm signature detection In Proceedings of the 13th Usenix Security Symposium  2004  A Manjhi V  Shkapen yuk K Dhamdhere and C Olston Finding recently frequent items in distributed data streams Technical report 2004 http://www cs.cmu.edu/manjhi/freqItems.pdf   G S Manku and R Motw ani Approximate frequenc y counts over data streams In VLDB  2002  Open Source De v elopment Netw ork Inc Slashdot http://slashdot.org  Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00  2005 IEEE 


12] S. Fujiwara, J. D. Ullman and R. Motwani, Dynamic Miss-Counting Algorithms: Finding Implications and Similarity Rules with Confidence Pruning. Proceedings of the IEEE ICDE \(San Diego, CA 13] F. Glover  Tabu Search for Nonlinear and Parametric Optimization \(with Links to Genetic Algorithms  Discrete Applied Mathematics 49 \(1-3 14] M. Klemettinen, H. Mannila, P. Ronkainen, H Toivonen, and A. Verkamo, Finding interesting rules from large sets of discovered association rules. Proceedings of the ACM CIKM, International Conference on Information and Knowledge Management \(Kansas City, Missouri 1999 15] C. Ordonez, C. Santana, and L. de Braal, Discovering Interesting Association Rules in Medical Data. Proceedings of the IEEE Advances in Digital Libraries Conference Baltimore, MD 16] W. Perrizo, Peano count tree technology lab notes Technical Report NDSU-CS-TR-01-1, 2001 http://www.cs.ndsu.nodak. edu /~perrizo classes/785/pct.html. January 2003 17] Ron Raymon, An SE-tree based Characterization of the Induction Problem. Proceedings of the ICML, International Conference on Machine Learning \(Washington, D.C 275, 1993 18] N. Shivakumar, and H. Garcia-Molina, Building a Scalable and Accurate Copy Detection Mechanism Proceedings of the International Conference on the Theory and Practice of Digital Libraries, 1996 19] P. Tan, and V. Kumar, Interestingness Measures for Association Patterns: A Perspective, KDD  2000 Workshop on Post-processing in Machine Learning and Data Mining Boston, 2000 20] H.R. Varian, and P. Resnick, Eds. CACM Special Issue on Recommender Systems. Communications of the ACM 40 1997 Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE pre></body></html 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


