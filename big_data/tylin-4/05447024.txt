 1  Field Test Implementation to Evaluate a Flash Lidar as a Primary Sensor for Safe Lunar Landing  Jason A. Keim, Sohrab Mobasser Da Kuang, Yang Cheng  Tonislav Ivanov Andrew E. Johnson Hannah R. Goldberg  Garen Khanoyan and  David B. Natzic  Jet Propulsi on Laboratory  California Institute of Technology  Pasadena, CA 91109  Jason.A.Keim  jpl.nasa.gov    A bstract 321 From May 2 through May 7 of 2008 the Autonomous Landing and Hazard Avoidance Technology ALHAT Exploration Technology Development Program carried out a helicopter field test to assess the use of a flash LIDAR as a primary sensor during lunar landing Th e field test d ata  has been used to evaluate the performance of the LIDAR system and of algorithms for LIDAR Hazard 
Detection and Avoidance, Hazard Relative Navigation, and Passive Optical Terrain Relative Navigation Reported here is a comprehensive description of the field test hardware ground infrastructure and trajectory reconstruction  methodologies 1,2  T ABLE OF C ONTENTS  1  I NTRODUCTION  1  2  S ENSORS AND S UPPORT E QUIPMENT  1  3  D ATA C OLLECTION  3  4  T ARGET S ITES  4  5  T RAJECTORY R ECONSTRUCTION  5  6  I NSTRUMENT V ERIFICATION  10 
 7  C ONCLUSIONS   10  A CKNOWLEDGMENTS  10  R EFERENCES  11  B IOGRAPHY  11  A PPENDIX  13   1  I NTRODUCTION  The  ALHAT  Project  is  funded  by  NASA  to develop an integrated AGNC Autonomous Guidance Navigation and Control\ hardware and software system capable of detecting and avoiding surface hazards and guiding  humans  and  cargo  safely  precisely  and  repeatedly to  designated lunar  landing  sites  2  Achieving this necessitates advancing the state of the art in Terrain Relative Navigation TRN Hazard 
Detection and Avoidance and Hazard Relative Navigation HRN algorithms.  This will also require the investigation and advancement of sensor technologies  Currently the technologies under consideration are passive and active optical systems and radio frequency systems  To  characterize applicable sensor technolo gie s and  algorithms   1 978 14244 3888 4/10/$25.00 \2512010 IEEE  2 IEEEAC paper #1686, Version 5 Updated January 5th 20 10  and  identify  key  performance  parameters a nd sensitivities several field tests are planned  The objective s of first ALHAT field test, which we refer to as FT1  Field Test 1  are to asses s the performance of a commercial LIDAR sensor an d the performance of Hazard Detection \(HD\, HRN 
and TRN algorithms The test results are already in the literature 1 provides the LIDAR data processing and performance 9 present the TRN results  and 5 gives the HD and HRN results.  This paper documents the FT1 hardware ground infrastructure and trajectory reconstruction methodologies   Table 1 List of Acronyms and Abbreviations  ALHAT  Autonomous Landing and Hazard Avoidance Technology  APL  Johns Hopkins University Applied Physics Laboratory  CDSU  Command and Data Storage Unit  DEM 
 Digital Elevation Model  ECI  Earth Centered Inertial  EKF  Extended Kalman Filter  FT1  Field Test One  GNSS  Global Navigation Satellite Systems  GPS  Global Positioning System  HD  Hazard Detection  HRN  Hazard Relative Navigation  IGS 
 International GNSS Service  IMU  Inertial Measurement Unit  JPL  NASA Jet Propulsion Laboratory  LaRC  NASA Langley Research Center  LIDAR  Light Detection And Ranging  NASA  National Aeronautics and Space Administration  PPS  Pulse Per Second  RANSAC  
Random Sample Consensus  SIFT  Scale Invariant Feature Transform  TRN  Terrain Relative Navigation  2  S ENSORS AND S UPPORT E QUIPMENT  Figure 1 is a  block diagram of the entire FT1 system  The FT1 test platform was the  S.N.I.A.S  AS350D ASTAR  helicopter  N145BH  To carry the ALHAT payload, a two axis gimbal was mounted to the aircraft as seen in Figure 2  The gimbal assembly is seen in Figure 3 with the enclosure 


2 removed  T he sensor s mounted to the gimbal are the f lash LIDAR  two optical still  shot  cameras an Inertial Measurement Unit IMU an orientation sensor and a context  witness video camera  The commercial f lash LIDAR is the device under test  One of the still shot  cameras provide d the attitude of the platform relative to the surface when surveyed targets  fell into its field of view. The other camera was co aligned with the LIDAR and had a comparable field of view.  This camera was used to identify what the LIDAR had imaged The IMU provides velocity and angle rate information and the magnetometer provides heading information  An additional IMU is seen in  Figure 3 This IMU is a member of the gimbal stabilization system and is not an ALHAT sensor  The o ther ALHAT sensors were also mounted external ly to the helicopter  I nside of the gimbal enclosure but not on the gimbal was an environment sensor that recorded  the temperature and humidity of the instrument environment Directly above the gimbal was a non articulating platform to which the GPS antenna was mounted  Additionally two digital cameras were mounted to the back of the ballast box at the rear of the helicopter  These cameras seen in  Figure 4  were the primary sensors for the  APLNav 9  terrain relative navigation system   Inside the helicopter was an equipment rack  As seen in Figure 5 it housed power supplies, a power distribution and switching box the PXI chassis and the LIDAR computer and computer switch.  For pre and post flight checkout, t he power supplies where plugged into a wall socket and powered the instruments  When helicopter power was available, the operator could use the power distribution box  to switch  from the power supplies to the helicopter power The box also controlled the power of the individual d evice s  The PXI chassis contained a National Ins truments PXI 6682  time synchronization card a computer switch and a computer.  This system commanded and collected data from the LIDAR and APLNav passive optical cameras   The remainder of this section documents each device on the Gimbal Assembly and the rack mounted equipment   Inertial Measurement Unit  The IMU utilized during the test is a Litton LN 200.  Three solid state fiber optic gyros and three solid state silicon accelerometers comprise the IMU  and were sampled at a rate of 400 Hz  GPS Base Station  The GPS base station is a statically placed GPS antenna and receiver  The data is stored internal to the receiver and periodically downloaded to a computer  By recording the static location of the GPS the  position measurement accuracy of the helicopter can be increased via relative GPS In order to support the long data sessions required the receiver was set to sample data only once a second   Figure 1 Payload block diagram    Figure 2  Helicopter test platform    Figure 3 Gimbal assembly    Figure 4 Passive o ptical TRN cameras  


3  Figure 5 Equipment r ack   Flight GPS Receiver  The A shtech MicroZ CGRS GPS receiver combined with a Magellan Dual band L1/L2 antenna recorded the position of the helicopter.  As stated above, the receiver was housed in the equipment rack and the antenna was mounted externally to the helicopter on a fixed platform above the gimbal.  To protect against data loss the GPS data was stored redundantly in the receiver and by the Command and Data Storage Unit  CDSU described below  In addition to measuring position, the receiver provided a digital pulse per second PPS The PPS combined with a 33 MHz 32 bit counter provided the GPS time tags for each instrument Initially for FT1, the flight receiver operated at a rate of 10 Hz  However during post flight data quality analysis it was observed that  this rate  caused gaps in the GPS data Consequentially the receiver sample rate was reduced to 5 Hz  Backup GPS Receivers  NovAtel Inc ProPak V3 GPS receivers were the FT1 backup receivers Although in our experience  it is relatively easier to operate and config ure a NovAtel receiver than an Ashtech receiver the ProPak V3 receiver does not output the necessary PPS Thus a NovAtel receiver cannot be the primary flight receiver. Instead, the HG1700 SPAN 58 system \(ProPak V3 GPS receiver with IMU was used only as a backup  Gimbal Mounted Digital Cameras  Two Sony XCD SX910 digital cameras were mounted to the gimba l   One camera had a Fujinon HF12.5SA 1 lens that produced a thirty degree field of view  The other camera had a Fujinon DF6HA 1B  lens and seven degree  field of view  The wide rangle d  camera served as the attitude sensor of the trajectory reconstruction system.  By observing surveyed ground targets the camera attitude in the GPS frame could be determined  The narrow angle camera was co aligned with th e LIDAR.  With a field of view similar to that of the LIDAR, it provided an image that roughly coincided with the LIDAR samples  For this test both cameras produced monotone grayscale 1280  960 pixel images.  Set to OneShot mode, the cameras capture d a single Portable Graymap Graphic  image when triggered to by the CDSU  The CDSU triggered the narrow angle camera at a rate of 3 Hz and the wide angle camera at 1  Hz The exposure times of each camera were manually modified to account for changing lighting conditions  Flash LIDAR  FT 1 sought to establish the feasibility of using a flash LIDAR as the primary sensor for Hazard Detection and Hazard Relative Navigation algorithms.  To this end, NASA Langley Resea rch Center \(LaRC\ provided the f lash LIDAR seen in  Figure 3  Manufactured by Advanced Scientific Concepts, the LIDAR was a combination of a 1.57 micron diode pumped Nd:YAG laser source and receiver optics The laser source is dif fused to provide a three degree cone to actively illuminate the target site observed by the receiver optics.  This receiver  with a field of view slightly less than three degree  was  the combination of a 128  128 pixel InGaAs array and a 120 mm diameter 250 mm focal length aperture.  By calibrating the f ocal plane, each pixel of the receiver provides a simultaneous range and bearing measurement   For this test the sensor operated at eight Hertz  Environment Sensor  A  HOBO  U1 2-0 13 was attached inside of the gimbal enclosure  Set to sample every 30 seconds both the temperature and  the  humidity inside the enclosure were recorded  This device operates independently of the data collection system and was not time synchronized.  Instead the device\325s internal clock was manually set at the beginning of the field test and allowed to free run  The HOBO logs data to its own internal drive.  There it is stored until the gimbal enclosure could be opened, and the device removed.  Once it was accessible, the HOBO was connected to a laptop the data was downloaded and the HOBO memory was cleared  Command and Data Storage Unit  The CDSU is a computer running the open source, real time RTAI LINUX operating system  It was the command and data collection unit for the trajectory reconstruction system It directly in terfaced with the GPS receiver, the IMU, both gimbal cameras and the orientation sensor  In addition to these devices the CDSU also received a synchronization pulse from the LIDAR data collection system.  The purpose of this pulse was to inform the CDSU when the Flash LIDAR collected a data frame. When the CDSU collected a data sample from a device it would generate  the time tag for that sample  3  D ATA C OLLECTION  During FT1 different systems independently collect flight data  As noted above t he CDSU collected the reference system  data  A separate c omputer  operating LabView   


4 collected the f lash LIDAR and Passive TRN  data  I n addition to flight data collection systems other support system data was  recorded  The environmental sensor recorded the temperature and humidity within the gimbal enclosure The GPS base station receivers recorded their position to act as a local stationary GPS reference  The flight GPS receiver also recorded the flight data to its internal memory adding redundancy to GPS sy stem.  Lastly a voice recorder captured all communications between the flight operator, the pilot and the ground crew  At the end of each flight data was downloaded from each system as illustrated by    Initially t he CDSU collected  GPS data was parsed into navigation data and timing data Then, all CDSU flight data was  copied to an external two terabyte drive Simultaneously, LaRC personnel retrieved a re movable hard drive containing the f lash LIDAR and passive optical TRN data The data was copied into a data storage server and to the  external two terabyte drive  Between flights the GPS base station receiver and flight receiver data was converted to a Rinex file and saved to a laptop running Ashtech\250 Micro Manager  Once the receiver data was successfully downloaded and checked for quality the receivers were erased in preparation for the next flight.  This data was then copied from the laptop to the tw oterabyte drive.  Overnight the data on the external drive was duplicated onto a second external drive to prevent data loss  At the end of the field test all data  from all sources were copied from the external drive s onto a JPL server  4  T ARGET S ITES  During FT 1 data was collected at three target sites designated as the Borrow Pit, Lakebed and Mars Hill The Borrow Pit and Lakebed sites were artificially created and surveyed for this experiment  The Mars Hill  site was a natural terrain sight select ed for its varying slopes and rock collections  Borrow Pit   Figure 7  Image of Borrow Pit captured by the wide angle camera  An artificially cr e ated target site located at NASA Dryden Flight Research Center Borrow P it  seen in Figure 7  was constructed to test the f lash LIDAR\325s ability to detect hazards at various slant ranges and angles  To this end sever al targets were located at the Borrow P i t.  The targets  includ ed  polystyrene and acrylic hemispheres one meter  cardboard cubes man made craters and plywood camera attitude targets  The hemispherical targets were placed into two areas the 10 rock field and the 5 rock field  T he 10 rock field contained forty two 18\323 diameter hemispheres eight 24\323 diameter hemispheres and six 36\323 diameter hemispheres.  The 5% rock field contained fifteen 18\323 diameter hemispheres, seven 24\323 diameter hemispheres and one 36\323 diameter hemisphere  The locations of the hemispheres were surveyed so that the positions were known in the GPS frame.  Using this information, the truth DEM  Digital Elevation Model  seen in Figure 8  was created  Also seen here are the man made craters and the cardboard boxes  Figure 6 Data collection block diagram   5% Rock Field  Cardboard  Boxes  Artificial C raters  10% Rock Field  


5 Figure 8 Borrow Pit truth DEM   Figure 9  Image of the Lakebed captured by the thirty degree fov camera   Figure 10: Lakebed DEM  Lakebed  The Lakebed is the second man made target site and is also located at NASA Dryden Flight Research Center.  With its flat, featureless surface, the naturally occurring dry lakebed is an ideal site for determining the LIDAR\325s ability to detect targets.  To this end, nine hemispheres of various sizes and albedos, a s ingle one meter cardboard cube, a cluster of four one meter cardboard cubes and eleven attitude targets where placed there.  The site was surveyed and a truth DEM was created  Figure 9  and Figure 10 show t he Lakebed site as seen by the thirty degree field of view camera and the truth DEM, respectively  Mars Hill  Seen in Figure 11 Mars Hill is the only naturally occurring target sight of FT1.   Located within Death Valley National Pa rk this site is approximately 200 miles away from the Lakebed and Borrow Pit.  With its natural rocks, slopes and lack of vegetation this site is a perfect analog for lunar terrain.  Plywood attitude targets, seen as white dots on the right of the hill and cardboard boxes were placed at Mars Hill   Figure 11  Mars Hill captured by the thirty degree fov camera   5  T RAJECTORY R E CONSTRUCTION  Overview  For FT1 the objective of the trajectory reconstruction process is to provide the position and attitude of the LIDAR for every LIDAR sample relative to a target Initially, it was proposed to develop an Extended Kalman Filter EKF to produce the LIDAR attitude and position by combining the GPS position  data the IMU  angular rate data  and the camera based attitude estimates However the development of the filter stalled  In order to provide results in a more timely fashion a simpler method was adopted  The new process is as follows  To begin the raw GPS data is processed to pro duce the position measurements accurate to a standard deviation of two centimeters.  Second, the images produced by the wide angle camera are manually sorted through to find images 5% Rock Field  10% Rock Field  Artificial  Craters  Cardboard  Boxes  


 6  that contain six or more attitude targets  Once the images were identified  they are used to determine the camera attitude Because the cameras infrequently captured images of the attitude targets the camera generated attitudes are used to initialize IMU based attitude  propagation   Finally g yro propagation is combined with t he GPS measurement to provide the full LIDAR pose \(postion and attitude  T his section details the reconstruction process  Initially w e present the GPS processing methodology followed by the imagery based attitude estimation process  After that t he equations used to propagate the attitude estimate based on the IMU data are given   Finally we provide how the camera estimates, gyroscope propagation and GPS position data are all combined to re produce the LIDAR pose  G PS Data Processing  The  GPS hardware co nfiguration of FT1 consists of two receivers, one is a static base station on the ground, and the other mounted on the helicopter. The data recorded by both receivers is analyzed to derive the precise positions of each  To obtain the precise position of a receiver, the data logged by the receiver is processed together with satellite orbit position and clock information. The quality of the GPS orbit and clock information used in the data processing defines the accuracy of the receiver position solution  In the final processing of the GPS data for FT1  we use the precise GPS orbit and clock information called the FLINN product. This product is produced at JPL by processing tracking data from 80 globally distributed ground stations, with about 10 days  latency and 3 cm accuracy The product is routinely generated and submitted to Interna tional GNSS Service IGS\to support precise applications i n science and industry communities           Figure 12  GPS data post processing for trajectory reconstruction  Our solution approach is illustrat ed in  Figure 12 At the left of the image is the data collected during a flight.  The center column represents a GPS processing method and the right hand side is the product and its solution accuracy  Each method is now described in detail  Base Station Static Point Positioning 321 The base station receiver d ata is processed using the FLINN GPS orbit and clock product The combination of GPS measurements at two different frequencies, commonly known as L1 and L2 are used to remove the effect of the ionosphere on the  measurements To further improve accuracy data points with tracking elevation angle below seven degrees are excluded Given two hours or longer hours of data the typical accuracy of the static point position solution is three centimeters  Helicopter Kinematic Point Positioning 321 Initially, the flight receiver data is processed to determine the helicopter\325s position using a kinematic point positioning technique  Again L1 and L2 GPS measurem ents are used to remove the effect of the ionosphere  Then using the best FLINN GPS orbit and clock information, the absolute the kinematic point positioning is determined to within twenty centimeters Here, the main error source for the solution is the correlation between the troposphere delay and the position height component   This solution is used as the initial trajectory for the relative positioning solution  Helicopter Kinematic Relative Positioning 321 The base station receiver data and the flight r eceiver data are processed together to determine the helicopter\325s position relative to the base station. In the relative positioning, those errors that are common to both the base station receiver data and the flight receiver data, such as the GPS orbit an d clock errors and troposphere and ionosphere delay errors, cancel out over a short baseline In our process the base station position is fixed to the st atic point positioning solution and the helicopter\325s position is solved iteratively starting with the kinematic point positioning solution To do so we use the GPS pseudorange and carrier pha se measurement at frequency L1   Typically the helicopter\325s position error relative to the bas e station has a two centimeter standard deviation   Image Based Attitude Estimation  The objective of the image based attitude estimation is to provide attitude measurements while over a test site  Because the image based attitude estimation does not drift as the IMU does it serves as an anchor point  for the IMU data processing and the final trajectory reconstruction  The initially proposed procedure for the image based attitude estimation was the following  First the image sequences containing a sufficient number of attitude targets were manually extracted  Then using these images the image pose was estimated.  Last we appl ied a bundle adjustment using the estimated pose and selected track features of an image series to improve attitude and position estimates  However many image sequences  contained a lack of distinguishable features  or the target field was Flight  Receiver  Data  Kinematic  Point  Positioning  20 cm level  Trajectory  GPS  Orbit & Clock  Data  Differential  Positioning  Process  2 cm level  Relative  Trajectory  Base  Receiver  Data  Static  Point  Positioning  3 cm level  Static  Position  


7 relatively small compared to the full  image For these  bundle adjustment produced bad results  Therefore a method was adopted that relied on the  kinemat ic relative GPS position rather than estimating it from imagery This method is as follows  Image Rectification  and  Attitude Target Extraction 321 Initially  seq uences of images containing six or more attitude targets were found Prior to inspection we use d the camera CAHVOR model 4  to rectify each image.  Then  a manual scan was  made of every eighth frame  If any attitude targets were  seen the skipped frames were  examined to determine the exact frame in which the targets initia lly appeared.  By comparing this frame to the surveyed locations of the attitude targets each target was  identified and its x  and y pixel loca tion were  recorded Th e se  locations  are  then used to initialize an automatic tracking algorithm  To track  the target locations each image frame in a sequence  containing the attitude targets wa s passed through a Scale Invariant Feature Transform SIFT Keypoint De tector 7  Comparing the SIFT points between consecutive images allow ed  for the computation of the Homography transform which map ped  the pixels from one image into the next   By i nitializing the locations of the targets in the first frame with a location recorded earlier the  Homography transform is  applie d to propagate the x and y pixel locations forward through the entire  series When  h owever there was  poor matching between pictures the propagation fail ed  When  the above f ail ed  the algorithm automatically shift ed  the target grid of one image horizontally and vertically over the other.  For every shift, the normalized sum of the pixel  values  under the targets is computed and the optimal shift was  found  Since the targets a re white this is the shift at which the sum of pixels is the maximum  In order for automatic shifting to succeed the targets must clearly contrast the background and the target grid s in both frames must be relatively in the same configuration  When  either condition was not met, the automatic shift algorithm fail ed  In this case a target was  hand selected and the shift from one image to the next was manually determined.  All targets were shifted by this amount and the x and y pixel location propagation continue d  Starting with the manual shift the local maximum was found to find the best local target location  When the propagation of the target locations completed the results were visually inspected for correctness by plotting the predicted attitude target locations back onto the image frames Figure 13 illustrates the entire target tracking algorithm  Solving for the Camera Attitude 321 Once a series of rectified images are produced and the pixel locations of the targets a re known, the attitude of the camera can be found.  For a CAHVOR calibrated camera, every x and y pixel location corresponds to a known three dimensional ray expressed in the camera frame.  Given the set of three dimensional points P i  expressed in the GPS  frame and a corresponding set of two dimensional points v i  expressed in image coordinates the x,y pixel locations  it is possible to derive a unit length ray v i in the camera frame using the camera calibration data Let r  be the camera location as measu red by the  GPS  and w i    w 1  311 w n  be a set of unit length rays from the camera to each P i i.e    1  where || . || is the vector norm  T hen solving for the full camera pose amounts to computing the rotation between w i and set v i  v 1, \311 v n To do so, we define V as the n  3 matrix consisting of  v i   W as the n  3 matrix consisting of w i  and R as the rotation matrix that rotates V T such that  W T RV T   2  In practice 2 does not have an exact solution and is computed either by  R=AB T  where A  B T W T V  is the singular value decomposition of W T V  or by  R=Pinv\(W where Pinv is the Moore Penrose pseudoinverse Note that if W T W  is non singular  then  Pinv\(W T W  W T W 1 W T  and, therefore  R = \(W T W 1 W T V    Figure 13: Block diagram of target tracking algorithm   Auto Adjustment Automatic Shift  


8 Next, we find the best attitude estimate using the RANdom SAmple Consensus RANSAC method  3 From the set P i   P 1  311 P n  th r ee points are selected and used to form an attitude estimate.  Using the resulting R matrix, the attitude targets are pro jected into the image and the projection error is calculated  Using a predetermined threshold erroneous members of  P i  are identi fied and rejected  Then the best attitude estimate is that formed from the remaining points  The resulting matrix R is the image based attitude estimate   IMU Based Attitude Propagation  For a given flight the resulting imagery based attitude estimates exist only when the wide angle camera capture d a sufficient number of attitude targets which occurred at a maximum rate of once a second Given the high rotational velocities of the gimbal this was  insufficient for  reconstructing the LIDAR trajectory  To this end we determine the attitude ever y 2.5 milliseconds using the LN 200 gyroscope s  To use the gyroscope s we first define an inertial frame.  Then, using the camera attitude estimates  to initialize the attitude  we propagated the attitude over the t ime that the  LIDAR was sampling data   The following sections provide the details of each step  T he inertial frame 321 We utilize an  Earth Centered Inertial ECI\frame We define the inertial frame to be the location of the WGS84 frame at the initial time  in a flight that the wide angle camera captures enough  attitude targets to produce an attitude estimate  Determining the initial attitude  from a series of attitude estimates 321 In this section, we find the initial attitude of the IMU relative to our ECI frame   The attitude of the IMU relative to the inertial frame is denoted M R I   This was  determined for every imagery based attitude estimate The product of the image based estimates was the attitude of the camera with respect to the GPS  frame.  Call it  E R C  From the i th estimate  E R C i   the IMU attitude relative to the initial frame was determined by the successive rotations  M R I i  M R C  E R C i  1  E R I t    3 Here  M R C  is the kn own rotation between IMU frame and the camera frame  and E R I accounts for the rotation between the GPS frame and the ECI frame  For propagating attitudes quaternions are numerically better suited. Therefore, each  M R I i  is converted to the equivalent quaternion  M q I i   Now that we expressed the attitude of the IMU re lative to an inertial frame in quaternion form we  propagated  it using the delta angles measured by the gyroscopes and using the 4     M q k 1 I  1 4  1 2 T  1  1 4 T    2 2  1 8 T    3 3                     M q k I  4 Here   and 1 4 is the 4  4 identity matrix The angles  x   y and  z are the angles provided by the x  y and z gyroscope measurements, respectively.  This equation is the result of truncating the infinite series expansion of the quaternion kinematic equation 8   Recall that image based attitude estimates are only available when the camera images at least six attitude targets.  Also the estimates come from sequen ces of consecutive  images For each sequence we attempt to find a good initial attitude to begin our  propagation of the attitude over the entire flight To start, we s elected  the initial estimate  from a sequence  and propagate d  the attitude  forward until the end of it  Note that each sequence  was  a small  portion of the entire flight Figure 14 shows an example of  M q I  generated from a section of data collected over the  Borrow Pit target site.  As seen here, the image based attitude estimates agree with the gyroscope propagation    Figure 14 Plot of attitude estimate based on camera vers us attitude propagated with IMU  Having both a propag ated quaternion and camera based  estimates during the same time span we found the mean error between them  For clarity we now denote the propagated quaternion as M p I and retain the M q I notation for attitude estimates  The  error quaternion mapping the Camera Estimate  321 IMU Propagation  q 1  q 4  q 3  q 2  Note   Jumps at  t 40 and  t 120 are the result of quaternion properization   Seconds since targets appeared in camera field of view   0  50  100  150  200  150  0.8  0 6  0 4  0 2  0.0  0.2  0.4  0.6  0.8  1  0 0 0 0.0 0.2 0.4 0.6 0.8 Quaternion Elements \(Unitless   


9 difference between the propagated attitude and the estimated attitude is given by  q e  M p I  1    M q I   5  Here   M p I  1  is the inverse  quaternion of  M p I  and   is the quaternion product  From the error quaternion the equivalent Euler angle of rotation   and unit Euler axis of rotation a  can be determined. Using   and a we define the scalar error quantity e s    and the vector error e v e s a   At this point   we have an error vector e v  and scalar e s  for every attitude estimate Taking u e to be the mean of all e v and u s to be the mean off all e s  we create the correction quaternion q c  u e T  sin u s 2  cos u s 2 T   Applying the correction quaternion to first camera based estimate  M q I 1 provide d the initial quaternion used to propagate the attitude for the entire flight, such that M q I 0  M q I 1  q c   To illustrate the effects of this procedure, we include  Figure 16  which because of its size has been placed  in the Appendix Here, f rames \(A\ and \(B\ are the value s of e s and e v  respectively created when the attitude was propagated starting at  M q I 1 a nd C and D was propagated starting at M q I 0  By improving the starting point the maximum scalar error e s has dropped from 0.26 degrees to 0.18 degrees and the standard deviation has dropped from 0.06 degrees to 0.03 degrees.  The vector error has the same shape, but now has zero mean  Determining the attitude of the IMU with respect to ECI 321 During a flight several passes at the target site are made At each pass, the wide angle camera takes a series of images used to estimate the attitude  S ee Figure 15   This figure shows the IMU LIDAR and camera timestamps for the second Borrow Pit flight  Here nine sequences  of attitude estimates are clearly visible  However it can be seen that LIDAR data is available between each sequence To provide attitude estimates between each series we propagate the estimates using the IMU.  As the propagation continues the estimate accuracy degrades due to error sources such as the gyroscope bias and numerical inaccuraci es Here we attempt to compensate for these errors   Minutes since start of data acquisition  Figure 15 Sample t imes of IMU, LIDAR and camera  Initially, it was proposed to combine GPS, camera data and the IMU with an EKF  However  development of the filter stalled and remains the subject of future work.  For FT1  we utilize d the following ad hoc method We beg an by finding the initial quaternio n for each sequence of image based attitude estimates  as determined by the previous section In addition to q 0 the standard deviation of e s for each  sequence  was calculated.  Starting from the initial quaternion of each sequence we propagate the attitude across the entire flight While propagating the accuracy of the attitude was  also tracked via   k t  0  b\(t k t q0   6 where  0   is the standard deviation of e s  for a given sequence    indicates the absolute value b is an arbitrary scale factor  t k is the timestamp of the k th measurement and t q0  is the timestamp  of  q 0  Take  M  to be the number of imaged based attitude estimate.  For the flight illustrated in Figure 15  M 9.  Since we propagated the attitude starting at each sequence 325s  q 0  there were  M  different propagated attitudes at every time step  Each of these were  combined using a weigh t ed average     7  where   j  and  a j  are the Euler angle and axis that defin e a propagated quaternion p j w j   j 1 is the weight of p j and W is the sum of all weights  Notice, that by using  and a  u is the average of the attitudes axis angle form    Finally for every time step u  was  converted to the quaternion M q I u     v T  sin   _ cos   T where v  u  u and   u 2 Also note that this scheme favors samples where there is good camera and IMU agreement  Constructing the LIDAR trajectory  The objective of the trajectory reconstruction  effort was  to determine the position and attitude of the LIDAR with respect to a target  site  Finding the attitude requires the following successive rotations  T q L  T q E    E q I t   M q I  1    M q L   8  Above T q E  and M q L  are the known relative attitudes The first i s the attitude of the target frame relative to the GPS  frame and the latter is the attitude of the LIDAR with respect to the IMU frame.  The other term introduced in \(8 E q I t is the attitude of the GPS frame with r espect to the ECI frame at time t  To  find the position of the LIDAR in the target frame we first linearly interpolate d  the 1 Hz GPS position measurement  r L/E  to the 8 Hz necessary for the LIDAR Then   we remove d  the offset between the GPS  and target frame r E/T  and rotate d the interpolated measurements from the GPS  frame into the target frame  using the rotation matrix T R E which is the rotation matrix equivalent to \(8  r T  T R E r L/E 320 r E/T   9  The attitude provided by \(8\ and the GPS measured position via 9 gave  the full pose of the LIDAR for each flight  0  80  70  60  50  40  30  20  10  


 10  6  I NSTRUMENT V ERIFICATION  To verify the accuracy of the reconstructed trajectory we  us ed the LIDAR by shift ing the LIDAR data until the minimum correlation error between the sensor data and truth DEM was found T he LIDAR camera boresite vector corresponds to the LIDAR z axis and t he sensor array rows and columns define the x and y axes  Therefore, any position shift in the z direction can be interpreted as a range error.  Shifts in the x and y axes co rrespond to yaw   and pitch  errors as follows   tan 1  y d  z  1    10   tan 1   x d  z  1    11  where  x   y and  z  represent the x, y and z shifts, and d is the distance to the target. Notice that yaw is the rotation abou t the LIDAR y axis  and pitch is the rotation about the LIDAR x axis R oll and pitch errors have been calculated using this method for Lakebed and Borrow Pit flights and plotted in Figure 17 A and Figure 18 A respectively  Additionally, we compare d the attitude determined by \(7\ to the image based estimates and plotted the results in  Figure 17 B and Figure 18  B   The maximum observed errors  observed by the LIDAR during the Borrow Pit flight were 0.25 and 0.16 degrees for pitch and yaw respectively These are comparable to the camera generated values of 0.12 and 0.15 degrees \(seed Figure 17 B\\.  For the Lakebed flight, seen in Figure 18  A the maximum LIDAR determined error was 0.37 and 0.58 degrees for pitch and yaw respectively  However the pitch and yaw errors seen in Figure 18  B for the cameras were significantly worse 0.33 and 2.50 degrees, respectively  In Figure 17 B and Figure 18  B  a non random process noise is apparent and is the likely cause of the larger than expected errors.  Currently, the source of this error has not been identified  however f or the ad hoc method described here  gyroscope scale factor errors, sense axis misalignments and bias have not been accounted for Additional error sources for the camera and IMU include the numerical inaccuracies introduced by 4 and timing errors For the LIDAR the largest source of e rror is the timing uncertainty. Efforts have been made to identify the timing of the LIDAR sampling.  Unfortunately, it changes from flight to flight and is not easily identifiable  Despite the larger than expected error the trajectory reconstruction me thod was s ufficient enough to place the LIDAR hazards within one meter horizontally of the truth hazards  The combination of the mainly horizontal trajectory errors and the time varying range bias inherent to the LIDAR caused the LIDAR data to be misaligne d relative to the truth DEM. To eliminate this misalignment, the flash LIDAR data and truth DEM were correlated using a procedure based on the HRN algorithm 6 was used. The end result was precise alignment to 1 DEM pixel \(0.1m or less  Even though the results from the method presented here were sufficient to achieve the ALHAT objectives i t may be desirable in the future to try to  achieve the expected attitude performance of 0.71 degrees   In that case  methods to identify the error sources will be developed  One such option is the use of an unconstrained nonlinear optimization  such as the MATLAB\250 function fminsearch   By formulating the effects that gyroscope misalignments, biases and timing offsets has on the propagated trajectory the parameter  values that minimize error between the camera estimates and the propagated trajectories can be found Although this approach is simple, the time for the nonlinear optimizer to find the parameters will be gre at  and it will not compensate for the numerical inaccuracies  A more complex alternative is to complete the EKF and extend it to an optimal smoother.  The filter will directly solve for gyroscope biases and drift due to the averaging between sensors and  reduce the impact of the numerical By running the filter multiple times and observing the effects that shifting timing offsets has on the filter residuals the best timing offset of the camera and IMU can be identified  Similarly by shifting the timing of the LIDAR samples and observing the effects on  x   y and  z the best LIDAR offset for each flight can be found  7  C ONCLUSIONS  The expected performance of the reference system was .12 cm for position and 0 071 degrees for attitude.  Although the ma ximum observed attitude error of 0.33 degrees was five times worse than expected the error for the Borrow Pit flight never exceeded the FT1 requirement However this was not the case for the Lakebed flight Examination of the error  seen in  Figure 17 B and Figure 18   B clearly demonstrates a yet unidentified, non random process noise By compensating for gyroscope misalignments, scale factor errors, gyro biases and timing, this error should dramatically reduce Despite the larger than expected errors, this method was sufficient to achieve the FT1 ALHAT objectives.  In case a better level of accuracy is required in the future, the  sources of error will be analyzed and the methods to compensate for them will be developed  Two approaches are the use of an unconstrained nonlinear optimizer or the development of an Extended Kalman Filter.  The results of these efforts are expected to achieve the expected 0.071 degree performance for FT1  A CKNOWLEDGMENT S  We thank Asif Ahmed of the Jet Propulsion Laboratory for his guidance  patience and tutelage during the trajectory reconstruction effort  The work described in this publication was performed at the Jet Propulsion Laboratory California Institute of 


 11  Technology under contract from the National Aeronautics and Space Administration The work was funded by the NASA Exploration Technology Development Program and would not have been possible without the flash LIDAR  sensor provide by NASA Langley and the field test system provided by JPL  R EFERENCES  1  Bulyshev Alexander Pierrotte t Diego Amzajerdian Farzin; Busch, George; Vanek, Michael; Reisse, Robert 322Processing of three dimensional flash lidar terrain images generating from an airborne platform\323 roc SPIE, Vol. 7329 April 2009  2  Epp  Chirold and Tom Smith, \322Autonomous Precisio n Landing and Hazard Detection and Avoidance Technology ALHAT\,\323 Proc IEEE Aerospace Conf Big Sky, MT, March 2007  3  Fischler Martin A and Robert C Bolles Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography  Comm Of the ACM 24 June 1981 381 320 395  4  Gennery D.B 322Least Squares Camera Calibration Including Lens Distortion  and Automatic Editing of Calibration Points,\323 Workshop Calibration  and Orientation of Cameras in Computer Vision XVI I Congress  of the International Society of Photogrammetry and Remote Sensing Washington DC, August  2, 1992  5  Johnson Andrew E Jason A Keim and Tonislav Ivanov  322Analysis of Flash Lidar Field Test Data  for Safe Lunar Landing\323 IEEE Aerospace Conferenc e  March 6 13, 2010, Big Sky, Montana  expected  6  Johnson  Andrew E  and Miguel SanMartin Motion Estimation from Laser Ranging for Autonomous Comet Landing Proc Int'l Conf Robotics and Automation pp. 132 138, April 2000  7  Lowe, David G. "Distinctive image features from scale invariant keypoints International Journal of Computer Vision vol. 60.2 \(2004\: 91 110  8  Wertz, James R Spacecraft Attitude Determination and Control  Boston Kluwer Academic Publishers 1978  558 566  9  Melvin \(Jay\ White II, Tom Criss, and Dewey Adams  322 APLNav Terrain Relative Navigation Helicopter Field Testing 323 AIAA Guidance Navigation and Control Conference  August 2009, Chicago, Illinois   B IOGRAPHY  Jason Keim received his B S.B.M.E from the University of Southern California and  M.S.M.E from California State University, Los Angeles. Since 2002, Jason has been a member of the Guidance and Control Analysis Group at the NASA Jet Propulsion Laboratory. His primary focus has been the development and validation of formation flight algorithms and technologies for missions such as NASA Starlight TPF and DARPA F6  Additionally, he has contributed to the autonomous surface operations of the Mars Science Laboratory data processing and t rajectory reconstruction for the Autonomous Landing and Hazard Avoidance Technology program and other research and technology development programs  Dr Sohrab Mobasser is a Senior Member of the Engineering Staff at the National Aeronautics and Space Admi nistration\325s NASA Jet Propulsion Laboratory JPL Sohrab has more than 26 years of aerospace industry experience most of it in spacecraft attitude determination His work can be found on many planetary missions from the Galileo mission to Jupiter to t he successful Pathfinder mission to Mars and the Cassini mission to Saturn His current interests are new technology and applications for autonomous spacecraft attitude determination  Sohrab is the Field Test Lead for ALHAT project    Dr Yang Cheng  Dr Y ang Cheng is senior staff member at JPL and he has been involved in many NASA and reimbursable robotic projects for many years He is the key algorithm developer for the Descent Image Motion Estimation System \(DIMES which played a critical role for Mars Exploration Rovers landing safely on Mars. He was also the key software developer for the MER onboard visual odometry which has been used during critical rover maneuvers and traverses.  In addition, he was also involved in novel vision algorithm developme nt for future Mars and Lunar safe and pinpoint landing   Tonislav Ivanov is an Associate Member of Technical Staff in the Computer Vision Group at JPL He works on lidar data processing  field test setup, and is helping develop the hazard detection algori thm for the Autonomous  Landing and Hazard Avoidance Project He also works on recognition using stereo data for robot human awareness and lunar terrain characterization for future missions to the Moon    320  


12 Dr Da Kuang  received his Ph.D in Aerospace Engineering from The University of Texas at Austin in 1995. He joined the Orbiter and Radio Metric Systems Group at JPL in 1996 His work has been focused on analyzing GPS and GPS like tracking data for precise orbit determ ination and precise relative positioning  Dr Andrew Johnson  is a Principal Member of Technical Staff in the Optical Navigation Group at JPL  He is the JPL Project Manager and  terrain sensing algorithm lead for the Autonomous Landing and Hazard Avoida nce Project which is developing technology for safe and precise landing for the next generation manned lunar lander At JPL he works on development validation and flight implementation of computer vision systems for planetary landers and Mars rovers  Ha nnah R Goldberg  received her M.S.E.E and B.S.E from the Department of Electrical Engineering and Computer Science at the University of Michigan in 2004 and 2003 respectively. She has been employed at the Jet Propulsion Laboratory California Institute of Technology since 2004 as a member of the technical staff in the Precision Motion Control and Celestial Sensors group Her research interests include the development of nano class spacecraft and microsystems   Garen Khanoyan  is a member of the Advanced Co mputer Systems  Technologies group at JPL He has been involved with field testing activities and the development of the Command  Data Storage Unit since 2008 for ALHAT and MSL projects. Garen received his B S.E.E and M S.C.S from the University of Southern California   David B. Natzic received his B.S.C.S. from the Department of  Computer Science at the Universi ty of Management and Technology He has been employed as a n Associate Member of Technical Staff in the Guidance Navigation and Control Group at JPL Dave Joined JPL in 1992 and currently serves as a n  essential ALHAT team member focusing on the design integration and field testing of Flash LIDAR instruments onboard aerial platforms 


13 A PPENDIX    Seconds since targets appeared in the camera field of view   Seconds since the targets appeared in the camera filed of view    A         B      Seconds since targets appeared in the camera field of view   Seconds since the targets appeared in the camera filed of view   C         D Figure 16  The error between the propagated and estimated attitude  A is the scalar error and B is the vector error before the corrections.  \(C\ is the scalar error and \(D\ is the vector error after the correction   200  180  160  140  120  100  80  60  40  20  0  200  180  160  140  120  100  80  60  40  20  200  180  160  140  120  100  80  60  40  20   200  180  160  140  120  100  80  60  40  20  0.35  0.30  0.25  0.10  0.05  0  0  Degrees  0.20 0.15  0.35  0.30  0.25  0.10  0.05  0  Degrees  0.20  0.15  0  0.15  0.10  0  0.15  0.20  0.25  Degrees  0.05  0.10  0.05  0.15  0.10  0  0.15  0.20  Degrees  0.05  0.10  0.05  0  


14   A        B  Figure 17: Yaw and pitch errors from second Borrow Pit flight for \(A\ the LIDAR and \(B\ the camera      A        B  Figure 18: Yaw and pitch errors from the third Lakebed flight for \(A\ the LIDAR and \(B\ the camera  Degrees  Seconds into flight  Seconds into flight  0 3 0  0.25  0.20  0.15 0.10  0.05  0  0 05  0 10  0 15  Degrees  0 3 0  0.25  0.20  0.15 0.10  0.05  0  0 05  0 10  0 15  Degrees  1.0  0.5  0  0.5  1.0  1.5  2.0  2.5  Degrees  1.0  0.5  0  0.5  1.0  1.5  2.0  2.5  500  2000  2500  1000  1500  3000  3500  Seconds into flight  500  2000  2500  1000  1500  3000  3500  Seconds into flight  1000  1500  500  2000  2500  0  0.20  1000  1500  500  2000  2500  0  0.20  


                                                  S J       


                                                      


                         L A                                        


          L A  Table 7. Table of Granules at left-hand-side is isomorphic to  at right- hand-side: By Theorem  3.1 one can ?nd patterns in either table as a single generalized concept  Internal points  are:[4]\(1, 1, 0, 0 tions; [5]\(0, 1, 1, 0  0, 1, 0, 1  0, 1, 1, 1  1, 1 1, 0  1, 1, 0, 1  1, 0, 1, 1 11]\(1, 1, 1, 1 form and simplify them into disjoint normal forms 1  T E N    S J    T E N    S J 2  T W E N T Y    L A    T H I R T Y   A 3  T W E N T Y      T H I R T Y   A 4  T W E N T Y            L A 5  T E N      T W E N T Y    L A    T E N    T W E N T Y   A   S J    T W E N T Y   A      T H I R T Y    L A     Y 7  T E N      T W E N T Y      T H I R T Y   L A    T E N   L A    S  J   A 8  T W E N T Y      T E N      T W E N T Y    L A    T E N   T W E N T Y      T H I R T Y 9  T W E N T Y    N Y    T E N    S J    T H I R T Y    L A      T W E N T Y    L A  1 0  T W E N T Y    N Y    T W E N T Y    L A    T H I R T Y   A    J 1 1  T W E N T Y          T W E N T Y    L A   T H I R T Y    L A    a l l If the simpli?ed expression is a single clause \(in the original symbols non-generalized the following associations 1   T E N     S J    T E N    S J  2. SJ   J 4   L A    T W E N T Y    L A    T H I R T Y    6 Conclusions Data, patterns, method of derivations, and useful-ness are key ingredients in AM. In this paper, we formalize the current state of AM: Data are a table of symbols. The patterns are the formulas of input symbols that repeat. The method of derivations is the most conservative and reliable one, namely, mathematical deductions. The results are somewhat surprising 1. Patterns are properties of the isomorphic class, not an individual relation - This implies that the notion of patterns may not mature yet and explains why there are so many extracted association rules 2. Un-interpreted attributes \(features can be enumerated 3. Generalized associations can be found by solving integral linear inequalities. Unfortunately, the number is enormous. This signi?es the current notion of data and patterns \(implied by the algorithms 4. Real world modeling may be needed to create a much more meaningful notion of patterns. In the current state of AM, a pattern is simply a repeated data that may have no real world meaning. So we may need to introduce some semantics into the data model [12],[10],[11 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE References 1] R. Agrawal, T. Imielinski, and A. Swami  Mining Association Rules Between Sets of Items in Large Databases  in Proceeding of ACM-SIGMOD international Conference on Management of Data, pp. 207216, Washington, DC, June, 1993 


216, Washington, DC, June, 1993 2] Richard A. Brualdi, Introductory Combinatorics, Prentice Hall, 1992 3] A. Barr and E.A. Feigenbaum, The handbook of Arti?cial Intelligence, Willam Kaufmann 1981 4] Margaret H. Dunham, Data Mining Introduction and Advanced Topics Prentice Hall, 2003, ISBN 0-13088892-3 5] Fayad U. M., Piatetsky-Sjapiro, G. Smyth, P. \(1996 From Data Mining to Knowledge Discovery: An overview. In Fayard, Piatetsky-Sjapiro, Smyth, and Uthurusamy eds., Knowledge Discovery in Databases AAAI/MIT Press, 1996 6] H Gracia-Molina, J. Ullman. &amp; J. Windin, J, Database Systems The Complete Book, Prentice Hall, 2002 7] T. T. Lee  Algebraic Theory of Relational Databases  The Bell System Technical Journal Vol 62, No 10, December, 1983, pp.3159-3204 8] T. Y. Lin  Deductive Data Mining: Mathematical Foundation of Database Mining  in: the Proceedings of 9th International Conference, RSFDGrC 2003 Chongqing, China, May 2003, Lecture Notes on Arti?cial Intelligence LNAI 2639, Springer-Verlag, 403-405 9] T. Y. Lin  Attribute \(Feature  The Theory of Attributes from Data Mining Prospect  in: Proceeding of IEEE international Conference on Data Mining, Maebashi, Japan, Dec 9-12, 2002, pp. pp.282-289 10] T. Y. Lin  Data Mining and Machine Oriented Modeling: A Granular Computing Approach  Journal of Applied Intelligence, Kluwer, Vol. 13, No 2, September/October,2000, pp.113-124 11] T. Y. Lin, N. Zhong, J. Duong, S. Ohsuga  Frameworks for Mining Binary Relations in Data  In: Rough sets and Current Trends in Computing, Lecture Notes on Arti?cial Intelligence 1424, A. Skoworn and L Polkowski \(eds 12] E. Louie,T. Y. Lin  Semantics Oriented Association Rules  In: 2002 World Congress of Computational Intelligence, Honolulu, Hawaii, May 12-17, 2002, 956961 \(paper # 5702 13  The Power and Limit of Neural Networks  Proceedings of the 1996 EngineeringSystems Design and Analysis Conference, Montpellier, France, July 1-4, 1996 Vol. 7, 49-53 14] Morel, Jean-Michel and Sergio Solimini, Variational methods in image segmentation : with seven image processing experiments Boston : Birkhuser, 1995 15] H. Liu and H. Motoda  Feature Transformation and Subset Selection  IEEE Intelligent Systems, Vol. 13 No. 2, March/April, pp.26-28 \(1998 16] Z. Pawlak, Rough sets. Theoretical Aspects of Reasoning about Data, Kluwer Academic Publishers, 1991 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 





