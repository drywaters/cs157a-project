  0 


 


 


 


associated with is full, it is placed in the queue with the next highest priority which has available space If an incoming packet arrives and there is no space available in any of the virtual sub-queues, then the incoming packet replaces the oldest lowest priority packet in the queue, unless it is of a higher priority than the incoming packet. For example, if a packet arrives with priority 5 and all of the virtual sub-queues are full, then it is compared with the oldest lowest priority packet. Note that the oldest lowest priority packet will be the last element in the lowest priority virtual queue, thus, no searching is necessary. Since the incoming packet is priority 5, it will replace the last element unless the old packet has a higher priority \(priority 6 or greater\sure that if the last element is also priority 5, that the newer incoming packet replaces the older packet to preserve the freshness of the data. Thus, an incoming packet will only be dropped if none of the virtual sub-queues have available space and the incoming packet is of a lower priority than all other packets in the queue  4.2. Dequeuing  The dequeuing portion of the algorithm takes into account four distinct parameters in its computation current network congestion \(packets per virtual subqueue pv 1 pv 2  pv n he free or available space in the pending virtual queue \(packets in pending virtual queue pp\e number of virtual sub-queues \(n\d the weight of each virtual sub-queue \(w 1 w 2 w 3   w n  The virtual queue weights are defined as a function of congestion. Congestion is defined as the total used space in the lightweight queue divided by the size of the lightweight queue. We implemented the congestion levels as ranges.  The number of ranges is application specific and can be altered to fit the user s needs. For our application we implemented four congestion levels: 0-40%, 41-75%, 76-90%, 91100%. Thus, each of the four congestion levels is associated with a set of weights, one for each of the virtual sub-queues  Assignment of weights \(w 1  w 8 for each virtual queue, is determined as a function of both the free space in the virtual pending queue \(pp\d the congestion levels, as shown in Table 2. Note, that the sum of the weights for each congestion level must equal pp. By assigning the weights of each virtual sub-queue based on the current free space in the virtual pending queue, we continually and dynamically re-adjusting the weights, as the network changes in order to reflect its current conditions  Table I: Virtual queue weights based on congestion level  Index Level 1 0-40 Level 2 41-75 Level 3 76-90 Level 4 91-100 VQ1 0.20*pp 0.30*pp 0.40*pp 0.50*pp VQ2 0.20*pp 0.20*pp 0.20*pp 0.15*pp VQ3 0.15*pp 0.15*pp 0.15*pp 0.10*pp VQ4 0.15*pp 0.10*pp 0.10*pp 0.10*pp VQ5 0.10*pp 0.10*pp 0.05*pp 0.05*pp VQ6 0.10*pp 0.05*pp 0.05*pp 0.05*pp VQ7 0.05*pp 0.05*pp 0.025*pp 0.025*pp VQ8 0.05*pp 0.05*pp 0.025*pp 0.025*pp  4.2.1. Dequeuing algorithm details There are four steps that take place in the dequeuing algorithm First, the current congestion level at the node must be determined. Second, the weight of each of the virtual sub-queues must be calculated based on the current congestion, the available space in the virtual pending queue, and the application constants associated with each congestion level. Once the weight of each virtual sub-queue is calculated, the third step is to determine the number of packets to be dequeued from each virtual sub-queue. The number of packets to be dequeued from virtual sub-queue 1 \(DP 1  determined by dividing the free space in the virtual pending queue by the number of virtual sub-queues then multiplying the result by the weight of virtual sub-queue 1. For example, if the network is 50 congested \(note:  w 1 30*pp\ere is room for 24 packets in the virtual pending queue, and there are 8 virtual sub-queues, then the most packets that can be dequeued from virtual sub-queue 1 is  \(.30*24 24/8\1. However, we cannot dequeue more packets from the virtual sub-queue than it has \(S 1  Say that there are only 4 packets in virtual sub-queue 1, thus, we can only take 4 packets from virtual subqueue 1. Thus   S 1 21- 4 = 17.  This is shown below in Equations 4 and 5    n pp w pv DP   min 1 1 1   4 1 1 1 1   max DP n pp w pv S    5  Now, the number of packets that we can dequeue from virtual sub-queue 2 \(DP 2 note w 2 20*pp 20*24\ \(24/8\4, but we can also add the 17 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 5 


packets that we did not dequeue from virtual subqueue 1. Thus, the most we can dequeue from virtual sub-queue 2 is 14 + 17 = 31. However, if we only have 3 packets in virtual sub-queue 2 then that is the most that we can dequeue \(S 2 his is depicted below in Equations 6 and 7    1 2 2 2   min S n pp w pv DP  6  2 1 2 2 2   max DP S n pp w pv S    7  This process continues until the number of packets to be removed form the last virtual priority sub-queue is determined. The general forms of the equations are shown below in Equations 8 and 9           2 2 2 1 1 1   max   max n n n n n n DP n pp w pv n pp w pv S 8    1   max n n n n S n pp w pv DP 9  The final step is to remove the appropriate number of packets form each of the virtual sub-queues. This is done according to the number of packets to be removed from each virtual priority sub-queue described above  5. Results  In order to evaluate our Tiny-DWFQ algorithm we designed and ran a series of tests. Our goal was to design a Quality of Se m a n a g e m e n t  algorithm for wireless sensor networks. Thus, in order to accurately test whether or not we had accomplished this we felt that it was necessary to forgo all simulations and physically implement this algorithm on a real wireless sensor network Naturally, implementation on real sensors brings to the forefront all of the limitations and obstacles associated with these resource constrained devices For example, measuring the packet loss, for each priority level, with simulations is a relatively easy task. It simply requires the counting and recording of the number of packets sent and received for each priority level which can be displayed through print or output statements. However, due to the limited resources of the individual nodes, this addition of a print statement \(which is actually an additional packet\or each packet, can cause serious congestion and in some cases failure of nodes. This caused us to use alternative measures to compute the packet loss for each priority level while not overloading the nodes. Our approach is discussed in detail in Section 5.2 Evaluation of our algorithm was done by measuring two network statistics, packet loss and throughput. Each of these was measured for each of the different priority levels. In our evaluations we compared our Tiny D-WFQ algorithm with Tiny WFQ. Additionally, we also implemented a simple FIFO queue for additional comparison which, when full, drops newly arriving packets. For each algorithm we implemented a queue size of 40, in order to achieve a fair comparison   5.1. Network scenario  Our experiments were all conducted using a testbed consisting of 10 Imote2 wireless sensor nodes. The testbed was setup inside our laboratory Due to the limited space and the goal of capturing a realist scenario we turned down the radio range of each node so that we could induce the multihop nature which would be present in the field. The nodes were arranged in a random or scattered fashion. The tesbed includes Panorama \(see Figure 3\which we designed and used as a monitoring tool for simulation of the senor nodes on Mount St. Helens. Each node s generated and multihop data was sent to a sink node through a dynamic multihop path\d then forwarded to its final destination which was a laptop computer. This laptop computer is representative of the computer which would be housed at the command and control center in a real volcanic monitoring scenario    Figure 3: Panorama monitoring tool Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 


We implemented the tests by generating seven different types of data streams, emulating data collected at a volcanic site, at each node: seismic Real-Time Seismic-Amplitude Measurement RSAM\mic triggered event data, seismic interevent data \(continuous\rasonic RSAM data infrasonic triggered event data, infrasonic inter-event data \(continuous\, and lightning data. The reason for the simulation of these data types is due to the cost of acquiring 10 of each of these sensors. Additionally we were able to adjust the data rates to that which would be sampled from the actual sensors thus causing no difference in the evaluation of our algorithms. The sampling rates and the priorities assigned to each of the different data types are shown below in Table 3   Table 3: Data rates and priorities  Data Type Sampling Rate bytes/sec Priority Seismic RSAM 4 6 Seismic Event 0-200 4 Seismic Inter-Event 100 2 Infrasonic RSAM 4 5 Infrasonic Event 0-200 3 Infrasonic Inter-Event 100 1 Lightning 2 5  It should be noted that these sampling rates and priority assignments were not arbitrary, rather there were used as recommended by Earth Scientists who specialize in volcanic monitoring. Additionally, for the event data for both seismic and infrasonic the sampling rates are 0-200 bytes per second. This is due to the fact that event data refers to data that is representative of a triggered or actual physical event or activity. Thus, at some times there is not event data \(0 bytes/sec\owever, when event data is being generated it can be sampled at a very high rate \(200 bytes/sec   5.2. Packet loss  We had two goals to accomplish in evaluating the packet loss. First, we wanted to make sure that our experiment was performed in as realistic an environment as possible.  In order to accomplish this we ran all of our tests on a real testbed of Imote2 sensors. Additionally, we ran the tests with data types, rates, and priorities defined by Earth Scientists as discussed in Section 5.1. These are real scenario parameters that are being used in volcanic monitoring scenarios. Second, we wanted to evaluate the packet loss for each individual priority level for each of the three algorithms. In order to compute packet loss PL i or each individual priority level i, we measured the total number of packets generated at each node for each priority level, x k where k is the node s id. Next, we measured the total number of packets received by the destination \(laptop\or each priority level, y. This allowed us to compute the packet loss for each priority level by computing one minus the sum of each of the number of packets generated at each node divided by the number of packets received by the destination \(laptop\or each priority level. The total pack et loss for each priority level is displayed below in Equation 10    1 11 1 y x PL k k i    10 In order to accurately capture a realistic scenario we felt that it was necessary take the average of multiple runs. More specifically, since we used realistic data generated on real motes we were not necessarily getting the exact same data set for each run, which introduces some variances in the results To reduce the affects of this we ran 10 experiments each for 20 minutes and took the average of these 10 runs. The results are shown in Figures 1-6  0 10 20 30 40 Packet Loss 5 101520 Time \(minutes Priority 1: Packet Loss FIFO Tiny-WFQ Tiny-DWFQ  Figure 1 0 10 20 30 40 Packet Loss 5101520 Time \(minutes Priority 2: Packet Loss FIFO Tiny-WFQ Tiny-DWFQ  Figure 2 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 


 0 10 20 30 40 Packet Loss 5 101520 Time \(minutes Priority 3: Packet Loss FIFO Tiny-WFQ Tiny-DWFQ  Figure 3 0 10 20 30 40 Packet Loss 5101520 Time \(minutes Priority 4: Packet Loss FIFO Tiny-WFQ Tiny-DWFQ  Figure 4 0 10 20 30 40 Packet Loss 5101520 Time \(minutes Priority 5: Packet Loss FIFO Tiny-WFQ Tiny-DWFQ  Figure 5 0 10 20 30 40 Packet Loss 5101520 Time \(minutes Priority 6: Packet Loss FIFO Tiny-WFQ Tiny-DWFQ  Figure 6  It can be seen in Figures 1-6 that Tiny-DWFQ performs better than both Tiny-WFQ and FIFO algorithms in terms of reducing the packet loss. It is interesting to note that a simple FIFO algorithm outperforms the Tiny-WFQ in most cases, especially for low priority messages \(Figures 5 and 6 messages with higher sampling rates \(Figures 3 and 4\. Additionally, it can be seen that Tiny-DWFQ s best relative improvements are for the mid range priorities \(priorities 3 and 4 in Figures 3 and 4\. This is due to the high rate at which these priority packets were generated. It can be seen from Table 3 that priorities 3 and 4 have the highest sampling rates Therefore, as the number of packets increases so does the relative improvement of Tiny-DWFQ over the other algorithms However, Tiny-DWFQ s primary goal is to ensure a high level Quality of Service for high priority data. Thus, we are particularly interested in the performance of Tiny-DWFQ for the high priority data \(note priority 6 is the highest\igure 6 shows that for the highest priority data Tiny-DWFQ only lost between 12.09% and 14.93% of the data while Tiny-WFQ lost between 17.09% and 21.82% of the data and the FIFO queue lost between 15.68% and 18.25% of the data  5.3. Throughput   Computation of the total throughput of our system using each of the three algorithms was measured with the same goals in mind as we had with the packet loss. We wanted to ensure a realistic environment and compute the total throughput for each individual priority level. The total throughput \(TP i or each priority level i is defined as the number of packets p k which traverse from the source to the destination per unit of time, u.  This is shown below in Equation 11  u p TP k k i  11 1     11  However, since we are using realistic data generation, as discussed in the previous section, the precise number of packets generated for each of the different runs varied slightly. Thus, in order to accurately evaluate the throughput it was necessary for us to take the throughput, TP i and divide it by the total number of packets generated g i for priority level i during that experimental run. The throughput percentage, TPP i is shown below in Equation 12  100    i i i g TP TPP   12  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


As described for the packet loss we also ran 10 experiments each for 20 minutes and took the average of the 10 runs for throughput. The results are shown below in Figures 7-12  50 60 70 80 90 Throughput  5101520 Time \(minutes Priority 1: Throughput FIFO Tiny-WFQ Tiny-DWFQ  Figure 7  50 60 70 80 90 Throughput  5101520 Time \(minutes Priority 2: Throughput FIFO Tiny-WFQ Tiny-DWFQ  Figure 8 50 60 70 80 90 Throughput  5101520 Time \(minutes Priority 3: Throughput FIFO Tiny-WFQ Tiny-DWFQ  Figure 9 50 60 70 80 90 Throughp ut 5101520 Time \(minutes Priority 4: Throughput FIFO Tiny-WFQ Tiny-DWFQ  Figure 10  50 60 70 80 90 Throughput  5101520 Time \(minutes Priority 5: Throughput FIFO Tiny-WFQ Tiny-DWFQ  Figure 11  50 60 70 80 90 Throughput 5101520 Time \(minutes Priority 6: Throughput FIFO Tiny-WFQ Tiny-DWFQ  Figure 12   In all cases, Figures 7-12 show that Tiny-DWFQ outperformed both of the other algorithms in terms of improving the network throughput. Similar to the previous results, the FIFO algorithm performs better than Tiny-WFQ for high frequency data \(Figures 9 and 10\w priority data \(Figures 11 and 12 Additionally, it can be seen that Tiny-DWFQ s best relative improvements are for the mid range priorities priorities 3 and 4 in Figures 9 and 10\se of the very high sampling rates of these data While we are primarily concerned with the highest priority data, we also do not want to starve the lower level data. If we did not care about any of the lower priority data we would just not transmit it at all. Therefore, it is good that we are able to maintain better Quality of Service than the other algorithms even for low level traffic For the highest priority data, Figure 12 shows that Tiny-DWFQ achieved a throughput percentage between 85.07% and 88.18% while Tiny-WFQ only achieved a throughput percentage between 78.63 and 82.59% and the FIFO queue s throughput percentage was between 81.75% and 84.32 Hence, Tiny-DWFQ was able to show improvement over the other algorithms for all priorities of data but most importantly for high priority data Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


We believe that the reason for this improvement is twofold. First, our enqueuing algorithm allowed us to fully utilize the entire queue structure for high priority packets in the midst of congestion. Secondly our dequeuing algorithm utilized not only information about the DWP of the packet but also the current network congestion as well as the available network resources. This allowed us to essentially store the higher priority packets in the queue until there was room for them in the network  6. Conclusions and future work  In conclusion, we have designed and implemented a lightweight dynamic weighted fair queuing algorithm, Tiny-DWFQ on a real wireless sensor network testbed. Our goal was to introduce a novel dynamic scheduling algorithm to increase the bandwidth for high priority data. We accomplished this by ensuring a high level of Quality of Service for high priority data without starving the lower priority data. In order to compare our algorithm against others we implemented a lightweight version of WFQ Tiny-WFQ, as well as a simple FIFO queue. The metrics that we used for evaluation were packet loss and throughput. After conducting our tests we were able to show improvement in both an increased throughput percentage as well as decreased packet loss for all types of data We are currently in the process of implementing a large-scale Optimized Autonomous Space In-Situ Sensorweb \(OSAIS\or monitoring Mount St Helens, with Tiny-DWFQ integrated into the middleware component. This sensorweb will be deployed continuously for one year \(2009-2010   10. References   A  Berf i e l d an d D M o sse  Efficient Scheduling for Sensor Networks Mobile and Ubiquitous Systems: Third Annual International Conference on Networking Services, July 2006, pp. 1-8   A  D e m e rs S  D e sh av  an d S  S h en ker  Analysis and Simulation of a Fair Queuing Algorithm In Proceedings of SIGCOMM 89, September 1989, pp. 1-12   Y Drou gas an d  V  Kal o geraki   RASC: Dynamic Rate Allocation for Distributed Stream Processing Applications Parallel and Distributed Processing Symposium IPDPS 2007, March 2007, pp. 1-10  4 J. Frolik  Q oS Contro l f o r ra ndom a cces s w i rele ss  sensor networks", WCNC 2004  IEEE Wireless Communications and Networking Conference, 2004 pp. 1510-1515   J-Y Hu a n g Y-K T s en g M  S  L i n  a n d W S  Hsieh  Error Rate-Based Dynamic Weighted Fair Queuing In Wireless Networks," The IEEE Conference on Local Computer Networks 30th Anniversary \(LCN'05\5, pp 546-553  6 R. Iy er, L  Klein ro ck  QoS control for sensor networks IEEE International Conference on Communications ICC 03, 2003, pp. 517 521  7 N-S. Ko, a nd H-S P a rk  Emulated Weighted Fair Queuing Algorithm for High-speed packet-switched networks In Proceedings of 15th International Conference on Information Networking, 2001, pp 52-58   Op t i m i zed  A u t o no m o u s S p ace In S i t u S e n s o r w e b   http://sensorweb.vancouver.wsu.edu/wiki/index.php/Main_ Page  9 N  O u f e rha t a n d A  Me ll ouk  A Q o S Sc he du le r Packets for Wireless Sensor Networks," IEEE/ACS International Conference on Computer Systems and Applications, 2007, pp. 211-216  1 A  P a rekh  Co n t r o l i n I n t e grat ed S e rvi ces Net w o r ks  PhD Thesis, MIT, February 1992  11 S. T a i, R.R. Be nk oc z i H   H a s s a n e i n, a nd S. G   A k l QoS and data relaying for wireless sensor networks  Journal Parallel Distribution Computing, 2007, pp. 715726  12 S. W a ng Y  W a ng a nd K  L i n A Priority-Based Weighted Fair Queuing Scheduler for Real-Time Network In Proceedings of the Sixth international Conference on Real-Time Computing Systems and Applications, IEEE Computer Society, Washington, DC 1999, pp. 312  13 K  W u Y  G a o, F L i a n d Y   Xia o  Lightweight Deployment-Aware Scheduling for Wireless Sensor Networks Mobile Networks and Applications, December 2005, Volume 10, Issue 6, pp. 837-852  14 D-B. Yin a n d J-Y. X i e   Probability Based Weighted Fair Queueing Algorithm with Adaptive Buffer Management for High-Speed Network Lecture notes in computer science. International Conference on Natural Computation No2, CHINE, 2006, pp. 992-998  1  X  Z h an g Y Ch en an d  X  W a n g   PAS: A PowerAware Static Scheduling Algorithm for Sensor Network  Communications and Networking in China ChinaCom 06 October 2006, pp. 1-4  16 Q  Zha o a nd L  T ong  QoS Specific Medium Access Control for Wireless Sensor Networks with Fading In Proceedings of 8th Intern ational Workshop on Signal Processing for Space Communications \(SPSC 03\, 2003 pp.1-8 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


selection is dominated by O   C k  2   Finally the space requirement is O  MAX  C k  2    Since the size of a community is small both the time and space are also small VII E XPERIMENTAL R ESULTS We now evaluate the performance of our algorithm for object connection discovery We run all experiments on an AMD Opteron 248 with 1GB RAM running Linux 64-bit We use the DBLP co-authorship dataset modeled as a graph The graph has approximate ly 316K nodes and 1,834K edges where a node represents an author and the edge weight is the number of papers co-authored between two authors A Performance of Community Partition We rst evaluate the performance of community partition by our greedy algorithm We test three settings of c local 10 50 and 100 and four settings of c global 10 100 1000 10000 We also compare with Newmans algorithm 13 Figure 11\(a shows that our algorithm is about an order of magnitude faster than Newmans when c local 10 Theresult also shows that when c local increases the efciency decreases which demonstrates the effectiveness of Heuristic 1 10 100 1000 10000 10 2 10 3 10 4 c g lobal Running Time \(sec c local 10 c local 50 c local 100 Newman a Running Time 10 100 1000 10000 0 50 100 150 200 250 300 c g lobal Peak Memory Consumption \(MB c local 10 c local 50 c local 100 Newman b Memory Consumption Fig 11 Performance of Community Partition For the effect of Heuristic 2 i.e c global  the performance is the best when c global  1000 When c local 10  the running time is 682 613 570 and 667 seconds for c global  10 100 1000 and 10000 respectively The result can be explained as follows When c global is too small many items are not kept in the global max-heap and hen ce we need to rebuild the heap more often When c global is too large there are too many items in the heap and hence the update of the heap takes longer Figure 11\(b shows that the peak memory consumption of our algorithm increases when c local increases since the size of the local max-heaps increases when c local increases Increasing c global  i.e the size of the global heap from 10 to 10000 only increases the memory usage for less than 1 MB since we have only one global heap However in all cases our algorithm consumes considerably less memory than Newmans We also record that the value of the modularity of the optimal community partition obtained by the greedy algorithm is 0.71 note that all the algorithms compute the same partition According to Newman 13 a m odul ari t y v a l u e o f g reat er t h an 0.3 indicates a signicant comm unity structure Therefore 0.71 is a very high value of modularity and indicates a highquality community partition B Semantics of Answer Graph A Case Study We conduct a case study to compare our answer graph with the center-piece subgraph  CEPS  10 Thi s s t udy ai ms to rst provide a more intuitive view on the answer graphs obtained by our algorithm and CEPS Then we perform a more systematic comparison in the following subsection We use the query  Jim Gray  Jennifer Widom  Michael I Jordan  Geoffrey E Hinton   The four scholars are from two different communities Gray and Widom are from the database community while Jordan and Hinton are from the machine learning community This is clearly captured by our answer graph as shown in Figure 1 which is displayed in Section I Figure 12 shows the answer graph of CEPS which is very similar to our answer graph The similarity is because both our algorithm and CEPS nd nodes that are closely related to the query nodes in order to connect them Thus the result shows that both algorithms are able to capture important nodes and paths related to the query nodes However our method not only nds a good connection between all query nodes but also for query nodes that are in the same context we put more emphasis on their connection than the existing methods Compare Figure 1 with Figure 12 we clearly see a stronger connection between Gray and Widom through both Ceri and Hellerstein in our answer graph than CEPS Michael I Jordan Jim Gray Alexander Aiken 1 3 Jennifer Widom Michael Stonebraker Tommi Jaakkola Lawrence K. Saul 3 Zoubin Ghahramani 9 Geoffrey E. Hinton 7 1 2 5 11 2 1 9 1 1 5 3 Joseph M Hellerstein Fig 12 The Answer Graph of CEPS Although the quality of the answer graphs is comparable our algorithm signicantly outperforms CEPS we take only 0.33 seconds to compute our answer graph while the computation of the CEPS takes 925 seconds We further compare the two methods using more systematic measures as follows C Performance of Object Connection Discovery We compare the performance of our algorithm PCquery with CEPS 10  W e s et t h e b udget t o b e t w i ce of t h e query size We also verify that the answer graphs obtained by PCquery and CEPS are of roughly the same size Other settings of CEPS are as its default We generate two types of queries in-community queries and random queries  which are abbreviated as cq and rq in the gures For in-community queries the nodes in a query are randomly selected from a randomly selected community For random queries the nodes in a query are randomly selected from the set of all nodes in the dataset We generate 100 queries for each type and test the query size from 2 nodes to 20 nodes Figure 13\(a reports the average running time of nding the connection for a query The result shows that PCquery is more 
866 
866 
 


than three orders of magnitude faster than CEPS for both query types We nd that PCquery takes more time to process an incommunity query than a random query This is because the computation of the intra-community connection by dynamic programming is more costly than that of the inter-community connection by tracing the c ommunity hierarchy tree 2 4 8 12 16 20 10 2 10 0 10 2 10 4 Quer y Size \(number of nodes Average Response Time \(sec PCquery \(cq CEPS \(cq PCquery \(rq CEPS \(rq a Average Response Time 2 4 8 12 16 20 0 100 200 300 400 500 600 700 Quer y Size \(number of nodes Peak Memory Consumption \(MB PCquery \(cq CEPS \(cq PCquery \(rq CEPS \(rq b Memory Consumption Fig 13 Efciency of PCquery and CEPS Figure 13\(b reports the peak memory consumption during the entire running process The result shows that PCquery also consumes signicantly less memory than CEPS in all cases In addition to the comparison on efciency we also compare the quality of the answer graphs obtained by PCquery and CEPS For the fairness of comparison We use the quality metrics proposed in CEPS 10  NRatio and ERatio which indicate the percentage of important nodes and edges that are captured by an answer graph respectively We report the result in Figure 14 2 4 8 12 16 20 0 20 40 60 80 100 Quer y Size \(number of nodes NRatio PCquery \(cq CEPS \(cq PCquery \(rq CEPS \(rq a Average NRatio 2 4 8 12 16 20 0 20 40 60 80 100 Quer y Size \(number of nodes Eratio PCquery \(cq CEPS \(cq PCquery \(rq CEPS \(rq b Average ERatio Fig 14 Quality of Answer Graph Figure 14 shows that both PCquery and CEPS obtain high-quality answer graphs Both NRatio and ERatio of our answer graphs are comparable to those of CEPS although on average those of CEPS are slightly better Considering our algorithm is three orders of magnitude faster and also consumes signicantly less memory we can conclude that our method is both efcient and effective VIII C ONCLUSIONS We propose context-aware object connection discovery in a large graph We adopt a partition-and-conquer approach to achieve both high performance efciency and high quality results Our method rst partitions a large graph into a set of communities The concept of community not only naturally denes the context of the nodes but also signicantly improves the efciency of connection d iscovery since a community is much smaller than the original graph We compute the connection between query nodes rst at the intra-community level by maximizing the information throughput of the nodes and the information ow of the paths in the answer graph and then at the inter-community level by retaining the close relation between the commun ities as dened by modularity The quality of both the intraand intercommunity connection is thus controlled by the integration of information throughput/ow and modularity We verify by experiments that our community partition algorithm is efcient and the set of communities obtained has high quality We also show that our method obtains comparable high-quality answers as the state-of-the-art algorithm but is more than three orders of magnitude faster and consumes signicantly less memory Acknowledgement This work is partially supported by RGC GRF under grant number CUHK419008 and HKUST617808 We thank Mr Hanghang Tong and Prof Christos Faloutsos for providing us the source code of CEPS R EFERENCES  X  Y an P  S  Y u and J  H an  Graph i nde xing bas e d o n d is crim inati v e frequent structure analysis ACM TODS  vol 30 pp 960993 2005  J  C he ng Y  K e  W  N g a n d A  L u  F g-i nde x t o w a rds v e r i  c a t i on-fre e query processing on graph databases in SIGMOD  2007 pp 857872  P  Z hao J  X Y u  a nd P  S Y u   Graph i nde xing T r ee  d elta   graph in VLDB  2007 pp 938949 4 Y  K e J  Cheng and W  N g Cor r e lation s ear ch in gr aph d atabas es   in KDD  2007 pp 390399 5 Y  K e J  Cheng and W  N g E f  cient c or r e lation s ear ch f r o m g r a ph databases To appear in TKDE  2008  A  I nokuchi T  W as hio and H  M ot oda An apriori-based algorithm for mining frequent substruc tures from graph data in PKDD  2000 pp 1323  X  Y an and J  H an  Clos e g raph m i ni ng closed frequent graph patterns in KDD  2003 pp 286295  J  H uan W  W a ng J  Prins  and J  Y ang Spin mining maximal frequent subgraphs from graph databases in KDD  2004 pp 581586 9 C  F alouts o s  K  S  M c Cur l e y  a nd A  T o m k ins  F as t d is co v e r y of connection subgraphs in KDD  2004 pp 118127  H  T ong and C  F alouts o s  Center piece s ubgraphs  p roblem de n ition and fast solutions in KDD  2006 pp 404413  Y  K o ren S C North and C  V olins k y  Meas uring a nd e x tracting proximity in networks in KDD  2006 pp 245255  M E  J  Ne wm an and M  G irv a n Finding and e v a luating c om m unity structure in networks Physical Review E  vol 69 p 066113 2004  M E  J  Ne wm an  F a s t algorithm f or detecting c om m unity s t ructure i n networks Physical Review E  vol 69 p 066133 2004  F  W u and B  A  H uber m an  F i nding com m unities i n linear tim e a physics approach The European Physical Journal B Condensed Matter and Complex Systems  vol 38 no 2 pp 331338 2004  R K u m a r  P  Ragha v a n S Rajagopalan and A  T om kins   T r a w ling the web for emerging cyber-communities Comput Netw  vol 31 no 11-16 pp 14811493 1999  R K u m a r  P  Ragha v a n S Rajagopa lan and A Tomkins Extracting large-scale knowledge bases from the web in VLDB  1999 pp 639 650  R K u m a r  U Mahade v a n and D  S i v akum ar   A g raph-theoretic approach to extract storylines from search results in KDD  2004 pp 216225  D Gibs on R K u m a r  and A  T om kins   Dis c o v e ring lar g e d ens e subgraphs in massive graphs in VLDB  2005 pp 721732  Y  Douris boure F  Geraci a nd M Pe llegrini Extraction and classication of dense communities in the web in WWW  2007 pp 461470  A Claus e t M E  J  Ne wm an a nd C Moore Finding com m unity structure in very large networks Physical Review E  vol 70 p 066111 2004 
867 
867 
 


  13 false \(double\argets Figure 12 illustrates this situation. The conditional update correctly updates the tracker wh ich is tasked with following the target. However, the detector which is partially spatially coincident with the tracker also receives energy from the conditional update. This can lead the detector to initiate falsely\second target nearby the first target This effect can be countered a number of ways. First, we can adjust the speed at which the tracker re-centers itself The double initialization phenomenon occurs when the PDF peaks near the edge of the tracker grid. However, this method has the side effect of potentially allowing probability to fall off of the grid in low SNR environments causing track loss. Of course if the SNR is low enough or measurement outages occur tracks will be dropped. Second a guardband around the tracker that does not allow any detector sufficiently near the tracker to receive reinforcement via the conditional density can mitigate the double target problem. However, this has the side effect of preventing detection of closely spaced targets. Third increasing the spatial extent of the tracker has a similar effect as the using a guardband. It does require increased computation, but generates a better representation of the posterior There are several engineering tradeoffs. The first is that large tracker grids \(or large guard bands\ prevent falsely detecting new targets because of conditional probability spill over. However, if applied too aggressively, this will prevent correctly detecting cl osely spaced targets. Second quick tracker grid translation correctly centers the target mass, again preventing spillover into nearby detectors However, overly liberal trac ker repositioning may in fact move trackers to spurious energy locations and drop true targets off of the finite grid On Ambiguous Targets As discussed earlier, ambiguous targets will eventually move non-physically and this will cause the tracker to remove them via its natural prediction and update process Figure 13 illustrates this phenomenon. There are two real targets that create two persistent ambiguities. All four are detected and tracked automati cally. The ambiguous targets however, eventually move non-physically due to their reliance on the node bearing angles. The tracker automatically penalizes the non-physical motion and the targets\222 present hypothesis decrease quickly over time Ambiguous target removal is done automatically in the Bayesian framework as follows The PDF on target state is predicted forward in time according to the kinematic model True targets will have behavior consistent with the kinematic model \(note the kinematic model is a statistical model so it is predicting a range of possibilities for the future target state\biguous targets may behave consistently with this model for a period of time, but eventually they will appear to perform a non-physical maneuver \(these epochs typically come when the ambiguous target crosses a line of symmetry in the sensor\this point, the predicted target position will be in strong disagreement with the inco ming measurements on that target. This mismatch in predicted target position and measurements leads to a decr ease in the target present hypothesis as calculated in eq. \(4\long, only true targets remain   Figure 12 \226 Improper selection of grid resolution leads to multiple initializations on the same target. Left Measurement update of a Tracker \(red=highest likelih ood, blue=lowest\Right Measurement update of a detector which lies near the Tracker.  Since the track er size has been improperly chosen, some energy from the measurements of a single target leak s on to the detector. This can le ad to false double-initializations 


  14 8  C ONCLUSION  This paper has described a Bayesian approach to detecting and tracking multiple moving targets using acoustic data from multiple passive arrays In contrast to traditional undersea acoustic systems, which develop tracks at the single array level and require track association, our approach fuses data at the m easurement level and operates directly in the target state space We have detailed a well known nonlinear filtering approach to single target detection and tracking [1, 4 and desc ri be d our computationally efficient finite-grid approach to the required density estimation. We have furthermore extended this to the multiple target case by employing a bank of single target detector tracke rs and approximation methods that adjust for closely spaced targets. This approximate approach avoids fully treating the computationally complex joint multitarget problem Future work includes modified approaches to posterior estimation including dynamic grid extent, dynamic grid resolution, and particle filtering. It is anticipated that adaptive sampling of the posterior will lead to computational savings. Furthermore, future work includes more detailed modeling and estimation of closely spaced targets allowing a more accurate representation of the joint target density. Naively implemented, this implies exponential growth \(in the number of targets\r the probability state space being es timated. However, recent work in a related tracking domain on adaptive density factorization [5 c h a stic sa m p lin g  p article filtering    pr ovi de m e t h o d s t h at m i t i g at e t h i s com put at i o n gr owt h  when the full joint density is treated  A PPENDIX  This section discusses the details of how the single target probability density is time evolved on a discrete grid. This discussion is similar to that found elsewhere [15, 14, 13 We wish to compute the single target probability density at time      from the density at time     The relation between these two densities can be expressed using the law of total probability as                We expand     using a second order Taylor series as               where  is the vector of partial derivatives, i.e and is the matrix of second order partial derivatives Then the relation of \(23\ approximated as   Figure 13\226 Left: P h1 over time for four targets, two of which are real and two of which are ambiguous. Although the ambiguous intersections are persistent, eventually the false targets ha ve non-physical motion. The target present hypothesis quickly goes to zero for these targets and they are elimin ated. Right: the tracker estimate of target position and red circles indicating the removal point fo r the false targets 


  15             Where denotes the expectation with respect to the transition distribution    and the omitted terms involve similar terms involving and and cross terms between the and coordinates We use the nearly constant velocity \(NCV\model to specify the transition distribution    This assumption corresponds to one where the target moves at constant velocity except for random jump changes \(i.e nearly constant velocity\is is a plausible model when  is small as it is here Specifically, the NCV model assumes step changes in target velocity defined by the Ito Equations     This model implies  and likewise for  It is furthermore assumed that th e noise processes in each coordinate are independent Under this model, we can eval uate the required terms from 25\ as follows          And likewise for terms involving and Notice that all cross terms \(e.g  have expectation due to the assumption that the noise process is independent in the two coordinates This model simplifies \(25\ to         where the terms omitted are replicas involving the  coordinate Under the assumption that is small, this can be rewritten as    For implementation, this is approximated using an implicit Euler scheme wh ere      Where the indices  represent the discrete    locations where the probability mass is captured Likewise, using forward differencing      and          and similarly for the y coordinate system When substituted into \(28\is leads to a series of equations of the form                This series of equations defi ne the probability at each point at time  It can be efficiently solved via Thomas\222 algorithm \(rather than simply inverted\he matrix is tridiagonal     


  16 R EFERENCES    R o y E. Bet h el Benjam i n Shapo, C h r i st opher M   Kreucher, \223PDF Detection and Tracking\224, under review IEEE Transactions on Aerospace and Electronic Systems  2 y. E B eth e l an d G. J. Paras, \223A PDF Mu lt it arg et Tracker\224 IEEE Transactions on Aerospace and Electronic Systems vol. 30, no. 2, pp. 386-403, April 1994 3   R o y E  B e t h e l a n d G  J  P a r a s  223 A P D F M u l t i s e n s o r  Multitarget Tracker\224 IEEE Transactions on Aerospace and Electronic Systems vol. 34, no. 1, pp. 153-168 January 1998  L   D   Stone, C. A. Bar l ow, and T. L Corwin, \223Bayesian  Multiple Target Tracking\224  Boston: Artech House, 1999  l la and A. Hero, \223Multitarget Tracking using the Joint Multitarget Probability Density\224 IEEE Transactions on Aerosp ace and Electronic Systems  vol. 41, no. 4, pp. 1396-1414, October 2005  M  M o relande, C. Kreucher, K. Kastella, \223A Bay e sian  Approach to Multiple Target Detection and Tracking\224 IEEE Transactions on Signal Processing vol. 55, no. 5 pp. 1589-1604, May 2007  B  Shapo, and R  E B e t h el  223An Overvi ew of t h e Probability Density Function \(PDF\er\224 Oceans 2006 Boston, Sept. 2006  R oy L. St r e it 223M ult i s ensor M ul tit arget Int e nsit y Fil t er 224  International Conference on Information Fusion  Cologne, Germany July 2008  M  Ort on and W Fi t z geral d 223A B a y e si an approach t o  tracking multiple targets using sensor arrays and particle filters\224 IEEE Transactions on Signal Processing, vol. 50 no. 2, pages 216-223, Feb 2002  A. Doucet B Vo, C Andri e u, and M Davy 223Par t i c le filtering for multi-target tracking and sensor management\224, IEEE International Conference on Information Fusion, 2002  H Van T r ees, \223Det ecti o n Est i m a t i on, and M odul at i o n  Theory IV:  Optimum Array Processing\224  J. C St ri k w erda, Fi nit e  Di fference Sch e m e s and Partial Differential Equations, Ch apman & Hall, New York 1989   K. Kast el la and C Kreucher, \223M ult i p l e  M odel Nonl i n ear  Filtering for Low Signal Ground Target Applications\224 IEEE Transactions on Aerospace and Electronic Systems vol. 41, no. 2, April 2005, pp. 549-564  Z. Tang and \334. \326zg\374n er, \223Sensor Fu si on for Target Track Maintenance with Multiple UAVs based on Bayesian Filtering Method and Hospitability Map\224 Proceedings of the 42 nd IEEE Conference on Decision and Control pages 19-24, December 2003  K. Kast ell a 223Fi n it e di ff erence m e t hods for no nl i n ear filtering and automatic target recognition\224 MultitargetMultisensor Tracking: Applications and Advances vol III, pages 233-258, Artech House, 2000 B IOGRAPHY  Chris Kreucher received his Ph.D. in Electrical Engineering from the University of Michigan in 2005. He is currently a Senior Systems Engineer at Integrity Applications Incorporated in Ann Arbor, Michigan. From 1998 to 2007, he was a Staff Scientist at General Dynamics Advanced Information Systems' Michigan Research & Development Facility \(formerly ERIM\. His current research interests include nonlinear filtering \(specifically particle filtering Bayesian methods of multitarget tracking, self localization information theoretic sensor management, and distributed swarm management Ben Shapo earned his Ph.D. in El ectrical Engineering in 1996 from the University of Michigan.  He is currently a Senior Systems Engineer at Integrity Applications Incorporated in Ann Arbor, Michigan.  From 2003 to 2008 he was a Lead Engineer at General Dynamics, where he contributed to a number of RF and acoustics signal processing and tracking efforts.  Dr. Shapo has 12 years experience in the DoD research community in the areas of detection, tracking, and data fusion, with emphasis on highfidelity simulations and applying new methods to real data  Dr. Roy Bethel is currently employed at The MITRE Corporation in McLean, VA. He has been actively involved in development, testing, and evaluation of signal processing and detection and tracking systems. In particular, he has developed many systems that have been implemented on United States Navy airborne, surface, and submerged platforms. He is currently engaged in research and development of innovative approaches to multitarget detection and tracking  A CKNOWLEDGEMENTS  This work was partially funded by the Office of Naval Research contract N00014-08-C-0275. The authors would like to thank Dr. John Tague for his support, and Mr. Scott Spencer and Dr. Charles Choi for their assistance 


2 1 0 00                4 G ro w th 1  1 3 1 0 9 2 0 2 3 0 10  0 56                So ci ode m og ra ph ic c ha ra ct er is tic s 


s 5 A ge y ea rs   21 7 8 7 3 9 0 01 0 22  0 1 4 0 0 8              6 G en de r i s fe m al e2   0 2 4  0 0 6 0 0 2 0 00 0 0 3 0 10    


           7 C ur re nt ly n ot w or ki ng 2  0 0 5  0 0 8 0 04 0 04 0 0 1 0 16  0 16             8 C ur re nt ly in e du ca tio n2   0 6 


6 7  0 01 0 1 9 0 08  0 03 0 6 8 0 0 7 0 3 2           9 C ur re nt ly w or ki ng 2  0 2 8  0 03 0 18  0 1 1 0 0 3 0 64  0 00 0 1 4 0 8 9   


        10 E du ca tio n ac hi ev ed 3  3 5 7 1 5 2  0 04 0 02 0 2 1 0 1 2 0 16  0 02 0 1 6 0 13  0 0 6         11 D is pe ns ab le in co m e   


  21 0 9 2 72 7  0 14  0 0 1 0 09  0 08  0 2 0 0 00 0 0 4 0 18  0 1 6 0 0 1        In te rn et u sa ge                     


  12 A ct iv e in te rn et u sa ge 1  0 0 2 0 9 6 0 2 1 0 25  0 11  0 12  0 10  0 0 4 0 05  0 0 8 0 0 5 0 0 1 0 12        13 H ou rs o nl in e h ou rs 


rs   2 6 5 3 0 3  0 04 0 12  0 1 1 0 0 3 0 40  0 0 7 0 0 7 0 4 7 0 5 3 0 07  0 1 1 0 07       14 W illi ng ne ss to p ay 1  1 8 3 0 6 3  0 03 0 10 


10  0 07  0 08  0 0 2 0 0 4 0 0 1 0 01  0 00 0 0 5 0 14  0 04 0 05      G am e sp ec ifi c va ria bl es                      15 T en 


ur e w ee ks   2 8 2 3 5 2 0 2 6 0 31  0 0 9 0 01 0 12  0 0 4 0 02 0 0 9 0 0 9 0 07  0 02 0 13  0 08  0 0 4    16 C ro ss o ve r on o ffl in e 4  0 1 5 


5 1 1 1 0 1 9 0 11  0 13  0 18  0 2 0 0 1 4 0 0 7 0 14  0 1 1 0 0 4 0 08  0 15  0 0 5 0 01 0 07    17 S at is fa ct io n1   18 7 5 1 3 16  0 18  0 00 


00 0 44  0 52  0 1 4 0 0 3 0 02 0 07  0 0 9 0 1 4 0 10  0 08  0 0 6 0 09  0 0 1 0 13   18 C om m itm en t1  0 6 2 0 8 3 0 3 1 0 13  0 37  0 39  0 0 7 


7 0 0 6 0 02 0 03  0 0 4 0 1 3 0 14  0 17  0 0 5 0 09  0 07  0 19  0 58  S ou rc e O w n ca lc ul at io n N ot e N  1 3 89 o bs er va tio ns S ig ni fic an ce le ve ls 


ls  p  0 05 S D  S ta nd ar d de vi at io n 1 5 po in t L ik er t s ca le ra ng in g fro m 2 to 2  2 du m m y va ria bl e 3 o rd in al v ar ia bl e ra ng in g fro m v oc at io na l e du ca 


tio n to P h D 4 n um be r o f c on ta ct s   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


