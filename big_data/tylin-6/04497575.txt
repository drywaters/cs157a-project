Adaptive Bitmap Indexes for Space-Constrained Systems Rishi Rakesh Sinhal Marianne Winslett2 Kesheng Wu3 Kurt Stockinger3 Arie Shoshani3 Microsoft Corporation USA rsinha[microsoft.com 2Department of Computer Science University of Illinois d Urbana-Champaign USA winslett@cs.uiuc.edu 3Lawrence Berkeley National Lab USA kwu,stockinger,shoshani}@lbl.gov Abstract Data management systems for big science often have tight memory and disk space constraints In this paper we introduce adaptive bitmap indexes which conform to both space limits while dynamically adapting to the query load and offering excellent performance So that adaptive bitmap indexes can use optimal bin boundaries we show how to improve the scalability of optimal binning algorithms so that they can be used with 
realworld workloads As the removal of false positives is the largest component of lookup time for a small-footprint bitmap index we propose a novel way to materialize and drop auxiliary projection indexes to eliminate the need to visit the data store to check for false positives Our experiments with real-world data and queries show that adaptive bitmap indexes offer approximately 100300 performance improvement compared to standard binned bitmap indexes at a cost of 5 MB of dedicated memory under disk storage constraints that would cripple other indexes I INTRODUCTION Bitmap indexing for business data was proposed in the 1980s 1 and analysed extensively 
in the 1990s it is included today in data warehousing products Several research groups have examined bitmap indexes for scientific data Both application areas are a good fit for bitmap indexes as they involve high-dimensionality queries over very large data sets in a read-and-append-only environment When we tried to convince scientists to use bitmap indexes in their projects we quickly learned that big science is not willing to accept the space over heads traditionally associated with indexing Bitmap indexes have roughly a 100-150 space overhead for indexed data which is much smaller than most high-performance indexes 5 but is 
not currently acceptable for big science A typical reaction If we can allow efficient access to the raw data at around 20-30 additional storage cost then we can help science a lot Joseph Mohr Dark Energy Survey data system designer Scientists are reluctant to invest in more storage because their data sizes are so large their new instruments can generate in one day as much data as is in a typical business data warehouse Doubling the amount of storage is not an option under current funding priorities In this paper we introduce adaptive bitmap indexes which combine the strengths 
of today's leading approaches to bitmap indexes while greatly reducing their size and retaining excellent performance In the full version of this paper 4  We extend current methods to calculate optimal bin boundaries to create locally optimal bin boundaries in a cost effective manner  We materialize and drop auxiliary projection indexes without ever exceeding predefined memory and disk limits to greatly reduce data lookup costs  We use large synthetic and real workloads to show that for a given space limit adaptive bitmap indexes are much faster than standard binned indexes and are comparable to large multi-resolution bitmap indexes Figure 1 Answering queries 
with an unbinned center binned left or multiresolution right bitmap index II RELATED WORK Figure 1 shows the simplest possible bitmap index BI for attribute A where we store one bit vector bitmap for each possible value of A The length of the bitmap is equal to the number of objects or tuples in the database The objects are numbered serially and if object k has value v for attribute A then the bitmap for value v has a 1 as its kth bit The bitmaps for all other values of A have 0 as their kth bit The resulting set of bitmaps is the 
BI for A and the total number of Is in all its bitmaps is the total number of objects If more objects are added to the database we append more bits to the bitmaps To find all objects whose value for A is in 10,12 we fetch the bitmaps for values 10 and 11 into memory perform a bitwise OR on them then OR the result with the bitmap for 12 For a query that restricts 20 attributes we compute the bitmaps resulting from each individual attribute restriction then AND together those 20 bitmaps to get the final result We can reduce the BI size 
by partitioning the domain of A into bins ranges and keeping only one bitmap per bin Then queries will fetch fewer bitmaps but can have false positives whenever the bin boundaries do not align exactly with the query restrictions Researchers have tried to combine the advantages of binned BIs fast bitmap lookups but slow false positive removal and unbinned BIs no false positives but slower lookups due to having to AND and OR so many bit vectors The resulting approach called multi-resolution BIs MBIs uses a hierarchy of bitmaps 3 At the highest level 978-1-4244-1837-4/08/$25.00 251 2008 IEEE 1418 ICDE 2008 


Bound 30 500-00 100 00 50 00 020 35 0891 Lo eolto 800 2 0 00 5 00 50 00 21ue Spta 23alt ofySDSSf qnueries wt Aedons Bitmap Index      Mlmor Fiue Achtcur fa w-evladpie0imp k of an MBI are WAH-compressed bitmaps whose bins are individual values At the next lowest level each bin contains several adjacent bins from the level above and so on down to level 1 which has the widest bins see Figure 1 To find all obj ects whose value for A is in 1I0 12 we start at level 1 of the MBI Any bins there lying entirely inside the query range have their bitmaps fetched and ORed together If the query range partially overlaps a bin we go to level 2 of the MBI to answer that part of the query and so on until if necessary we have reached the highest level of the MBI MBIs incur no false positives and fetch few bitmaps but do require about twice as much space as when there is exactly one WAHcompressed bitmap for each value in the domain III LocALLY OPTimAL BINS Rotem et al use dynamic programming to place bin boundaries to minimize total query processing time for a particular workload 2 They observe that only the endpoints of the range restrictions in queries query endpoints must be considered to find optimal boundaries They use dynamic programming to find optimal bins for a query workload and data distribution in O\(n 2 time where n is the number of endpoints Their approach does not scale to large workloads In the full version of the paper we solve the scale-up problem by first using a fast heuristic approach to choose the number of bins and assign their boundaries The resulting structure is the lowest level of a new MBI Then for each bin at the lowest level we compute a set of bin boundaries that are optimal within that bin to serve as bins at the next highest level We can do this efficiently because only a small fraction of the workload will have query endpoints in that particular bin so n is small We repeat the process with the other bins at the first level of the index then generate additional levels in the same manner This is the locally optimal NMIB or LOMBI IV EXPLOITING QUjERY LOCALITY Even with a LOMBI the cost of removing false positives is still extremely high as shown by experiments presented later Fortunately scientific workloads exhibit spatial and/or temporal locality with respect to the time an observation was made We exploit this locality to reduce workload cost The Sloan Digital Sky Survey SDSS is an astronomy survey collecting images of the sky and generating a map of a million galaxies and quasars In SDSS spatial locality takes the form of similar values for the right ascension RA and declination DEC coordinates used to describe the location of obj ects in the sky Figure 2 shows a graph of the RA values used as query endpoints Y axis during 18 months of SDSS queries X axis Left endpoints are shown in dark blue and are mostly obscured by the magenta-colored right endpoints The graph shows that consecutive queries tend to request very similar values of RA except for brief periods We exploit this locality to reduce the cost of false positive removal When we go to the data to check for false positives we create an auxiliary projection index PI for that particular bin consisting of a objectID attributeValue pair for each object in the bin and sorted on attributeValue A subsequent query whose endpoint e falls in the same bin can do binary search for e in the PI all objects in the PI beyond e are false positives and can be eliminated without visiting the data Scanning a PI is much quicker than parsing a data file and searching for the objects belonging to a particular bin Lower Bound ne-\(B Asshw i igre3 a datiebimp3ndx\(AI cosit o LMI ls mal e o Is s6nAB ano ever exed h ds spc1 etitosipsdb cetss we craete 2OB o hti i1lgtl4mlerta h spcelii heletve ds sac1i5urdik1ahe\(D con Rsoltrans ecnmtraieol ml rion o h I We storedithe RAevalueso asitgrsfion 0,3,0000].Th with re1.6 108cobjects.o Fworlve adrepriesentmativdexwoklAd,Iwe thehow binnngFagorithm and thereapineingm15 quderie forI tenssting We use aOB dlusalsal coreP2.4GAs machne wBIt 1anGB mvemry anced 2t50 GBs dskaers,twicthibndwidthse uptb10M scetsts The creat LOMB hadB 2o level withi 1000tl bismatlleve 1ha 27 MB 15,000 bins aetolvel 2is 120e MB isace\(D Csdonsilde the effect ofe MClasizel onathe ruGimen four RAtorag cnthe a5,00tetqris we a aeithlthe ABIy smlmte tac250o of thePl datae lesizt11M igr hw h result uedPs foro6 MC sizC 400 Upper o aes.oo 1419 


i r0 0 5 10 25 50 1X 0 5 S 25 50 1X D Rotem K Wu Optimizing candidate check costs for bitmap indices In CIKM 2005 a 2level R R Sinha and R R Sinha and For each MC size one X axis entry is for a configuration with both MC and DC and the other has just MC The Y axis for the bar graph shows the time to answer the queries The Y axis for the line graph shows the percent of queries answered from MC and if present DC DC size is 15 MB the amount of disk space left after creating the LOMBI readFP time to read data file to create PIs not already cached writePI time to move overflow PIs from MC to DC readPI time spent moving PIs from DC to MC as needed checkPI time to parse PIs in MC to filter out false positives readBV time to read bit vectors from disk entirely IPO calcBV time spent ORing and ANDing bitmaps no 11O Mem rory iUthe Onhl MarnoC ryor Mem ory ache MC Si ze 100 MB Figure 4 Time to run all P O'Neil Model 204 architecture and performance In Proceedings of the Conference on High Performance Transaction Systems 1987 K Stockinger and K Wu E J Otoo and A Shoshani On the performance of bitmap indices for high cardinality attributes In VLDB 2004 1420 M Winslett Multi-resolution bitmap indexes for scientific data ACM TODS 32:3 August 2007 M Winslett Adaptive bitmap indexes Submitted for publication and available at dais.cs.uiuc.edu/dais/sdm/sdmpubs.php Di k ache 60 E 40 30 4 20 10 L 4it P1B ireadPi 0 0 RA queries in the 15,000 test query log with ABI and 25 space limit The line shows the percent of queries answered without visiting the data The Figure 4 workload times are completely dominated by the time spent visiting the data to find false positives readFP any increase in readFP also increases total run time Such visits are unavoidable when the index has no single-value bins Query locality gives much faster answers once PIs are cached Figure 4 run times improve a lot as MC increases from 0 to 5 MB regardless of DC size As MC grows beyond 5 MB performance continues to improve if there is no DC but plateaus if there is DC This is because the second largest component in MC-without-DC run times is computeBV while in the MC+DC version it is writePI and readBV whose contention for the disk arm and space in the file system cache slows down both components In the full version of the paper we present additional experiments showing that this DC is too small to benefit performance and that any extra disk space available for DC is better spent increasing the LOMBI size With no DC and no MC the average time to answer one query in Figure 4 is 4 seconds From experiments in the full version of the paper we know it takes 7.1 seconds on average to answer a query with a cold file system cache so the 4 second response time reflects very positively on the file system caching algorithms The file system cache would be much less effective in a production run however because this experiment only involves 644 MB of RA on a machine with 1 GB memory A production SDSS run devotes file cache space to many other less popular attributes fetched by concurrent queries with memory orders of magnitude smaller than the recently-accessed data Thus MC and even DC will be very important in production runs For a clearer picture of the expected file system cache behavior in a production system consider the following analysis The first check of a bin for false positives must read the corresponding objects from disk If the file system cache does not already have those objects cached as expected in production runs the average query execution time will be close to 7 seconds For queries whose PIs are already in MC the query time will be 1 second Thus even with 5 MB MC and no DC we can answer 4500 of the queries in 1 second and 5500 of the queries in 7 seconds This means that ABI provides a considerable advantage for the 4500 of queries answered from the MC VI CONCLUSIONS In this paper we introduced adaptive bitmap indexes ABIs as a way to satisfy scientists demands for a high performance index with a tiny footprint An ABI includes a locally-optimal multi-resolution bitmap index and a set of auxiliary projection indexes PIs that are materialized while removing false positives from current query answers then kept in an LRU cache in memory and/or disk for use in answering subsequent queries The PIs greatly reduce the cost of false positive removal which is the dominant factor in query performance The full version of this paper 4 includes much material omitted here In particular we present the algorithms for building adaptive bitmap indexes and include a complete set of experiments explorations of query performance with both RA and DEC for a variety of space limits and with synthetic queries and/or data as well as the SDSS workload We also show that an adaptive bitmap index for RA with a 5000 space limit performs 32 faster on the SDSS workload than an ordinary multi-resolution bitmap index twice its size In other words space limits need not harm lookup performance and can actually improve it by reducing CPU costs which are high for bitmap index lookups Acknowledgements This work was done while R Sinha was at the University of Illinois This research was supported by a Computational Science Fellowship by the Department of Energy under subcontracts B341494 DOE DEFC0201ER25508 and DE-AC03-76SF00098 and by NSF grant NSF ACI 02-05611 REFERENCES 1 2 3 4 5 


expectations are ambiguous or unrealistic. In this situation, the hardworking efforts of people go unrecognized. The “order taker” mindset stands in the way of IT sitting at the table as an equal partner offering valid perspectives on the best way to deliver real value making and meeting commitments, and delighting our business partners  3.2 IT Investment Change Management    Two of the inherent mindsets underlying the legacy processes of managing change in the IT investment agenda are “maximize utilization” and “get it done  The “maximize utilization” mindset is characterized by the notion that 100% utilization of people means maximum efficiency and maximum productivity. Less than 100% allocation for a person means an opportunity to work on another project. Slack time is bad; it means that a person has unproductive time  To leverage our people who are most skilled and have domain expertise, we tend to over-allocate them; they even become unavailable to even casually help others Too much time is spent transitioning from one task to another, reducing overall productivity and quality  Although difficult to prove, striving to achieve maximum efficiency can reduce effectiveness and decrease the ability to respond to change. Slack time actually improves productivity by providing time to think 9 u cin g ti m e to t h in k i m p acts th e creativ it y a n d  quality of the results we are able to deliver  When we over-allocate our skilled resources we cause costly task-switching.  This thrashing results in lost productivity and delays product delivery ou r experience, this loss of time is not accounted for in work estimates and puts even more pressure on our best people to deliver. People become increasingly exhausted decreasingly satisfied at work, and are at risk for burnout  The “get it done” mindset is characterized by the notion that there is a way to accomplish our goals, and our job is to find it. This is a derivative of the “order taker” mindset - our business partners have high expectations, and we must find a way to meet them. We cannot walk away from a request and leave funding on the table. When the going gets tough, the tough get going  The “get it done” mindset sometimes causes us to believe that a best-case plan will succeed. Over the course of the year, we consume our contingency and compromise our ability to deliver. We find ourselves “in the box” and it becomes easy to delude ourselves about the situation. The recognition of situational reality may be delayed until late in the year as we never give up; in these situations we lose credibility with our business partners  3.3 Governance and Oversight    Our processes and behaviors for feedback and control are second-nature to us; as a regulated energy company part of our genetic predisposition is to ensure our work is well-engineered and carefully monitored  Our traditional approach to IT governance shared this control through data” mindset, characterized by the notion that we seek more information and greater details When agile projects flourished and propagated throughout our portfolios, this instinctive need for more data at a portfolio level was brought into question  On agile projects we did not use detailed four-level work breakdown structures, earned value metrics, and fully resource-loaded Gantt charts.  Rather, our burn-up charts, value velocity trends, and team engagement metrics [11 p r o v id ed actio n a b l e in si g h ts   Agile metrics did not resonate with the legacy “control through data” mindset where complex data was needed to make up-front decisions and even more exhaustive information was required to control change over time  The legacy mindset resisted these new factors for project success and new gauges of progress. This was especially prevalent at the project portfolio level where the legacy mindset was comfortable with its ability to understand some of its projects and was frustrated with its inability to understand others  Finally, the “control through data” mindset asserted we could plan out a full year’s slate of projects; this was turned on its head as agile projects ebbed and flowed based on evolving business needs over time  4. Actions Taken and Results Achieved    In this section we discuss the changes initiated to adjust the legacy processes and change mindsets related to IT funding, managing change, and governance. Here we describe the strategies adopted to address the underlying beliefs and to shape the observable behaviors  The previous sections explored three legacy processes and summarized the underlying legacy mindsets that impacted our ability to effectively deliver value; in the subsequent section we characterize our intentions for further enhancements and improvements  4.1 IT Investment Funding    To address the opportunities related to the IT funding and business case development processes in light of the widget engineering” and “order taker” mindsets, we tackled the business case development process  Our strategy was to change the business case development process to provide more flexibility into how 
255 


we managed individual projects and to extend that flexibility to the project portfolio  The goal was to establish a common understanding that a business case was ideal for high-level problem and opportunity statements, with a funding level reasonable to address the need and a general implementation timeframe This approach allows flexibility at a project level as well as a project portfolio level to adjust scope, dollars, or time as an appropriate solution becomes clearer  We established a process improvement team under the auspices of our Software Engineering Process Group SEPG\2  T h e g o al o f th is tea m  w a s to reco n c ile t h e business case development approaches in use, identify the best aspects of each, establish a standard process, and drive toward a common understanding of the approach  We found it a challenge to reconcile viewpoints about the level of detail in the various business case processes Those who used a more detailed approach did so because of a perceived need for specificity \(the “widget engineering” and “order taker” mindsets at work  The work group was successful in their efforts to define a common process and work products, but many of us continue to drive for much lower levels of detail  4.2 IT Investment Change Management    Our approach to managing the set of IT investment projects, coupled with our “maximize productivity” and get it done” mindsets, offered a significant improvement opportunity. We explored insights to leverage lean techniques to better manage the flow of work  To raise awareness and to educate our leadership team we retained Mary Poppendieck, a leading expert on the application of lean manufacturing techniques to IT work  ry prov i d ed u s  w i t h i n s i gh t abou t ou r part i c u l ar situations, and initiated dialog amongst the leadership team about how we could use lean techniques. This was generally well received by our leadership team: Mary’s perspectives about thrashing and applying the theory of constraints to manage the flow work were particularly compelling  At about the same time, we developed and initiated the use of a new portfolio management model in one of the IT groups serving a business unit. We setup a simple model to represent our capacity, the projects that were on the agenda, and the available resources for these projects Similar to agile project planning, we based this portfolio management model on the capacity of our available staff and would run no more projects than the level our staff could support. We recognized it was not feasible to respond to unplanned change by acquiring new contractors to reactively form new project teams  Rather than plan out the entire year, the plan looked out three months and initially allocated our staff to get the most important set of projects started first. We placed the remainder of the projects on a backlog list with a projected start date based upon the expected availability of staff due to completion of in-flight projects  We strove to ensure that the people were focused only on their areas of domain expertise by forming teams of domain experts, using a queue and pull work system to release work to the domain team, and to develop more domain expertise over time  We explained the new portfolio management model to the affected business partners. We also explained that we were limited in what we would take on because of the people constraint. As engineers, they understood the notion of a system constraint, and expressed willingness to work with us using the new model. As the year progressed, the business partners began to realize they were not going to get everything they asked for. This drove healthy conversations within the business unit and with other affected constituents to identify and rally around the most important priorities for the business as a whole \(rather than just a single business unit  4.3 Governance and Oversight    The legacy mindset of “control through data encouraged us to measure success with traditional metrics, and to use this information to control projects and portfolios  We sought to shift our behaviors and processes from Big Plan Up Front” \(BPUF\ to rolling wave planning with clear, well-understood decision gates along the way Specifically, we wanted to employ our company’s FourGate, Nine-Step process for IT projects, each gate having deliberate intentions to be satisfied along the way  First, we brought the implicit gaps within our portfolio management and funding models into the open through a series of retrospectives and staff meetings  From those discussions we sought to address the ways in which the portfolio of projects would be measured recognizing that each project tailored our standard project methodology based on its context and specific challenges  Second, we introduced terminology and concepts from lean manufacturing to help us better understand our constraints and how we could reorganize the way we prioritize our commitments and fund our work  Principles like “sustainable pace” and “make the main thing the main thing” came into focus and we realized that we needed to manage the queue of projects within a portfolio and across the business units based on our scarce resources and subject matter experts, rather than to spread those resources across many parallel initiatives  Thus, we viewed our key resources as platform teams and began managing the queue of pending projects for those teams. As a team completed one project, it began work on the next most important project for its platform 
256 


 Over time we successfully moved from the “control through data” mindset, which sought greater amounts of detail, to an “enable and ensure” mindset, which sought to more effectively manage each platform team given its context and operating parameters  5. Sustaining Results and Moving Forward    In this section we characterize our intentions to sustain the progress we have made thus far, and to further embrace change as we refine our processes and mindsets related to IT funding, managing change, and governance  Previous sections explored three legacy processes summarized the underlying legacy mindsets, and discussed the changes that we inculcated into the organization; in the concluding section we offer some of our biggest lessons learned and recommendations  5.1 IT Investment Funding    One of our overarching strategies in IT is to leverage the DTE Energy Operating System, a combination of lean and Six Sigma thinking, seeing, and doing tools and techniques based upon the Plan-Do-Check-Act cycl The company has invested in, and seen benefits from, this approach by training and rolling it out across the organization  For the business case, we are implementing the Operating System’s Project Charter tool. This tool provides a one-page overview of a proposed project. For the IT business case process, we are using the Project Charter as a means to provide “just enough” description of an IT investment opportunity and to defer definition of a formal business case until the confidence level for commencing the project becomes high  Previously, our standard was to transform the business case document \(and all its details\to a project initiation document. This document tended to be redundant with the business case and other standard documents \(like project budget, timeline, risks, etc.\ a recent process improvement we effort identified the opportunity to phase out the project initiating document and to further leverage the Operating System’s Project Charter tool. We will conduct retrospectives and After Action Reviews \(another Operating System tool\o identify opportunities for further refinement  These actions will move us toward “right sizing” the amount of detail in each business case, thus enabling flexibility to adapt and evolve a solution over the life of the project to better meet the real needs and deliver more value to our business partners. A smaller investment in developing the business case should also enable more flexibility at the portfolio level as there will be less invested interest in the business case document itself  5.2 IT Investment Change Management    Our revised portfolio management model directly addresses our strategic IT goal to broaden our business alignment. Our intention is to further leverage our portfolio management model on two fronts: first, by further refining the model and applying it in the business area for which it was first developed, and second, by leveraging our experience for other IT groups and the business areas they support  To further refine the portfolio management model, we are developing a systematic model of the portfolio production line. Our goal is to see more clearly how the system of delivering projects actually works. Managing the project portfolio first seemed like controlled chaos then like each project was a special situation. We now realize we can view portfolio management as a systematic process. Our goal is to develop a model with simple measures that we can use to quantitatively identify where bottlenecks exist. Then, we can apply classic theory of constraint techniques to further optimize the system  We also hold bimonthly retrospectives to review what’s working and what’s not, and make decisions about what we’re going to do differently. Going forward, we intend to keep revisiting our principle that each person works on a single project \(or task\ntil it is complete  Serendipitously, we are encountering similar problems in other IT groups and business areas. We are working on applying these portfolio management approaches in one additional area at the moment. We are also planning how we will roll out these approaches on a broader scale to other IT and business groups as well  5.3 Governance and Oversight    In our approach to portfolio governance, we moved from a “control through data” legacy mindset to a “enable and ensure” mindset, where we more effectively understood and dispatched queues of work by constraints  One key enabler was to better identify our constraints most prevalently, technical and business expertise\nd more effectively balance our queue of work to the capacity of those constraints  Going forward, we will continue delivering results by continuing to improve how we manage our constraints in the areas that are already using this approach  For our areas that have not yet made this transition, we will manage the culture change in an intentional manner We seek to avoid a “big bang” approach; instead, we will use a consistent pattern - standardizing terminology identifying constraints, prioritizing and re-sequencing the work, and so on  A second enabler to this mindset shift was to embrace a suite of fact-based measures that gave us the insights we 
257 


needed without force-fitting a comprehensive suite of metrics upon all projects and programs  Going forward, we will continue to remind ourselves that the impulse to dive deeper into the details is one of the more prevalent legacy mindsets, to remain aware of when we slip into that old habit, and to challenge ourselves to take a step back rather than a step down  6. Lessons Learned and Recommendations    To those who seek to foster an agile approach to project portfolio funding, change management, and governance, the following reflections and suggestions may provide some degree of utility  First, be aware of your organization’s processes and the mindsets that shape those processes. Rather than simply performing a process because “that’s just how it is,” challenge the underlying assumptions – this will either reinforce its value or prompt meaningful change  We found that agile principles related to process improvement may be of additional benefit in this context Specifically, the habit of introducing frequent, small improvements \(in an incremental and iterative manner tends to ensure that processes do not become too stale  Second, be engaged in your organization’s leadership communities \(both IT and business\her than hoping they recognize their own legacy processes and mindsets guide them to identify what is working well and where problems exist, and to introduce change in a deliberate and intentional manner  We found that quite often, leaders were too close to the situation and its complexity \(accumulated over the years\ to see any other way to operate. We created a safe and enabling environment to take a step back and view those complexities from alternative perspectives \(such as constraints\ and proceed with a fresh set of ideas  Third, be patient in rolling out an agile approach to funding, change management, and governance across a corporate enterprise.  Rather than change everything all at once, select and collaborate with one or two business units first, reflect on the results, and then solidify those gains while branching out to another set of business units  We found that while our business units had variations on the same themes of legacy processes and mindsets each was different enough that a single approach would be inadequate. By adopting and adapting lean and agile techniques within each business unit \(and in general, one unit at a time\we enabled agile funding and portfolio management capability and improved our business-IT alignment along the way  7. References    g ile A l l i an ce Manifesto for Agile Software Development http://agilemanifesto.org   e r, S t ev en W Formalizing Agility: An Agile Organization’s Journey toward CMMI Accreditation  Agile 2005 Proceedings, IEEE Press, 2005   e r, S t ev en W Formalizing Agility, Part 2: How an Agile Organization Embraced the CMMI Agile 2006 Proceedings, IEEE Press, 2006   a k e r, Stev en W an d J o s e ph C  T h o m as  Agile Principles as a Leadership Value System: How Agile Memes Survive and Thrive in a Corporate IT Culture  Agile 2007 Proceedings, IEEE Press, 2007   atis  c om  Business-IT Alignment  http://whatis.techtarget.com/definition/0,,sid9_gci118549 4,00.html   g ile P r oj ect L eaders h ip Net w or k   Declaration of Interdependence http://www.pmdoi.org   lle m a n  Gle n  Agile Program Management: Moving from Principles to Practice Agile Product & Project Management, Vol. 6 No. 9, Cutter Consortium September, 2005   lle m a n  Gle n  ibid    a rco, T. 20 02.S l ac k  Getti n g P a s t Bu rn o u t  Busywork, and the Myth of Total Efficiency. Broadway Publishing, New York   Pop p en di eck M. 2008 Thrashing: What it is, what causes it, and what to do about it  www.agilebazaar.org/DeepLean/DeepLean-2%20%20Poppendieck%20-%20Thrashing%20-%20color.pdf   B a k e r, Stev en W  an d Am bros e, Stev e n B   Measuring Engagement and Predicting Project Performance: The TEAM Score Cutter IT Journal Volume 18, No. 7, 2005    o w l er, P. an d R i f k i n  S   Software Engineering Process Group Guide Software Engineering Institute 1990   Pop p en di eck M. an d Poppen d i eck T   Implementing Lean Software Development: From Concept to Cash The Addison-Wesley, 2006   ean  L earn i ng C e n t e r L ean a n d DT E: L e a n  Transformation Reaps Millions In Savings For DTE Energy http://www.leanlearningcenter.com/aboutus/our_experien ex__dte_energy.cfm 
258 


of the nodes is not assigned to any ground node as described earlier The topology for airborne network is again a ring Notice that the communication links between airborne and ground nodes are shorter now signifying greater upload capacity This is due to the fact that the extra airborne node allows for the network to spread out more and hence get closer to the ground nodes 4 Ground nodes 4 Airborne nodes Ix 32 3464 250 P 200 f68 3215 150 49~2218 100 501541 98 99 74 52 0632 1 Choose uniformly and randomly N cluster centers 2 Assign each ground node to the cluster center that is nearest to it Let Qcl  c be the ground nodes assigned to the cth cluster 3 For each cluster say c compute the new cluster center as k  Argmin max dA-gm i A i{fci  I 44 4 Repeat the above steps till the locations of the cluster centers do not change beyond a certain threshold Let A AN be the cluster centers kl  kN obtained as the output of the algorithm Next the convex optimization problem in Equations 33 and 34 is solved to obtain the desired airborne trajectory centers   760~53 52 0632 200 150 100 50 0 50 100 150 x Figure 5 Coverage using multiple airborne nodes when locations of ground nodes are known Here we have 4 ground nodes denoted by s and 4 airborne nodes denoted by circles The axes represent x,y lLocations on a 2 D plane The optimal placement results in each airborne node providing coverage to a unique ground node while maintaining a connected topology between themselves 1-76-77-72142265 150 100 50 0 x 39 5904 50 100 Figure 6 In thLs case there is an extra airborne node The op timal placement in this case results in higher maximum data upload capacity to each ground node Note that one airborne node approximately at 60,130 acts purely as a relay 2 Number of ground nodes greater than the number of airborne nodes In this case the problem in general is NP hard An efficient and popular method of solving this problem is a heuristic similar to the k-means clustering algorithm described as follows 4 CONCLUSIONS AND FUTURE WORK Conclusions In this paper we have studied the placement planning problem of the airborne network We first define two metrics maximum data upload capacity for air-ground links and worst case link activity percentage for air-air links We have shown that the maximum data upload capacity is a measure of the coverage provided to a single ground node and the worst case link activity percentage for a single air-air link is a decreasing function of the distance between the centers of the loiter orbits of the airborne nodes We have also discussed how this measure is useful in the design of networks for delay-tolerant applications This paper considers three practically interesting scenarios for the deployment of the airborne network backbone where the purpose of the airborne network is to provide coverage to the ground nodes while maintaining a certain degree of reliability and connectivity in the inter-airborne node topol8 250 200 7.3458 46.9364 Figure 7 A snapshot of the optimal coverage of 4 ground nodes using 5 airborne nodes The rings at the top indicate the loiter orbits of the airborne nodes the shaded circles on the ground indicate their respective instantaneous coverage disk and the fixed ground node positions are indicated by s 150  100 _ 50 24.9954 200 4 Ground nodes 5 Airborne nodes 15.8688 


ogy Results of this paper can be directly useful to the Objective Gateway program of the Air Force where several Battlefield Airborne Communication Nodes BACNs and their ground counterparts the Rapid Attack Information Dissemination Execution Relay RAIDERs The goal of the Objective Gateway program is to provide networking technology to the forward edge of the battlefield and create a high-capacity airborne communication backbone Future Work This paper is the first step towards developing a mission planning toolbox for the airborne network deployment problem A few further research directions could be accounting for link disruptions due to airborne platform banking expanding the optimization search space to include non-circular or tilted loiter orbits accounting for terrain effects providing coverage to mobile airborne nodes that are involved in a mission tactical edge networks planning so as to optimally use satellite communication when certain airborne links are inactive developing multiple tiers of airborne backbones located at different elevations and incorporating topology control such that the airborne nodes can change power levels thus radio range during periods where airborne topology is sparse ACKNOWLEDGMENTS The authors would like to thank Maneesh Varshney and Prof Mario Gerla for insightful discussions on the subject matter of this paper This work was supported under Air Force phase I SBIR grant number FA8750-07-C-0158 7 K Schroth and D Kiwior Interdomain Routing for Mobile Nodes Proceedings of the Military Communications Conference 2007 to appear 8 E G I D Kiwior and S V Pizzi Quality of Service QoS Sensitivity for the OSPF Protocol in the Airborne Networking Environment Proceedings of the Military Communications Conference 2005 9 L V J Cooley 0 Huang and S McGarry Mobile Airborne Networking Experience with Paul Revere Proceedings of the Military Communications Conference 2005 10 K Q Weinberger and L K Saul Unsupervised Learning of Image Manifolds by Semidefinite Programming International Journal of Computer Vision vol 70 no 1 pp 77-90 2006 11 S Boyd and L Vandenberghe Convex Optimization Cambridge University Press 2006 12 J Lfberg Yalmip A toolbox for modeling and optimization in MATLAB in Proceedings of the CACSD Conference Taipei Taiwan 2004 Online Available http://control.ee.ethz.ch joloef/yalmip.php 13 R A Horn and C R Johnson Matrix Analysis Cambridge University Press 1985 14 N Megiddo and A Tamir New results on the complexity of p-center problems SIAM Journal on Computing vol 12 no 4 pp 751-758 November 1983 REFERENCES 1 T A Kostas and T G Macdonald A Methodology for Evaluating and Planning Future Airborne Networks Proceedings of the Military Communications Conference 2004 2 R Ramirez Link Management in the Air Force Airborne Network Proceedings of the Military Communications Conference 2005 3 Y Wang and Y J Zhao Fundamental Issues in Systematic Design of Airborne Networks for Aviation Proceedings of the IEEE Aerospace Conference 2006 4 B Epstein and V Mehta Free Space Optical Communications Routing Performance in Highly Dynamic Airspace Environment Proceedings of the IEEE Aerospace Conference 2004 5 M Dehkordi K Chandrashekhar and J S Baras A placement algorithm for enhanced connectivity and reliability in wireless ad-hoc networks in Conference on Future Networking Technologies CoNEXT Toulouse France October 2005 6 D Kiwior and L Lam Routing Protocol Performance over Intermittent Links Proceedings of the Military Communications Conference 2007 to appear 9 


  10 Airborne Collision Avoidance System and Enhanced Vision System  Akira Ishide was born in Tokyo Japan.  He received the B.S. degree in 1971 from the Tokyo Institute of Technology, Tokyo, Japan.   Since 1971, he has been with the Electronic Navigation Research Institute, Ministry of Transport Tokyo, Japan.   He received the Ph D. in 1996 from the Tokyo Institute of Technology, Tokyo, Japan Since 2001, he has been with the Electronic Navigation Research Institute, Independent Administrative Institution He has been engaged in research on Aeronautical Satellite Communication System, Array Antenna System, and so on 


Alan Little is the MEDLI Project Manager at NASA's Langley Research Center He previously served on a variety of earth remote sensing missions and recently served as the NASA-CNES interface manager and the payload assembly integration and test manager on the joint NASAICNES CALIPSO Mission that was launched in April 2006 He has a MS in Optics from the University of Rochester Neil Cheatwood earned B.S MS and Ph.D degrees in aerospace engineering from NC State University He has played key roles in a number of NASA's planetary atmospheric flight programs and is a nationally recognized expert in aerosciences and flight mechanics for planetary entry systems He is currently serving as the Hypersonics Project Scientist for the Fundamental Aeronautics Program with NASA's ARMD Dr Cheatwood is also the Principle Investigator for the Mars Science Laboratory Entry Descent and Landing Instrumentation MEDLI project In recent years he led NASA LaRC efforts to develop inflatable aeroshell technologies He served as CoInvestigator to Claude Graves of NASA JSC on the NASA ESMD ESR&T Inflatable Aeroshell and TPS Development IA TD Project He served as the Principle Investigator for NASA LaRCs Inflatable Reentry Vehicle Experiment IRVE as well as the follow-on Program to Advance Inflatable Decelerators for Atmospheric Entry PAI-DAE Dr Cheatwood was responsible for aerodynamic databases of Stardust Mars Microprobe Genesis and Mars Exploration Rovers He has also contributed to the Mars Global Surveyor and Mars Sample Return flight projects Dr Cheatwood is an AIAA Associate Fellow and the principle author or co-author of 60 technical publications in the fields of fluid dynamics atmospheric entry and systems engineering Jeff Herath serves as the Lead Systems Engineer and Chief Engineer for the Mars Science Laboratory Entry Descent and Landing Instrumentation MEDLI He also serves as the Assistant Head of the Atmospheric Flight and Entry Systems Branch AFESB to plan direct and coordinate Branch activities in the areas offlight and entry systems research and development Mr Herath previously was the Acting Assistant Branch Head for Electronic Systems and served as the branch leadfor new business activities proposals and their development He was also the Principal Investigator PI for the Radiation Tolerant Intelligent Memory Stack RTIMS Project which successfully developed and demonstrated an in-flight reconfigurable radiation tolerant stacked memory array He co-founded Vianix LC a company developing and licensing voice compression technology and served as its Chief Technology Officer He developed the company's voice compression technology and was responsible for all research  development engineering personnel and production efforts He has 6 patents As Manager of Hardware Development at Arc Second Inc he designed and built a unique laser based three-dimensional positioning system which opened new markets for the company At E-Systems he successfully completed several military avionics programs and payload that were classified and consisted of system box and board level designs Michelle Munk has been a NASA employee for nearly 20 years first at the Johnson Space Center then at the Langley Research Center She has been involved in Mars advanced mission studies for many years both robotic and human contributing interplanetary trajectory analysis and entry and descent analysis She has managed the delivery of International Space Station hardware and was on the Mars Odyssey aerobraking operations team In 2002 Ms Munk accepted a detail assignment to become the Lead Engineer for Aerocapture Technology Development under In-Space Propulsion at Marshall Space Flight Center She managed the technical work of ISP Aerocapture for nearly 5 years before becoming the Project Area Manager and returning to Langley in 2007 Ms Munk is also a subsystem leadfor the Mars Science Laboratory Entry Descent and Landing Instrumentation MEDLI project and contributes to other NASA projects developing entry system technologies She has a BSAE from Virginia Tech and completed graduate coursework at the University of Houston Frank Novak is an Assistant Branch Head for Remote Sensing Flight Systems Branch RSFSB at the NASA Langley Research Center LaRC in Hampton VA He serves as the MEDLI Subsystem Manager for the Sensor Support Electronics SSE system Mr Novak has over 20 years of experience in the design development and test of spaceflight electronics He served as the lead development manager for the EVA IR Camera Project lead engineer for the visible imager for the GIFTS project lead electronics engineer for the pointing spectrometer for the Mars ARES project lead integration and test engineer for the SAGE III project and lead engineer for the interface adaptor module for SAGE III He earned a BS in Physics from Christopher Newport University in 1999 11 


Ed Martinez is a Project Manager/Lead Scientist/Project Engineer with 20 years experience in the aerospace and electromechanical field He is responsible for leading the Thermal Protection System TPS instrumentation programs for the NASA Ames Research Center As Project Manager he simultaneously managed multiple teams of scientists engineers and engineering technicians responsible for TPS projects including test analysis and technology advancement As Lead Scientist he was engaged in the characterization and operations of the world's largest shock tube This facility produced simultaneous overpressure and thermal environments at shock speeds up to Mach 5 As a Project Engineer his experience included project initiation coordination and providing design and instrument criteria for operating multi-100 million dollar DoD facilities Mr Martinez also managed data handling performed analysis reporting of test results and maintained technical proficiency in shockwave phenomenology 12 


  13 B IOGRAPHY  Brian Paczkowski is currently the Deputy Section Manager of the Planning and Execution Section within the Systems and Software Division at JPL. Prior to that he spent 9 years as the Cassini Science Planning Manager responsible for the development and implementation of the Science Operations Plan. Prior to Cassini, he was the Science Planning and Operations Team Chief for the Galileo Mission to Jupiter. He has also been involved with the pre-launch development of the science instruments on Galileo, Comet Rendezvous and Asteroid Flyby \(CRAF\ and Cassini missions. He has a BS in Astronomy from Villanova University and did graduate studies in Astronomy at Ohio State University  Barbara Larsen  is the Mission Operations System Engineer for the Cassini Mission. She is also on the science planning staff and previously worked in system engineering for the Mission Sequence Subsystem. She has a MS in Mathematics from California State University Long Beach and a BS in Mathematics from USC Trina Ray  is currently the Titan Orbiter Science Team \(TOST\ co-chair and the Science System Engineer for the Project Scientist for Cassini. She has been working on the Cassini Mission since before launch as an instrument operations lead for the Radio Science Team, and then as part of the Science Planning Team supporting Titan integrati on and sequence development She has a MS in Astronomy from San Diego State University and a BS in Physics, Astronomy option from CSUN  


  14  


 15 Fourier transform spectrometers at the Internationa l Scientific Station of the Jungfraujoch Switzerland  for atmospheric measurements and at the Institute of Astrophysics in Liège for laboratory measurements He was hired by JPL in August 1990 as MkIV cognizant engin eer and participated in all the MkIV campaigns since th en \(one DC-8 campaign 19 balloon campaigns Dr J.-F Bla vier obtained his Ph.D in Physics from the University o f Liège in July 1998 Paula Pingree is a Senior Engineer in the Instruments and Science Data Systems Division at JPL She has been involved in the design integration test and operation of several JPL flight projects most recently Deep Impact DI She has worked on the Tunable Laser Spectrometer development for the 2009 Mars Rover and is presently the Electronics CogE for the Juno Mission s Microwave Radiometer She also enjoys research and technology development for Smart Payloads in her s pare time Paula has a Bachelor of Engineering degree i n Electrical Engineering from Stevens Institute of Te chnology in Hoboken, NJ, and an MSEE degree from California State University Northridge.  She is a member of IEEE 


 16  A High Capacity Solid Sate Recorder \(HC-SSR\is suggested using devices predicated to be available for each decade.   The design is robu st and easily implemented using conservative design and manufacturing techniques D EFINITIONS  rad radiation absorbed dose\:   the dose causing 0.01 joule of energy to be absorbed per kilogram of matter.   As the absorption is greatly affected by the molecular structure of the material, citations should al so indicate the material as a subscript to the term 223rad\224, as in rad Si indicating Silicon equivalency.  For the purposes of this paper, radiation equivalency always assumes Silicon For completeness, it should be noted that System International replaced the \223rad\224 with the unit Gr ay \(Gy\nd having an equivalency of 100 rads = 1 Gy [27 How e v e r   the use of rads, kilorads, megarads remains in the industry vernacular and is used in this document Moore\222s Law Named after Fairchild Semiconductor technologist Gordon Moore, Moore\222s law was derived from empirical data which shows that the dimensions of basic memory cells will shrink by approximately 50% of the previous value every 30 to 36 months.  It is Moore\222s Law more or less, that forms the backbone of the ITRS examinations for memory devices A DDITIONAL M ATERIAL  Standard Dose Rates for Various Orbits and Missions per year Earth    LEO  100 rad \(protons  MEO  100 krad \(protons electrons  GEO  1 krad \(electrons  Transfer Orbit  10 krad \(protons electrons Mars     Surface  2 krad \(electrons  Orbit  5 krad \(protons  Transit  5 krad \(protons Jovian     Transfer  100 Mrad \(protons electrons A CKNOWLEDGEMENT  The research described in this paper was carried out at the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration.   The Author thanks the many who guided the concept and offered support all along the way.   With special thanks to fellow-JPL\222ers Gary Noreen who provided funding\nd Taher Daud who provided editing ad-hoc extraordinaire  R EFERENCES    G M o o r e C r a m m i ng m o re C o m pone nt s o n t o  Integrated Circuits Electronics vol. 38, no. 8, April 1965 2 G M o ore, "No Expo n e n tial is Forev er: B u t 223Forever\224 Can Be Delayed Digest of Technical Papers, International Solid State Circuits Conference pp. 1.1-1 thru 1.1-19, 2003  S eagate Tec hnology Com p any. Seagate Tec hnical Corporation. [Onlin http://www.seagate.com/docs/pdf/marrketing/Article _Perpendicular_Recording.pdf   B l u R ay Di sc Ass o ci at i on  2 00 5 M a rc h B l u-R a y  Disc Technical Papers  J Vel e v K  D B e l a shche n ko  an d et _al   2 0 0 7  October\ MSREC - University of Nebraska Onlin http://www.mrsec.unl.edu/research/nuggets/nugget_2 6.shtml  6 R Katti, "Honeywell Rad i atio n Hard en ed  No nVolatile Memory \(MRAM\ Product Development in Proceedings, IEEE No n-Volatile Memory Technology Symposium Orlando, 2004, pp. L2:1-15 7 S aif u n  Sem ico n d u c tor  2 008 NV M Techno log y  Over http://www.saifun.com/content.asp?id=113  


 17    J. Ta usc h  S. T y son a n d T T a i r ba nks  Multigenerational Radiation Response Trends in SONOSb ased NROM Flash Memories with Neutron Latch-up Mitigation," in NSREC Radiation Effects Workshop Honolulu, 2007, pp. 189-193 9  Semico n du c to r In du str y A s sociatio n SIA  2 008   August\ Home. [Online  www.itrs.net  1  S. Ty s o n P ri v a t e C o m m uni que Tra n sEl  Semiconductor, Albuquerque, NM, 2008 1  T. M i k o l a ji c k  and C U Pi n n o w  2 00 8 N o vem b er Indo-German Winter Academy, 2008, Course 3 Onlin http://www.leb.eei.unierlangen.de/winterakadem ie/2008/courses/course3_ material/futureMemory/Mikolajick_TheFutureofNV M.pdf   BAE System s North Am erica, [Data Sheet Microcircuit, CMOS, 3.3V, NVRAM 8406746, April 28, 2008, Rev A 1  N Ha dda d a n d T Scot t  A da pt i n g C o m m erci al  Electronics to the Natura lly Occurring Radiation Environment," in IEEE Nuclear and Space Radiation Effects Conference Short Course Tucson, 1994, pp iv-14 1  D. R  R o t h a n d et _al S EU a n d TI D Test i n g of t h e Samsung 128 Mbit and the Toshiba 256 Mbit flash memory," in Radiation Effects Data Workshop  Reno, 2000 1  F. I r o m and D N guy e n  S i n gl e E v ent  Ef fe ct  Characterization of High Density Commercial NAND and NOR Nonvolatile Flash Memories Honolulu, 2007 1  C Ha fer M  L a hey a n d et _al R adi a t i o n H a rd ness  Characterization of a 130nm Technology," in Proceedings IEEE Nuclea r and Space Radiation Effects Conference Honolulu, 2007 17  T. R O l dh am J. Fr iend lich  an d et_ a l, "TID  an d SEE Response of an Advanced Samsung 4Gb NAND Flash Memory," , Honolulu, 2007  R. C. Lac o e C MOS Scaling, Desi gn Princi ples a n d Hardening-by-Design Methodologies," in Nuclear and Space Radiation Effects Conference Short Course Notebook Monterey, 1993, pp. II-1 thru II142 1 J. Pat t e rs o n a n d S  Gue rt i n   E m e rgi ng S E F I M o des and SEE Testing for Highly-Scaled NAND FLASH Devices," in Proceedings 2005 Non-Volatile Memory Technology Symposium vol. CD-ROM, Dallas, TX 2005, pp. G-3, Session G ; Paper 3 2 J. Ta usc h  S. T y son a n d T F a i rba nks  Mulitgenerational Radiation Response Trends in SONOSb ased NROM Flash Memories with Neutron Latch-up Mitigation," in Honolulu Radaition Effects Data Workshop, NSREC, 2007, pp. 189-193 2 M Janai  B Ei t a n A Sha p pi r I B l o o m and G  Cohen, "Data Retention Reliability Model of NROM Nonvolatile Memory Products IEEE Transactions on Device and Materials Reliability vol. 4, no. 3, pp 404-415, September 2004 2 D N g uy en a n d F I r o m Tot al Io ni zi n g  Do se \(T ID  Tests on Non-Volatile Memories: Flash and MRAM," in 2007 IEEE Radiation Effects Workshop  vol. 0, Honolulu, 2007, pp. 194-198  G. Noree n  a n d et_al L ow Cost Deep Space Hybrid Optical/RF Communications Architecture," , Big Sky, Montana, 2009, Pre-print 2 T. Sasa da a n d S. I c hi kawa  A p p l i cat i o n o f  Sol i d  State Recorders to Spacecraft," in Proceedings, 54th International Astronautical Cogress Bremen, 2003 2 H Ka nek o  E rr or C o nt r o l C odi ng f o r  Semiconductor Memory Systems in the Space Radiation Environment," in Proceedings, 20th IEEE International Symposium in Defect and Fault Tolerance in VLSI Systems, DFT2005 Monterey 2005 2 T. Sasa da a n d H Ka nek o  D evel o p m e nt an d Evaluation of Test Circuit for Spotty Byte Error Control Codes," in Proceedings, 57th International 


 18  Astronautical Congress Valencia, 2006 27  Bu reau  In tern atio n a l d e s Po ids et Mesures. \(2 008  August\SI Base Units. [On http://www.bipm.org/en/si/base_units   B IOGRAPHY  Author, Karl Strauss, has been employed by the Jet Propulsion Laboratory for over 22 years.  He has been in the Avionics Section from day One.  He is considered JPL\222s memory technology expert with projects ranging from hand-woven core memory \(for another employer\o high capacity solid state designs.  He managed the development of NASA\222s first Solid State Recorder, a DRAM-based 2 Gb design currently in use by the Cassini mission to Satu rn and the Chandra X-Ray observatory in Earth Orbit.  Karl was the founder, and seven-time chair of the IEEE NonVolatile Memory Technology Symposium, NVMTS, deciding that the various symposia conducted until then were too focused on one technology.  Karl is a Senior IEEE member and is active in the Nuclear and Plasma Scie nce Society, the Electron Device Society and the Aerospace Electronic Systems Society Karl is also an active member of SAE Karl thanks his wonderful wife of 28 years, Janet, for raising a spectacular family: three sons, Justin, Jeremy Jonathan.  Karl\222s passion is trains and is developing a model railroad based upon a four-day rail journey across Australia\222s Northern Outback   


 19 Bollobás, B. 2001. Random Graphs. Cambridge University Press; 2nd edition. 500pp  Cawley, G. C., B. L. C. Talbot, G. J. Janacek, and M. W Peck. 2006. Sparse Bayesian Ke rnel Survival Analysis for Modeling the Growth Domain of Microbial Pathogens  Chiang C. L. 1960. A stochastic study of life tables and its applications: I. Probability distribution of the biometric functions. Biometrics, 16:618-635  Cox,  D. R. 1972. Regression models and life tables J. R Stat. Soc. Ser. B 34:184-220  Cox, D. R. 1975.   Partial likelihood Biometrika 62:269276  Cox, D. R. & D. Oakes. 1984 Analysis of Survival Data  Chapman & Hall. London  Cressie, N. A. 1993 Statistics for Spatial Data John Wiley Sons. 900pp  Duchesne, T. 2005. Regression models for reliability given the usage accumulation history. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty Y. Armijo. pp.29-40. World Scientific, New Jersey  Eleuteri, A., R. Tagliaferri, L. Milano, G. Sansone, D D'Agostino, S. De Placido,  M. Laurentiis. 2003.  Survival analysis and neural networks. Proceedings of the International Joint Conference on Neural Networks, Vol. 4 20-24 July 2003 Page\(s\:2631 - 2636  Ellison, E., L. Linger, and M Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013, 1997  Fleming, T. R. & D. P. Harrington. 1991. Counting process and survival analysis. John Wiley & Sons. 429pp  Graver, J. and M. Sobel 2005. You may rely on the Reliability Polynomial for much more than you might think Communications in Statistics: Theory and Methods  34\(6\1411-1422  Graves, T. and M. Hamada. 2005. Bayesian methods for assessing system reliability: models and computation. In Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson, et al. pp.41-53  Grimmett, G. 2006 The Random-Cluster Model Springer  Grimmett, G. 1999 Percolation Springer  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis.  Springer. 481pp  Jin Z. 2005. Non-proportional semi-parametric regression models for censored data. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.279-292 World Scientific  Kalbfleisch, J. D. & R. L. Prentice. 1980 The Statistical Analysis of Failure Time Data John Wiley & Sons.  New York. 1980  Kalbfleisch, J. D. &  R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data.  Wiley-InterScience, 2nd ed 462pp  Lisboa, P. J. G. and H. Wong. 2001. Are neural networks best used to help logistic regression? Proceedings of International Joint Conference on Neural Networks, IJCNN 01. Volume 4, 15-19,  July 2001. Page\(s\:2472 - 2477 vol.4  Kauffman, R. J. and B. Wang. 2002. Duration in the Digital Economy. Proceedings of th e 36th Hawaii International Conference on System Sciences \(HICSS’03\ Jan 2003  Kaplan, E. L. & P.  Meier.  1958.  Nonparametric estimation from incomplete observations J. Amer. Statist. Assoc  53:457-481  Klein, J. P. and P. K. Goel 1992. Survival Analysis: State of the Art.  Kluwer Academic Publishes. 450pp  Klein, J. P. and  M. L Moeschberger. 20 03. Survival analysis techniques for ce nsored and truncated data Springer  Krings, A. and Z. S. Ma. 2006.  "Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks MILCOM 2006, Military Communications Conference, 2325 October, 7 pages, 2006  Krings, A. W. 2008.  Survivable Systems.  in Information Assurance: Dependability and Security in Networked Systems Yi Qian, James Joshi, David Tipper, and Prashant Krishnamurthy, Morgan Kaufmann Publishers. \(in press  Lawless, J. F. 1982. Statistical models and methods for lifetime data.  John Wiley & Sons. 579pp  Lawless, J. F. 2003. Statistical models and methods for lifetime data.  John Wiley & Sons. 2nd ed. 630pp  Li, M. and P. Vitanyi. 1997. Introduction to  Kolmogorov Complexity and Its Applications. 2nd ed, Springer  Ma, Z. S. 1997.  Survival analysis and demography of Russian wheat aphid populations.  Ph.D dissertation, 307pp University of Idaho Moscow, Idaho, USA 


 20 Ma, Z. S., and E. J. Bechinski. 2008.  Developmental and Phenological Modeling of Russian Wheat Aphid Annals of Entomological Soc. Am In press  Ma, Z. S. and A. W. Krings. 2008a. The Competing Risks Analysis Approach to Reliability Survivability, and Prognostics and Health Management.  The 2008 IEEEAIAA AeroSpace Conference. BigSky, Montana, March 18, 2008. \(In Press, in the same volume  Ma, Z. S. and A. W. Krings 2008b. Multivariate Survival Analysis \(I\e Shared Frailty Approaches to Reliability and Dependence Modeling. The 2008 IEEE-AIAA AeroSpace Conference. BigSky Montana, March 1-8, 2008 In Press, in the same volume  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(II\ Multi-State Models in Biomedicine and Engineering Reliability. 2008 IEEE International Conference on Biomedical Engineering and Informatics BMEI 2008\27th-30th, 2008 Accepted   Mani, R., J. Drew, A. Betz, P. Datta. 1999. Statistics and Data Mining Techniques for Lifetime Value Modeling ACM Conf. on Knowledge Discovery and Data Mining  Mazzuchi, T. A., R Soyer., and R. V Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Meeker, W. Q. and L. A. Escobar. 1998. Statistical Methods for Reliability Data. Wiley-Interscience  Munson, J. C. 2003. Software Engineering Measurement Auerbach Publications  Nelson, W. 1969. Hazard plotting for incomplete failure data J. Qual. Tech 1:27-52  Nakagawa, T. 2006.  Shock and Damage Models in Reliability Theory. Springer  Osborn, B. 2005. Leveraging remote diagnostics data for predictive maintenance.   In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp. 353-363  Pena, E. A. and E. H. Slate. 2005. Dynamic modeling in reliability and survival analysis. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.55-71  Reineke, D. M., E. A. Pohl, and W. P. Murdock. 1998 Survival analysis and maintenance policies for a series system, with highly censore d data.  1998 Proceedings Annual Reliability and Maintainability Symposium. pp 182-188  Schabenberger, O. and C. A. Gotway. 2005. Statistical Methods for Spatial Data Analysis.  Chapman & Hall/CRC  Severini, T. A. 2000. Likelihood methods in statistics Oxford University Press  Shooman, M. L. 2002. Reliability of Computer Systems and Networks: Fault Tolerance, Analysis and Design. John Wiley and Sons. 551pp  Stillman, R. H. and M. S. Mack isack, B. Sharp, and C. Lee 1995. Case studies in survival analysis of overhead line components. IEE Conferen ce of the Reliability and Distribution Equipment. March 29-31, 1995. Conference Publication No. 406. pp210-215  Therneau, T. and P. Grambsch. 2000 Modeling Survival Data: Extending the Cox Model Springer  Wilson, A.  N. Limnios, S Kelly-McNulty, Y. Armijo 2005. Modern Statistical and Mathematical Methods in Reliability. World Scientific, New Jersey  Xie, M. 1991. Software Reliability Modeling. World Scientific Press    B IOGRAPHY   Zhanshan \(Sam\ Ma holds a Ph.D. in Entomology and is a Ph.D. candidate in Computer Science at the University of Idaho. He has published approximately 30 journal and 30 conference papers, mainly in the former field.  Prior to his recent return to academia, he worked as senior network/software engineers in software industry.  His current research interests include reliability and survivability of wireless sensor networks, fault tolerance survival analysis, evolutionary game theory, evolutionary computation and bioinformatics  Axel W. Krings is a professor of Computer Science at the University of Idaho.  He received his Ph.D. \(1993\ and M.S 1991\ degrees in Computer Science from the University of Nebraska - Lincoln, and his M.S. \(1982\ in Electrical Engineering from the FH-Aachen, Germany.  Dr. Krings has published extensively in the area of Computer Network Survivability, Security, Fault-Tolerance and Realtime Scheduling. In 2004/2005 he was a visiting professor at the Institut d'Informatique et Mathématiques Appliquées de Grenoble, at the Institut National Polytechnique de Grenoble, France.  His work has been funded by DoE/INL DoT/NIATT, DoD/OST and NIST 


States\nWAb-3.4: NEW RESULTS IN THE ANALYSIS OF DECISION-FEEDBACK 2118\nEQUALIZERS\nAhmed Mehana, Samsung Electronics, Co Ltd., United States; Aria Nosratinia, University of Texas at \nDallas, United States\nWAb-5: TARGET TRACKING II\nWAb-5.1: POSTERIOR DISTRIBUTION PREPROCESSING FOR PASSIVE 2125\nDTV RADAR TRACKING: SIMULATED AND REAL DATA\nEvan Hanusa, Laura Vertatschitsch, David Krout, University of Washington, United States\nWAb-5.2: DEPTH-BASED PASSIVE TRACKING OF SUBMERGED SOURCES  ............................................2130\nIN THE DEEP OCEAN USING A VERTICAL LINE ARRAY\nLisa Zurk, John K. Boyle, Jordan Shibley, Portland State University, United States\nWAb-5.3: GENERALIZED LINEAR MINIMUM MEAN-SQUARE ERROR 2133\nESTIMATION WITH APPLICATION TO SPACE-OBJECT TRACKING\nYu Liu, X. Rong Li, Huimin Chen, University of New Orleans, United States\nWAb-5.4: FEATURE-AIDED INITIATION AND TRACKING VIA TREE SEARCH ..........................................2138\nHossein Roufarshbaf Jill Nelson, George Mason University, United States\nxxxiii\nWAb-6: DIRECTION OF ARRIVAL ESTIMATION\nWAb-6.1: A SELF-CALIBRATION TECHNIQUE FOR DIRECTION 2145\nESTIMATION WITH DIVERSELY POLARIZED ARRAYS\nBenjamin Friedlander, University of California, Santa Cruz, United States\nWAb-6.2: CRAMER-RAO PERFORMANCE BOUNDS FOR SIMULTANEOUS  ..............................................2150\nTARGET AND MULTIPATH POSITIONING\nLi Li, Jeff Krolik, Duke University, United States\nWAb-6.3: COPY CORRELATION DIRECTION-OF-ARRIVAL ESTIMATION  .................................................2155\nPERFORMANCE WITH A STOCHASTIC WEIGHT VECTOR\nChrist Richmond, Keith Forsythe, MIT Lincoln Laboratory, United States; Christopher Flynn, Stevens nInstitute of Technology, United States\nWAb-6.4: LOCATING CLOSELY SPACED COHERENT EMITTERS USING 2160\nTDOA TECHNIQUES\nJack Reale, Air Force Research Laboratory / Binghamton University, United States; Lauren Huie, Air \nForce Research Laboratory, United States Mark Fowler, State University of New York at Binghamton, \nUnited States\nWAb-7: ENERGY- AND RELIABILITY-AWARE DESIGN\nWAb-7.1: LOW-ENERGY ARCHITECTURES FOR SUPPORT VECTOR 2167\nMACHINE COMPUTATION\nManohar Ayinala, Keshab K Parhi, University of Minnesota, United States\nWAb-7.2: TRUNCATED MULTIPLIERS THROUGH POWER-GATING FOR 2172\nDEGRADING PRECISION ARITHMETIC\nPietro Albicocco, Gian Carlo Cardarilli, University of Rome Tor Vergata, Italy; Alberto Nannarelli, \nTechnical University of Denmark Denmark; Massimo Petricca, Politecnico di Torino, Italy; Marco Re, \nUniversity of Rome Tor Vergata Italy\nWAb-7.3: A LOGARITHMIC APPROACH TO ENERGY-EFFICIENT GPU 2177\nARITHMETIC FOR MOBILE DEVICES\nMiguel Lastras Behrooz Parhami, University of California, Santa Barbara, United States\nWAb-7.4: ON SEPARABLE ERROR DETECTION FOR ADDITION ..................................................................2181\nMichael Sullivan, Earl Swartzlander, University of Texas at Austin, United States\nWPb-1: PAPERS PRESENTED IN 2012\nWPb-1.1 DYNAMICALLY RECONFIGURABLE AVC DEBLOCKING FILTER  .............................................2189\nWITH POWER AND PERFORMANCE CONSTRAINTS\nYuebing Jiang, Marios Pattichis, University of New Mexico\nxxxiv\n 


on science teams for numerous planetary missions including Magellan, Mars Observer, Mars Global Surveyor and Rosetta. He was the US Project Scientist for the international Mars NetLander mission, for which he was also principal investigator of the Short-Period Seismometer experiment, and is currently the Project Scientist for the Mars Exploration Rovers. He led the Geophysics and Planetary Geology group at JPL from 1993-2005, and is the JPL Discipline Program Manager for Planetary Geosciences. He has held several visiting appointments at the Institut de Physique du Globe de Paris. He has a BS in physics and a PhD in geophysics from the University of Southern California  David Hansen is a member of the technical staff in the Communications Systems and Operations Group at the Jet Propulsion Laboratory. Current work includes the development of the telecom subsystem for the Juno project. David received a B.S. in Electrical Engineering from Cornell University and an M.S. in Electrical Engineering from Stanford University  Robert Miyake is a member of the technical staff in the Mission and Technology Development Group at the Jet Propulsion Laboratory. Current work includes the development of thermal control subsystems for interplanetary flagship missions to Jupiter and Saturn missions to Mars and the Earth Moon, and is the lead Thermal Chair for the Advanced Project Design Team Robert graduated with a B. S. from San Jose State University, with extensive graduate studies at UCLA University of Washington, and University of Santa Clara  Steve Kondos is a consultant to the Structures and Mechanisms group at the Jet Propulsion Laboratory. He currently is generating the mechanical concepts for small Lunar Landers and Lunar Science Instrument packages in support of various Lunar mission initiatives. He also provides conceptual design, mass and cost estimating support for various Team X studies as the lead for the Mechanical Subsystem Chair. Steve is also involved with various other studies and proposals and provides mentoring to several young mechanical and system engineers. He graduated with a B.S. in Mechanical Engineering from the University of California, Davis and has 28 years of experience in the aerospace field ranging from detail part design to system of systems architecture development. He has worked both in industry and in government in defense, intelligence commercial and civil activities that range from ocean and land based systems to airborne and space systems. Steve has received various NASA, Air Force, Department of Defense and other agency awards for his work on such projects as the NASA Solar Array Flight Experiment, Talon Gold, MILSTAR, Iridium, SBIRS, Mars Exploration Rovers ATFLIR, Glory Aerosol Polarimeter System and several Restricted Programs  Paul Timmerman is a senior member of technical staff in the Power Systems Group at the Jet Propulsion Laboratory Twenty-five years of experience in spacecraft design including 22 at JPL, over 250 studies in Team-X, and numerous proposals. Current assignments include a wide variety of planetary mission concepts, covering all targets within the solar system and all mission classes. Paul graduated from Loras College with a B.S. in Chemistry in 1983  Vincent Randolph is a senior engineer in the Advanced Computer Systems and 


the Advanced Computer Systems and Technologies Group at the Jet Propulsion Laboratory. Current work includes generating Command and Data Handling Subsystem conceptual designs for various proposals and Team X.  He also supports Articulation Control and Electronics design activities for the Advanced Mirror Development project. Vincent graduated from the University of California at Berkeley with a B.S. in Electrical Engineering 18  pre></body></html 


i models into time and covariate dependent dynamic counterparts  ii models and reliability analysis in a more realistic manner  iii level  whether or not functional components \(loyal generals diagnose correctly and take proper actions such as fault mask of failed components \(traitors asymmetric  iv survivability analysis. Evolutionary game modeling can derive sustainable or survivable strategies \(mapped from the ESS in EGT such as node failures such as security compromise level modeling in the so-called three-layer survivability analysis developed in Ma \(2008a this article  v offer an integrated architecture that unite reliability survivability, and fault tolerance, and the modeling approaches with survival analysis and evolutionary game theory implement this architecture. Finally, the dynamic hybrid fault models, when utilized to describe the survival of players in EGT, enhance the EGT's flexibility and power in modeling the survival and behaviors of the game players which should also be applicable to other problem domains where EGT is applicable  5. OPERATIONAL LEVEL MODELING AND DECISION-MAKING  5.1. Highlights of the Tactical and Strategic Levels  Let's first summarize what are obtainable at both tactical and strategic levels. The results at both tactical and strategic levels are precisely obtainable either via analytic or simulation optimization. With the term precisely, we mean that there is no need to assign subjective probabilities to UUUR events. This is possible because we try to assess the consequences of UUUR events \(tactical level ESS strategies \(strategic level time prediction of survivability. The following is a list of specific points. I use an assumed Wireless Sensor Network WSN  i of UUUR events: \(a actions which can be treated as censored events; \(b Cont' of Box 4.2 It can be shown that the replicator differential equations are equivalent to the classical population dynamics models such as Logistic differential equation and LotkaVolterra equation \(e.g., Kot 2001 Logistic equation, or the limited per capital growth rate is similar to the change rate of the fitness  xfxfi which can be represented with the hazard function or survivor functions introduced in the previous section on survival analysis.  This essentially connects the previous survival analysis modeling for lifetime and reliability with the EGT modeling. However, EGT provides additional modeling power beyond population dynamics or survival analysis approaches introduced in the previous section. The introduction of evolutionary theory makes the games played by a population evolvable. In other words, each player \(individual 


other words, each player \(individual agent and players interact with each other to evolve an optimized system Box 4.3. Additional Comments on DHF Models  The above introduced EGT models are very general given they are the system of ordinary differential equations. Furthermore, the choice of fitness function f\(x complexity to the differential equation system.  The system can easily be turned into system of nonlinear differential equations. The analytical solution to the models may be unobtainable when nonlinear differential equations are involved and simulation and/or numerical computation are often required  In the EGT modeling, Byzantine generals are the game players, and hybrid fault models are conveniently expressed as the strategies of players; the players may have different failure or communication behaviors Furthermore, players can be further divided into groups or subpopulations to formulate more complex network organizations. In the EGT modeling, reliability can be represented as the payoff \(fitness, the native term in EGT of the game. Because reliability function can be replaced by survivor function, survival analysis is seamlessly integrated into the EGT modeling. That is, let Byzantine generals play evolutionary games and their fitness reliability function  The evolutionary stable strategy \(ESS counterpart of Nash equilibrium in traditional games ESS corresponds to sustainable strategies, which are resistant to both internal mutations \(such as turning into treason generals or nodes such as security compromises represent survivable strategies and survivability in survivability analysis. Therefore, dynamic hybrid fault models, after the extension with EGT modeling, can be used to study both reliability and survivability 13 risks such as competing risks which can be described with CRA; \(c captured with the shard frailty.  We believe that these UUUR events are sufficiently general to capture the major factors/events in reliability, security and survivability whose occurrence probabilities are hard or impossible to obtain  Instead of trying to obtain the probabilities for these events which are infeasible in most occasions, we focus on analyzing the consequences of the events.  With survival analysis, it is possible to analyze the effects of these types of events on survivor functions. In addition, spatial frailty modeling can be utilized to capture the heterogeneity of risks in space, or the spatial distribution of risks \(Ma 2008a d UUUR events introduced previously. These approaches and models that deal with the effects of UUUR events form the core of tactical level modeling  To take advantage of the tactical level modeling approaches it is obviously necessary to stick to the survivor functions or hazard functions models. In other words, survival analysis can deal with UUUR events and offer every features reliability function provides, but reliability function cannot deal with UUUR events although survivor function and reliability function have the exactly same mathematical definition. This is the junction that survival analysis plays critical role in survivability analysis at tactical level. However, we 


recognize that it is infeasible to get a simple metric for survivability similar to reliability with tactical level modeling alone. Actually, up to this point, we are still vague for the measurement of survivability or a metric for survivability. We have not answered the question: what is our metric for survivability? We think that a precise or rigorous definition of survivability at tactical level is not feasible, due to the same reason we cited previously  the inability to determine the probabilities of UUUR events However, we consider it is very helpful to define a work definition for survivability at the tactical level  We therefore define the survivability at tactical level as a metric, Su\(t t function or reliability function with UUUR events considered. In the framework of three-layer survivability analysis, this metric is what we mean with the term survivability. The "metric" per se is not the focus of the three-layer survivability analysis. It is not very informative without the supports from the next two levels  strategic and operational models.  However, it is obvious that this metric sets a foundation to incorporate UUUR effects in the modeling at the next two levels  Due to the inadequacy of tactical level modeling, we proposed the next level approach  strategic level modeling for survivability. As expected, the tactical level is one foundation of strategic level modeling ii objectives: \(a affect survivability which survival analysis alone is not adequate to deal with; \(b survivability at tactical level is necessary but not sufficient for modeling survivability, we need to define what is meant with the term survivability at strategic level  With regard to \(a behaviors or modes which have very different consequences. These failure behaviors can be captured with hybrid fault models. However, the existing hybrid fault models in fault tolerance field are not adequate for applying to survivability analysis. There are two issues involved: one is the lack of real time notion in the constraints for hybrid fault models \(e.g., N&gt;3m+1 for Byzantine Generals problem synthesize the models after the real-time notions are introduced. The solution we proposed for the first issue is the dynamic hybrid fault models, which integrate survivor functions with traditional hybrid fault models. The solution we proposed for the second issue is the introduction of EGT modeling  With regard to \(b modeling our problem at strategic level, EGT modeling is essentially a powerful optimization algorithm.  One of the most important results from EGT modeling is the so-called evolutionary stable strategies \(ESS We map the ESS in EGT to survivable strategies in survivability analysis.   Therefore, at the strategic level, our work definition for survivability refers to the survivable strategies or sustainable strategies in the native term of EGT, which can be quantified with ESS  In addition to integrating dynamic hybrid fault models another advantage for introducing EGT modeling at strategic level is the flexibility for incorporating other node behaviors \(such as cooperative vs. non-cooperative those behaviors specified in standard hybrid fault models, as well as anthropocentric factors such as costs constraints  Without UUUR events, both tactical and strategic level 


Without UUUR events, both tactical and strategic level models default to regular reliability models. This implies that, in the absence of UUUR events, reliable strategies are sustainable or survivable.  This also implies that three-layer survivability analysis defaults to reliability analysis however, the three-layer approach does offer some significant advantages over traditional reliability analysis, as discussed in previous sections. Nevertheless, when UUUR events exist, reliable strategies and survivable strategies are different. This necessitates the next operational level modeling  5.2. Operational Level Modeling and Decision-Making  When UUUR events are involved, we cannot make real time predictions of survivability at tactical and strategic levels This implies that the implementations of survivable 14 strategies need additional measures that we develop in this section.  Box 5.1 explains the ideas involved with possibly the simplest example  Figure 4 is a diagram showing a simplified relationship between action threshold survivability \(TS survivability \(ES view since both TS and ES are multidimensional and dynamic in practice. Therefore, the sole purpose of the diagram is to illustrate the major concepts discussed above The blue curve is the survivability when survivable strategies specified by ESS are implemented at some point before time s.  The system is then guaranteed to hold survivability above ES. In contrary, if no ESS implemented before time s, then the system quickly falls below to the survivable level at around 40 time units  T i m e 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 1 0 0 Su rv iv ab ili ty M et ric S u t 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 E S S  i s  I m p lm e n t e d N o  E S S  is  I m p lm e n t e d ts E S T S  Figure 4. A Diagram Showing the Relationship Between TS and ES, as well as timing of s and t, with s &lt; t  6. SUMMARY  The previous sections discussed the major building blocks 


The previous sections discussed the major building blocks for the new life-system inspired PHM architecture. This section first identifies a few minor aspects that have not been discussed explicitly but are necessary for the implementation of the architecture, and then we summarize the major building blocks in a diagram  6.1. Missing Components and Links  Optimization Objectives  Lifetime, reliability, fault tolerance, and survivability, especially the latter two, are application dependent. Generally, the optimization of reliability and survivability are consistent; in that maximization of reliability also implies maximization of survivability. However, when application detail is considered, optimization of lifetime is not necessarily consistent with the optimization of reliability. Consider the case of the monitoring sensor network as an example. The network reliability is also dependent on connectivity coverage, etc, besides network lifetime. What may be further complicated is the time factor. All of the network metrics are time-dependent. A paradoxical situation between lifetime and reliability could be that nodes never 'sleep                                                   


          Box 5.1 Operational Level Modeling  Assuming that the ESS solution for a monitoring sensor network can be expressed with the following simple algebraic conditions: survivability metric at tactical level SU = 0.7, Router-Nodes in the WSN &gt; 10%, Selfish Nodes &lt; 40%. Even with this extremely simplified scenario, the ESS strategies cannot be implemented because we do not know when the actions should be taken to warrant a sustainable system.  These conditions lack a correlation with real time  The inability to implement ESS is rooted in our inability to assign definite probabilities to UUUR events, which implies that we cannot predict when something sufficiently bad will jeopardize the system survivability What we need at the operational level is a scheme to ensure ESS strategy is in place in advance  The fundamental idea we use to implement the ESS strategy is to hedge against the UUUR events. The similar idea has been used in financial engineering and also in integrated pest management in entomology. This can be implemented with the following scheme  Let us define a pair of survivability metrics: one is the expected survivability \(ES threshold survivability or simply threshold survivability \(TS ES is equivalent to the survivability metric at tactical level. ES corresponds to ESS at strategic level, but they are not equivalent since ESS is strategy and ES is survivability. TS is the survivability metric value \(at tactical level and TS can be obtained from strategic level models. For example, TS = SU\(s t condition for the implementation of ESS. In other words, the implementation of strategies that ensures TS at time s will guarantee the future ES level at time t.  To make the implementation more reliable and convenient multiple dynamic TSs can be computed at time s1, s2 sk, with si &lt; t for all i.  These TS at times s1, s2, ..., sk should be monitored by some evaluation systems  Unlike tactical and strategic levels, the operational level modeling is approximate. The term "approximate means that we cannot predict the real time survivability or we do not know the exact time an action should be taken. Instead, the action is triggered when the monitored survivability metric SU\(r survivability \(TS scheme of TS and ES, we ensure the ES by taking preventative actions \(prescribed by ESS and triggered by the TS consequences of UUUR events  Figure 4 is a diagram showing the above concepts and the decision-making process involved 15 This wakefulness \(never 'sleep short period but at the expense of network lifetime. Of course, when the network is running out of lifetime, network reliability ultimately crashes. This example reminds us that 


reliability ultimately crashes. This example reminds us that multi-objective optimization should be the norm rather than exception  Constraints and Extensions  Many application specific factors and constraints are ignored in this article. For example, we mentioned about spatial heterogeneity of environment, but never present a mathematical description The spatial heterogeneity can be modeled with the so-called spatial frailty in multivariate survival analysis \(Ma 2008a  Evolutionary Algorithm  Evolutionary game modeling when implemented in simulation, can be conveniently implemented with an algorithm similar to Genetic Algorithms \(GA ESS in the evolutionary game model with simulation is very similar to GA. Dynamic populations, in which population size varies from generation to generation \(Ma &amp; Krings 2008f of node failures. Another issue to be addressed is the synchronous vs. asynchronous updating when topology is considered in the simulation. This update scheme can have profound influences on the results of the simulation. Results from cellular automata computing should be very useful for getting insights on the update issue  6.2. Summary and Perspective  To recapture the major points of the article, let us revisit Figure 3, which summarizes the principal modules of the proposed life-system inspired PHM architecture. The main inspiration from life systems is the notion of individuals and their assemblage, the population. Population is an emergent entity at the next level and it has emergent properties which we are often more concerned with. Survival analysis, which has become a de facto standard in biomedicine, is particularly suitable for modeling population, although it is equally appropriate at individual level. Therefore, survival analysis \(including competing risks analysis and multivariate survival analysis comprehensively in the context of PHM in a series of four papers presented at IEEE AeroSpace 2008 \(Ma &amp; Krings 2008a, b, c, &amp; d proposed architecture. Survival analysis constitutes the major mathematical tools for analyzing lifetime and reliability, and also forms the tactical level of the three-layer survivability analysis  Besides lifetime and reliability, two other major modules in Figure 3 are fault tolerance and survivability. To integrate fault tolerance into the PHM system, Dynamic Hybrid Fault DHF 2008e, Ma 2008a make real-time prediction of reliability more realistic and make real-time prediction of fault tolerance level possible DHF models also unite lifetime, reliability and fault tolerance under a unified modeling framework that consists of survival analysis and evolutionary game theory modeling  DHG models also form the partial foundation, or strategic level, for the three-layer survivability analysis. At the strategic level, the Evolutionary Stable Strategies \(ESS which is mapped to survivable or sustainable strategies, can be obtained from the evolutionary game theory based DHF models. When there is not any UUUR event involved reliability and survivability are consistent, and reliable strategies are survivable. In this case, the strategic level modeling up to this point is sufficient for the whole PHM system modeling, and there is no need for the next level  operational level modeling  When there are UUUR events in a PHM system, the 


When there are UUUR events in a PHM system, the inability to determine the occurrence probabilities of UUUR events makes the operational level modeling necessary Then the principle of hedging must be utilized to deal with the "hanging" uncertainty from UUUR events. In this case reliability strategies are not necessarily survivable strategies At the operational level modeling, a duo of survivability metrics, expected survivability \(ES survivability \(TS the survivable strategies \(ESS level are promptly implemented based on the decisionmaking rules specified with the duo of survivability metrics then the PHM system should be able to endure the consequences of potentially catastrophic UUUR events. Of course, to endure such catastrophic events, the cost may be prohibitively high, but the PHM system will, at least, warn decision-makers for the potentially huge costs.  It might be cheap to just let it fail  Figure 3 also shows several other modules, such as security safety, application systems \(such as Automatic Logistics CBM+, RCM, Life cycle cost management, Real-time warning and alert systems architectures, but we do not discuss in this paper. Generally the new architecture should be fully compatible with existing ones in incorporating these additional modules. One point we stressed is that PHM system can be an ideal place to enforce security policies. Enforcing security policies can be mandatory for PHM systems that demand high security and safety such as weapon systems or nuclear plant facilities.  This is because maintenance, even without human-initiated security breaches, can break the security policies if the maintenance is not planned and performed properly  In perspective, although I did not discuss software issues in this paper, the introduced approaches and models should provide sufficient tools for modeling software reliability and survivability with some additional extension. Given the critical importance of software to modern PHM systems, we present the following discussion on the potential extension to software domain. Specifically, two points should be noted: \(1 architecture to software should be a metric which can 16 replace the time notion in software reliability; I suggest that the Kolmogorov complexity \(e.g., Li and Vitanyi 1997 be a promising candidate \(Ma 2008a change is because software does not wear and calendar time for software reliability usually does not make much sense 2 software reliability modeling.  Extending to general survivability analysis is not a problem either. In this article I implicitly assume that reliability and survivability are positively correlated, or reliability is the foundation of survivability. This positive correlation does not have to be the case. A simplified example that illustrates this point is the 'limit order' in online stock trading, in which limit order can be used in either direction: that stock price is rising or falling.  The solution to allow negative or uncorrelated relationships between reliability and survivability are very straightforward, and the solutions are already identified in previous discussions. Specifically, multiple G-functions and multi-stage G-functions by Vincent and Brown \(2005 very feasible solution, because lifetime, reliability and survivability may simply be represented with multiple Gfunctions. Another potential solution is the accommodation of the potential conflicts between reliability and survivability with multi-objective GA algorithms, which I previously suggested to be used as updating algorithms in the optimization of evolutionary games  


 The integration of dynamic hybrid fault models with evolutionary game modeling allows one to incorporate more realistic and detailed failure \(or survival individual players in an evolutionary game. This is because dynamic hybrid fault models are supported by survival analysis modeling, e.g., time and covariate dependent hazard or survivor functions for individual players. If necessary, more complex survival analysis modeling including competing risks analysis and multivariate survival analysis, can be introduced.  Therefore, any field to which evolutionary game theory is applicable may benefit from the increased flexibility in modeling individual players.  Two particularly interesting fields are system biology and ecological modeling.  In the former field, dynamic hybrid fault models may find important applications in the study of biological networks \(such as gene, molecular, and cell networks 2008g conjecture that explains the redundancy in the universal genetic code with Byzantine general algorithm. In addition they conducted a comparative analysis of bio-robustness with engineering fault tolerance, for example, the strong similarity between network survivability and ecological stability \(Ma &amp; Krings 2008g survivability analysis can be applied for the study of survivals or extinctions of biological species under global climate changes \(Ma 2008b  In this paper, I have to ignore much of the details related to the implementation issues to present the overall architecture and major approaches clearly and concisely. To deal with the potential devils in the implementation details, a well funded research and development team is necessary to take advantages of the ideas presented here. On the positive side I do see the great potential to build an enterprise PHM software product if there is sufficient resource to complete the implementation. Given the enormous complexity associated with the PHM practice in modern engineering fields, it is nearly impossible to realize or even demonstrate the benefits of the architecture without the software implementation. The critical importance of PHM to mission critical engineering fields such as aerospace engineering, in turn, dictates the great value of such kind software product  6.3. Beyond PHM  Finally, I would like to raise two questions that may be interested in by researchers and engineers beyond PHM community. The first question is: what can PHM offer to other engineering disciplines? The second question is: what kinds of engineering fields benefit most from PHM? Here, I use the term PHM with the definition proposed by IEEE which is quoted in the introduction section of the paper  As to the first question, I suggest software engineering and survivability analysis are two fields where PHM can play significant roles. With software engineering, I refer to applying PHM principles and approaches for dealing with software reliability, quality assurance, and even software process management, rather than building PHM software mentioned in the previous subsection. For survivability analysis, borrowing the procedures and practices of PHM should be particularly helpful for expanding its role beyond its originating domain \(network systems that control critical national infrastructures is a strong advocate for the expansion of survivability analysis to PHM. Therefore, the interaction between PHM and survivability analysis should be bidirectional. Indeed, I see the close relationships between PHM, software engineering, and survivability as well-justified because they all share some critical issues including reliability survivability, security, and dependability  


 The answer to the second question is much more elusive and I cannot present a full answer without comparative analysis of several engineering fields where PHM has been actively practiced. Of course, it is obvious that fields which demand mission critical reliability and dependability also demand better PHM solutions. One additional observation I would like to make is that PHM seems to play more crucial roles for engineering practices that depend on the systematic records of 'historical' data, such as reliability data in airplane engine manufacturing, rather than on the information from ad hoc events.  This may explain the critical importance of PHM in aerospace engineering particularly in commercial airplane design and manufacturing.  For example, comparing the tasks to design and build a space shuttle vs. to design and manufacture commercial jumbo jets, PHM should be more critical in the latter task  17    Figure 2. States of a monitoring sensor node and its failure modes \(after Ma &amp; Krings 2008e     Figure 3. Core Modules and their Relationships of the Life System Inspired PHM Architecture    REFERENCES  Adamides, E. D., Y. A. Stamboulis, A. G. Varelis. 2004 Model-Based Assessment of Military Aircraft Engine Maintenance Systems Model-Based Assessment of Military Aircraft Engine Maintenance Systems. Journal of the Operational Research Society, Vol. 55, No. 9:957-967  Anderson, R. 2001. Security Engineering. Wiley  Anderson, R. 2008. Security Engineering. 2nd ed. Wiley  Bird, J. W., Hess, A. 2007.   Propulsion System Prognostics R&amp;D Through the Technical Cooperation Program Aerospace Conference, 2007 IEEE, 3-10 March 2007, 8pp  Bock, J. R., Brotherton, T., W., Gass, D. 2005. Ontogenetic reasoning system for autonomic logistics. Aerospace Conference, 2005 IEEE 5-12 March 2005.Digital Object Identifier 10.1109/AERO.2005.1559677  Brotherton, T., P. Grabill, D. Wroblewski, R. Friend, B Sotomayer, and J. Berry. 2002. A Testbed for Data Fusion for Engine Diagnostics and Prognostics. Proceedings of the 2002 IEEE Aerospace Conference  Brotherton, T.; Grabill, P.; Friend, R.; Sotomayer, B.; Berry J. 2003. A testbed for data fusion for helicopter diagnostics and prognostics. Aerospace Conference, 2003. Proceedings 2003 IEEE  Brown, E. R., N. N. McCollom, E-E. Moore, A. Hess. 2007 Prognostics and Health Management A Data-Driven Approach to Supporting the F-35 Lightning II. 2007 IEEE AeroSpace Conference  Byington, C.S.; Watson, M.J.; Bharadwaj, S.P. 2008 Automated Health Management for Gas Turbine Engine Accessory System Components. Aerospace Conference 2008 IEEE, DOI:10.1109/AERO.2008.4526610 


2008 IEEE, DOI:10.1109/AERO.2008.4526610 Environment Covariates &amp; Spatial Frailty Applications: AL; Life Cycle Mgmt; Real-Time Alerts CBM+, RCM, TLCSM; Secret Sharing and Shared Control 18 Chen, Y. Q., S. Cheng. 2005. Semi-parametric regression analysis of mean residual life with censored survival data Biometrika \(2005  29  Commenges, D. 1999. Multi-state models in Epidemiology Lifetime Data Analysis. 5:315-327  Cook, J. 2004. Contrasting Approaches to the Validation of Helicopter HUMS  A Military User  s Perspective Aerospace Conference, 2004 IEEE  Cook, J. 2007. Reducing Military Helicopter Maintenance Through Prognostics. Aerospace Conference, 2007 IEEE Digital Object Identifier 10.1109/AERO.2007.352830  Cox, D. R. 1972. Regression models and life tables.  J. R Stat. Soc. Ser. B. 34:184-220  Crowder, M. J.  2001. Classical Competing Risks. Chapman amp; Hall. 200pp  David, H. A. &amp; M. L. Moeschberger. 1978. The theory of competing risks. Macmillan Publishing, 103pp  Ellison, E., L. Linger, and M. Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013  Hanski, I. 1999. Metapopulation Ecology. Oxford University Press  Hallam, T. G. and S. A. Levin. 1986. Mathematical Ecology. Biomathematics. Volume 17. Springer. 457pp  Hess, A., Fila, L. 2002.  The Joint Strike Fighter \(JSF concept: Potential impact on aging aircraft problems Aerospace Conference Proceedings, 2002. IEEE. Digital Object Identifier: 10.1109/AERO.2002.1036144  Hess, A., Calvello, G., T. Dabney. 2004. PHM a Key Enabler for the JSF Autonomic Logistics Support Concept. Aerospace Conference Proceedings, 2004. IEEE  Hofbauer, J. and K. Sigmund. 1998. Evolutionary Games and Population Dynamics. Cambridge University Press 323pp  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Huzurbazar, A. V. 2006. Flow-graph model for multi-state time-to-event data. Wiley InterScience  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis. Springer. 481pp  Kacprzynski, G. J., Roemer, M. J., Hess, A. J. 2002. Health management system design: Development, simulation and cost/benefit optimization. IEEE Aerospace Conference Proceedings, 2002. DOI:10.1109/AERO.2002.1036148  Kalbfleisch, J. D., and R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data. Wiley-InterScience, 2nd ed  Kalgren, P. W., Byington, C. S.   Roemer, M. J.  2006 Defining PHM, A Lexical Evolution of Maintenance and Logistics. Systems Readiness Technology Conference 


Logistics. Systems Readiness Technology Conference IEEE. DOI: 10.1109/AUTEST.2006.283685  Keller, K.; Baldwin, A.; Ofsthun, S.; Swearingen, K.; Vian J.; Wilmering, T.; Williams, Z. 2007. Health Management Engineering Environment and Open Integration Platform Aerospace Conference, 2007 IEEE, Digital Object Identifier 10.1109/AERO.2007.352919  Keller, K.; Sheahan, J.; Roach, J.; Casey, L.; Davis, G Flynn, F.; Perkinson, J.; Prestero, M. 2008. Power Conversion Prognostic Controller Implementation for Aeronautical Motor Drives. Aerospace Conference, 2008 IEEE. DOI:10.1109/AERO.2008.4526630  Klein, J. P. and M. L. Moeschberger. 2003. Survival analysis techniques for censored and truncated data Springer  Kingsland, S. E. 1995. Modeling Nature: Episodes in the History of Population Ecology. 2nd ed., University of Chicago Press, 315pp  Kot, M. 2001. Elements of Mathematical Ecology Cambridge University Press. 453pp  Krings, A. W. and Z. S. Ma. 2006. Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks Military Communications Conference, 23-25 October, 7 pages, 2006  Lamport, L., R. Shostak and M. Pease. 1982. The Byzantine Generals Problem. ACM Transactions on Programming Languages and Systems, 4\(3  Lawless, J. F. 2003. Statistical models and methods for lifetime data. John Wiley &amp; Sons. 2nd ed  Line, J. K., Iyer, A. 2007. Electronic Prognostics Through Advanced Modeling Techniques. Aerospace Conference 2007 IEEE. DOI:10.1109/AERO.2007.352906  Lisnianski, A., Levitin, G. 2003. Multi-State System Reliability: Assessment, Optimization and Applications World Scientific  Liu, Y., and K. S. Trivedi. 2006. Survivability Quantification: The Analytical Modeling Approach, Int. J of Performability Engineering, Vol. 2, No 1, pp. 29-44  19 Luchinsky, D.G.; Osipov, V.V.; Smelyanskiy, V.N Timucin, D.A.; Uckun, S. 2008. Model Based IVHM System for the Solid Rocket Booster. Aerospace Conference, 2008 IEEE.DOI:10.1109/AERO.2008.4526644  Lynch, N. 1997. Distributed Algorithms. Morgan Kaufmann Press  Ma, Z. S. 1997. Demography and survival analysis of Russian wheat aphid. Ph.D. dissertation, Univ. of Idaho 306pp  Ma, Z. S. 2008a. New Approaches to Reliability and Survivability with Survival Analysis, Dynamic Hybrid Fault Models, and Evolutionary  Game Theory. Ph.D. dissertation Univ. of Idaho. 177pp  Ma, Z. S. 2008b. Survivability Analysis of Biological Species under Global Climate Changes: A New Distributed and Agent-based Simulation Architecture with Survival Analysis and Evolutionary Game Theory. The Sixth 


International Conference on Ecological Informatics. Dec 25, 2008. Cancun, Mexico  Ma, Z. S. and E. J. Bechinski. 2008. A Survival-Analysis based  Simulation Model for Russian Wheat Aphid Population Dynamics. Ecological Modeling, 216\(2 332  Ma, Z. S. and A. W. Krings. 2008a.  Survival Analysis Approach to Reliability Analysis and Prognostics and Health Management \(PHM  AIAA AeroSpace Conference, March 1-8, 2008, Big Sky, MT, 20pp  Ma, Z. S. and A. W. Krings. 2008b. Competing Risks Analysis of Reliability, Survivability, and Prognostics and Health Management \(PHM  AIAA AeroSpace Conference, March 1-8, 2008.  Big Sky, MT. 20pp  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(I Dependence Modeling", Proc. IEEE  AIAA AeroSpace Conference, March 1-8, 2008, Big Sky, MT. 21pp  Ma, Z. S. and A. W. Krings., R. E. Hiromoto. 2008d Multivariate Survival Analysis \(II State Models in Biomedicine and Engineering Reliability IEEE International Conference of Biomedical Engineering and Informatics, BMEI 2008.  6 Pages  Ma, Z. S. and A. W. Krings. 2008e. Dynamic Hybrid Fault Models and their Applications to Wireless Sensor Networks WSNs Modeling, Analysis and Simulation of Wireless and Mobile Systems. \(ACM MSWiM 2008 Vancouver, Canada  Ma, Z. S. &amp; A. W. Krings. 2008f. Dynamic Populations in Genetic Algorithms. SIGAPP, the 23rd Annual ACM Symposium on Applied Computing, Ceara, Brazil, March 16-20, 2008. 5 Pages  Ma, Z. S. &amp; A. W. Krings. 2008g. Bio-Robustness and Fault Tolerance: A New Perspective on Reliable, Survivable and Evolvable Network Systems, Proc. IEEE  AIAA AeroSpace Conference, March 1-8, Big Sky, MT, 2008. 20 Pages  Ma, Z. S.  and A. W. Krings. 2009. Insect Sensory Systems Inspired Computing and Communications.  Ad Hoc Networks 7\(4  MacConnell, J.H. 2008. Structural Health Management and Structural Design: An Unbridgeable Gap? 2008 IEEE Aerospace Conference, DOI:10.1109/AERO.2008.4526613  MacConnell, J.H. 2007. ISHM &amp; Design: A review of the benefits of the ideal ISHM system. Aerospace Conference 2007 IEEE. DOI:10.1109/AERO.2007.352834  Marshall A. W., I. Olkin. 1967. A Multivariate Exponential Distribution. Journal of the American Statistical Association, 62\(317 Mar., 1967  Martinussen, T. and T. H. Scheike. 2006. Dynamic Regression Models for Survival Data. Springer. 466pp  Mazzuchi, T. A., R. Soyer., and R. V. Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Millar, R.C., Mazzuchi, T.A. &amp; Sarkani, S., 2007. A Survey of Advanced Methods for Analysis and Modeling of 


of Advanced Methods for Analysis and Modeling of Propulsion System", GT2007-27218, ASME Turbo Expo 2007, May 14-17, Montreal, Canada  Millar, Richard C., "Non-parametric Analysis of a Complex Propulsion System Data Base", Ph.D. Dissertation, George Washington University, June 2007  Millar, R. C. 2007. A Systems Engineering Approach to PHM for Military Aircraft Propulsion Systems. Aerospace Conference, 2007 IEEE. DOI:10.1109/AERO.2007.352840  Millar, R. C. 2008.  The Role of Reliability Data Bases in Deploying CBM+, RCM and PHM with TLCSM Aerospace Conference, 2008 IEEE, 1-8 March 2008. Digital Object Identifier: 10.1109/AERO.2008.4526633  Nowak, M. 2006. Evolutionary Dynamics: Exploring the Equations of Life. Harvard University Press. 363pp  Oakes, D. &amp; Dasu, T. 1990. A note on residual life Biometrika 77, 409  10  Pintilie, M. 2006. Competing Risks: A Practical Perspective.  Wiley. 224pp  20 Smith, M. J., C. S. Byington. 2006. Layered Classification for Improved Diagnostic Isolation in Drivetrain Components. 2006 IEEE AeroSpace Conference  Therneau, T. and P. Grambsch. 2000. Modeling Survival Data: Extending the Cox Model. Springer  Vincent, T. L. and J. L. Brown. 2005. Evolutionary Game Theory, Natural Selection and Darwinian Dynamics Cambridge University Press. 382pp  Wang. J., T. Yu, W. Wang. 2008. Research on Prognostic Health Management \(PHM on Flight Data. 2008 Int. Conf. on Condition Monitoring and Diagnosis, Beijing, China, April 21-24, 2008. 5pp  Zhang, S., R. Kang, X. He, and M. G. Pecht. 2008. China  s Efforts in Prognostics and Health Management. IEEE Trans. on Components and Packaging Technologies 31\(2             BIOGRAPHY  Zhanshan \(Sam scientist and earned the terminal degrees in both fields in 1997 and 2008, respectively. He has published more than 60 peer-refereed journal and conference papers, among which approximately 40 are journal papers and more than a third are in computer science.  Prior to his recent return to academia, he worked as senior network/software engineers in semiconductor and software industry. His current research interests include: reliability, dependability and fault tolerance of distributed and software systems behavioral and cognitive ecology inspired pervasive and 


behavioral and cognitive ecology inspired pervasive and resilient computing; evolutionary &amp; rendezvous search games; evolutionary computation &amp; machine learning bioinformatics &amp; ecoinformatics                 pre></body></html 


