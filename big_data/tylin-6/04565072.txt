254 255 Abstract 227As digital resources increasingly growing and the economic benefit of digital intellectual property rights being increasingly important, people has been increasingly emphasis on information security issues brought by the data remnants in storage devices. They try their best to prevent the potential risks In this paper we survey comprehensively related technologies standards and trends of erasure discuss the shortcomings of techniques on adding secure deletion to file systems and on cryptographic to prevent deleted data from being accessible. We focus on secure deletion mechanism in the NTFS file system combining the asynchronous I/O multi-threading technology Finally we present a novel data erasing algorithm, Quick Erase which not only greatly exceeds the speed of the existing international data erasing algorithms but also can be easily combined with a variety of standard algorithms to form various high-speed mixed erase algorithms It can be used to erase file data and metadata to ensure the security and reliability of data erasing.  The test result of Quick Erase indicates that the erasure speed of one big file has reached 10-12 s/100MB, faster than that of existing secure deletion tools 60-80s/100MB The algorithm 
has good application prospects for practical applications Index Terms 227Secure Deletion Multi-thread asynchronous I/O Data wiping I I NTRODUCTION n modern networks and information storage systems data recording systems have become essential equipment, which carry a large number of important or sensitive data, such as confidential business information or national secrets even personal privacy Once these data recording system disappeared or data stolen the consequences would be disastrous if there is no effective high-tech means to removal or destroy these data Currently a number of important research institutions governments and enterprises are undertaking researches experiments and product sales in secure deletion area 1  3   5    T h i s p a p e r  h a s  a  c o m p r e h e n s i v e  i n v e s t i g a t i o n  i n  t h e  field of disk wiping technology and comparison the erasure effectiveness of various data erasure methods Finally we present a novel algorithm that can effectively improve Manuscript received March 28, 2008. This work was supported in part by the National Science Foundation of China the project code 70471064 and Research Foundation of National Innovation Base in Philosophy and Social 
Science of 985 II, the project No. 107008200400024 Guomeng Wang is with School of Management and Economics Beijing Institute of Technology Beijing 100081 P.R.China phone 8610\13401072820; e-mail: guomeng.wang@yahoo.com.cn Yanping Zhao is with School of Management and Economics Beijing Institute of Technology, Beijing 100081, P.R.China  \( corresponding author to provide phone: \(+8610\3911558038; e-mail: zhaoyp@bit.edu.cn efficiency of all existing data erasure methods such as Gutmann 35 times data erasure algorithm The paper is organized as follows: section II presents related progress in data erasure; section III presents the Limitation of techniques on adding secure deletion to file systems and on cryptographic; section IV a fast data erasing algorithm; section V, test results of secure deletion algorithms; the last section, a summary II RELATED PROGRESS IN DATA ERASURE EREA Data Erasing is a kind of technology that using software\222s or hardware\222s means to erase data on storage media including hard drives and flash disk\, so as to achieve the purpose that 
ensures data not be recovered. Nowadays, many countries are studying secure deletion algorithms and detection mechanisms Therefore this paper surveys comprehensively researches on this issues and divided existing erasing technology into five categories 1\ Cryptographic technique is a good technique in protection of Dynamic data. MS\222s Encryption File System \(EFS\ [6  c a n  encrypt files, directories, or whole file systems with individual keys, but the key management in encryption system is a very crucial problem to resolve, because once the key was sniffed, it could be used to decrypt both deleted and undeleted data 2 Changing operating systems so that files are actually overwritten when they are unlinked from the system is another solution [8    3\ Physical destruction is the best and the safest way to erase data Generally include degaussing chemical corrosion shredding, burning 4\ Using a novel method, Dead on Demand Digital Shredder to secure erase data NIST Special Publication 800-88 states that the method is quickly earning market acceptance, as well as receiving growing public awareness [4  5 Data erasing Based on the software model is also often called secure deletion tools there are usually two erasure modes, one is to use disk I/O mode to overwrite, and the other is 
to use the operating system file drive interface to overwrite the disk. Here are some typical methods Peter Gutmann presented a deletion algorithm, now is widely used in the world, in his paper "Secure Deletion of Data from Magnetic and Solid-State Memory" in 1996 [2   c r e a t e d  a  n e w  field of data erasing based on software and formed the industry-recognized Gutmann standards We propose our Quick Erase erasing system based on Quick Erase algorithm here The survey results of famous erasing tools[8  9   1 0   a r e  i n  t a b l e  1   A Fast Algorithm for Data Erasure Guomeng Wang and Yanping Zhao I 
227Cyberactivism refers to the use of the Internet to advocate vigorous or intentional actions to bring about social or political change Cyberactivism analysis aims to improve the understanding of cyber activists and their online communities. In this paper, we present a case study of online Free Tibet activities For web site analysis, we use the inlink and outlink information of five selected seed URLs to construct the network of Free Tibet web sites. The network shows the close relationships between our five seed sites Centrality measures reveal that tibet.org is probably an information hub site in the network. Further content analysis tells us that common hub site words are most popular in tibet.org whereas dalailama.com focuses mostly on religious 
Abstract 
words For forum analysis descriptive statistics such as the number of posts each month and the post distribution of forum users illustrate that the two large forums FreeTibetAndYou and RFAnews-Tibbs have experienced significant reduction in activities in recent years and that a small percentage of their users contribute the majority of posts Important phrases of several long threads and active forum users are identified by using mutual information and TF-IDF scores Such topical analyses help us understand the topics discussed in the forums and the ideas and interest of those forum users Finally social network analyses of the forum users are conducted to reflect their interactions and the social structure of their online communities 
I I NTRODUCTION CTIVISM often refers to vigorous or intentional actions that aim to bring about social or political change The rapid development of the Internet makes it an ideal source for information and propaganda dissemination 2   3    C y b e r  activists use web sites forums and other types of computer media to build relationships with the public voice their opinions and spread their propaganda 4   5    C o n s e q u e n t l y   Cyberactivism which refers to activism on the Internet has emerged as a new and important social phenomenon A comprehensive and systematic framework for analysis of Cyberactivism needs to be developed for researchers, analysts and related government agents to get a better understanding of 
Index Terms 
227 Cyberactivism Forum Analysis Free Tibet Web Site Analysis 
cyber activists and their online communities For example it has been observed that web sites of related activism groups are linked to each other through hyperlinks creating their own community network 6   A  g o o d  r e s e a r c h  f r a m e w o r k  w o u l d  include an effective method to identify such networks and the roles of the web sites in the networks In forums social networks of forum users, representative topics and phrases, and active users are useful information that can help understand member interactions This paper presents a case study of online Free Tibet activities and is an initial attempt toward a framework to analyze Cyberactivism. Two popular Internet media, web sites and forums of Free Tibet activities are identified collected and analyzed. We aim to shed light on the following research questions 1 What automated methods can be used to analyze Cyberactivism 2 What knowledge and insights can we gain through social network and topical analysis of Cyberactivism II F REE T IBET Tibet generally refers to the Tibet Autonomous Region a province-level entity governed by the People's Republic of China \(PRC\. Religion is extremely important to the Tibetans and Tibet is the traditional center of Tibetan Buddhism Panchen Lamas and Dalai Lamas are two well-known spiritual leaders of Tibetans Tibet was first included into China\222s territory in the 1300s At the beginning of the 20th century Tibet and Mongolia signed a treaty proclaiming mutual recognition and their independence from the Republic of China  When the China 
Civil War ended in 1949, Tibet became a part of the PRC. In 1959, PRC armies thwarted the Tibetan resistance movement and Tenzin Gyatso, the 14th Dalai Lama, fled to India The Central Tibetan Administration \(CTA\also referred to as the Tibetan Government in Exile TGE is headed by the 14th Dalai Lama and claims to be the rightful and legitimate government of Tibet  Free Tibet activities refer to activities that aim to support the independence of Tibet. Most Free Tibet activities are guided by the CTA III R ESEARCH D ESIGN Free Tibet related web sites and forums are the focuses of this case study Such web sites and forums are created and maintained by activists supporters and sympathizers in a variety of languages, including English, Chinese, and Tibetan In this study, we propose different methodologies for analysis of each medium 
Since there are many Free Tibet web sites available, the first step is to identify which ones are important. Fig. 1 shows our identification process Analysis of Cyberactivism A Case Study of Online Free Tibet Activities Tianjun Fu and Hsinchun Chen, Artificial Intelligence Lab, University of Arizona A 
A Web Site Analysis 1 Identifying and Collecting Important Web Sites 


2 5 4 254 2 5 5 255 Table.1: Test Results III T HE L IMITATION OF ADDING SECURE DELETION TO FILE SYSTEM FUNCTION AND CRYPTOGRAPHIC TECHNIQUES Using the way of adding secure deletion to file system [7  o r  cryptographic techniques to erase sensitive data is very efficient and useful in many aspects but also has some limitations, here are shortcomings in A and B. So developing secure deletion tools still has a large demand especially in record devices reusable cases A Limitations of Adding secure deletion to file systems The Ext2, Ext3, and Reiserfs file systems support a special per-file attribute to mark files or directories that contain sensitive information and require secure deletion Unfortunately the attribute is different thus difficultly maintained for different OS versions Bauer 2  u s e d  s p e c i a l  per-file attribute setting method and modified unlink  and truncate \(\ operations to Ext2 to erase the data and meta-data Joukov and Zadok 11  d e v e l o p e d  a n  e x t e n s i o n  o f  t h e  f i l e  system which is good to incorporate other file system to implement synchronous or asynchronous data erasure work These approaches have many advantages but still have some shortcomings 1 Need to modify and increase the source code of file system 2\ Data erasing may not completely overwrite 3\ Synchronous and asynchronous data erasing B Cryptographic techniques Cryptographic erasing 5  a p p r o ach m a inly uses either an encrypted file system or encrypted tools The idea is that destroy the encryption key will make encoded data not be accessible without overwriting The advantage of cryptographic techniques is that it has quick erasing speed, but there are a number of deficiencies as follows 1\ Difficulty in key management 2\ Plaintext files may remain visible 3\ Affect system performance IV A F AST DATA ERASING ALGORITHM MODEL A Asynchronous I/O algorithm in Data Erasing system The main drawback of secure deletion tools is that speed is much slow, because it needs to use data to cover the disk sector when you do the overwriting work, so in order to improve the speed and efficiency of erasure, we introduce multi-threading technology base on asynchronous I/O in data erasing system to erase data Our Data Erasing system use the multi-threading technology base on asynchronous I O to erase data for improving speed, because of data erasing system need frequent to write data to disk when it is overwriting threads will be blocked when it is writing data to disk if the data erasing system use synchronous single-threaded to erase data programming instructions can not be executed until the data have been completely written to disk B Core algorithm of data overwrite Although disk drives are often thought of as \221random access\222 devices the fact is that mass storage access is usually more efficient when done in a sequential fashion. Sequential access on a disk drive is relatively efficient because the OS can move the read/write head one track at a time assuming the file appears in sequential blocks on the disk\. This is much faster than accessing one block on the disk, moving the read/ write head to some other track, accessing another block, moving the head again, and so on.  So our data erasing system attempt to read or write large blocks of data or whole disk blocks on each file access rather than reading or writing small amounts more frequently. Reasons as following 1\OS external file system calls are not fast, so if you want the application run twice as fast, you can make half as many calls by reading or writing twice as much data on each access 2\ The OS must read or write whole disk blocks, it cannot transfer partial blocks between the disk and memory.  It bring two advantages first slack space can be overwritten for improving security Second it follows the rules that the OS must read/write whole disk block so improving data erasure speed 


2 5 6 256 3\  We can\222t control the number of the OS file system calls because the number of files is choired by the user. But we can control read/write mode of the file system calls. Windows file system calls read/write approach can be determined by set up read/write flag SDelete software use FILE_FLAG_WRITE_THROUGH flag to make system write data to the disk through fast disk cache V T EST R ESULTS OF SECURE DELETION TOOLS We evaluate our system on a P4 3.2GHz machine with 1GB of memory The disk is a 30GB 7200 RPM Western Digital Caviar IDE and formatted with NTFS. The operation system is windows XP sp2 The tested software are Eraser Sdelete Quick Erase. The tested files are set as two types, one is single file that file size increased by the order is 4 MB, 16MB, 32MB, 50MB, 100MB 1GB, the other multi-files in directories that make total file size 100MB The 100MB directory includes 24 subdirectory and 440 files. Every file size is between 1K and 10MB Fig. 1 three-passes overwrite We can see from above Fig. 1 that the distance between the two lines has linear growth with the file size increasing.  If the file has the same size our Quick Erase\222s erasure time is the fastest, Sdelete is the second, and Eraser is the last. For example if the single-file size is 100 MB, our Quick Erase\222s erasure time is 10.12s, the Eraser is 18.32s, so our erasure time is 8s faster than the latter. If the single-file size is 1GB, our erasure time is 96.62s, the Eraser is 172.45s, so our erasure time is 78s faster than the latter Thus we conclude that our Quick Erase\222s erasure time is faster than Eraser twice, and is 33 percents faster than Sdelete VI C ONCLUSIONS This paper comprehensively overviews technologies standards and trends related to data erasure discusses the shortcomings of techniques on adding secure deletion to file system and on cryptographic to prevent deleted data from being accessible Focusing on secure deletion mechanism in the NTFS file system and combining the asynchronous I/O multi-threading technology we develop Quick Erase data erasure software, to erase both the file data and metadata Comparison with the best international secure deletion tools our Quick Erase has much improved the erasure speed The principle of the algorithm shows ability to combine with other effective data erasure methods to form different levels of erasure algorithms that can be applicable to variety of security situations The technology we provide can be used for both good and bad purposes, for good it benefits to the protection of people\222s privacy, of confidential information; for bad is that it would help hacker hide their activities by erasing evidence of criminals and allow them to evade the law more easily Our future work is to extend our Quick Erase algorithm to other file systems and other file types such as complex compression, encryption and sparse files A CKNOWLEDGMENT This research is supported by the NSF of China, the project code 70471064 and the Foundation for National Innovation Base in Philosophy and Social Science, \223985 Project II\224 R EFERENCES 1 N  J o u k o v   E  Z ad o k   A d d i n g  S e c u r e D e l e t i o n  t o  Y o u r  F a v o r i t e  F i l e  System. In Proceedings of the third international IEEE Security In Storage Workshop \(SISW 2005\, San Francisco, CA, December 2005 2 P   G u t m a n n   S e c u r e D e l e t i o n  o f  D a t a  f r o m  M a g n e t i c  a n d  S o l i d S t a t e  Memory, Proc. Sixth Usenix Security Symposium, 1996 3 R y k  E d e l s t e i n   T h e  L im i t a t i o n s  o f  S o f t w a r e B a s e d  H a r d  D r i v e  Sanitization. Converge Net Inc, September 2007 4 G   H u g h e s   M R R  P r o t o c o l s  f o r  D i s k  D r i v e  S e c u r e E r a s e   T e c h n i c a l  r ep o r t   Center for Magnetic Recording Research, University of California, San Diego, October 2004. Available http://cmrr.ucsd.edu/Hughes/CmrrSecureEraseProtocols.pdf 5 S i m s o n  L   G a r f i n k e l   C o m p l e t e  d e l e t e  v s   t i m e  m a c h i n e  c o m p u t i n g   A C M SIGOPS Operating Systems Review, Jan.2007, pp.42-44 6 M i c r o s o f t  T e c h N e t   A v a i l a b l e   h t t p    w w w  m ic r o s o f t  c o m c h i n a  t e c h n e t    security/sgk/protect_data_EFS.mspx 7 S i m s o n  L   G a r f i n k e l   D a v i d  J   M al a n   O n e  B i g  F i l e  I s  N o t  E n o u g h   A  Critical Evaluation of the Dominant Free-Space Sanitization Technique ,ACM 2006 8 M ar k  R u s s i n o v i c h   D e l e t e   2 0 0 3   9 Eraser Available http://sourcef orge.net/projects/eraser 10 R o b i n  H o o d  S o f t w a r e L td   A v a i l a b l e   http://www.evidence-eliminator.com 11 S   B a u e r  N   B   P r i y a n t h a   S e c u r e D a t a  D e l e t i o n  f o r  L in u x  F i l e  S y s t e m s   USENIX Association In Proceedings of the 10th Usenix Security Symposium, Washington, DC, August 2001, pp.153 164 Guomeng Wang Master student of the School of Management and Economics Beijing Institute of Technology. Research on the Methods of key technology of data erasing in Data security Yanping Zhao  Professor the chief director for the research direction on operation research and control theory in the School of Management and Economics, Beijing Institute of Technology, and Consultant and experts of the National Professional Identification Committee for Ecommerce, and expert of the China Agenda 21 and UNDP and has presided the NSF of China titled Analysis and Monitoring for Network Text Content Security \(2004-2007\, and as a major member of the Key NSF of China 2000-2003 And important project for Beijing Education Committee to undergraduate network for applying jobs. She has won twice awards of Beijing Science and Technology Progress, and has published many papers in the influenced academic journals and conferences such as Management Science of China, Computer Science, and the first IEEE International Conference on Service Systems and Service Management, WSEAS Transactions On Information Science And Applications the Pan-Pacific Business Conference and so on, and cited by SCI, EI or ISTPs 


measures the relative pose The the sensor fault recovery rate characterizes mean time the sensor is off-line because of a fault The main sources of intermittent faults for optics-based sensors are environmental factors such as glint or lighting The lighting conditions depend on the sun angle For low Earth orbit with a 120 min orbital period a 100 second time interval corresponds to a 5 degree change in the sun angle This should provide a sufficient lighting environment change for the glint to go away The 100 second average persistency of the fault corresponds to the sensor fault recovery rate in Table 1 oAVGS SHORT RANGE DATA 1 15 DATA POINT NUMBER Figure 3 Sensor test data for determining outliers Sensor model discussion Let us discuss the sensor failure model in more detail An extensive program of testing several types of relative navigation same time on the FDIR logic to handle the outliers The trades between the two described approaches more reliance are described in 14 In this work we used the sensor test data to build a more detailed characterization of the faults In the test the target is moved with respect to the sensor which are recorded along with the ground truth data obtained with sensor failure rate in Table 1 is ascertained by having sufficiently relaxed sensor data the residuals observation noise are assumed zero-mean normally distributed One we can estimate coordinate residuals and three attitude angle residuals The low sensors data must be detected and removed The drawback of the approach is that it has accuracy of the navigation filter At the AVGS SHORT RANGE DATA _ _ _-0 20 40 60 80 MAHALANOBIS RADIUS 100 120 Figure 4 Sensor outlier rate vs the accuracy threshold solid line and cumulative X6 distribution dashed line X 10 a higher rate of intermittent sensor faults Having zero mean In practice when tuning sensor accuracy specifications assume that the assume a normal distribution model for outlier detection There are two possible approaches to handling this discrepancy 1 Assume that the covariance is larger than the empirical such that all the data points are no outliers but the an independent highaccuracy measurement system The differences between the two measurements residuals an empirical probability of the outlier Figsensor noise covariance which influences the FDIR-related performance probabilities discussed in the next section m m m 0 0 00 D 0 100 10o2 10-4 sensor failure rate in Table 1 Another model parameter is the same time engineering approaches to FDIR algorithm design sensor could be characterized using the empirical covariance matrix 3 N Q N-E ri j=l where ri C 6 is the j-th residual vector and N is the number of the data points in the test set We can then characterize the residual sensor output data sensor error specifications Tightening these specifications would lead to sensor is not biased and the residuals have are bounded by the X6 distribution envelope Current design follows this approach There sensors was carried at NASA The tests can be then analyzed At each point in time the residual vector consists of three linear an FDIR logic and FT/RM architecture in place allows trading a higher sensors fault rate against the improvement in sensor accuracy specs This paper analyses the tradeoffs with respect to mission risks The residual data for the a navigation filter using the relative a set of N  256 412 residual data vectors obtained in the tests The Mahalanobis distances 4 are shown in Figure 3 By introducing a threshold dashed line and counting a fraction of the data points above the threshold ure 4 shows the results obtained for different thresholds solid line together with the cumulative probability density for X6 dashed line The empirical cumulative probability distribution solid line in Figure 4 has heavy tail It decays slower than the cumulative probability distribution for normally distributed residuals dashed line At the are relaxed 2 Assume the empirical covariance obtained from the data This covariance is by a factor of 20 smaller compared to the first approach The smaller covariance improves the more outliers faults in the can be evaluated by using the developed PRA framework One model parameter that is impacted is vectors through the squared Mahalanobis distances d rTQ lrj 4 For the normally distributed residuals d2 should follow X6 distribution where 6 is the number of degrees of freedom We processed 


sensor is initially in No Fault state 1 Hence the probability of being in state 4 is zero initially and subsequently Table 2 shows the transition rates qij assumed for the model in Figure 5 The transitions probabilities related to hardware first three lines in Table 2 were defined based on literature and expert evaluations The rate of permanent avionics faila system failure have probabilities of was assumed to be about three orders of magnitudes higher This reflects the activation time being sensor is at state 4 From this state it can transition into two states nominal operation state 1 nominal activation sensor being not healthy If the main sec This activation time 4 AvIONICs/FDIR MODEL Figure 5 Markov chain model for sensor avionics Initially the backup causes of sensor avionics same as in Figan inverse of the backup activation time of 5 assumes that the temperature of Tr ansition from state to Rate sec-1 Mean Tim'e state Avionics failure  3 2.8 10 100,000 h Backup activation failure 1 10 10,000 s 4 3 Backup activation 4 1 092 causes The assumed backup activation rate is sensor avionics is the ure 5 it is just assumed that the on the main are shown by dash-dotted lines They are operating nominally then the backup or permanent failure state 3 activation attempt failed These transitions are conditional sensor is kept inactive state 4 The model for the main a million hours each We assumed the failure rate of 10 per million hours The backup activation failure rate a few seconds out of the 1000 second mission but yielding a number of failures per mission comparable with other ure transition between states 1 and 3 is assumed similar to what is considered in 12 for navigation system avionics Several a few points per s Recovery for healthy sensor 0.1 10 s 2 1 Fault detection for faulty sen10 0.1I s sor 1 2 Type 11 erTor sensor healthy 1 10 28 h but fault detected 1 2 Type I error recovery for 2.5  10-4 4000 s faulty sensor 2 41 _____________ Table 2 Avionics fault model parameters the backup sensor is controlled and stabilized and is mainly time needed to boot up the sensor avionics The backup activation delay is a parameter of the trade study described in the next section and 5 sec is just one of the values considered the nominal value Transitions probabilities related to FDIR last four lines in Table 2 were determined as a result of a Monte Carlo simulation analysis of the FDIR algorithm performance in low Earth orbit docking approach The FDIR algorithms and the simulation scenario are described in our earlier paper 10 FDIR model and Monte Carlo simulation The Monte Carlo simulation was set up as follows The chaser and target spacecraft were simulated with 6 degrees of freedom each in low earth orbit A visualization of the approach scenario is shown in Figure 6 and described in detail in 10 The spacecraft pictures and error ellipses are magnified in the picture The spacecraft dynamics were simulated as two bodies in low Earth orbit with 6 degrees of freedom each Motion noise sensor noise and a proportional-integralderivative PID control are simulated with the control system using linear programming LP to schedule sensor firing The simulation generated relative navigation sensor measurements The simulation included essential components for autonomous rendezvous The simulated relative navigation sensor had a nominal operating rate of 5 Hz with time jitter added to simulate potential variation in the sensor's output It provided relative position measurements throughout the flight and relative attitude measurements 70%0 of the time The nominal noise of the relative navigation sensor and the inertial navigation system was modeled as Gaussian with covariances equal to those expected for a typical human-rated spacecraft sensor suite The chosen values were for a Honeywell SIGI found in 13 16 Inertial navigation measurements were simulated at 100 Hz 


a fixed interval of time and evaluating the probabilities of the three mission outcomes success abort mishap in the end The used simulation duration of 1000 seconds is representative For the nominal as a result of Monte Carlo study L 20 0 Figure 6 Visualization of R-bar approach used for Monte Carlo simulations The simulated docking procedure mimics the R-bar approach of a chaser vehicle to the International Space Station 9 Alternative approaches such as those currently proposed for the Orion vehicle 6 are similar in terms of sensing requirements The simulated approach begins at a 100 m range The control system uses a PID controller to track the approach path and a linear program solver to optimally fire the thrusters The navigation system fuses inertial and relative navigation data using a multi-rate Extended Kalman Filter EKF as described in 24 For the Monte Carlo analysis 5000 random trajectories can be considered acceptable The mission mishap probability below 10-4 is acceptable cite 8 Figure 8 shows how the system performance characterization the two probabilities varies when the backup use in the avionics fault model validate the choice of time delay models and provide reasonable Type I and Type II error probabilities for the FDIR algorithm 5 ANALYSIS RESULTS The PRA study are specific to the parameters chosen for this simulation they provide 100 80 LL 60 z 40 Ut 40 were injected at were observed The smaller the random fault magnitude the unreliability numbers abort and mishap probabilities stay zero hot stand-by to infinity no backup sensor The on the target spacecraft the distribution of time delays no backup sensor infinite activation delay has acceptable performance A sensitivity analysis more delay before it is detected Regardless of fault magnitude the expected time to detection decays monotonically after the fault onset For smaller fault magnitudes case parameters in Table 1 and Table 2 the mission abort probability is 8.3 10-3 and the mishap probGAUSSIAN RANDOM FAULTS 0 0.5 1 1.5 2 TIME DELAY TO DETECTION s RANDOM FAULTS WITH BOUNDED MAGNITUDE 45 40  35 uJ 30 25 0 20 J 15 L10 0 0.5 1 1.5 2 TIME DELAY TO DETECTION s Figure 7 Time delay from fault onset to fault detection in Monte Carlo simulations of a spacecraft approach x 10 4 x 10-8 8 6 lnf 1000500 200 100 50 20 10 5 3 2 1 05 0 BACKUP START DELAY Figure 8 Nominal case results depending were generated with variation in the initial on the scale of the spacing of retro-reflectors was close to exponential The time to detection histograms for random faults with standard deviation of 0.7 m in each axis and for faults bounded to be less than 0.6 in magnitude are shown in Figure 7 The range at which the fault as are the fault magnitudes The data shown is for faults under 0.6 m in magnitude Although the numerical results one representative set of values for was carried out by running the integrated Markov chain model for on the backup activation time ability is 3.4 10-7 The abort probability below 0.01 sensor activation delay varies from acceptable through the range of activation delays In particular for the assumed model parameters the system with was performed by varying each of the parameters in Table 2 an order of magnitude and observing the change in the system performance characterization A strong dependence on FDIR performance parameters which were obtained was observed The sensitivity to other problem parameters is relatively small Of all FDIR parameters the highest sensitivity is to the Type I error rate false positive probability The mission abort probability increases two orders of magnitude per order of magnitude increase of the Type I error rate Changes in other parameters influence the results to occurs is randomly generated a random range with random magnitude Several trends a lesser extent MISSION ABORT PROBABILITY MISHAP PROBABILITY cross track position along track position and orientation Faults 


sec In that The sensitivity to the FDIR parameters is high at the same time accuracy of computing these parameters is not very high The FDIR parameters obtained in the Monte Carlo simulation study on the backup activation time Study limitations The most important limitation of the above analysis results is that the abort guidance and control G&C logic is not as follows 1 The considered configuration with no chance for 2 Type 11 error sensor 5 10 2000 s healthy but fault detected I 2 Type I error recovery for 0.001 1,000 s faulty sensor 2 1  1_ ____ I one of the study parameters The summary of the findings is one cold backup more The mishap is caused by uncovered failures of the backup was on the relative navigation system and the G&C logic design for CEV AR&D was not yet available at the time of this study In fact the abort G&C logic needs to be considered in interaction with relative navigation Our PRA model assumed that abort is always possible if can observe that the mission abort probability has we felt compelled to consider FDIR parameters that ensure an acceptable risk of mission abort once the backup navigation sensor activation Transition from state to Rate sec1 Mean Time state Fault detection for faulty 0.1 10 s sensor 1 sec or less 2 For the worst-case model parameters the mission abort probability does not exceed 0.01 in the 1000 seconds of the final approach In all studied considered This is because the study focus come up on-line in time for the docking This might be untrue in the end of the approach and depends rendezvous and docking of CEV spacecraft An architecture with a single off-line backup or mishap for backup s Decreasing the activation delay below this number does not visibly improve the mishap probability The mishap probability decreases for large activation delay This is somewhat counterintuitive and is because the abort probability increases much cases the mishap probability is less than 3.5 10-7 This is true for backup sensor failure the backup sensor are detected The sensitivity analysis for the worst-case data set shows little sensitivity to fault detection rate and Type II FDIR are shown in Table 3 We will call these worst-case parameters Table 3 Worst are shown in Figure 9 One are shifted into the unfavorable direction Such Markov chain transition parameters for FDIR model a worst-case FDIR performance for parameters in Table 3 The remaining model parameters a knee point for backup activation delay of 20-30 a mishap Thus the mishap probablity decreases if one active relative case avionics tault model parameters In addition to the above described results are sensitive to many simulation parameters which in turn are known only roughly Therefore we considered are in Tables 1 and 2 The worst-case parameters yield mission abort probability of 2 10-4 and the mishap probability of 2.9 10-8 The results for different backup sensor activation delays in the worst-case parameter set sensor In cases when the mission is aborted because of the main sensor is not activated and there is more faults of the main error rate false positives The performance could become critically bad for i slow FDIR recovery and ii a large rate of Type I FDIR errors false negatives This finding provides important requirements to the FDIR algorithm design and tuning The overall finding is that the considered FT/RM architecture provides acceptable performance even for the worst-case parameters as long as the backup activation delay does not exceed 20-30 a sensor is lost In fact this might be untrue in the immediate vicinity of the target depending on the approach speed The PRA model also implicitly assumed that sensor is activated it will on the G&C logic The reported study was necessarily limited in scope The mentioned limitations provide possible directions of enhancing the developed PRA model and improving the analysis 6 CONCLUSIONS This paper presented a probabilistic risk assessment study for redundant relative navigation sensors in automated sensor was considered The backup activation delay is sensor and sensor appears to activation delay of 20 case mission abort probability is less than 0.01 and mishap probability is less than 10-6 0.1 _ 0.08 0.06 0.04 0.02 MISSION ABORT PROBABILITY ol 0 l Inf 1000 300 100 50 30 20 10 5 3 1 0 x 10 41 MISHAP PROBABILITY Inf 1000 300 100 50 30 20 10 5 3 1 BACKUP START DELAY 0 Figure 9 Worst case results depending 


aelay less that 20 sec This finding does not consider abort guidance and control 3 The results specific probability numbers strongly depend on the Markov chain model of the FDIR performance The previous conclusions assume the worst-case model The Monte Carlo simulation study undertaken to quantify FDIR performance parameters in the model was quite sensitive to configuration and tuning of the FDIR algorithms This emphasizes the importance of FDIR in the overall system design REFERENCES 1 D J Allerton and H Jia A review of multisensor fusion methodologies for aircraft navigation systems The Journal of Navigation Vol 58 2005 pp 405-417 2 P S Babcock G Rosh and J J Zinchuk An automated environment for optimizing fault-tolerant systems design IEEE Annual Maintainability and Reliability Symposium 1991 Orlano FL 31 P Buchholz Structured analysis techniques for large Markov Chains ACM Workshop on Tools for Solving Structured Markov Chains October 2006 Pisa Italy 4 R W Butler and S C Johnson Techniques for Modeling the Reliability of Fault-Tolerant Systems With the Markov State-Space Approach NASA Reference Publication 1348 September 1995 5 Current State ofReliability Modeling Methodologies for Digital Systems and Their Acceptance Criteria for Nuclear Power Plant Assessments The Ohio State University U.S Nuclear Regulatory Commission Office of Nuclear Regulatory Research Washington DC 205550001 6 C D'Souza C Hanak P Spehar F D Clark and M Jackson Orion Rendezvous Proximity Operations and Docking Design and Analysis AIAA GN&C Conference Hilton Head SC August 2007 7 S Everett K Markin P Wroblewski and M Zeltser Design considerations for achieving MLS Category III requirements Proceedings of IEEE Vol 77 No 11 1989 pp 1752-1761 8 Exploration Systems Architecture Study Final Report Chapter 8 Risk and Reliability NASA 2005 available http://www.nasa.gov/pdf/140639main ESAS 08.pdf 9 W Fehse Automated Rendezvous and Docking of Spacecraft Cambridge University Press New York NY 2003 10 G Hoffmann D Gorinevsky R Mah C Tomlin and J Mitchell Fault tolerant relative navigation using inertial and relative sensors AIAA GN&C Conference August 2007 Hilton Head SC 11 R.T Howard A.S Johnston T.C Bryan and M L Book Advanced video guidance sensor AVGS development testing NASA Marshall Space Flight Center 2004 NTRS 2004-11-03 Document ID 20040071003 12 M K Jeerage Reliability analysis of fault-tolerant IMU architectures with redundant inertial sensors IEEE AES Magazine July 1990 13 R Majure Demonstration of a ring laser gyro system for pointing and stabilization applications IEEE PLANS 90 Position Location and Navigation Symposium Las Vegas NV March 1990 14 J D Mitchell S P Cryan D Strack L L Brewster M L Williamson R T Howard A S Johnston Automated rendezvous and docking sensor testing at the flight robotics laboratory IEEE Aerospace Conference 3-10 March 2007 15 Mobius Model based environmentfor validation of system reliability availablity security and performance available http://www.mobius.uiuc.edu 16 Navigation Accelerometers QA2000-020 Performance Specifications available http://www.honeywell.com July 2007 17 M E Pate-Cornell and L M Lakats Organizational warning systems a probabilistic approach to optimal design IEEE Trans on Engineering Management Vol 51 No 2 2004 pp 183-196 18 M E Polites Technology of automated rendezvous and capture in space Journal of Spacecraft and Rockets Vol 36 No 2 1999 19 V S Sharm and K S Trivedi Reliability and performance of component based software systems with restarts retries reboots and repairs 17th Internat Symp on Software Reliability Engineering Nov 2006 L20 L C G Rogers and D Williams Diffusions Markov Processes and Martingales Volume 1 Foundations Series Cambridge Mathematical Library 2nd Edition Cambridge University Press 2000 21 W E Vesely M Stamatelatos J B Dugan J Fragola J Minarick and J Railsback Fault Tree Handbook with Aerospace Applications Version 1.1 NASA Office of Safety and Mission Assurance Washington DC 20546 August 2002 22 B K Walker Performance evaluation of systems that include fault diagnostics Joint Automatic Control Conference June 1981 Charlottesville VA 23 S Tamblyn H Hinkel and D Saley NASA CEV reference GN&C architecture 30th Annual AAS Guidance and Control Conference February 3-7 2007 Breckenridge CO AAS 07-071 NASA/JSC/EG FltDyn-CEV-06-151 NASA DAA 11564 24 S Thrun W Burgard and D Fox Probabilistic Robotics MIT Press Cambridge MA 2005 25 0 Yong and J.B Dugan Approximate sensitivity analysis for acyclic Markov reliability models IEEE Transactions on Reliability Vol 52 No 2 2003 pp 220-230 


BIOGRAPHY Dimitry Gorinevsky is a Consulting Professor of Electrical Engineering in Information Systems Laboratory at Stanford University and a Principal of Mitek Analytics LLC He received a Ph.D from Moscow Lomonosov University and a M.Sc from the Moscow Institute of Physics and Technology He held research engineering and academic positions in Moscow Russia Munich Germany Toronto and Vancouver Canada He spent 10 years with Honeywell His interests are in decision and control systems applications across many industries He has authored a book more than 140 reviewed technical papers and a dozen patents Dr Gorinevsky is an Associate Editor of IEEE Transactions on Control Systems Technology He is a recipient of Control Systems Technology Award 2002 and Transactions on Control Systems Technology Outstanding Paper Award 2004 of the IEEE Control Systems Society He is a Fellow of IEEE Gabriel Hoffmann is a PhD candidate at Stanford He is also with Mitek Analytics LLC working on NASA AR&D project At Stanford Gabriel develops hardware flight software and control laws for a fleet of quadrotor helicopter unmanned aerial vehicles UAVs He has been the team lead for controls systems for the Stanford Racing Team since 2004 He designed the control system that ran the autonomous unmanned vehicle Stanley though a 132 mile race across off-road desert terrain to win DARPA Grand Challenge in 2005 He also designed the control system that ran the autonomous unmanned vehicle Junior through 56 miles of urban terrain to place second in the DARPA Urban Challenge Gabriel interned at Boeing Space Systems Houston working on the International Space Station He interned at the NASA Ames Academy in 2001 Marina Shmakova is working on modeling and analysis of fault tolerant systems with Mitek Analytics LLC She is also a Staff Scientist at Physics Department of Stanford University She worked on analysis of Hubble Space Telescope data including the attitude and pointing data and observational data As a research associate at Stanford Linear Accelerator Center Dr Shmakova worked on different aspects of astrophysics and high energy physics She has Ph.D and in MS in Physics from University of Tennessee Robert Mah is a Senior Scientist in Intelligent Systems Division at NASA Ames Research Center ARC He received a Ph.D in Applied Mechanics and M.S in Mechanical Engineering from Stanford University in 1988 and 1976 He has led and managed NASA projects and groups for more than 30 years He is a recipient of many NASA awards including NASA Exceptional Achievement Medal three Space Act awards several awards for Technology Transfer and Commercialization Spacecraft Docking Simulation Award and several others He published a few dozen papers and has several patents received and pending He was a keynote or invited speaker at many international and NASA conferences his work was featured in the media Dr Mah currently is a Project Scientist for Integrated Vehicle Health Management project in NASA Aviation Safety Program Scott Cryan received BS Aerospace Engineering from University of Buffalo in 1988 Since then he has been working Space Shuttle and Intermational Space Station GPS receivers In addition to the GPS receiver role on Space Shuttle and ISS he had been working on GPS _ii lreceivers for several projects at NASA JSC Recently Mr Cryan has been investigating relative navigation sensor performance via testing and analyses with open loop simulations Currently Mr Cryan is the relative navigation subsystem manager for the CEV Project at NASA-JSC Jennifer Mitchell is the Crew Exploration Vehicle Flight Dynamics Deputy Functional Area Manager at NASAs Johnson Space Center and also serves as the Project Manager for the Exploration Systems Technology Development Au_0 tomated Rendezvous and Docking Sensor Technology Project She has supported guidance navigation and control system development and testing for the Autonomous Extravehicular Robotic Camera AERCam the International Space Station and the X38 Crew Return Vehicles Space Integrated Global Positioning System  Inertial Navigation System She has a BS in Aerospace Engineering from Texas A&M University 


Table 2. Measurement Results \(all numbers are in milliseconds Tokenization Parsing Transformation 100% Cache Hit 0% Cache Hit 100% Cache Hit 0% Cache Hit Document Nature Swif t Hash kDOM Sender Assign Receiver Assign Sender Assign Receiver Assign XT Sender Assign Receiver Assign Sender Assign Receiver Assign axis 7.1 4.8 9.5 11.4 5 9.6 10.7 13 36.3 6.9 11.4 36.3 39.3 Backwards 33.3 7.4 43.4 60.7 7.6 43.7 56.1 66.6 232.2 12.9 49.5 232.7 252.4 chart 18.5 7.7 24 32.1 7.8 24.9 30.1 36.3 117.8 21.9 39.7 118 129.3 book 15.1 8.6 18.2 24.4 8.8 18.5 21.1 25.5 48.3 11.5 20.9 48.9 55.2 game 33.1 7.2 43.2 60.7 7.6 43.5 56.4 67.3 92.2 8.9 45.1 92 105.5 midsummer 1990 913.4 2311 5 4686.9 997 2332.5 4012.8 4360.8 7398.5 1475 3068 7268.1 7555.2 nitfstylized 67.3 25.1 76 105.3 27 77.3 93.8 107.7 178.3 34.4 85 188.5 204.3 recipes 164.3 99 180.1 249.2 107.6 184.8 209.9 234.3 574.4 144.3 224.1 566 585.1 sort 129.8 32.2 159.8 248.2 33 163.6 242.3 281.5 1094.9 55.6 188.1 1121.9 1174.4 sp 36.2 21.5 40 53.7 23.2 40.6 45.5 51.3 122.8 28.2 45.2 126.6 138.9 total 18.6 6.8 24.5 32.4 7.4 25 30.6 36.2 53.9 7.6 25.5 54 61.5 trend 36.5 10.8 41 62.1 10.4 41.1 66.3 70.9 2249.3 56.9 89.1 2249.3 2279.5 wai 9.1 4.5 12.5 14.2 4.6 12.6 13.4 17.1 96.4 7.7 15.8 99.9 105 with the structure \(Note that document structures and template values can be flushed out of the cache and reconstructed from files stored in persistent storage The first document of a given structure will be sent uncompressed. But an entry will be added to the encoding tables on both sender and receiver sides, with the attribute and text valu es of this first document used as template values. A following document of the given structure will be encoded as a two-byte integer indicating the index of th e structure in the encoding table, followed by the list of attribute and text string values. A special 1-byte symbol replaces any value that is an exact match of its corresponding template value Such compression is used between a mobile handset and its service gateway in cellular environments. Note that compression happens on a document send from the handset, through the gateway, to a server on the Internet, as well as on a document received by the handset through the gateway Table 1. XML Documents Used in the Experiments, and Th eir Corresponding Stylesheets Document Size in KB Stylesheet axis 0.38 Tests XPath selection along the different axes backwards 2.62 Reverses order of elements using the document used in game chart 1.29 Generates an HTML chart of some sales data book 1.21 Convertin book record to HTML game 2.62 Produces a HTML table of the data midsummer 146 Converting the play to HTML nitf-stylized 5.79 NITFML to HTML recipes 16.7 Converting Recipes in XML to HTML sort 10.3 Sorting input tree according to element name sp 3.53 Web site construction kit total 1.31 Reports on sales data trend 1.9 Computes trends in the input data wai 0.58 Schematron validator for WAI docs 
320 


5 EVALUATION Evaluation experiments described in this section are conducted on a TI OMAP 1511 Innovator device which runs in frequencies up to 200MHz. It has 32MB ROM as well as 32MB RAM. The OS used is a version of embedded Linux, and the JVM used is Intent from TAO Group The source documents and stylesheets used for evaluation are randomly selected from Sarvega’s XSLTBench and DataPower’s XSLTMark [17  after initially excluding some unrealistic transformations. We select these two benchmarks as they are influential indust ry benchmark and that they both support XSLT benchmarking. Note that some of the large source documents are less likely to have recurrent structures in real-world applications. We included them regardless as they might help us understand the implication of document size on system performance. Table 1 lists sizes of sample documents and the transformation stylesheets used for them. Table 2 lists the raw numbers used in following discussions Documents are fully read into memory before any measurement starts to minimize external disturbance Warm-up runs are always conducted prior to real measurement runs 5.1 Tokenization Figure 12 compares the speed of Swift mode and Hash mode against the speed of Nature mode tokenization The Swift mode is by far the fasted, having a median speedup of 2.40 over Nature mode. The median speed of Hash mode is about 81% of that of Nature mode which means a median hashing overhead of about 23.5%. In the worst case, this overhead is about 37.4 for wai  5.2 DOM-style Parsing Figure 13 and Figure 14 compare speed of Structure Encoding based parsing against that of kXML parsing for both Receiver ID Assigning and Sender ID Assigning. In Receiver ID Assigning, the underlying tokenizer always uses Hash mode, while in Sender ID Assigning, the tokenizer uses Swift mode when cache hit ratio is 100% \(i.e., structure of incoming document is always in cache\ode when cache hit ratio is 0 Figure 13 shows the results for the ideal case when cache hit ratio is 100%: the median relative speed is 4.34 when Sender ID Assigning is used, and is 1.35 when Receiver ID Assigning is used. The best case is in backwords and game which use the same source document\to 8 under Sender ID Assigning Figure 14 shows the results for the worst case when cache hit ratio is 0%: the median relative speed is 1.07 when Sender ID Assigning is used, and is 0.90 \(or an overhead of %11.1\ID Assigning is used. The worst sample is wai where the overhead is about 20.5% under Receiver ID Assigning. The reason Figure 12. Comparing Performance of Tokenization Modes Figure 14. Comparing Structure Encoding Based XML Parsing and kXML Parsing \(Cache Hit Ratio = 0 Figure 13. Comparing Structure Encoding Based XML Parsin g and kXML Parsin g Cache Hit Ratio 100  
321 


that Structure Encoding is a little faster than kXML even when cache hit ratio is 0 is likely because of the fact that, while we use kXML for tree building, we modified it slightly to interface it with our tokenizer which resulted in differences in execution traces and slight differences in runtime performance when execute in JVM Overall, these two figures demonstrate that Structure Encoding based parsing provides substantial speedup when structure recurrence is frequent, and incurs low overhead when such recurrence is rare 5.3 Transformation Similarly, Figure 15 and Figure 16 compare transformation speed of Structure Encoding based XSL processor against that of XT for both Sender ID Assigning and Receiver ID Assigning. Again, under Receiver ID Assigning, the underlying tokenizer always uses Hash mode, while under Sender ID Assigning, the tokenizer uses either Swift mode or Nature mode Figure 15 shows the results for the ideal case when cache hit ratio is 100%: the median relative speed of Structure Encoding based implem entation is 5.38 when Sender ID Assigning is used, and is 2.72 when Receiver ID Assigning is used. The best case is trend  where the relative speed is over 39 for Sender ID Assigning, and over 25 for Receiver ID assigning Figure 16 shows the results for the worst case when cache hit ratio is 0%: the median relative speed is 0.996 when Sender ID Assigning is used, and is 0.918 or an overhead of 8.9 is used. The worst sample is nitf-stylized where the overhead is about 14.6% under Receiver ID Assigning These two figures show that Structure Encoding based XSL processing offers even higher potential speedup than parsing, yet incurs lower overhead in worst cases This is understandable as, typi cally, a large portion of operations in transformation is for costly structurerelated operations, which we have pre-processed offline. Note that, as long as structure recurrence is frequent, the potential improvement is very high \(up to a median relative speed of 2.72\even if the receiver has to calculate structure ID through hashing 5.4 Compression We conducted an experiment to examine the effectiveness of Structure Encoding based compression. In the experiment, for each sample document, we randomly changed the values of 3 rd 8 th  13 th i.e., one in every 5, starting from the 3 rd  attribute and text node values. Thus our experiment Figure 16. Comparing Structu re Encoding Based Transformation and XT  Cache Hit Ratio 0  Figure 15. Comparing Structu re Encoding Based Transformation and XT  Cache Hit Ratio = 100  Table 3. Compression and its effect on tokenization speed \(Assuming changes in 20% of the text and attribute values Document Size in KB Comp Size Byte Natur e ms Swift ms Comp  ms axis 0.38 48 7.1 4.8 0.8 backwards 2.62 81 33.3 7.4 1.8 chart 1.29 91 18.5 7.7 1.3 book 1.21 129 15.1 8.6 1.1 game 2.62 81 33.1 7.2 1.8 midsummer 146 20760 1990 913.4 111.7 nitf-stylized 5.79 564 67.3 25.1 4.0 recipes 16.7 2355 164.3 99 11.4 sort 10.3 752 129.8 32.2 8.8 sp 3.53 554 36.2 21.5 2.6 total 1.31 91 18.6 6.8 1.3 trend 1.9 169 36.5 10.8 2.7 wai 0.58 15 9.1 4.5 0.6 
322 


assumes that around 20% of the attribute and text values are different from the template values Table 3 lists results from this experiment. The third column of the table shows the number of bytes transmitted for each sample document, while the 6 th column shows the time used to construct tokens from the compressed document. The table shows that, under above-described setup, the amount of data transmitted are reduced to 2.6% to 15.7% of the original size second column, listed in KB time is reduced to 5.4% to 11.3% of the Nature mode tokenization time, or 11.5% to 25% of the Swift mode tokenization time 5.5 Discussion Since in our implementations, we make changes to base systems \(KXML and XT changes are required to implement Structure Encoding performance differences demonstrated in this section are caused by differences in techniques rather than differences in implementations Our experiments clearly show that Structure Encoding can, potentially, greatly improve the efficiency of XML processing with relatively low penalty for worst cases. The worst case scenario happens when a receiver uses hash function to identify the structure of a document, only to find out that the document structure is not in cache. The penalty it pays for such worst case scenario, 11.1% for DOM-style parsing and 8.9% for transformation, can easily be compensated by future structure recurrence. Such cache-miss penalty is negligible when sender-assigning scheme is used Systems that are conscious of such client-side penalty can let the sender or anyone in the middle of the transmission path to assign ID without altering the semantics of the message or document Although we didn’t measure the impact of compression on parsing and transformation, it can be inferred from tables 2 and 3. For example, in the book case, with tokenization time reduced from 8.6ms Swift mode\s \(compressed\e will likely be further reduced from 8.8ms to around 1.3ms compared with 24.4ms for KDOM\Similarly transformation time may be further reduced from 11.5ms to around 4.0ms \(compared with 48.3ms for XT\Encoding based XML compression offers additi onal, significant performance improvements for XML parsing and transformation in mobile environments, where closelycoupled proxies commonly exists. With compression turned off, Structure Enc oding is fully compatible with Web specifications and can be used between any two Internet hosts, and it still offer very significant performance improvements when there is structure recurrence Without compression, for documents with recurrent structures, tokenization and structure hashing cost dominates the overall cost for parsing, and is the major part of the cost for transformation. Tokenization and structure hashing however are relatively simple operations that may be implemented in hardware with low cost. \(In fact, some mobile chipsets already have hardware implementation of hash functions such as MD5 for security purposes implementation can reduce tokenization and structure hashing cost to a fifth of the current cost, then there will be additional substantial improvement for the parsing and transformation of most of the documents used in our experiments In our system, a document havi ng a structure slightly different \(e.g., added a new element\from a previous structure will not be able to reuse the structure processing result of the previous structure. However our system pays off as long as this new, slightly different structure recurs in future documents 6 CONCLUSION, LIMITATIONS, AND FUTURE WORK In this paper we motivated exploiting structure recurrence to speedup XML processing in mobile environments, by reduci ng structure related transmission and processing costs. We presented the concept of Structure Encoding, and described approaches to quickly identifying recurring structures including one using collision-resistant hash function We explained in detail how to use structure encoding to speedup XML transmission, tokenization, treebuilding, and transformation. We described our implementation of st ructure encoding based tokenizer DOM parser \(based on kXML\processor \(based on XT\pression scheme. Our experiments conducted on a mobile test-bed demonstrated dramatic performance improvement in the presence of structure recurrence and low overhead otherwise. In ideal cases structure encoding offers speedups of up to 7 for parsing and over 38 for XSL transformation, and up to 97.4% in size reduction when 20% of the text and attribute values change Structure encoding, however, is not applicable to all XML applications. Rather, it is more applicable to data-centric XML processing than to document-centric XML processing. A user randomly browsing Web pages is not likely to have high structure recurrence probabilities 
323 


Our current implementa tion does not support documents with “variable-le ngth arrays” – lists of identically structured elements with non-fixed lengths Otherwise identically structured documents with different array lengths are currently considered as having different structure We are currently working on supporting “variablelength arrays” to extend the applicability of Structure Encoding. We are also looking at provide similar, but less aggressive, optimization support for schemaconforming documents REFERENCES  Nokia Web Services – Helping Operators Mobilize the Internet Http://www.projectliberty.org/resources/whitepapers/W S_Operators_A4_0408.pdf  The SAX Project. http://www.saxproject.org  XML Pull Parsing. http://www.xmlpull.org  W3C Document Object Model http://www.w3.org/DOM  WAP Binary XML Content Format http://www.w3.org/TR/wbxml  Efficiency Structured XML. http://www.esxml.org  VTD-XML. http://vtd-xml.sourceforge.net  XSLTC Documentation. http://xml.apache.org/xalanj/xsltc  kXML. http://www.kxml.org  Liefke, H. and D. Suciu. XMill: An Efficient Compressor for XML Data. In Proc. of the ACM SIGMOD Conference on Management of Data. May 2000  Liu, L., C. Pu, and W. Tang. WebCQ: Detecting and Delivering Information Changes on the Web" In the Proceedings of International Conference on Information and Knowledge Management \(CIKM  The XT XSLT processor http://www.blnz.com/xt/index.html  Sarvega,Inc. http://www.sarvega.com  DataPower Technology, Inc http://www.datapower.com  Rax Content Processor http://www.tarari.com/rax/index.html  The Sarvega XSLT Benchmark Study, Sarvega Inc http://www.sarvega.com/xslt-benchmark.php  XSLTMark http://www.datapower.com/xmldev/xsltmark.html  Eisenhauer, G. and L. K. Daley. Fast Heterogenous Binary Data Interchange. In Proceedings of the 9th Heterogeneous Computing Workshop \(HCW 2000 90-101  Bustamente, F., G. Eisenhauer, K.Schwan, and P Widener. Efficient Wire Formats for High Performance Computing. In Proceedings of High Performance Networking and Computing Conference, 2000 SC’2000  Toshiro Takase, Hisashi Miyashita, Toyotaro Suzumura, and Michiaki Tatsubori, An Adaptive, Fast and Safe XML Parser Based on Byte Sequence Memorization. In Proc. of WWW’2005  XML-RPC. http://www.xmlrpc.com  Open  Mobile Alliance http://www.openmobilealliance.org  RSS 2.0 Specification http://blogs.law.harvard.edu/tech/rss  Open Mobile Alliance http://www.openmobilealliance.org  M ogul, J., F. Douglis, A. Feldm an, and B Krishnamurthy. Potential benefits of deltaencoding and compression for HTTP In Proc SIGCOMM’97 1997  Spring, N. T., and D. W e therall. A protocolindependent technique for eliminating redundant network traffic In Proc. SIGCOMM’00 2000  Chiu, K., and W  Lu. A Com piler-Based Approach to Schema-Specific XML Parsing. In First Internati onal Workshop on High Performance XML Processing, May 2004  Matsa, M., E. Perkins, A. Heifets, M. G.aitatzes Kostoulas, D. Silva, N. Mendelsohn, M. Leger. A high-performance interpretive approach to schema-directed parsing. In Proceedings of the 16th International Conference on World Wide Web, 2007  Noga, M  L., Schott, S., and Löwe, W  2002. Lazy  XML processing. In Proceedings of the 2002 ACM Symposium on Document Engineering McLean, Virginia, USA, November 08 - 09 2002\02. ACM, New York, NY  Farfán, F., V. Hristidis and R. Rangaswam i Beyond Lazy XML Parsing. In Proceedings of the 18th International Conference \(DEXA 200 September 3-7, 2007  
324 


 15 Fourier transform spectrometers at the Internationa l Scientific Station of the Jungfraujoch Switzerland  for atmospheric measurements and at the Institute of Astrophysics in Liège for laboratory measurements He was hired by JPL in August 1990 as MkIV cognizant engin eer and participated in all the MkIV campaigns since th en \(one DC-8 campaign 19 balloon campaigns Dr J.-F Bla vier obtained his Ph.D in Physics from the University o f Liège in July 1998 Paula Pingree is a Senior Engineer in the Instruments and Science Data Systems Division at JPL She has been involved in the design integration test and operation of several JPL flight projects most recently Deep Impact DI She has worked on the Tunable Laser Spectrometer development for the 2009 Mars Rover and is presently the Electronics CogE for the Juno Mission s Microwave Radiometer She also enjoys research and technology development for Smart Payloads in her s pare time Paula has a Bachelor of Engineering degree i n Electrical Engineering from Stevens Institute of Te chnology in Hoboken, NJ, and an MSEE degree from California State University Northridge.  She is a member of IEEE 


 16  A High Capacity Solid Sate Recorder \(HC-SSR\is suggested using devices predicated to be available for each decade.   The design is robu st and easily implemented using conservative design and manufacturing techniques D EFINITIONS  rad radiation absorbed dose\:   the dose causing 0.01 joule of energy to be absorbed per kilogram of matter.   As the absorption is greatly affected by the molecular structure of the material, citations should al so indicate the material as a subscript to the term 223rad\224, as in rad Si indicating Silicon equivalency.  For the purposes of this paper, radiation equivalency always assumes Silicon For completeness, it should be noted that System International replaced the \223rad\224 with the unit Gr ay \(Gy\nd having an equivalency of 100 rads = 1 Gy [27 How e v e r   the use of rads, kilorads, megarads remains in the industry vernacular and is used in this document Moore\222s Law Named after Fairchild Semiconductor technologist Gordon Moore, Moore\222s law was derived from empirical data which shows that the dimensions of basic memory cells will shrink by approximately 50% of the previous value every 30 to 36 months.  It is Moore\222s Law more or less, that forms the backbone of the ITRS examinations for memory devices A DDITIONAL M ATERIAL  Standard Dose Rates for Various Orbits and Missions per year Earth    LEO  100 rad \(protons  MEO  100 krad \(protons electrons  GEO  1 krad \(electrons  Transfer Orbit  10 krad \(protons electrons Mars     Surface  2 krad \(electrons  Orbit  5 krad \(protons  Transit  5 krad \(protons Jovian     Transfer  100 Mrad \(protons electrons A CKNOWLEDGEMENT  The research described in this paper was carried out at the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration.   The Author thanks the many who guided the concept and offered support all along the way.   With special thanks to fellow-JPL\222ers Gary Noreen who provided funding\nd Taher Daud who provided editing ad-hoc extraordinaire  R EFERENCES    G M o o r e C r a m m i ng m o re C o m pone nt s o n t o  Integrated Circuits Electronics vol. 38, no. 8, April 1965 2 G M o ore, "No Expo n e n tial is Forev er: B u t 223Forever\224 Can Be Delayed Digest of Technical Papers, International Solid State Circuits Conference pp. 1.1-1 thru 1.1-19, 2003  S eagate Tec hnology Com p any. Seagate Tec hnical Corporation. [Onlin http://www.seagate.com/docs/pdf/marrketing/Article _Perpendicular_Recording.pdf   B l u R ay Di sc Ass o ci at i on  2 00 5 M a rc h B l u-R a y  Disc Technical Papers  J Vel e v K  D B e l a shche n ko  an d et _al   2 0 0 7  October\ MSREC - University of Nebraska Onlin http://www.mrsec.unl.edu/research/nuggets/nugget_2 6.shtml  6 R Katti, "Honeywell Rad i atio n Hard en ed  No nVolatile Memory \(MRAM\ Product Development in Proceedings, IEEE No n-Volatile Memory Technology Symposium Orlando, 2004, pp. L2:1-15 7 S aif u n  Sem ico n d u c tor  2 008 NV M Techno log y  Over http://www.saifun.com/content.asp?id=113  


 17    J. Ta usc h  S. T y son a n d T T a i r ba nks  Multigenerational Radiation Response Trends in SONOSb ased NROM Flash Memories with Neutron Latch-up Mitigation," in NSREC Radiation Effects Workshop Honolulu, 2007, pp. 189-193 9  Semico n du c to r In du str y A s sociatio n SIA  2 008   August\ Home. [Online  www.itrs.net  1  S. Ty s o n P ri v a t e C o m m uni que Tra n sEl  Semiconductor, Albuquerque, NM, 2008 1  T. M i k o l a ji c k  and C U Pi n n o w  2 00 8 N o vem b er Indo-German Winter Academy, 2008, Course 3 Onlin http://www.leb.eei.unierlangen.de/winterakadem ie/2008/courses/course3_ material/futureMemory/Mikolajick_TheFutureofNV M.pdf   BAE System s North Am erica, [Data Sheet Microcircuit, CMOS, 3.3V, NVRAM 8406746, April 28, 2008, Rev A 1  N Ha dda d a n d T Scot t  A da pt i n g C o m m erci al  Electronics to the Natura lly Occurring Radiation Environment," in IEEE Nuclear and Space Radiation Effects Conference Short Course Tucson, 1994, pp iv-14 1  D. R  R o t h a n d et _al S EU a n d TI D Test i n g of t h e Samsung 128 Mbit and the Toshiba 256 Mbit flash memory," in Radiation Effects Data Workshop  Reno, 2000 1  F. I r o m and D N guy e n  S i n gl e E v ent  Ef fe ct  Characterization of High Density Commercial NAND and NOR Nonvolatile Flash Memories Honolulu, 2007 1  C Ha fer M  L a hey a n d et _al R adi a t i o n H a rd ness  Characterization of a 130nm Technology," in Proceedings IEEE Nuclea r and Space Radiation Effects Conference Honolulu, 2007 17  T. R O l dh am J. Fr iend lich  an d et_ a l, "TID  an d SEE Response of an Advanced Samsung 4Gb NAND Flash Memory," , Honolulu, 2007  R. C. Lac o e C MOS Scaling, Desi gn Princi ples a n d Hardening-by-Design Methodologies," in Nuclear and Space Radiation Effects Conference Short Course Notebook Monterey, 1993, pp. II-1 thru II142 1 J. Pat t e rs o n a n d S  Gue rt i n   E m e rgi ng S E F I M o des and SEE Testing for Highly-Scaled NAND FLASH Devices," in Proceedings 2005 Non-Volatile Memory Technology Symposium vol. CD-ROM, Dallas, TX 2005, pp. G-3, Session G ; Paper 3 2 J. Ta usc h  S. T y son a n d T F a i rba nks  Mulitgenerational Radiation Response Trends in SONOSb ased NROM Flash Memories with Neutron Latch-up Mitigation," in Honolulu Radaition Effects Data Workshop, NSREC, 2007, pp. 189-193 2 M Janai  B Ei t a n A Sha p pi r I B l o o m and G  Cohen, "Data Retention Reliability Model of NROM Nonvolatile Memory Products IEEE Transactions on Device and Materials Reliability vol. 4, no. 3, pp 404-415, September 2004 2 D N g uy en a n d F I r o m Tot al Io ni zi n g  Do se \(T ID  Tests on Non-Volatile Memories: Flash and MRAM," in 2007 IEEE Radiation Effects Workshop  vol. 0, Honolulu, 2007, pp. 194-198  G. Noree n  a n d et_al L ow Cost Deep Space Hybrid Optical/RF Communications Architecture," , Big Sky, Montana, 2009, Pre-print 2 T. Sasa da a n d S. I c hi kawa  A p p l i cat i o n o f  Sol i d  State Recorders to Spacecraft," in Proceedings, 54th International Astronautical Cogress Bremen, 2003 2 H Ka nek o  E rr or C o nt r o l C odi ng f o r  Semiconductor Memory Systems in the Space Radiation Environment," in Proceedings, 20th IEEE International Symposium in Defect and Fault Tolerance in VLSI Systems, DFT2005 Monterey 2005 2 T. Sasa da a n d H Ka nek o  D evel o p m e nt an d Evaluation of Test Circuit for Spotty Byte Error Control Codes," in Proceedings, 57th International 


 18  Astronautical Congress Valencia, 2006 27  Bu reau  In tern atio n a l d e s Po ids et Mesures. \(2 008  August\SI Base Units. [On http://www.bipm.org/en/si/base_units   B IOGRAPHY  Author, Karl Strauss, has been employed by the Jet Propulsion Laboratory for over 22 years.  He has been in the Avionics Section from day One.  He is considered JPL\222s memory technology expert with projects ranging from hand-woven core memory \(for another employer\o high capacity solid state designs.  He managed the development of NASA\222s first Solid State Recorder, a DRAM-based 2 Gb design currently in use by the Cassini mission to Satu rn and the Chandra X-Ray observatory in Earth Orbit.  Karl was the founder, and seven-time chair of the IEEE NonVolatile Memory Technology Symposium, NVMTS, deciding that the various symposia conducted until then were too focused on one technology.  Karl is a Senior IEEE member and is active in the Nuclear and Plasma Scie nce Society, the Electron Device Society and the Aerospace Electronic Systems Society Karl is also an active member of SAE Karl thanks his wonderful wife of 28 years, Janet, for raising a spectacular family: three sons, Justin, Jeremy Jonathan.  Karl\222s passion is trains and is developing a model railroad based upon a four-day rail journey across Australia\222s Northern Outback   


 19 Bollobás, B. 2001. Random Graphs. Cambridge University Press; 2nd edition. 500pp  Cawley, G. C., B. L. C. Talbot, G. J. Janacek, and M. W Peck. 2006. Sparse Bayesian Ke rnel Survival Analysis for Modeling the Growth Domain of Microbial Pathogens  Chiang C. L. 1960. A stochastic study of life tables and its applications: I. Probability distribution of the biometric functions. Biometrics, 16:618-635  Cox,  D. R. 1972. Regression models and life tables J. R Stat. Soc. Ser. B 34:184-220  Cox, D. R. 1975.   Partial likelihood Biometrika 62:269276  Cox, D. R. & D. Oakes. 1984 Analysis of Survival Data  Chapman & Hall. London  Cressie, N. A. 1993 Statistics for Spatial Data John Wiley Sons. 900pp  Duchesne, T. 2005. Regression models for reliability given the usage accumulation history. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty Y. Armijo. pp.29-40. World Scientific, New Jersey  Eleuteri, A., R. Tagliaferri, L. Milano, G. Sansone, D D'Agostino, S. De Placido,  M. Laurentiis. 2003.  Survival analysis and neural networks. Proceedings of the International Joint Conference on Neural Networks, Vol. 4 20-24 July 2003 Page\(s\:2631 - 2636  Ellison, E., L. Linger, and M Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013, 1997  Fleming, T. R. & D. P. Harrington. 1991. Counting process and survival analysis. John Wiley & Sons. 429pp  Graver, J. and M. Sobel 2005. You may rely on the Reliability Polynomial for much more than you might think Communications in Statistics: Theory and Methods  34\(6\1411-1422  Graves, T. and M. Hamada. 2005. Bayesian methods for assessing system reliability: models and computation. In Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson, et al. pp.41-53  Grimmett, G. 2006 The Random-Cluster Model Springer  Grimmett, G. 1999 Percolation Springer  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis.  Springer. 481pp  Jin Z. 2005. Non-proportional semi-parametric regression models for censored data. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.279-292 World Scientific  Kalbfleisch, J. D. & R. L. Prentice. 1980 The Statistical Analysis of Failure Time Data John Wiley & Sons.  New York. 1980  Kalbfleisch, J. D. &  R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data.  Wiley-InterScience, 2nd ed 462pp  Lisboa, P. J. G. and H. Wong. 2001. Are neural networks best used to help logistic regression? Proceedings of International Joint Conference on Neural Networks, IJCNN 01. Volume 4, 15-19,  July 2001. Page\(s\:2472 - 2477 vol.4  Kauffman, R. J. and B. Wang. 2002. Duration in the Digital Economy. Proceedings of th e 36th Hawaii International Conference on System Sciences \(HICSS’03\ Jan 2003  Kaplan, E. L. & P.  Meier.  1958.  Nonparametric estimation from incomplete observations J. Amer. Statist. Assoc  53:457-481  Klein, J. P. and P. K. Goel 1992. Survival Analysis: State of the Art.  Kluwer Academic Publishes. 450pp  Klein, J. P. and  M. L Moeschberger. 20 03. Survival analysis techniques for ce nsored and truncated data Springer  Krings, A. and Z. S. Ma. 2006.  "Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks MILCOM 2006, Military Communications Conference, 2325 October, 7 pages, 2006  Krings, A. W. 2008.  Survivable Systems.  in Information Assurance: Dependability and Security in Networked Systems Yi Qian, James Joshi, David Tipper, and Prashant Krishnamurthy, Morgan Kaufmann Publishers. \(in press  Lawless, J. F. 1982. Statistical models and methods for lifetime data.  John Wiley & Sons. 579pp  Lawless, J. F. 2003. Statistical models and methods for lifetime data.  John Wiley & Sons. 2nd ed. 630pp  Li, M. and P. Vitanyi. 1997. Introduction to  Kolmogorov Complexity and Its Applications. 2nd ed, Springer  Ma, Z. S. 1997.  Survival analysis and demography of Russian wheat aphid populations.  Ph.D dissertation, 307pp University of Idaho Moscow, Idaho, USA 


 20 Ma, Z. S., and E. J. Bechinski. 2008.  Developmental and Phenological Modeling of Russian Wheat Aphid Annals of Entomological Soc. Am In press  Ma, Z. S. and A. W. Krings. 2008a. The Competing Risks Analysis Approach to Reliability Survivability, and Prognostics and Health Management.  The 2008 IEEEAIAA AeroSpace Conference. BigSky, Montana, March 18, 2008. \(In Press, in the same volume  Ma, Z. S. and A. W. Krings 2008b. Multivariate Survival Analysis \(I\e Shared Frailty Approaches to Reliability and Dependence Modeling. The 2008 IEEE-AIAA AeroSpace Conference. BigSky Montana, March 1-8, 2008 In Press, in the same volume  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(II\ Multi-State Models in Biomedicine and Engineering Reliability. 2008 IEEE International Conference on Biomedical Engineering and Informatics BMEI 2008\27th-30th, 2008 Accepted   Mani, R., J. Drew, A. Betz, P. Datta. 1999. Statistics and Data Mining Techniques for Lifetime Value Modeling ACM Conf. on Knowledge Discovery and Data Mining  Mazzuchi, T. A., R Soyer., and R. V Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Meeker, W. Q. and L. A. Escobar. 1998. Statistical Methods for Reliability Data. Wiley-Interscience  Munson, J. C. 2003. Software Engineering Measurement Auerbach Publications  Nelson, W. 1969. Hazard plotting for incomplete failure data J. Qual. Tech 1:27-52  Nakagawa, T. 2006.  Shock and Damage Models in Reliability Theory. Springer  Osborn, B. 2005. Leveraging remote diagnostics data for predictive maintenance.   In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp. 353-363  Pena, E. A. and E. H. Slate. 2005. Dynamic modeling in reliability and survival analysis. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.55-71  Reineke, D. M., E. A. Pohl, and W. P. Murdock. 1998 Survival analysis and maintenance policies for a series system, with highly censore d data.  1998 Proceedings Annual Reliability and Maintainability Symposium. pp 182-188  Schabenberger, O. and C. A. Gotway. 2005. Statistical Methods for Spatial Data Analysis.  Chapman & Hall/CRC  Severini, T. A. 2000. Likelihood methods in statistics Oxford University Press  Shooman, M. L. 2002. Reliability of Computer Systems and Networks: Fault Tolerance, Analysis and Design. John Wiley and Sons. 551pp  Stillman, R. H. and M. S. Mack isack, B. Sharp, and C. Lee 1995. Case studies in survival analysis of overhead line components. IEE Conferen ce of the Reliability and Distribution Equipment. March 29-31, 1995. Conference Publication No. 406. pp210-215  Therneau, T. and P. Grambsch. 2000 Modeling Survival Data: Extending the Cox Model Springer  Wilson, A.  N. Limnios, S Kelly-McNulty, Y. Armijo 2005. Modern Statistical and Mathematical Methods in Reliability. World Scientific, New Jersey  Xie, M. 1991. Software Reliability Modeling. World Scientific Press    B IOGRAPHY   Zhanshan \(Sam\ Ma holds a Ph.D. in Entomology and is a Ph.D. candidate in Computer Science at the University of Idaho. He has published approximately 30 journal and 30 conference papers, mainly in the former field.  Prior to his recent return to academia, he worked as senior network/software engineers in software industry.  His current research interests include reliability and survivability of wireless sensor networks, fault tolerance survival analysis, evolutionary game theory, evolutionary computation and bioinformatics  Axel W. Krings is a professor of Computer Science at the University of Idaho.  He received his Ph.D. \(1993\ and M.S 1991\ degrees in Computer Science from the University of Nebraska - Lincoln, and his M.S. \(1982\ in Electrical Engineering from the FH-Aachen, Germany.  Dr. Krings has published extensively in the area of Computer Network Survivability, Security, Fault-Tolerance and Realtime Scheduling. In 2004/2005 he was a visiting professor at the Institut d'Informatique et Mathématiques Appliquées de Grenoble, at the Institut National Polytechnique de Grenoble, France.  His work has been funded by DoE/INL DoT/NIATT, DoD/OST and NIST 


States\nWAb-3.4: NEW RESULTS IN THE ANALYSIS OF DECISION-FEEDBACK 2118\nEQUALIZERS\nAhmed Mehana, Samsung Electronics, Co Ltd., United States; Aria Nosratinia, University of Texas at \nDallas, United States\nWAb-5: TARGET TRACKING II\nWAb-5.1: POSTERIOR DISTRIBUTION PREPROCESSING FOR PASSIVE 2125\nDTV RADAR TRACKING: SIMULATED AND REAL DATA\nEvan Hanusa, Laura Vertatschitsch, David Krout, University of Washington, United States\nWAb-5.2: DEPTH-BASED PASSIVE TRACKING OF SUBMERGED SOURCES  ............................................2130\nIN THE DEEP OCEAN USING A VERTICAL LINE ARRAY\nLisa Zurk, John K. Boyle, Jordan Shibley, Portland State University, United States\nWAb-5.3: GENERALIZED LINEAR MINIMUM MEAN-SQUARE ERROR 2133\nESTIMATION WITH APPLICATION TO SPACE-OBJECT TRACKING\nYu Liu, X. Rong Li, Huimin Chen, University of New Orleans, United States\nWAb-5.4: FEATURE-AIDED INITIATION AND TRACKING VIA TREE SEARCH ..........................................2138\nHossein Roufarshbaf Jill Nelson, George Mason University, United States\nxxxiii\nWAb-6: DIRECTION OF ARRIVAL ESTIMATION\nWAb-6.1: A SELF-CALIBRATION TECHNIQUE FOR DIRECTION 2145\nESTIMATION WITH DIVERSELY POLARIZED ARRAYS\nBenjamin Friedlander, University of California, Santa Cruz, United States\nWAb-6.2: CRAMER-RAO PERFORMANCE BOUNDS FOR SIMULTANEOUS  ..............................................2150\nTARGET AND MULTIPATH POSITIONING\nLi Li, Jeff Krolik, Duke University, United States\nWAb-6.3: COPY CORRELATION DIRECTION-OF-ARRIVAL ESTIMATION  .................................................2155\nPERFORMANCE WITH A STOCHASTIC WEIGHT VECTOR\nChrist Richmond, Keith Forsythe, MIT Lincoln Laboratory, United States; Christopher Flynn, Stevens nInstitute of Technology, United States\nWAb-6.4: LOCATING CLOSELY SPACED COHERENT EMITTERS USING 2160\nTDOA TECHNIQUES\nJack Reale, Air Force Research Laboratory / Binghamton University, United States; Lauren Huie, Air \nForce Research Laboratory, United States Mark Fowler, State University of New York at Binghamton, \nUnited States\nWAb-7: ENERGY- AND RELIABILITY-AWARE DESIGN\nWAb-7.1: LOW-ENERGY ARCHITECTURES FOR SUPPORT VECTOR 2167\nMACHINE COMPUTATION\nManohar Ayinala, Keshab K Parhi, University of Minnesota, United States\nWAb-7.2: TRUNCATED MULTIPLIERS THROUGH POWER-GATING FOR 2172\nDEGRADING PRECISION ARITHMETIC\nPietro Albicocco, Gian Carlo Cardarilli, University of Rome Tor Vergata, Italy; Alberto Nannarelli, \nTechnical University of Denmark Denmark; Massimo Petricca, Politecnico di Torino, Italy; Marco Re, \nUniversity of Rome Tor Vergata Italy\nWAb-7.3: A LOGARITHMIC APPROACH TO ENERGY-EFFICIENT GPU 2177\nARITHMETIC FOR MOBILE DEVICES\nMiguel Lastras Behrooz Parhami, University of California, Santa Barbara, United States\nWAb-7.4: ON SEPARABLE ERROR DETECTION FOR ADDITION ..................................................................2181\nMichael Sullivan, Earl Swartzlander, University of Texas at Austin, United States\nWPb-1: PAPERS PRESENTED IN 2012\nWPb-1.1 DYNAMICALLY RECONFIGURABLE AVC DEBLOCKING FILTER  .............................................2189\nWITH POWER AND PERFORMANCE CONSTRAINTS\nYuebing Jiang, Marios Pattichis, University of New Mexico\nxxxiv\n 


on science teams for numerous planetary missions including Magellan, Mars Observer, Mars Global Surveyor and Rosetta. He was the US Project Scientist for the international Mars NetLander mission, for which he was also principal investigator of the Short-Period Seismometer experiment, and is currently the Project Scientist for the Mars Exploration Rovers. He led the Geophysics and Planetary Geology group at JPL from 1993-2005, and is the JPL Discipline Program Manager for Planetary Geosciences. He has held several visiting appointments at the Institut de Physique du Globe de Paris. He has a BS in physics and a PhD in geophysics from the University of Southern California  David Hansen is a member of the technical staff in the Communications Systems and Operations Group at the Jet Propulsion Laboratory. Current work includes the development of the telecom subsystem for the Juno project. David received a B.S. in Electrical Engineering from Cornell University and an M.S. in Electrical Engineering from Stanford University  Robert Miyake is a member of the technical staff in the Mission and Technology Development Group at the Jet Propulsion Laboratory. Current work includes the development of thermal control subsystems for interplanetary flagship missions to Jupiter and Saturn missions to Mars and the Earth Moon, and is the lead Thermal Chair for the Advanced Project Design Team Robert graduated with a B. S. from San Jose State University, with extensive graduate studies at UCLA University of Washington, and University of Santa Clara  Steve Kondos is a consultant to the Structures and Mechanisms group at the Jet Propulsion Laboratory. He currently is generating the mechanical concepts for small Lunar Landers and Lunar Science Instrument packages in support of various Lunar mission initiatives. He also provides conceptual design, mass and cost estimating support for various Team X studies as the lead for the Mechanical Subsystem Chair. Steve is also involved with various other studies and proposals and provides mentoring to several young mechanical and system engineers. He graduated with a B.S. in Mechanical Engineering from the University of California, Davis and has 28 years of experience in the aerospace field ranging from detail part design to system of systems architecture development. He has worked both in industry and in government in defense, intelligence commercial and civil activities that range from ocean and land based systems to airborne and space systems. Steve has received various NASA, Air Force, Department of Defense and other agency awards for his work on such projects as the NASA Solar Array Flight Experiment, Talon Gold, MILSTAR, Iridium, SBIRS, Mars Exploration Rovers ATFLIR, Glory Aerosol Polarimeter System and several Restricted Programs  Paul Timmerman is a senior member of technical staff in the Power Systems Group at the Jet Propulsion Laboratory Twenty-five years of experience in spacecraft design including 22 at JPL, over 250 studies in Team-X, and numerous proposals. Current assignments include a wide variety of planetary mission concepts, covering all targets within the solar system and all mission classes. Paul graduated from Loras College with a B.S. in Chemistry in 1983  Vincent Randolph is a senior engineer in the Advanced Computer Systems and 


the Advanced Computer Systems and Technologies Group at the Jet Propulsion Laboratory. Current work includes generating Command and Data Handling Subsystem conceptual designs for various proposals and Team X.  He also supports Articulation Control and Electronics design activities for the Advanced Mirror Development project. Vincent graduated from the University of California at Berkeley with a B.S. in Electrical Engineering 18  pre></body></html 


i models into time and covariate dependent dynamic counterparts  ii models and reliability analysis in a more realistic manner  iii level  whether or not functional components \(loyal generals diagnose correctly and take proper actions such as fault mask of failed components \(traitors asymmetric  iv survivability analysis. Evolutionary game modeling can derive sustainable or survivable strategies \(mapped from the ESS in EGT such as node failures such as security compromise level modeling in the so-called three-layer survivability analysis developed in Ma \(2008a this article  v offer an integrated architecture that unite reliability survivability, and fault tolerance, and the modeling approaches with survival analysis and evolutionary game theory implement this architecture. Finally, the dynamic hybrid fault models, when utilized to describe the survival of players in EGT, enhance the EGT's flexibility and power in modeling the survival and behaviors of the game players which should also be applicable to other problem domains where EGT is applicable  5. OPERATIONAL LEVEL MODELING AND DECISION-MAKING  5.1. Highlights of the Tactical and Strategic Levels  Let's first summarize what are obtainable at both tactical and strategic levels. The results at both tactical and strategic levels are precisely obtainable either via analytic or simulation optimization. With the term precisely, we mean that there is no need to assign subjective probabilities to UUUR events. This is possible because we try to assess the consequences of UUUR events \(tactical level ESS strategies \(strategic level time prediction of survivability. The following is a list of specific points. I use an assumed Wireless Sensor Network WSN  i of UUUR events: \(a actions which can be treated as censored events; \(b Cont' of Box 4.2 It can be shown that the replicator differential equations are equivalent to the classical population dynamics models such as Logistic differential equation and LotkaVolterra equation \(e.g., Kot 2001 Logistic equation, or the limited per capital growth rate is similar to the change rate of the fitness  xfxfi which can be represented with the hazard function or survivor functions introduced in the previous section on survival analysis.  This essentially connects the previous survival analysis modeling for lifetime and reliability with the EGT modeling. However, EGT provides additional modeling power beyond population dynamics or survival analysis approaches introduced in the previous section. The introduction of evolutionary theory makes the games played by a population evolvable. In other words, each player \(individual 


other words, each player \(individual agent and players interact with each other to evolve an optimized system Box 4.3. Additional Comments on DHF Models  The above introduced EGT models are very general given they are the system of ordinary differential equations. Furthermore, the choice of fitness function f\(x complexity to the differential equation system.  The system can easily be turned into system of nonlinear differential equations. The analytical solution to the models may be unobtainable when nonlinear differential equations are involved and simulation and/or numerical computation are often required  In the EGT modeling, Byzantine generals are the game players, and hybrid fault models are conveniently expressed as the strategies of players; the players may have different failure or communication behaviors Furthermore, players can be further divided into groups or subpopulations to formulate more complex network organizations. In the EGT modeling, reliability can be represented as the payoff \(fitness, the native term in EGT of the game. Because reliability function can be replaced by survivor function, survival analysis is seamlessly integrated into the EGT modeling. That is, let Byzantine generals play evolutionary games and their fitness reliability function  The evolutionary stable strategy \(ESS counterpart of Nash equilibrium in traditional games ESS corresponds to sustainable strategies, which are resistant to both internal mutations \(such as turning into treason generals or nodes such as security compromises represent survivable strategies and survivability in survivability analysis. Therefore, dynamic hybrid fault models, after the extension with EGT modeling, can be used to study both reliability and survivability 13 risks such as competing risks which can be described with CRA; \(c captured with the shard frailty.  We believe that these UUUR events are sufficiently general to capture the major factors/events in reliability, security and survivability whose occurrence probabilities are hard or impossible to obtain  Instead of trying to obtain the probabilities for these events which are infeasible in most occasions, we focus on analyzing the consequences of the events.  With survival analysis, it is possible to analyze the effects of these types of events on survivor functions. In addition, spatial frailty modeling can be utilized to capture the heterogeneity of risks in space, or the spatial distribution of risks \(Ma 2008a d UUUR events introduced previously. These approaches and models that deal with the effects of UUUR events form the core of tactical level modeling  To take advantage of the tactical level modeling approaches it is obviously necessary to stick to the survivor functions or hazard functions models. In other words, survival analysis can deal with UUUR events and offer every features reliability function provides, but reliability function cannot deal with UUUR events although survivor function and reliability function have the exactly same mathematical definition. This is the junction that survival analysis plays critical role in survivability analysis at tactical level. However, we 


recognize that it is infeasible to get a simple metric for survivability similar to reliability with tactical level modeling alone. Actually, up to this point, we are still vague for the measurement of survivability or a metric for survivability. We have not answered the question: what is our metric for survivability? We think that a precise or rigorous definition of survivability at tactical level is not feasible, due to the same reason we cited previously  the inability to determine the probabilities of UUUR events However, we consider it is very helpful to define a work definition for survivability at the tactical level  We therefore define the survivability at tactical level as a metric, Su\(t t function or reliability function with UUUR events considered. In the framework of three-layer survivability analysis, this metric is what we mean with the term survivability. The "metric" per se is not the focus of the three-layer survivability analysis. It is not very informative without the supports from the next two levels  strategic and operational models.  However, it is obvious that this metric sets a foundation to incorporate UUUR effects in the modeling at the next two levels  Due to the inadequacy of tactical level modeling, we proposed the next level approach  strategic level modeling for survivability. As expected, the tactical level is one foundation of strategic level modeling ii objectives: \(a affect survivability which survival analysis alone is not adequate to deal with; \(b survivability at tactical level is necessary but not sufficient for modeling survivability, we need to define what is meant with the term survivability at strategic level  With regard to \(a behaviors or modes which have very different consequences. These failure behaviors can be captured with hybrid fault models. However, the existing hybrid fault models in fault tolerance field are not adequate for applying to survivability analysis. There are two issues involved: one is the lack of real time notion in the constraints for hybrid fault models \(e.g., N&gt;3m+1 for Byzantine Generals problem synthesize the models after the real-time notions are introduced. The solution we proposed for the first issue is the dynamic hybrid fault models, which integrate survivor functions with traditional hybrid fault models. The solution we proposed for the second issue is the introduction of EGT modeling  With regard to \(b modeling our problem at strategic level, EGT modeling is essentially a powerful optimization algorithm.  One of the most important results from EGT modeling is the so-called evolutionary stable strategies \(ESS We map the ESS in EGT to survivable strategies in survivability analysis.   Therefore, at the strategic level, our work definition for survivability refers to the survivable strategies or sustainable strategies in the native term of EGT, which can be quantified with ESS  In addition to integrating dynamic hybrid fault models another advantage for introducing EGT modeling at strategic level is the flexibility for incorporating other node behaviors \(such as cooperative vs. non-cooperative those behaviors specified in standard hybrid fault models, as well as anthropocentric factors such as costs constraints  Without UUUR events, both tactical and strategic level 


Without UUUR events, both tactical and strategic level models default to regular reliability models. This implies that, in the absence of UUUR events, reliable strategies are sustainable or survivable.  This also implies that three-layer survivability analysis defaults to reliability analysis however, the three-layer approach does offer some significant advantages over traditional reliability analysis, as discussed in previous sections. Nevertheless, when UUUR events exist, reliable strategies and survivable strategies are different. This necessitates the next operational level modeling  5.2. Operational Level Modeling and Decision-Making  When UUUR events are involved, we cannot make real time predictions of survivability at tactical and strategic levels This implies that the implementations of survivable 14 strategies need additional measures that we develop in this section.  Box 5.1 explains the ideas involved with possibly the simplest example  Figure 4 is a diagram showing a simplified relationship between action threshold survivability \(TS survivability \(ES view since both TS and ES are multidimensional and dynamic in practice. Therefore, the sole purpose of the diagram is to illustrate the major concepts discussed above The blue curve is the survivability when survivable strategies specified by ESS are implemented at some point before time s.  The system is then guaranteed to hold survivability above ES. In contrary, if no ESS implemented before time s, then the system quickly falls below to the survivable level at around 40 time units  T i m e 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 1 0 0 Su rv iv ab ili ty M et ric S u t 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 E S S  i s  I m p lm e n t e d N o  E S S  is  I m p lm e n t e d ts E S T S  Figure 4. A Diagram Showing the Relationship Between TS and ES, as well as timing of s and t, with s &lt; t  6. SUMMARY  The previous sections discussed the major building blocks 


The previous sections discussed the major building blocks for the new life-system inspired PHM architecture. This section first identifies a few minor aspects that have not been discussed explicitly but are necessary for the implementation of the architecture, and then we summarize the major building blocks in a diagram  6.1. Missing Components and Links  Optimization Objectives  Lifetime, reliability, fault tolerance, and survivability, especially the latter two, are application dependent. Generally, the optimization of reliability and survivability are consistent; in that maximization of reliability also implies maximization of survivability. However, when application detail is considered, optimization of lifetime is not necessarily consistent with the optimization of reliability. Consider the case of the monitoring sensor network as an example. The network reliability is also dependent on connectivity coverage, etc, besides network lifetime. What may be further complicated is the time factor. All of the network metrics are time-dependent. A paradoxical situation between lifetime and reliability could be that nodes never 'sleep                                                   


          Box 5.1 Operational Level Modeling  Assuming that the ESS solution for a monitoring sensor network can be expressed with the following simple algebraic conditions: survivability metric at tactical level SU = 0.7, Router-Nodes in the WSN &gt; 10%, Selfish Nodes &lt; 40%. Even with this extremely simplified scenario, the ESS strategies cannot be implemented because we do not know when the actions should be taken to warrant a sustainable system.  These conditions lack a correlation with real time  The inability to implement ESS is rooted in our inability to assign definite probabilities to UUUR events, which implies that we cannot predict when something sufficiently bad will jeopardize the system survivability What we need at the operational level is a scheme to ensure ESS strategy is in place in advance  The fundamental idea we use to implement the ESS strategy is to hedge against the UUUR events. The similar idea has been used in financial engineering and also in integrated pest management in entomology. This can be implemented with the following scheme  Let us define a pair of survivability metrics: one is the expected survivability \(ES threshold survivability or simply threshold survivability \(TS ES is equivalent to the survivability metric at tactical level. ES corresponds to ESS at strategic level, but they are not equivalent since ESS is strategy and ES is survivability. TS is the survivability metric value \(at tactical level and TS can be obtained from strategic level models. For example, TS = SU\(s t condition for the implementation of ESS. In other words, the implementation of strategies that ensures TS at time s will guarantee the future ES level at time t.  To make the implementation more reliable and convenient multiple dynamic TSs can be computed at time s1, s2 sk, with si &lt; t for all i.  These TS at times s1, s2, ..., sk should be monitored by some evaluation systems  Unlike tactical and strategic levels, the operational level modeling is approximate. The term "approximate means that we cannot predict the real time survivability or we do not know the exact time an action should be taken. Instead, the action is triggered when the monitored survivability metric SU\(r survivability \(TS scheme of TS and ES, we ensure the ES by taking preventative actions \(prescribed by ESS and triggered by the TS consequences of UUUR events  Figure 4 is a diagram showing the above concepts and the decision-making process involved 15 This wakefulness \(never 'sleep short period but at the expense of network lifetime. Of course, when the network is running out of lifetime, network reliability ultimately crashes. This example reminds us that 


reliability ultimately crashes. This example reminds us that multi-objective optimization should be the norm rather than exception  Constraints and Extensions  Many application specific factors and constraints are ignored in this article. For example, we mentioned about spatial heterogeneity of environment, but never present a mathematical description The spatial heterogeneity can be modeled with the so-called spatial frailty in multivariate survival analysis \(Ma 2008a  Evolutionary Algorithm  Evolutionary game modeling when implemented in simulation, can be conveniently implemented with an algorithm similar to Genetic Algorithms \(GA ESS in the evolutionary game model with simulation is very similar to GA. Dynamic populations, in which population size varies from generation to generation \(Ma &amp; Krings 2008f of node failures. Another issue to be addressed is the synchronous vs. asynchronous updating when topology is considered in the simulation. This update scheme can have profound influences on the results of the simulation. Results from cellular automata computing should be very useful for getting insights on the update issue  6.2. Summary and Perspective  To recapture the major points of the article, let us revisit Figure 3, which summarizes the principal modules of the proposed life-system inspired PHM architecture. The main inspiration from life systems is the notion of individuals and their assemblage, the population. Population is an emergent entity at the next level and it has emergent properties which we are often more concerned with. Survival analysis, which has become a de facto standard in biomedicine, is particularly suitable for modeling population, although it is equally appropriate at individual level. Therefore, survival analysis \(including competing risks analysis and multivariate survival analysis comprehensively in the context of PHM in a series of four papers presented at IEEE AeroSpace 2008 \(Ma &amp; Krings 2008a, b, c, &amp; d proposed architecture. Survival analysis constitutes the major mathematical tools for analyzing lifetime and reliability, and also forms the tactical level of the three-layer survivability analysis  Besides lifetime and reliability, two other major modules in Figure 3 are fault tolerance and survivability. To integrate fault tolerance into the PHM system, Dynamic Hybrid Fault DHF 2008e, Ma 2008a make real-time prediction of reliability more realistic and make real-time prediction of fault tolerance level possible DHF models also unite lifetime, reliability and fault tolerance under a unified modeling framework that consists of survival analysis and evolutionary game theory modeling  DHG models also form the partial foundation, or strategic level, for the three-layer survivability analysis. At the strategic level, the Evolutionary Stable Strategies \(ESS which is mapped to survivable or sustainable strategies, can be obtained from the evolutionary game theory based DHF models. When there is not any UUUR event involved reliability and survivability are consistent, and reliable strategies are survivable. In this case, the strategic level modeling up to this point is sufficient for the whole PHM system modeling, and there is no need for the next level  operational level modeling  When there are UUUR events in a PHM system, the 


When there are UUUR events in a PHM system, the inability to determine the occurrence probabilities of UUUR events makes the operational level modeling necessary Then the principle of hedging must be utilized to deal with the "hanging" uncertainty from UUUR events. In this case reliability strategies are not necessarily survivable strategies At the operational level modeling, a duo of survivability metrics, expected survivability \(ES survivability \(TS the survivable strategies \(ESS level are promptly implemented based on the decisionmaking rules specified with the duo of survivability metrics then the PHM system should be able to endure the consequences of potentially catastrophic UUUR events. Of course, to endure such catastrophic events, the cost may be prohibitively high, but the PHM system will, at least, warn decision-makers for the potentially huge costs.  It might be cheap to just let it fail  Figure 3 also shows several other modules, such as security safety, application systems \(such as Automatic Logistics CBM+, RCM, Life cycle cost management, Real-time warning and alert systems architectures, but we do not discuss in this paper. Generally the new architecture should be fully compatible with existing ones in incorporating these additional modules. One point we stressed is that PHM system can be an ideal place to enforce security policies. Enforcing security policies can be mandatory for PHM systems that demand high security and safety such as weapon systems or nuclear plant facilities.  This is because maintenance, even without human-initiated security breaches, can break the security policies if the maintenance is not planned and performed properly  In perspective, although I did not discuss software issues in this paper, the introduced approaches and models should provide sufficient tools for modeling software reliability and survivability with some additional extension. Given the critical importance of software to modern PHM systems, we present the following discussion on the potential extension to software domain. Specifically, two points should be noted: \(1 architecture to software should be a metric which can 16 replace the time notion in software reliability; I suggest that the Kolmogorov complexity \(e.g., Li and Vitanyi 1997 be a promising candidate \(Ma 2008a change is because software does not wear and calendar time for software reliability usually does not make much sense 2 software reliability modeling.  Extending to general survivability analysis is not a problem either. In this article I implicitly assume that reliability and survivability are positively correlated, or reliability is the foundation of survivability. This positive correlation does not have to be the case. A simplified example that illustrates this point is the 'limit order' in online stock trading, in which limit order can be used in either direction: that stock price is rising or falling.  The solution to allow negative or uncorrelated relationships between reliability and survivability are very straightforward, and the solutions are already identified in previous discussions. Specifically, multiple G-functions and multi-stage G-functions by Vincent and Brown \(2005 very feasible solution, because lifetime, reliability and survivability may simply be represented with multiple Gfunctions. Another potential solution is the accommodation of the potential conflicts between reliability and survivability with multi-objective GA algorithms, which I previously suggested to be used as updating algorithms in the optimization of evolutionary games  


 The integration of dynamic hybrid fault models with evolutionary game modeling allows one to incorporate more realistic and detailed failure \(or survival individual players in an evolutionary game. This is because dynamic hybrid fault models are supported by survival analysis modeling, e.g., time and covariate dependent hazard or survivor functions for individual players. If necessary, more complex survival analysis modeling including competing risks analysis and multivariate survival analysis, can be introduced.  Therefore, any field to which evolutionary game theory is applicable may benefit from the increased flexibility in modeling individual players.  Two particularly interesting fields are system biology and ecological modeling.  In the former field, dynamic hybrid fault models may find important applications in the study of biological networks \(such as gene, molecular, and cell networks 2008g conjecture that explains the redundancy in the universal genetic code with Byzantine general algorithm. In addition they conducted a comparative analysis of bio-robustness with engineering fault tolerance, for example, the strong similarity between network survivability and ecological stability \(Ma &amp; Krings 2008g survivability analysis can be applied for the study of survivals or extinctions of biological species under global climate changes \(Ma 2008b  In this paper, I have to ignore much of the details related to the implementation issues to present the overall architecture and major approaches clearly and concisely. To deal with the potential devils in the implementation details, a well funded research and development team is necessary to take advantages of the ideas presented here. On the positive side I do see the great potential to build an enterprise PHM software product if there is sufficient resource to complete the implementation. Given the enormous complexity associated with the PHM practice in modern engineering fields, it is nearly impossible to realize or even demonstrate the benefits of the architecture without the software implementation. The critical importance of PHM to mission critical engineering fields such as aerospace engineering, in turn, dictates the great value of such kind software product  6.3. Beyond PHM  Finally, I would like to raise two questions that may be interested in by researchers and engineers beyond PHM community. The first question is: what can PHM offer to other engineering disciplines? The second question is: what kinds of engineering fields benefit most from PHM? Here, I use the term PHM with the definition proposed by IEEE which is quoted in the introduction section of the paper  As to the first question, I suggest software engineering and survivability analysis are two fields where PHM can play significant roles. With software engineering, I refer to applying PHM principles and approaches for dealing with software reliability, quality assurance, and even software process management, rather than building PHM software mentioned in the previous subsection. For survivability analysis, borrowing the procedures and practices of PHM should be particularly helpful for expanding its role beyond its originating domain \(network systems that control critical national infrastructures is a strong advocate for the expansion of survivability analysis to PHM. Therefore, the interaction between PHM and survivability analysis should be bidirectional. Indeed, I see the close relationships between PHM, software engineering, and survivability as well-justified because they all share some critical issues including reliability survivability, security, and dependability  


 The answer to the second question is much more elusive and I cannot present a full answer without comparative analysis of several engineering fields where PHM has been actively practiced. Of course, it is obvious that fields which demand mission critical reliability and dependability also demand better PHM solutions. One additional observation I would like to make is that PHM seems to play more crucial roles for engineering practices that depend on the systematic records of 'historical' data, such as reliability data in airplane engine manufacturing, rather than on the information from ad hoc events.  This may explain the critical importance of PHM in aerospace engineering particularly in commercial airplane design and manufacturing.  For example, comparing the tasks to design and build a space shuttle vs. to design and manufacture commercial jumbo jets, PHM should be more critical in the latter task  17    Figure 2. States of a monitoring sensor node and its failure modes \(after Ma &amp; Krings 2008e     Figure 3. Core Modules and their Relationships of the Life System Inspired PHM Architecture    REFERENCES  Adamides, E. D., Y. A. Stamboulis, A. G. Varelis. 2004 Model-Based Assessment of Military Aircraft Engine Maintenance Systems Model-Based Assessment of Military Aircraft Engine Maintenance Systems. Journal of the Operational Research Society, Vol. 55, No. 9:957-967  Anderson, R. 2001. Security Engineering. Wiley  Anderson, R. 2008. Security Engineering. 2nd ed. Wiley  Bird, J. W., Hess, A. 2007.   Propulsion System Prognostics R&amp;D Through the Technical Cooperation Program Aerospace Conference, 2007 IEEE, 3-10 March 2007, 8pp  Bock, J. R., Brotherton, T., W., Gass, D. 2005. Ontogenetic reasoning system for autonomic logistics. Aerospace Conference, 2005 IEEE 5-12 March 2005.Digital Object Identifier 10.1109/AERO.2005.1559677  Brotherton, T., P. Grabill, D. Wroblewski, R. Friend, B Sotomayer, and J. Berry. 2002. A Testbed for Data Fusion for Engine Diagnostics and Prognostics. Proceedings of the 2002 IEEE Aerospace Conference  Brotherton, T.; Grabill, P.; Friend, R.; Sotomayer, B.; Berry J. 2003. A testbed for data fusion for helicopter diagnostics and prognostics. Aerospace Conference, 2003. Proceedings 2003 IEEE  Brown, E. R., N. N. McCollom, E-E. Moore, A. Hess. 2007 Prognostics and Health Management A Data-Driven Approach to Supporting the F-35 Lightning II. 2007 IEEE AeroSpace Conference  Byington, C.S.; Watson, M.J.; Bharadwaj, S.P. 2008 Automated Health Management for Gas Turbine Engine Accessory System Components. Aerospace Conference 2008 IEEE, DOI:10.1109/AERO.2008.4526610 


2008 IEEE, DOI:10.1109/AERO.2008.4526610 Environment Covariates &amp; Spatial Frailty Applications: AL; Life Cycle Mgmt; Real-Time Alerts CBM+, RCM, TLCSM; Secret Sharing and Shared Control 18 Chen, Y. Q., S. Cheng. 2005. Semi-parametric regression analysis of mean residual life with censored survival data Biometrika \(2005  29  Commenges, D. 1999. Multi-state models in Epidemiology Lifetime Data Analysis. 5:315-327  Cook, J. 2004. Contrasting Approaches to the Validation of Helicopter HUMS  A Military User  s Perspective Aerospace Conference, 2004 IEEE  Cook, J. 2007. Reducing Military Helicopter Maintenance Through Prognostics. Aerospace Conference, 2007 IEEE Digital Object Identifier 10.1109/AERO.2007.352830  Cox, D. R. 1972. Regression models and life tables.  J. R Stat. Soc. Ser. B. 34:184-220  Crowder, M. J.  2001. Classical Competing Risks. Chapman amp; Hall. 200pp  David, H. A. &amp; M. L. Moeschberger. 1978. The theory of competing risks. Macmillan Publishing, 103pp  Ellison, E., L. Linger, and M. Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013  Hanski, I. 1999. Metapopulation Ecology. Oxford University Press  Hallam, T. G. and S. A. Levin. 1986. Mathematical Ecology. Biomathematics. Volume 17. Springer. 457pp  Hess, A., Fila, L. 2002.  The Joint Strike Fighter \(JSF concept: Potential impact on aging aircraft problems Aerospace Conference Proceedings, 2002. IEEE. Digital Object Identifier: 10.1109/AERO.2002.1036144  Hess, A., Calvello, G., T. Dabney. 2004. PHM a Key Enabler for the JSF Autonomic Logistics Support Concept. Aerospace Conference Proceedings, 2004. IEEE  Hofbauer, J. and K. Sigmund. 1998. Evolutionary Games and Population Dynamics. Cambridge University Press 323pp  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Huzurbazar, A. V. 2006. Flow-graph model for multi-state time-to-event data. Wiley InterScience  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis. Springer. 481pp  Kacprzynski, G. J., Roemer, M. J., Hess, A. J. 2002. Health management system design: Development, simulation and cost/benefit optimization. IEEE Aerospace Conference Proceedings, 2002. DOI:10.1109/AERO.2002.1036148  Kalbfleisch, J. D., and R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data. Wiley-InterScience, 2nd ed  Kalgren, P. W., Byington, C. S.   Roemer, M. J.  2006 Defining PHM, A Lexical Evolution of Maintenance and Logistics. Systems Readiness Technology Conference 


Logistics. Systems Readiness Technology Conference IEEE. DOI: 10.1109/AUTEST.2006.283685  Keller, K.; Baldwin, A.; Ofsthun, S.; Swearingen, K.; Vian J.; Wilmering, T.; Williams, Z. 2007. Health Management Engineering Environment and Open Integration Platform Aerospace Conference, 2007 IEEE, Digital Object Identifier 10.1109/AERO.2007.352919  Keller, K.; Sheahan, J.; Roach, J.; Casey, L.; Davis, G Flynn, F.; Perkinson, J.; Prestero, M. 2008. Power Conversion Prognostic Controller Implementation for Aeronautical Motor Drives. Aerospace Conference, 2008 IEEE. DOI:10.1109/AERO.2008.4526630  Klein, J. P. and M. L. Moeschberger. 2003. Survival analysis techniques for censored and truncated data Springer  Kingsland, S. E. 1995. Modeling Nature: Episodes in the History of Population Ecology. 2nd ed., University of Chicago Press, 315pp  Kot, M. 2001. Elements of Mathematical Ecology Cambridge University Press. 453pp  Krings, A. W. and Z. S. Ma. 2006. Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks Military Communications Conference, 23-25 October, 7 pages, 2006  Lamport, L., R. Shostak and M. Pease. 1982. The Byzantine Generals Problem. ACM Transactions on Programming Languages and Systems, 4\(3  Lawless, J. F. 2003. Statistical models and methods for lifetime data. John Wiley &amp; Sons. 2nd ed  Line, J. K., Iyer, A. 2007. Electronic Prognostics Through Advanced Modeling Techniques. Aerospace Conference 2007 IEEE. DOI:10.1109/AERO.2007.352906  Lisnianski, A., Levitin, G. 2003. Multi-State System Reliability: Assessment, Optimization and Applications World Scientific  Liu, Y., and K. S. Trivedi. 2006. Survivability Quantification: The Analytical Modeling Approach, Int. J of Performability Engineering, Vol. 2, No 1, pp. 29-44  19 Luchinsky, D.G.; Osipov, V.V.; Smelyanskiy, V.N Timucin, D.A.; Uckun, S. 2008. Model Based IVHM System for the Solid Rocket Booster. Aerospace Conference, 2008 IEEE.DOI:10.1109/AERO.2008.4526644  Lynch, N. 1997. Distributed Algorithms. Morgan Kaufmann Press  Ma, Z. S. 1997. Demography and survival analysis of Russian wheat aphid. Ph.D. dissertation, Univ. of Idaho 306pp  Ma, Z. S. 2008a. New Approaches to Reliability and Survivability with Survival Analysis, Dynamic Hybrid Fault Models, and Evolutionary  Game Theory. Ph.D. dissertation Univ. of Idaho. 177pp  Ma, Z. S. 2008b. Survivability Analysis of Biological Species under Global Climate Changes: A New Distributed and Agent-based Simulation Architecture with Survival Analysis and Evolutionary Game Theory. The Sixth 


International Conference on Ecological Informatics. Dec 25, 2008. Cancun, Mexico  Ma, Z. S. and E. J. Bechinski. 2008. A Survival-Analysis based  Simulation Model for Russian Wheat Aphid Population Dynamics. Ecological Modeling, 216\(2 332  Ma, Z. S. and A. W. Krings. 2008a.  Survival Analysis Approach to Reliability Analysis and Prognostics and Health Management \(PHM  AIAA AeroSpace Conference, March 1-8, 2008, Big Sky, MT, 20pp  Ma, Z. S. and A. W. Krings. 2008b. Competing Risks Analysis of Reliability, Survivability, and Prognostics and Health Management \(PHM  AIAA AeroSpace Conference, March 1-8, 2008.  Big Sky, MT. 20pp  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(I Dependence Modeling", Proc. IEEE  AIAA AeroSpace Conference, March 1-8, 2008, Big Sky, MT. 21pp  Ma, Z. S. and A. W. Krings., R. E. Hiromoto. 2008d Multivariate Survival Analysis \(II State Models in Biomedicine and Engineering Reliability IEEE International Conference of Biomedical Engineering and Informatics, BMEI 2008.  6 Pages  Ma, Z. S. and A. W. Krings. 2008e. Dynamic Hybrid Fault Models and their Applications to Wireless Sensor Networks WSNs Modeling, Analysis and Simulation of Wireless and Mobile Systems. \(ACM MSWiM 2008 Vancouver, Canada  Ma, Z. S. &amp; A. W. Krings. 2008f. Dynamic Populations in Genetic Algorithms. SIGAPP, the 23rd Annual ACM Symposium on Applied Computing, Ceara, Brazil, March 16-20, 2008. 5 Pages  Ma, Z. S. &amp; A. W. Krings. 2008g. Bio-Robustness and Fault Tolerance: A New Perspective on Reliable, Survivable and Evolvable Network Systems, Proc. IEEE  AIAA AeroSpace Conference, March 1-8, Big Sky, MT, 2008. 20 Pages  Ma, Z. S.  and A. W. Krings. 2009. Insect Sensory Systems Inspired Computing and Communications.  Ad Hoc Networks 7\(4  MacConnell, J.H. 2008. Structural Health Management and Structural Design: An Unbridgeable Gap? 2008 IEEE Aerospace Conference, DOI:10.1109/AERO.2008.4526613  MacConnell, J.H. 2007. ISHM &amp; Design: A review of the benefits of the ideal ISHM system. Aerospace Conference 2007 IEEE. DOI:10.1109/AERO.2007.352834  Marshall A. W., I. Olkin. 1967. A Multivariate Exponential Distribution. Journal of the American Statistical Association, 62\(317 Mar., 1967  Martinussen, T. and T. H. Scheike. 2006. Dynamic Regression Models for Survival Data. Springer. 466pp  Mazzuchi, T. A., R. Soyer., and R. V. Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Millar, R.C., Mazzuchi, T.A. &amp; Sarkani, S., 2007. A Survey of Advanced Methods for Analysis and Modeling of 


of Advanced Methods for Analysis and Modeling of Propulsion System", GT2007-27218, ASME Turbo Expo 2007, May 14-17, Montreal, Canada  Millar, Richard C., "Non-parametric Analysis of a Complex Propulsion System Data Base", Ph.D. Dissertation, George Washington University, June 2007  Millar, R. C. 2007. A Systems Engineering Approach to PHM for Military Aircraft Propulsion Systems. Aerospace Conference, 2007 IEEE. DOI:10.1109/AERO.2007.352840  Millar, R. C. 2008.  The Role of Reliability Data Bases in Deploying CBM+, RCM and PHM with TLCSM Aerospace Conference, 2008 IEEE, 1-8 March 2008. Digital Object Identifier: 10.1109/AERO.2008.4526633  Nowak, M. 2006. Evolutionary Dynamics: Exploring the Equations of Life. Harvard University Press. 363pp  Oakes, D. &amp; Dasu, T. 1990. A note on residual life Biometrika 77, 409  10  Pintilie, M. 2006. Competing Risks: A Practical Perspective.  Wiley. 224pp  20 Smith, M. J., C. S. Byington. 2006. Layered Classification for Improved Diagnostic Isolation in Drivetrain Components. 2006 IEEE AeroSpace Conference  Therneau, T. and P. Grambsch. 2000. Modeling Survival Data: Extending the Cox Model. Springer  Vincent, T. L. and J. L. Brown. 2005. Evolutionary Game Theory, Natural Selection and Darwinian Dynamics Cambridge University Press. 382pp  Wang. J., T. Yu, W. Wang. 2008. Research on Prognostic Health Management \(PHM on Flight Data. 2008 Int. Conf. on Condition Monitoring and Diagnosis, Beijing, China, April 21-24, 2008. 5pp  Zhang, S., R. Kang, X. He, and M. G. Pecht. 2008. China  s Efforts in Prognostics and Health Management. IEEE Trans. on Components and Packaging Technologies 31\(2             BIOGRAPHY  Zhanshan \(Sam scientist and earned the terminal degrees in both fields in 1997 and 2008, respectively. He has published more than 60 peer-refereed journal and conference papers, among which approximately 40 are journal papers and more than a third are in computer science.  Prior to his recent return to academia, he worked as senior network/software engineers in semiconductor and software industry. His current research interests include: reliability, dependability and fault tolerance of distributed and software systems behavioral and cognitive ecology inspired pervasive and 


behavioral and cognitive ecology inspired pervasive and resilient computing; evolutionary &amp; rendezvous search games; evolutionary computation &amp; machine learning bioinformatics &amp; ecoinformatics                 pre></body></html 


