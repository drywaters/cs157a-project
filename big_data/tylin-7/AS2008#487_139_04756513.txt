Association Rules Mining Based on an Optimized Probability and Statistics Estimate Model  Yun Yu 1 Wei Chen 1 Chang Li 2  1 Wuhan Digital Engineering Institute 718 Luoyu Road, Wuhan, Hubei,430074, China 2 School of Remote Sensing and Information Engineering Wuhan University, Wuhan, Hubei,430079,  China realyuyun@gmail. com, realyuyun@163.com   Abstract  This paper has analysed the Apriori algorithm performance, and has pointed out performance bottleneck question of the Apriori algorithm. Currently those 
algorithms to mine association rules only pay attention to one aspect of efficiency or accuracy respectively. There is a paradox between efficiency and accuracy. In order to resolve to this conflict, a novel algorithm based on Probability estimate and least square estimate is proposed to mine the association rules from database with the high correlativity and the high conf idence. Probability estimate reduce the times of database scanning so as to increase efficiency; least square estimate is based on rigorous and classical mathematical model so as to enhance accuracy Furthermore, we deduce a recurrence formula to resolve K-itemsets issue. Experimental results have demonstrated 
that our algorithm is not only efficient but also keeps the completion of frequent items  1. Introduction  Association rule mining is an important embranchment of the artificial intelligence. Association rule in the military field, the most typical example of association rules is that it can be used to examine the failure of the military equipment. During the examination, one failure will often cause many failures. So during the diagnosis process, we need not only the default judge rules of fault 
in each subsystem but also the rules of transmission of failure information and interaction. Mining the association rules from a great deal of failures, we can get available association rules between failure and cause or get available association rules between one failure and another failure, and then we can provide important reference for eliminating the failure quickly and completely  1  There is a bottleneck question in classical Apriori algorithm that is frequent ly repeated scanning the transaction database 
 2][3 So we propose an algorithm based on probability estimate, which has an obvious improvement in efficiency. At the same time, in order to keep the completion of frequent items, that is to say accuracy, we utilize least square method to estimate parameters of probability model This paper is organized as follows: Section 2 gives a review of transitional Apriori algorithm, and analyses its performance. Section 3 presents our Probability model which increases the speed of data mining and decreases 
computerís I/O operation. In section 4, we propose an algorithm based on least square method to estimate parameters of probability model. Section 5 gives out the experimental results. And we compare Apriori algorithm with our algorithm. Finally, conclusions and future work will be illustrated in Section 6  2. Apriori algorithm  Now Apriori algorithm is core technology for other each kind of Boolean Mining association rules algorithm But the Apriori algorithm has two bottlenecks. Firstly, the algorithm performs as many passes over the database as 
the length of the longest itemsets. This incurs high I/O overhead for scanning large disk-resident database many times. For each item in the itemsets C k to verify it in the frequent itemsets L k must scans the database one time Secondly, it may come into being a large number of itemsets. The itemsets C k which produced by L k-1 grows rapidly 4][5  The most crucial step in the Apriori algorithm is finding frequent itemsets. The Apriori algorithm finds 
frequent itemsets by scanning the database frequently and it consumed too much time. It also may come into being a large number of item sets by JOIN operation and it consumed too much time too  In order to solve this issue, we present an algorithm based on probability estimate  3. Our Probability model  
2008 IEEE Pacific-Asia Workshop on Computational Intelligence and Industrial Application 978-0-7695-3490-9/08 $25.00 © 2008 IEEE DOI 10.1109/PACIIA.2008.21 3 
2008 IEEE Pacific-Asia Workshop on Computational Intelligence and Industrial Application 978-0-7695-3490-9/08 $25.00 © 2008 IEEE DOI 10.1109/PACIIA.2008.21 3 
2008 IEEE Pacific-Asia Workshop on Computational Intelligence and Industrial Application 978-0-7695-3490-9/08 $25.00 © 2008 IEEE DOI 10.1109/PACIIA.2008.21 3 


3.1. 2-itemsets probability algorithm  Association rules mining uses a support-confidence framework is useful for many applications. However, the support-confidence framework can be misleading in that it may identify a rule A B as interesting, when in fact, A does not imply B. In this section, we consider an alternative framework for finding interesting relationships between data items based on correlation Two events A and B are independent if    P AB P A P B  otherwise A and B are dependent and correlated. This definition can easily be extended to more than two variables. We can know that the correlation between A and B can be measured by computing      AB P AB corr P APB   1  If the   AB corr  is less than 1, A and B are negatively correlated, which means that one event rate restrain the occurrence of another If  AB corr is greater than 1, A and B are positively correlated, which means that one event rate promotes another If  AB corr  is equal to 1, A and B are independent each other, and there is no correlation between them 7   Obviously, we can conclude that min P AB P A P B   for correlations between each A and B Especially, in mining association rules, we only consider the situation of  1 AB corr  that all the event rate is positive correlation Suppose independent probabilities of each attribute item  12  n K KK  in Database are 12  n P PP   Simultaneous happening probability of arbitrary two attributes items i K  j K is ij P According to discussion above, in positive correlation, we have    min P BPA PAB PA PB   9   Then, we can deduce  111 1    ij j i j j i j i j P aP a PP a P PP PP      2  We can compute  1 a by following steps 1\canning mined database at the first time is in order to get each probability of 1-itemset 2\ning mined database at the second time is in order to get each probability of 2-itemset 3\alculate every 1 i a by expressions \(2\d then average to get 1 a   3.2. 3-itemset probability algorithm  There are four categories logistics facility in China such as Hub, Central Distribution Center, Regional Distribution Center and Distribution Center We only take the situation in positive correlation into account, and we have     m in        P C P AB P ABC P A P B P C   Then we can deduce 11 11 11 1  1  1  ijk ij ij ik ijk ij ij jk ijk ik ik jk P aP a PP P aP a PP P aP a P P        3  According to total probability formula 1    n ii i P XPXYPY   we can get  12 3 123 1 ijk ijk ijk ijk PkPkP kP kkk      4  Where  1 01 k   2 01 k   3 01 k  We use 123 1/3 kk k   in the calculation   3.3. Description of this algorithm  1. Scan the database, the algorithm simply scans all of the transactions in order to count the number of occurrences of each item. It can get frequent 1itemsets,L 1 It consists of the candidate 1-itemsets having minimum support. It can get each support of frequent 1itemsets, And Then the number of frequent 1-itemsets is m 2.To find L k a set of candidate k-itemsets is genetated by joining L k-1 with itself. This set of candidate is denoted C k The join 11 kk L L  is performed, where members of L k-1 are joinable if they have \(k-2\ms in common that is 11 1  2 kk k LLABABLABk     The algorithm uses 11 L L to generate a candidate set of 2itemsets C 2 And scan the database secondly. The set of frequent 2-itemsets, L 2 is then determined, consisting of those candidate 2-itemsets in C 2 having minimum support. And then the number of frequent 2-itemsets is f 3. From step one and two, the probability of K i is P i  and the probability of K j is P j when they happen at the same time, the probability of them is P ij We can get a k  from formula \(2\. Then  1 n k k a a n   and 2 m nC    4. The algorithm uses 22 L L to generate a candidate set of 3-itemsets, C 3  
4 
4 
4 


5. The algorithm uses formula \(4\ generate all occurrent probability of candidate 3-itemsets. It get candidate frequent 3-itemsets whose occurrent probability is greater than minimum support 6. Iterate the above process, estimate candidate frequent 4-itemsets and 5-itemsets and so on, and then analogy until get the longest candidate frequent itemsets 7. From step 6, we can get candidate frequent itemsets and then use it to scan the database to get the support of each candidate frequent itemsets; and then use the support to get the frequent itemsets; if the support of the candidate frequent itemsets is greater than the minimum or equal to it, we should get this frequent itemsets 8. From the frequent itemsets, we can get the associate rules However, we only average 1 i a  to get a which is not optima estimation. Thus, we propose to use least square method   4. Least square estimate  We have     min P C P AB P ABC P A P B P C  Then we can deduce  22221 112 2 2 1  1        ijk i k ij i k j i j i j kj ij kj ij kij kij i P aP a PP aP a P a P PP PP P P PP a aa P P PP a PP PP PP         5  Because, we have already known  ij P  after 2-itemset were computed. And  ij P  is substituend into ijk P so we can get ijk P   This is a non-linear function, so we expand Taylor formula to linearization      0 2 2 1 1 2 2 1 1 0 ijk ijk ijk ijk i ijk ijk ijk ijk P P da a P da a P v da a P da a P P P                 6  Where  T 12 n V v v  v   i v is a correction value  12  T da da  x  0  ijk P is an approximation for ijk P  and  11 12 12 nn kk kk  B             1 2 2 1 2 1 j i j k ij k j i j k j i j k ijk ijk i i P P P P a P P P P P P a P P P P a P a P k k             7  Generally speaking, this function model obeys normal distribution, whose probability density is  1 2 1/2 11  exp 2    2 n f     TD D  8  We introduce a random model 1 22 00 012\012   nn nn nn D Q P  Where  nn Q is a factor matrix \(inverse of weight matrix nn P  is a weight matrix. According to maximum likelihood estimation min  T VPV We get each element covariance by 2 0 012  ij ij DQ where LL Q=Q  2 0 2 n 012   T VPV  Program layout is as follow Figure 1  Figure 1  Program layout  We generalize an N-itemsets recurrence formula  12 1 12 1 2 12 1 1 12 1 12 1 2 1 2 1 12 2 123 2 3 2 3 12 12 1 m in            1        1        1     nn n n nn n n n n nnn n n n P AA A P A P AA A P A P A P A PAA A a PA a PA PAA A PAA A a PA a PA PAA A PAAA aPA a PA PAA PAA a               2121 1  PA a PA PA  9  5. Experiment  We have already achieved the Apriori algorithm which is in the environment of VC6.0, and we also program with VC++ to realize our algorithms. Then we compared them 
5 
5 
5 


in the same condition. The test condition is: P4 2.0 CPU 512M DDR; 80G HD and Windows xp-sp2 professional OS. And the test data that we use in the experiment is the failure detection history notes of certain system equipment which used by navy. The number of the total failure detection history notes is 103 638 notes. The minimum support is 2 the degree of confidence is 0.3. In the experiment, the failure detection history notes are different part of the all notes, and the numbers of them are 1◊10 4 2◊10 4 4◊10 4 6◊10 4 8◊10 4 1.0◊10 5 From figure 2, we can know the comparative result of Apriori algorithm and this algorithm In the execution speed. And from figure 3, we can know the comparative result in the quantity of rule sets Figure 2  Compare of implementation time  Figure 3  Compare of rules  From the experimental result, we can easily find that under the same conditions, if the notes in data sets are increasing, the executive time of Apriori algorithm and this algorithm will increase, but the slope of this algorithm is less than the slope of Apriori algorithm, the increaser of this algorithm is less than the increaser of Apriori algorithm. So the diffusibility of this algorithm is better than the diffusibility of Apriori algorithm. With the increase of items and notes, the difference will be more and more obvious. And in the experiment, we have found that it decreases I/O operation of computer and it only needs less memory In order to estimate parameter  a 5 8\, we can use sample mean, however this method is not optimal. Thus, we use least square method to estimate a   And from the experiment, we can prove that this algorithm is steady and effective. Experimental result show that ours spends a little more time than our algorithm with sample mean estimate, but algorithm results with least square estimate are  more approximate to Apriori algorithm results. Furthermore, the operating speed of algorithm with least square estimate is still fast and approximate to mean estimate. So we propose our algorithm based on probability model with least square estimate  6. Conclusions  In this paper, our probability model that is  based on probability estimate and least square estimate is proposed to mine the association rules from database. Our algorithm takes both efficiency and accuracy into account and it is proved and validated by experiment Sometimes, we need to know which factors have the most influence to an event rate and how much this influence is. Therefore, in the future, we will research principal factors leading up to even rate so as to distinguish correlativity in the association rules data mining  Acknowledgement  It is a project supported by Wuhan Digital Engineering Institute   References  1  ZHANG SU lan, HU Jun, ìA Fault Diagnosis Method Based on Simulation and Data Mining Computer Simulation January 2007 2  Srikant R, Agrawal R, ìMining Association Rules with Item Constraintsî. In Proceedings of the 1997 3rd International conference on Knowledge Discovery in Databases and Data Mining California :AAAI Press ,1997  
6 
6 
6 


3  M.Klemettinen, H.Mannila, P.Ronkiene, H.Toivonen, and AI.Verkamo, ìFinding Interesting Rules from Large Sets of Discovery Association Rules In  Proceedings of the 3rd International conference : On Information and Knowledge Management \(CIKM'94 PP401-407 Gaithersburg, Maryland, USA, November 1994  4  Tsur.D, Unman.J, Abiteboul.Setal, Query.Flocks, ìA Generalization of Association-Rule Mining [J    Proceedings of ACM SIGMOD International conference on Management of Data 1998  5  LIU Bing, HSUW, CHEN Shu, ìUsing General Impressions to Analyze Discovered Classification Rules C   California USA: Newport Beach, 1997: 31-36 6  AGRAWLR, SRIKANTR, ìFast algorithm formining association rules[C   Proceedings of International conference Very Large Database 1994,9: 487-499 7  FANG Wuyuan, LU  Jieping, XUAN Zhiyuan, ìAlgorithm for Reducing of Association Rules Generating Based on Correlation Journal of Jiangsu University of Science and Technology\(Natural Science Edition Vo1.21 No.1 February 2007  8  Jiawei Han, Micheline Kamber Data Mining Concepts and Techniques [M BeiJing: China M achine Press, P206P207 9  CHEN Jiangping, FU Zhongliang, XUZhihong, ìAn Impoved Algorithm of Apriori Geomatics and Information Science of Wuhan University February 2003 
7 
7 
7 


0 0.05 0.1 0.15 0.2 0.25 0.3 0 20 40 60 80 100 Error k Real data set Chess  Mushroom   Votes  0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0 100 200 300 400 500 600 700 800 Error k Synthetic data set tau=0.10  tau=0.05  Figure 5 Support estimation error increasing k  high when k is low We now turn our attention to the synthetic data set where we built models with much higher k  In this case error shows a slower rate of decrease The trend also seems asymptotic We can also see there is a clear gap between 002 0  05 and 002 0  10  error roughly grows 100 when 002 decreases by 50 Interestingly enough results are much better for real data sets than for the synthetic data set Error decreasing minimum support Figure 6 shows error growth as 002 decreases The left graph analyzes error for real data sets We can see error growth shows different behavior for each data set Error growth is slow for the Votes data set Error grows almost linearly for the Chess data set Error grows fast for the Mushroom data set The trend indicates we need to build more accurate models with higher k for Mushroom probably not for Chess and not necessary for the Votes data set The right graph analyzes error growth for the synthetic data set Theﬁrstmodelat k  800 is twice as accurate as k  400  We can see error grows linearly for high 002 values but it start growing faster at 002 0  1  The trend indicates the growth is not linear but it does not seem exponential 4.4 Comparing Speed and Scalability We rst compare our proposal versus the standard algorithm to mine association rules We then study time complexity and scalability Comparing clustering and A-priori Table 2 compares the efﬁci ency of the model with the standard A-priori algorithm We must stress our proposal does not intend to substitute fast association rule algorithms Table 2 Comparing model and A-priori 002 from model clustering+model A-priori 0.20 1 1672 24 0.15 1 1672 43 0.10 1 1672 156 0.05 3 1674 645 0.02 11 1683 3347 0.01 36 1708 14806 32 16 b u t w e i ncl ude t h es e c ompari s ons t o pro v i d e a rel ative performance benchmark We used the synthetic data with n 1 M  The clustering model used had the number of clusters set to k  100  These times include the time to compute exact support on a nal pass The rst column varies 002 the minimum support threshold The second column shows the time to discover frequent itemsets from the model excluding the time to compute the model The third column adds the time to compute the clustering model and the time to mine frequent itemsets Finally the fourth column shows the time for the traditional algorithm As can be seen the clustering model r epresents an efﬁcient mechanism to produce all frequent itemsets assuming the model is already tuned and stored That is we assume the model is computed a few times or even once The second column shows clustering the data set takes most of the time In this case the standard algorithm is faster at high support levels but the model becomes faster at low support levels Notice the third column represents a pessimistic case in which the model is recomputed every time Finally we can see the A-priori algorithm suffers scalability problems due to the exponential growth of patterns The basic reason the model is faster is because it uncovers long itemsets and it is efﬁciently manipulated in main memory 
614 
614 


0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0 0.1 0.2 0.3 0.4 0.5 0.6 Error tau Real data set Chess  Mushroom   Votes  0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0 0.05 0.1 0.15 0.2 Error tau Synthetic data set k=400  k=800  Figure 6 Support estimation error decreasing 002  0 100 200 300 400 500 600 700 0 200 400 600 800 1000 1200 1400 1600 Time in seconds n x 1000 Data set size d= 100  d=1000  0 50 100 150 200 250 300 0 20 40 60 80 100 120 140 160 Time in seconds k Number of clusters d= 100 n=100k  d=1000 n=100k  Figure 7 Time complexity for clustering large data sets with K-means 
615 
615 


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efﬁcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efﬁciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efﬁciency When the clustering model is available it is a signiﬁcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques Sufﬁcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efﬁcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efﬁcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conﬁdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conﬁdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conﬁdence The sufﬁcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efﬁcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efﬁciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufﬁciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70–81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207–216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487–499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145–154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146–153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9–15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597–600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern Classiﬁcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155–162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512–521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1–12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265–278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283–304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476–482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559–563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220–231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13–24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10–17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549–550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188–201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259–283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909–921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305–345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432–444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conﬁdence Intelligent Data Analysis  9\(4\:381–395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49–73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407–419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1–12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki Efﬁciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642–658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483–490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194–203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344–353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efﬁcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103–114 1996 
618 
618 


