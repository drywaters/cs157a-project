Abstract 
227Association rule discovery is one of kernel tasks of data mining Concept lattice induced from a binary relation between objects and features is a very useful formal analysis tool It represents the uni\036cation of concept intension and extension 
ARCA An Algorithm for Mining Association Rules based Concept Lattice Qing Xia Library Huaihai Institute of Technology Lianyungang China 222005 Email:xiaqing1@163.com Sujing Wang College of Computer Science and Technology Jilin University Changchun China 130012 Email sujingwang@hotmail.com Zhen Chen College of Computer Science and Technology Jilin University Changchun China 130012 Email chenzhen@jlu.edu.cn Tao Lv Weapon System Engineering Center Artillery Command Academy,XuanHua China 075100 Email lvtom@sohu.com Dongjing Wang Shizhuang Middle School Lianyungang China 222200 Email rpcwangdongjing@163.com 
It re\037ects the association between objects and features and the relationship of generalization and specialization among concepts There is a one-to-one correspondence between concept intensions and closed frequent itemsets This paper presents an ef\036cient algorithm for mining association rules based concept lattice called Arca Association Rule based Concept lAttice Arca algorithm uses concept-matrix to build a part of concept lattice in which the intension of every concept be put into one-to-one correspondence with a closed frequent itemset Then all association rules are discovered by 4 operators which are de\036ned in this paper performed on these concepts 
Index Terms 
I I NTRODUCTION Association rule mining from a transaction database has been a very active research area since the publication of the Apriori algorithm Se v eral impro v ements to the basic algorithm and many new approaches  ha v e been proposed during the last decade With the development of research Association rule discovery is one of kernel tasks of 
227Concept lattice rank of matrix formal concept analysis 
data mining Formal Concept Analysis FCA was developed by Pro Wille in 1982 Concept Lattice the core data structure in Formal Concept Analysis has been widely in machine learning data mining and knowledge discovery etc Every node of concept lattice is a formal concept consisting of extent and intent Concept lattice embodies the relations between extension and intension Here is a one-to-one correspondence between concept intensions and closed frequent itemsets There are various algorithms  of association rule mining using concept lattice However These algorithms need to build a complete concept lattice Based on CMCG algorithm  this paper presents an algorithm Arca of association rule mining using a part of concept lattice The paper is organized as follows Section 2 recalls basic de\036nitions of association rule and concept lattice Section 3 discusses Arca algorithm and four operator Section 4 gives an experimental evaluation on the time spent of Arca algorithm and Apriori algorithm Section 5 concludes the paper II T HE DEFINES OF ASSOCIATION RULE AND CONCEPT LATTICE 
I 
T 
i t 
TID 
002 
  
 
1 1 
I 
2 2 
k 
 
 
  
m t t t I I k 
 
I 
 
m n 
i t 
I 
 If 
from 
 
Let 212 
i t 
be a set of  then 
 the task-relevant data be a set of database transactions where each transaction is a set of items such that 
items.Let  Each transaction is associated with an identi\036er called  Each transaction consists of a set of items is called a 
I 
is said to contain 
I 
1 2 1 1 2 1 2 1 
I 
2 
Therule 
006 
003 004 005 003 007 
t I I I I I s s I 
 
002 
I 
I 
T T 
t 
 An association rule is an implication of ten form 
 A transaction 
and 
itemset I 
if and only if  where that contain 
I 
 where holds in the transaction set with support is the percentage of transactions in 
  
I I 
i.e both 
I 
Therule 
007 003  
if 
I I c c I I P 
 This is taken to be the probability 
T T 
I 
  
1 2 1 2 1 2 1 2 2 1 
I 
and has con\036dence 
I 
is the percentage of transactions in containing  This is taken to be the conditional probability 
P 
2 in the transaction set that also contain  That is 
I 
I I I 
  
1 2 1 2 1 2 2 1 
Example 2.1 
 
minconf 
002 
003 007 003  
P P minsupp I I minsupp 
For 
  
I 
t 
  
2 Given the user de\036ned minimum support 
I 
conf idence f requentitemset 
thresholds If the support of 
I I 
  
support T 
1 
I 
 
and minimum con\036dence 
be greater or equal to 
itemset is called a 
 
i 
 
 
is 978-1-4244-2108-4/08/$25.00  \251 2008 IEEE 1 
 
  
i 
 
  
t 
1 
2 
A B C D E A B C D E 
 
 
A data mining context is a triple 
 
t 
I I R 
T 
 
is a relation between 
n 
T 
 
b 
t 
n 
D I T R I T 
and and  each 
 
 
 
 t 
De\336nition 2.1 
 where 
 Table I represents a transaction database are two sets and 


002      m  B T i t t i f f t Y I i h f T I g T I B T C T B C C C C C C L C C B C C B C C C C I I I i i n i n r n m m C t T     1 1 1 2 2 1 1 2 1 1 2 2 1 1 2 1 2 1 3 1 1 2 1 2 2 1 R t for short a concept if and only if 002   T I R T I 2 1 2 2 2 2 3 2      D  C 135    t   R TABLE I A TRANSACTION DATABASE  TABLE II A TRANSACTION DATABASE    max   3 T I  g i   j I f C t Table II represents a data mining context corresponding with the transaction database showed in Table I Fig 2 a concept lattice for the context of Table II Fig 3 represents the concept-matrix of concept i I I AC     C De\336nition 2.6 978-1-4244-2108-4/08/$25.00  \251 2008 IEEE 2 of ABCDE  If i           I t i t i t i t i t i support support g            TID  De\336nition 2.4 t sinthe corresponding column of the item  D we say that the attribute n n  T   t I I t    if    m t I   j R T I I I I T I I minsupp i T g I I i of of t T C g      1   0 0 1       1          002 T I T i i t r     in context-matrix of 002 002 upper neighbors I R I R I R I R I R I R I R I R I R Example 2.2 Example 2.3 concept lattice Example 2.4 Example 2.5 superconcept subconcept support count frequent itemset De\336nition 2.2 De\336nition 2.3 De\336nition 2.5 De\336nition 2.7 De\336nition 2.8 De\336nition 2.9  each  or that  and and Let Let and Let and or and of and denoted by Let and Let Let Let  if the count of Let is denoted by is denoted by  Thus a data mining context can be represented by a matrix only with be a data mining context We de\036ne a function Dually we de\036ne denotes the set consisting of those objects in be a data mining context A pair is called a formal concept of be a data mining is denoted by are two concepts in We say that be a data mining context is called a lower neighbor of be a set of items from be a set of items from be a data mining context For given a item we say that the rank of item be a data mining context The concept-matrix of  In this paper  We say that the matrix is the context-matrix of that produces the set of their common attributes for every set of objects to know which attributes from that have all the attributes from are two concepts in and there is no concept  denoted by  denoted by is an attribute of the object veri\036es Fig 1 a context-matrix of the data mining context showed in Table II are common to these entire objects for subset of attributes is called extent is called intent  An partial ordering relation 241 is de\036ned on B\(D by is called a is called a and the partial ordering relation 241 form a compete lattice called the is called a of the itemset  I is called a in the concept-matrix of in concept-matrix of        b 004 BC E 3  is called an attribute In a data mining context in in is is is is D D D D D D T T           T,I T,I   I R I 110110 201101 311101 401001 511101  t wesay that the rank of the data mining contgext BE 5  t is the matrix consisting of these rows what are the corresponding rows of the each element in set ACD 2  Iterms   C C C ful\036lling T   t 013 004 013 f r     and  The set of all lower neighbors of a given concept is a subset of the set consisting of all subconcepts of it ABC E 4  ABC E  C   I If C The     I D T D T I T D T D D T D D D T D D D D T D D D D T i If    1  Fig 1 A context-matrix of the data mining context showed in TableII called an object  These two functions are used to determine a formal concept Fig 2 A Concept lattice for the context of TableII context The set of all concepts of   Let      


 r r         and NOT t 4 r  1                 D T D T I D T T T D T D T D T I I r        C  T of     t C B i C n i C n R n m C m C m C B I T i C B minsupp C I minsupp T I C B C m i g C C C T C m m C C C B C m g C i R m C C C C T C m C C T g C g g C C r I I r I r r C minsupp minsupp  C T C T B C r r  r r r I r r I r I I r r And For i T T R g g T T T T C T g T T C T g g g T I I I g minsupp m minsupp   0 Wehave  De\336nition 2.10 Property 2.1 De\336nition 2.11 De\336nition 2.12 Property 2.2 Proof Proof Proof De\336nition 2.13 Example 3.1          i  f I f I I I I I I I I minsupp  By Property 2.1 we have  Then  I    be a data mining context is a concept in s in the corresponding column of the item is equal or lesser than is a concept in Given subset be a data mining context is a concept in is a frequent concept be a data mining context is a concept in be a data mining context is a concept in  where implies that  where  where be a data mining context are two concepts in  the minimum supports of  respectively If  The minimum support of and its minimum support 978-1-4244-2108-4/08/$25.00  \251 2008 IEEE 3  T    T  g T  T 1    T 2  T 1    T 2  R r 005 R t C De\336nition 3.1 De\336nition 3.2 I frequent concept Theorem 2.1 Theorem 2.2 subconcept             T I I If r I C C C C C C C C C C C C C C C C   12 I R I R I R I R I R I R I I I  016 t I I   minsupp basic association rules C  I For Let C 2 2 1 3 2 1 3 2 1 2 1 2 1 2 3 T,I T,I T,I T,I T,I T,I T,I  I I  m I I     135    t 004 002 016 016 r   t  017 004 005 007  007 007  r 016 003 212 020 017 020 017 007 020 017 017 327 020 017 007 007 020 017 t I C I I I 1 1 1 1 1 1 2 2 1 2 2 2 2 1 1 1 1 1 1 1 2 2 2 2 2 1 2 1 3 3 1 3 1 1 3 3 2 2 2 2 1 3 1 1 3 1 3 1 1 2 1 1 2 1 2 1 2 1 2 1 1 2 1 1 1 2 2 1 2 2 1 2 1 1 2 2 3 4 1 2 1 2 3 1 2 1 2 1 4 1 4 3 4 5 1 2 3 3 4 5 1 2 3 1 2 3 1 6 I g BE BE 003 BE BCE 003 BE  If             327 I T  g    AC minconf minconf minconf conf conf conf conf conf idence conf idendce Let Let   I I     where    t  Then    conf 004 C,m   C C,m       If If 006 Example 3.2 t 016 minconf AC 003   D D D D D D so  016 n   016 004 004 I Let  If the count of The count of objects of subconcept of concept Let Let  Then  The rank of concept and and and and Let and and Let and and and and and  327  we say that the rank of item  denoted by  we say that the rank of concept be a data mining context  The rank of concept is a lower neighbor of  We can obtain is a lower neighbor of is a lower neighbor of  On the other hand is a lower neighbor of is called the antecedent of  Below we de\036ne the support and con\036dence of an association r holds is de\036ned as   The operator 222+\222 can be implemented on is de\036ned as  the operator 222+\222 can be implemented on denotes the set consisting of those transactions in that have all the itemsets from is called a is a frequent itemset   Suppose there exist  where  This result contradicts with Suppose there exist  it implies that  This result contradicts with NOT An association rule is an implication between itemsets of the form Fig 4 represents a concept lattice while is called a Table III represents basic association rules from Fig 4 with are and its minimum supports is I i i I I I i i g i i i i g i i T i i i i i r I r I I I r r so    max   is in concept-matrix of concept is is is a frequent itemset is is is a is called the consequent of        Fig 3 The concept-matrix of concept in the concept-matrix of TABLE III B ASIC ASSOCIATION RULES FROM F IG 4 WITH 4/5 002\003 4/5 3/4 3/4 3/4 2/3 2/3  b 016 C C     The minimum support of      therule 0 4 0 5                                                  I     0 conf If  i C I 4 5  Then  Then  Basic association rules can be mined from concept lattice      basic association rule minimum support  002\003  C  t    n t    n   I T I   I 0 I I 5 Mining association rules is to 036nd all rules  A A By De\036nition 2.10 And De\036nition 2.11 we have g   C C 003 C 003 C r m i t i support support support support    there NOT exist Fig 4 A concept lattice while  III A RCA ALGORITHM When a concept lattice is built each concept   


222 on basic association rules in Table III Using the following example S S 002 002 002 002 002 t              1 2 3 1 2 1 2 3 1 2 3 1 1 1 1 2 1 023\006 I g I I I I I minsupp I I I g I I I 327 327 020 017 020 017 007 021 020 017 007 017 007 327 020 017 007 007 021 021 020 017 007 007 327 327 020 017 017 007  005 007 022 022 020 017 017  022 004  t 023 023 023 004 006 If I I r I T 021 021 021 021 021 021 021 017 017 021   024 023 007 n 023 007   ABE I a 006 006 006 006 t 020\006\017 020\006\017  T  T I I C C C C C C I T   005 I I if  T 1 1 2 2 3 4 1 2 1 2 3 1 2 1 2 1 2 4 1 2 4 3 4 5 1 2 3 3 4 5 1 2 3 1 2 3 1 2 4 6 1 1 2 2 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 2 1 I     TABLE IV T HE RESULT OF PERFORMING 222+\222 AND 222 I I   conf conf conf conf conf conf conf conf conf conf conf conf conf minconf minconf Queue.EnQueue Queue.DeQueue minconf Queue.EnQueue subnodes subnodes subnodes subnodes  Extent Extent Intent Intent AC ABCE C minsupp minsupp 1 subnodes for all  I  I I I 022 do do if do do AC Algorithm 1 Algorithm 2 right move r Queue m S  002\003 r r while AC 023 016 023 SUBNODES  R  R 023\006 the set consisting of a attribute I I I I I from 021\020 222on basic association rules in Table III r r 123 12 123 BCE BE BE BE C BE C BE 002\003 002\003 002\003 002\003 002\003 002\003 002\003 002\003 002\003 007  1 1 1 1 1 2 2 2 while BE BCE BE BE BasicRules BasicRules BasicRules BasicRules   021\020\006\017 I I   D b 222  212 020 while T T,I 006 r I I  I r r r I r r I r I I r r C A r r minsupp Queue BasicRules C C C C r C C minsupp minsupp subnodes M C M m C S m I a M S T T m I 003 003 003 I AC ABCE ABE C ABCE AC C ABE ABCE  conf 023 022 023 002\002 002\002 002\002 002\002 De\336nition 3.3 De\336nition 3.4 De\336nition 3.5 Let and and and and 222 can be implemented on and Let Let the rank of concept and these attributes from Example 3.3  The minimum support of and its minimum support is 222 cannot be implemented on rule and rule  The antecedent of rule be equal to BE Therefore the operators 222+\222 and 222 222 cannot be implemented on rule and rule  All association rules which con\036dence be less than 1 are mining out by performing four operators on basic association rules The pseudo codes are given by Algorithm 1  and data mining context D such that  minimum support  and data mining context D the set of attributes which ranks equal to which corresponding columns are same as column of  The operator 222+\222 can be de\036ned among n rules by same way  respectively If  The operator 222 222 can be implemented on is de\036ned as  the operator 222 222 can be de\036ned among n rules by same way Let\222s examine how to perform 222+\222 and 222 Since  the union of the antecedent and consequent of rule  be not equal to the antecedent of rule  the operators 222+\222 and 222  minimum con\036dence SUBNODES Input given a concept of concepts which extent\222s count be greater or equal to the concept matrix of concept  such that NOT Basic rules The result of 222+\222 The result of 222 and its minimum supports is 6 7 Every basic association rule after rule is check whether the operators 222+\222 and 222 222 can be implemented on rule and itself and not yes Therefore we can obtain 8 9  The result of performing operator is a set of association rules  The result of performing operator decompose is a set of association rule Input minimum support Output a set of basic association rules Initialize a queue generate a rule Output:a set compute the rank of every attribute in 11 Table IV represents the result of performing 222+\222 and 222     Queue 016 017  T  004 10 is in   I I I A I S g C m I I I  GenBasicRules D I I                    Require Ensure then if then end if end if end for end while return Require Ensure then end if end while end while return 004 003 003 003 003 003  are   Let    The operator 222 Algorithm 1 is to generate basic association rules by build a concept lattice called function SUBNODES The pseudo codes of this function are given by Algorithm 2 978-1-4244-2108-4/08/$25.00  \251 2008 IEEE 4   002 002 002 023  212 021\021\020\006\017 r I  C A A 222 ON BASIC ASSOCIATION RULES IN T ABLE III  the minimum supports of    I 


minconf 01 minconf 5 IV E VA L UAT I O N In order to evaluate we implement Algorithm Arca and Algorithm Aprior by Visual C and STL The data set generated randomly by IBM dataset generator have 1000 items and 10000 transactions The result shows that the performance of Arca is as four times higher as Aprior on average Fig 5 represents Running time of algorithms versus lattice size with 0 0  Fig 5 Running time of algorithms with  V C ONCLUSION Now there are many algorithms of mining association rules There is a one-to-one correspondence between concept intensions and closed frequent itemsets Concept lattice is a good tool for mining association rules R EFERENCES  Agra w al R Imielinski T  Sw ami A Mining association rules between sets of items in large A CM SIGMOD Record 1993 22 207-216  Sa v asere A Omiecinski E Na v athe S An ef 036cient algorithm for mining association rules in large Proceedings of the 21st VLDB Conference the 21st VLDB Conference Zurich 1995 432-444  P ark J S Chen M S Y u P S An ef fecti v e hash-based algorithm for mining association A CM SIGMOD Record  A CM Press 1995 24 175-186  Zaki M J et al Ne w algorithms for f ast disco v ery of association rules[R University of Rochester  1997  Sheno y P  et al T urbo-char ging v ertical mining of lar ge databases[C ACM SIGMOD Record ACM Press 2000  22-33  P asquier N et al Disco v ering frequent closed itemsets for association  Proceeding of the 7th International Conference on Database Theory  The 7th International Conference on Database Theory Jerusalem 1999 Berlin Springer-Verlag 1999 1540 398-416  Pei J Han J  Mao R An ef 036cient algorithm for mining frequent closed  2000 A CM SIGMOD W orkshop on Research Issues in Data Mining and Knowledge Discovery Dallas 2000 21-30  Han J et al FreeSpan frequent pattern-projected sequential pattern min Proceedings of the s ixth A CM SIGKDD international conference on Knowledge discovery and data mining The sixth ACM SIGKDD international conference on Knowledge discovery and data mining 2000 Boston  ACM Press 2000 355-359  Han J et al Pre\036xSpan mining sequential patterns ef 036ciently by pre\036xprojected pattern gro The Proceedings of 17th International Conference The17th International Conference 2001 215-224  Han J  Pei J Y in Y  Mining frequent patterns without candidate  2000 A CM SIGMOD W orkshop on Research Issues in Data Mining and Knowledge Discovery Dallas 2000 1-12  Godin R Missaoui R An incremental concept formation approach for learning from Theoretical Computer Science 1994 133 387-419  V altche v P  Missaoui R Lebrun P  A partition-based approach to w ards constructing Galois concept Discrete Mathematics 2002 256\(3  P asquier N Bastide Y  T aouil R Lakhal L Disco v ering frequent closed itemsets for association Proceeding of the 7th International Conference on Database Theory  The 7th International Conference on Database Theory Jerusalem 1999 Berlin Springer-Verlag 1999 1540 398-417  Stumme G Ef 036cient data mining based on formal concept analysis[C Proceeding of the 13th International Conference Database and Expert Systems Applications  13th International Conference DEXA 2002 Aixen-Provence 2002 Berlin Springer-Verlag 2453:534-547  P asquier N Bastide Y  T aouil R Lakhal L Ef 036cient mining of association rules using closed Information System Else vier  1999 24\(1 25-46 16 Stumme G Taouil R Bastide Y Pasquier N Lakhal L Fast Computation of Concept Lattices Using Data Mining T  Proceeding of 7th Intl W orkshop on Kno wledge Representation Meets Databases KRDB\22200 The 7th Intl Workshop on Knowledge Representation Meets Databases KRDB\22200 Berlin 2000  Sujing W ang Zhen Chen and Dongjing W ang An Algorithm based on Concept Matrix for Building Concept Lattice with Proceeding of 2007 International Conference on Wireless Communications Networking and Mobile Computing  2007 International Symposium on Information Systems And Management Shanghai 2007 Volume 8 55935596 978-1-4244-2108-4/08/$25.00  \251 2008 IEEE  01 


In other words, an item would only appear at most one time in the i th row during a single round. In this way, we have few linked lists including n items if the hash function H is sufficiently uniform. The total memory required is again 000\013\000\014 On  In summary, the time an d space complexity of the candidate-generation process in our SERIT are asymptotically the same as BTH’s 4. experiment results In this section, we report the results of testing our SERIT on several data sets. Compar ing with algorithm TAPER and BTH, SERIT is proved to be effective. Bounded with a small false-negative tolerance 000W as 0.005, SERIT can not only prune the uncorrelated pairs efficiently to generate a small candidate set and save computi ng resources; but also cover the shortcoming of BTH, still achieving small running time when 000T is relatively small The experiment bases on both synthetic and real data sets We choose the famous “retail” data set, which contains data from a retail store and can be downloaded from FIMI website. The tool mentioned in [2 is also use d h e re  to generate three synthetic data sets S 1 S 2 and S 3 Table 3 lists the characteristics of these data sets. And table 4 shows the values of parameter k and t in algorithm SERIT and BTH for different 000T   T ABLE 3 THE CHARACTERISTICS OF DATA SETS  Data Set Num of Items Num of Records retail 16470 88163 S 1 20589 51316 S 2 33052 51292 S 3 42522 51337 All the experiments were performed on a personal computer, with a 2.0 GHz CPU and 512 Mbytes of memory running the windows XP operating system In section 5.1, we give th e executing time of the three algorithms, including candidate-generation running time and overall running time. We then show in section 5.2 the scalability of SERIT T ABLE 4   THE VALUES OF PARAMETER K AND T   Algorithm SERIT Algorithm BTH 000T  k t k t 0.8 7 8 2 10 0.7 11 6 2 18 0.6 14 6 2 38 0.5 17 6 2 47 0.4 28 4 2 204 0.3 30 4 2 370 0.2 110 4 2 2940 0.1 410 2 2 50000 4.1. The executing time  All the three algorith ms need two steps: 1\ generation of the candidate set 2\refinement. As a result, there are two types of executing time, candidate-generation running time and overall running time, which may present different properties. Although an algorithm executes efficiently during candidate-generation stage, it may produce a large candidate set inducing the incr ease of overall executing time The coefficient thresholds are sp lit to two parts with 0.4 as a dividing point. Figure 5 illust rates the candid ate-generation time, while Figure 6 shows the overall executin g time, both based on “retail” data set The corresponding executing time on S 1 is presented in Figure 7 and 8. Note that when 0.1 000T 000  we find that the executing time including the two types\ of BTH is too long, much longer than that of other algorithms Moreover, it is also much longer than the time when 0.2 000T 000  For the rationality of experiment and brevity to present, we don’t illustrate the execu ting time of BTH with 0.1 000T 000  From Figure 5~8\(a\s small executing time when 0.4 000T 000d And the advantage becomes more obvious when 000T  is smaller. Oppositely, the running time of BTH increases sharply with the decrease of 000T This is mainly due to the very large numbers of min-hash values caused by large t. Although BTH merely chooses two items in the same equivalence class to generate a candidate pair and the method in SERIT seem s a little complex, SERIT is still faster due to the r eason mentioned above When 0.5 000T 000t BTH computes less min-hash values, and consequentially it has smaller running time. SERIT is slightly slower than BTH in most cases, but always faster than TAPER. This is determin ed by the property of the probability function used in SERIT: With a big 000T the item pair whose correlation coefficient is smaller than 000T would be chosen into the candidate set in a relatively high probability \(compared with BTH\In this way, SERIT may have a slightly bigger candidate set than BTH From further analysis, the other two is much powerful than TAPER to remove pairs unwanted. As a result of the large candidate set generated in TAPER, in the refinement step more time is required. As shown in Figure 7 \(b\d 8 b\ the execution of TAPER can be one order of magnitude slower Moreover, comparing the experimental results on “retail with S 1 we can conclude that the size of item set has greater influence on TAPER than on SERIT and BTH. The running time increases obviously with the growth of item set 
424 
418 


0.10 0.15 0.20 0.25 0.30 0.35 0.40 0 100 200 300 400 500 600 BTH TAPER SERIT Execution Time \(sec Threshold  Fig 5\(a\ candidate-generati on time on “retail” with 0.4 000T 000d  0.50 0.55 0.60 0.65 0.70 0.75 0.80 5 10 15 20 25 30 Execution Time \(sec Threshold BTH TAPER SERIT  Fig 5\(b\ candidate-generati on time on “retail” with 0.5 000T 000t  0.10 0.15 0.20 0.25 0.30 0.35 0.40 0 100 200 300 400 500 600 BTH TAPER SERIT Execution Time \(sec Threshold  Fig 6\(a\ overall executing time on “retail” with 0.4 000T 000d  0.50 0.55 0.60 0.65 0.70 0.75 0.80 20 40 60 80 100 120 140 160 180 200 Execution Time \(sec Threshold BTH TAPER SERIT  Fig 6\(b\ overall executing time on “retail” with 0.5 000T 000t  0.10 0.15 0.20 0.25 0.30 0.35 0.40 0 100 200 300 400 500 600 BTH TAPER SERIT Execution Time \(sec Threshold  Fig 7\(a\ candidate-generation time on S 1 with 0.4 000T 000d  0.50 0.55 0.60 0.65 0.70 0.75 0.80 5 10 15 20 25 30 35 40 45 Execution Time \(sec Threshold BTH TAPER SERIT  Fig 7\(b\ candidate-generation time on S 1 with 0.5 000T 000t  0.10 0.15 0.20 0.25 0.30 0.35 0.40 0 100 200 300 400 500 600 700 800 900 BTH TAPER SERIT Execution Time \(sec Threshold  Fig 8\(a\ overall executing time on S 1 with 0.4 000T 000d  0.50 0.55 0.60 0.65 0.70 0.75 0.80 50 100 150 200 250 300 350 400 450 Execution Time \(sec Threshold BTH TAPER SERIT  Fig 8\(b\ overall executing time on S 1 with 0.5 000T 000t  4.2. The scalability of algorithm SERIT We use synthesized data sets list in Table 3 to examine the scalability of SERIT from the vi ew point of ex ecution time They have different sizes of item set. Figure 9 shows that the execution time increases with the number of items. This conclusion is consistent with th e complexity analysis shown in section 3 
425 
419 


2.0 2.5 3.0 3.5 4.0 4.5 40 60 80 100 120 140 160 Execution Time \(sec Num of items              X 10 4 Thre0.1 Thre0.3 Thre0.5 Thre0.7  Fig 9. scalability of algorithm SERIT 5. Conclusion and future work In this paper, given a coefficient threshold, we design and implement the algorithm SERIT which can find the correlated item pairs through efficient pruning in a massive market-basket data set. The method proposed by Zhang et al   o re suitable when 000T is relatively big. Hence, a new problem is generated when 000T is small. Our SERIT introduces a new probability functio n to select candidate pairs Experimental results show that the space and time complexity are asymptotically the same as BTH’s. In ad dition, with a relatively small 000T  0.4 000d eds a much smaller and more reasonable kt 000  than BTH. Theref ore, SERIT achieves much larger saving of computational resources  There are several potential dir ections in future research First, we will examine whether it is more efficient to combine SERIT and TAPER. However, the two different pruning methods need different pre-proces sings. In particular, we have to consider the trade-off between a more complex pre-processing and efficiency Second, we only considered pairs of items in this paper How about a correlated item set Also, there exist several other cr iterions to measure correlation association\, which may have peculiar properties  A CKNOWLEDGMENT  This work is supported by the National High-Tech Research and Development Plan of China \("863" plan\nder Grant No  2006AA01Z451 and No. 2007AA01Z474 6. References   Hui Xiong, Shashi Shekhar, P. N. Tan, and Vipin Kumar Exploiting a support-based upper bound of Pearson’s correlation coefficient for efficiently identi fying strongly correlated pairs KDD’04, pp. 334–343, August 22-25, 2004, USA   J i an Zh ang and J o an F e ig e nbaum, “Findind highly correlated pairs efficiently with powerfu l pruning”, CIKM’06, pp. 152–161 November 5-11,2006, USA  Hui Xiong, Mark Brodie a nd Sheng Ma, “TOP-COP: mining TOP-K strongly correlated pairs in large database”, ICDM’06, pp 1162–1166  R.Agrawal  T. Im i e linski   a nd A. S w am i   M ining associa tion rules between sets of items in large databases SIGMOD’93, pp 207-216  C. Jerm aine, “ Th e com putational complexity of high-dimensional correlation s earch”, ICDM’01, pp. 249-256  C.Bucila, J Gehrke, D  Kif e r and W. M. White, “ Dualminer: a dual-pruning algorithm for itemsets with constrains”, SIGKDD’02 pp. 241-272  E. Cohen   S ize-estim at i on fram e work with appl ic atio ns to transitive closure and reachability Journal of Computer and System Sciences, 1997, pp. 441-453  C. Jermaine, “Play i ng hide-a nd-seek with correlations SIGKDD’03, pp. 559-564  S  Brin R  M o tw ani and C S ilvers t ein   B e y ond m a rket bas k ets   Generalizing association rules to correlations”, SIGMOD’97, pp 265-276  Yu Li Miroslav, and  Kubat, “Searching for high-su pport itemsets in itemset trees”, Intellig ent Data Analysis, , March, 2006 Volume 10, Issue 2, pp. 105-120  H. T. R e ynolds The an aly s is of cross-classifications”, The Free Press, New York, 1977   Toon Cad e rs, B a rt Goeth a ls and S. Jaroszewicz, “Mining rank-correlated sets of numerical attributes KDD’06, August 20-23 2006, USA, pp. 96-105  C. K. S Leung, and Qu amru l l. Khan, “Efficient mining of constrained frequent patterns fro m Streams”, IDEAS’06, pp. 61-68  Motwani  E Cohe n, M Da ta r, S. Fujiwa re A Gionis P Indyk, J. Ullman, and C.Yang, “Finding interesting associations without support pruning”, IEEE Transactions on Knowledge and Data Engineering \(special issue  W. DuMouchel and D. Preg ibon Empirical bayes screening for multi-item associations”, KDD’01, pp. 67-76  S  J a ros zew icz and D a n A  Simovici, “ Inte restingness of frequent itemsets using Bayesi an networks as background knowledge”, SIGKDD’04, USA, pp. 178-186  
426 
420 


TESTS IN SECOND t INDICATES nl WAS LOWERED TO 2 Training BSTC Top-k RCBT 7 OC Holdout Validation Results RCBT outperforms BSTC on the single test it could finish by more then 7 although it should be kept in mind that RCBT's results for the 24 unfinished tests could vary widely Note that BSTC's mean accuracy increases monotonically with training set size as expected At 60 training BSTC's accuracy behaves almost identically to RCBT's 40 training accuracy see Figure 6 4 Ovarian Cancer OC Experiment For the Ovarian Cancer dataset which is the largest dataset in this collection the Top-k mining method that is used by RCBT also runs into long computational times Although Top-k is an exceptiounally fast CAR group upper bound miner it still depends on performing a pruned exponential search over the training sample subset space Thus as the number of training samples increases Top-k quickly becomes computationally challenging to tune/use Table VI contains four average classification test run times in seconds for each Ovarian Cancer\(OC training size As before the second column run times each give the average time required to build both class 0/1 BSTs and then use them to classify all test's samples with BSTC Note that BSTC was able to complete each OC classification test in about 1 minute In contrast RCBT again failed to complete processing most classification tests within 2 hours Table VI's third column gives the average times required for Top-k to mine the top 10 covering rule groups upper bouhnds for each training set test with the same 2 hour cutoff procedure as used for PC testing The fourth column gives the average run times of RCBT on the tests for which Topk finished mining rules also with a 2 hour cutoff Finally the  RCBT DNF column gives the number of tests that RCBT was unable to finish classifying in  2 hours each THE OC TESTS THAT RCBT FINISHED Training BSTC RCBT 40 92.05 97.66 60 95.75 96.73 80 94 12 98.04 1-133/077 9380 96.12 1070 cJ CZ C 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 BSTC RCBT d Median Median  Mean 260 Near outliers  Far outliers 40 Training 60 Training 0.90.80.70.6BSTC RCBT a 80 Training 1-52/0-50 Training 0.9DNFI 0.80.70.6BSTC RCBT b 1 u0.9DNFI 0.80.70.6BSTC RCBT  RCBT DNF 40 30.89 0.6186 273.37 0/25 60 61.28 41.21  5554.37 19/25 80 71.84  1421.80  7205.43 t 21/22 TIMES FOR THE OC 9 Mean 0 Near outliers  Far outliers 1.01 11 01 1.0 d Fig 6 PC Holdout Validation Results BSTC RCBT a Fig 0.80.8 0.8BSTC RCBT BSTC RCBT b c c i DNF cJ CZ C 40 Training 60 Training 80 Training 1-133/0-77 Training 0.95 DNF DNF DNF 0.9 0.90.90.90.85 0.8 BSTC RCBT TABLE VI AVERAGE RUN 1 133/0-77 70.38  1045.65  6362.86 t 20/23 over the number of tests for which Top-k finished Because RCBT couldn't finish any 80 or 1-133/0-77 tests within 2 hours with nl  20 we lowered nl to 2 Classification Accuracy Figure 7 contains boxplots for BSTC on all four OC classification test sets Boxplots were not generated for RCBT with 60 80 or 1-133/0-77 training since it was unable to finish all 25 tests for all these training set sizes in  2 hours each Table VII lists the mean accuracies of BSTC and RCBT over the tests on which RCBT was able to produce results Hence Table VII's 40 row consists of averages over 25 results Meanwhile Table VII's 60 row results are from 6 tests 80 contains a single test's result and 1-133/0-77 results from 3 tests RCBT has better mean accuracy on the 40 training size but the results are closer on the remaining sizes   4 difference over RCBT's completed tests Again RCBT's accuracy could vary widely on its uncompleted tests CAR Mining Parameter Tuning and Scalability We attempted to run Top-k to completion on the 3 OC 80 training and 2 OC 1-133/0-77 training tests However it could not finish mining rules within the 2 hour cutoff Top-k finished two of the three 80 training tests in 775 min 43.6 sec and 185 min 3.3 sec However the third test ran for over 16,000 mnm  11 days without finishing Likewise Top-k finished one of the two 1-133/0-77 tests in 126 min 45.2 sec but couldn't finish the other in 16,000 min  11 days After increasing Top-k's support cutoff from 0.7 to 0.9 it was able to finish the two unfinished 80 and 1-133/0-77 training tests in 5 min 13.8 sec and 35 min 36.9 sec respectively However RCBT with nl 2 then wasn't able to finish lower bound rule mining for either of these two tests within 1,500 min Clearly CAR-mining and parameter tuning on large training sets is TABLE VII MEAN AcCU1ACIES FOR 


support pruning gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis Biosystems vol 85 computationally challenging As training set sizes increase it is likely that these difficulties will also increase VI RELATED WORK While operating on a microarray dataset current CAR 1 2 3 4 and other pattern/rule 20 21 mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets Hence they are generally quite computationally expensive for datasets containing many training samples or genes as the case may be BSTC is explicitly related to CAR-based classifiers but requires no expensive CAR mining BSTC is also related to decision tree-based classifiers such as random forest 19 and C4.5 family 9 methods It is possible to represent any consistent set of boolean association rules as a decision tree and vice versa However it is generally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data 1 2 6 7 11 BSTC is explicitly related to and motivated by CAR-based methods To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here Perhaps the work closest to utilizing 100 BARs is the TOPRULES 22 miner TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci Hence TOP-RULES discovers all 100 confident CARs in a dataset However the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER 23 and so generally isn't polynomial time Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules 24 25 Once obtained fuzzy rules can be used for classification in a manner analogous to CARs However the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM 25 VII CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with preclassification CAR mining see Tables IV and VI we developed a novel method which considers a larger subset of CAR-related boolean association rules BARs These rules can be compactly captured in a Boolean Structure Table BST which can then be used to produce a BST classifier called BSTC Comparison to the current leading CAR classifier RCBT on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining see Section VB Hence BSTC extends generalized CAR based methods to larger datasets then previously practical Furthermore unlike other association rule-based classifiers BSTC easily generalizes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse then CAR-based methods after all exponential time CAR mining is completed O SlS CGl versus O Si CGi As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists ACKNOWLEDGM[ENTS We thank Anthony K.H Tung and Xin Xu for sending us their discretized microarray data files and Top-k/RCBT executables This research was supported in part by NSF grant DMS-0510203 NIH grant I-U54-DA021519-OlAf and by the Michigan Technology Tri-Corridor grant GR687 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies REFERENCES 1 G Cong K L Tan A K H Tung and X Xu Mining top-k covering rule Mining SDM 2002 5 R Agrawal T Imielinski and A Swami Mining associations between sets of items Y Ma Integrating classification and association rule mining KDD 1998 11 T McIntosh and S Chawla On discovery of maximal confident rules without pp 43-52 1999 24 S Vinterbo E Kim and L Ohno-Machado Small fuzzy and interpretable pp 165-176 2006 1071 pp 207-216 1993 6 G Dong pp 273-297 t995 9 pp 5-32 2001 20 W Li J R Quinlan Bagging boosting and c4.5 AAAI vol 1 V Vapnik Support-vector networks the best strategies for mining frequent closed itemsets KDD 2003 4 M Zaki and C Hsiao Charm L Wong Identifying good diagnostic genes or gene expression data SIGMOD 2005 2 G Cong A K H Tung X Xu F Pan and J Yang Farmer Finding interesting rule gene expression data by using the gene expression based classifiers BioiiiJcrmatics vol 21 l and Inrelligent Systenis IFIS 1993 16 Available at http://sdmc.i2r.a-star.edu.sg/rp 17 The dprep package http:/cran r-project org/doclpackages dprep pdfI 18 C Chang and C Lin Libsvm a library for support vector machines 2007 Online Available www.csie.ntu.edu.tw cjlin/papers/libsvm.pdf 19 L Breiimnan Random forests Maclh Learn vol 45 no 1 M Chen and H L Huang Interpretable X Zhang 7 J Li and pp 725-734 2002 8 C Cortes and Mac hine Learming vol 20 no 3 in microarray data SIGKDD Worikshop on Dtra Mining in Bioinfrrnatics BIOKDD 2005 12 R Agrawal and R Srikant Fast algorithms for mining association rules VLDB pp 1964-1970 2005 25 L Wong and J Li Caep Classification by aggregating emerging patterns Proc 2nd Iat Coif Discovery Scieice DS 1999 gene groups from pp 487-499 t994 13 Available ot http://www-personal umich edu/o markiwen 14 R Motwani and P Raghavan Randomized Algoriitlms Caim-bridge University Press 1995 15 S Sudarsky Fuzzy satisfiability Intl Conf on Industrial Fuzzy Contri J Han and J Pei Cmar Accurate and efficient classification based on multiple class-association rules ICDM 2001 21 F Rioult J F Boulicaut B Cremilleux and J Besson Using groups for groups in microarray datasets SIGMOD 2004 3 concept of emerging patterns BioinformJotics vol 18 transposition for pattern discovery from microarray data DMKD pp 73-79 2003 22 J Li X Zhang G Dong K Ramamohanarao and Q Sun Efficient mining of high confidence association rules without S Y Ho C H Hsieh H pp 725-730 1996 10 B Liu W Hsu and support thresholds Principles f Drata Mining aind Knowledge Discovery PKDD pp 406 411 1999 23 G Dong and J Li Efficient mining of emerging patterns discovering trends and differences KDD J Wang J Han and J Pei Closet Searching for An efficient algorithm for closed association rule mining Proc oJ the 2nd SIAM Int Con on Data in large databases SIGMOD 


