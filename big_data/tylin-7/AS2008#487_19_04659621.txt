Research on Fuzzy Genetics-Based Rule Classifier in Intrusion Detection System   YU-PING ZHOU  JIAN-AN FANG, DONG-MEI YU College of Information Science and T echnology, Donghua University, Shanghai 201620,China E-mail: yp_zhou@mail.dhu.edu.cn   Abstract  Intrusion detection technique has become the focus in the area of network security research. Various soft computing approaches have been applied to the 
intrusion detection field. The paper incorporate fuzzy logic and genetic algorithms into the classifying system based on fuzzy association rule to extract both accurate and interpretable fuzzy IF-THEN rules from network traffic data for classification, and utilize genetic algorithms to optimize the classifier, The experiments and evaluations of the proposed method were performed with the KDD Cup 99 intrusion detection dataset. Results indicate the high detection 
accuracy for intrusion attacks and low false alarm rate of the reliable system   1  Introduction The security of computer network plays a strategic role in modern computer systems with the widespread use of network. Intrusion Detection Systems \(IDS\ are effective security tools, placing inside a protected network and looking for known or potential threats in network traffic and/or audit data recorded by hosts 
Basically, an IDS analyzes information about userís behaviors from various sources such as audit trail system table, and network usage data. The problem of intrusion detection has been studied extensively in computer security 1,2,3  d h a s receiv e d a lot of attention in machine learning and data mining 4   The intrusion detection techniques can be divided 
into two groups according to the type of information they use: misuse detection and anomaly detection. Soft computing is an innovative approach to construct a computationally intelligent system which parallels the extraordinary ability of the human mind to reason and learn in an environment of uncertainty and imprecision  Fuzzy logic, as a robust soft computing method 
has demonstrated its ability in intrusion detection systems[6  M o r e o v e r  fuz z y s y ste m s ha ve se ve r a l  important features which make them suitable for intrusion detection a ri ou s  m e t h ods  h a v e been suggested for automatic generation and adjustment of fuzzy rules without using the aid of human experts, the genetic fuzzy is one of the most successful approaches 
in this rega  Genetic algorithm \(GA\[10 ha s b e e n s u c c e s s f ul l y  applied to solve many combinatorial optimization problems. The application of GA to the evolution of fuzzy rules can be found in Refs s i m p l e G A i s  applied to generate and evolve the fuzzy classi  ers that use complete expression tree and triangular membership function for the formulation of chromosome. To evaluate the 
 tness of individual solutions, the weighted sum of  tness values of multiple objective functions is proposed[1 h ere the proposed weights are user-de  ned and cannot be optimized dynamically for different cases The rest of the paper is as follows: Section 2 discusses fuzzy system and interpretability. Section 3 
2008 International Conference on Intelligent Computation Technology and Automation 978-0-7695-3357-5/08 $25.00 © 2008 IEEE DOI 10.1109/ICICTA.2008.241 914 


introduces the proposed work, Experiment results are reported in Section 4. Section 5 is conclusions 2. Fuzzy system and interpretability  2.1. Fuzzy logic  Fuzzy logic has been a powerful tool for decision making to handle imprecise and uncertain data. In contrast to classical set, a fuzzy set is a set without crisp boundaries, the transition from ìbelong to a set to ìnot belong to a setî is gradual. Membership function is utilized to reflect a degree of membership and indicated by a value in the range [0.0, 1.0   There are two alternative ways to denote a fuzzy set   1  A A X xX ii i x x if X is discret A x x if X is continuous               where  A x  is called the membership function which maps each object x from domain X to a continuous membership value between 0 and 1. We usually divide the universe into several fuzzy set covering by some confident consistent membership function, where x is denoted linguistic variable and those fuzzy set is denoted linguistic label. Fuzzy if-then rule is based on linguistic variable and linguistic label. An example conjunctive fuzzy rule  11 2 2 R IF x is A and x is A and and x is A THEN class is c qq q nqn q              where R q is the qth fuzzy rule, x=\(x 1 x 2 x n s an n-dimensional object of X, C q is the consequent class and corresponding antecedent A qi is an antecedent fuzzy set. Then the firing strength Aq of a rule using the max-min composition is Aq min\(m 1 m 2 m n  The interpretability issues of the genetic fuzzy rule based system is an important problem which must be solved. Redundant fuzzy rules and fuzzy sets, as well as inappropriate fuzzy set topology would be undesirably constructed if the interpretability criterion is not optimized. In order to improve the usage and performance of fuzzy rule-based system, we can discern and guarantee the classification of fuzzy system from the following factors 2.2.Completeness and distinguishability Each fuzzy variable should be partitioned completely and distinguishingly. For each input variable X with n fuzzy set 1 2  n A AA if the following condition hold true, then the partitioning of fuzzy sets is complete  0,1    0  1    iji xUi m Ax j n        Where U is the universe of X. Completeness and distinguishability can be interpreted by the fuzzy similarity measure[7  w h i c h id en ti f i e s 1  e  similarity between two fuzzy sets for a fuzzy variable 2\e similarity of a fuzzy set to the universal set U  3\e similarity of a fuzzy set to a singleton set. The similarity between two fuzzy sets A and B can be calculated using the following computationally efficient method    1     1 2    AB AB m xi xi i Similar A B m xi xi i          On a discrete universe 1,2 Uxj m j  where   and  represent the minimum and maximum operations respectively. If Similar\(A,B  is larger than a given threshold   then the partitioning of those two fuzzy set are not well distinguishable from each other resulting in a bad topology. Now it is necessary to merge the fuzzy set A and B, construct a new fuzzy set C. suppose A have the membership function 111 x; a, b, c A  B have the membership function 222 x; a, b, c B  then the resulting fuzzy set C with the member function 333 x; a, b, c c  is defined from merging A and B by 312 31 2 312 3 min   1 0,1 max   aaa bb b ccc         Where 0,1   If the similarity of a fuzzy set to the universal set 1  U Ux xX   is larger than an 
915 


upper threshold  U  or smaller than a lower threshold  s  then we can remove it from the rule base the similarity of a fuzzy set to a singleton set, the fuzzy set in the former case is very similar to the universal set, and in the latter case similar to a singleton set. Neither of these cases is desired to generate interpretable rule cases 2.3.Consistency and compactness  If two or more rules with similar antecedents are triggered simultaneously, then their consequents should also be similar. In other words, fuzzy rules are consistent if they are not contradictiv  The traditional weight\(fire-strength i  of the ith rule can be defined as follows 12 11 1        1,2  ii in iA A A xx x xi R      4 Where R is the number of fuzzy rules in the rule base. We can use the inclusion relation for indicating the degree of consistency, and introduce an inclusion factor which is given by 1   1,2  Rk Ri iukxkRki     012  5 Then the weight of the rule R i with the inclusion factor can be updated as  i  ii x    1 2  R i  6 The interpretability of fuzzy systems is depend on the compactness of fuzzy systems A compact fuzzy system indicates that it is easy to be comprehended. Compactness of fuzzy rules becomes more important when the system involves a large number of dimensi I n order t o  achieve the effect compactness of fuzzy systems there are three factors should be considered[1 1 a small number of fuzzy sets for each fuzzy variable 2\all number of fuzzy rules in rule base \(3 small number of conditions in the rule premise  When a fuzzy set is not used by any fuzzy rule in the fuzzy rule base, it is necessary to remove the fuzzy set from the rule base 3  Proposed algorithm  3. 1 Fuzzification of association rules Association rules are used to represent the relationships between the given data items. It refers to the usage of items and tries to establish relationships Its form is given as  X Ycs  where c represent confidence and s represent support Fuzzy rules of the form qq AC  is similar to the association rules, where the antecedent part is fuzzy but the consequent is not. Assume that Aq  is the firing strength of the rule antecedent determined by max-min composition. Support of the fuzzy association rule can be computed as   xclass pcq Aq p qq s AC xm    7 and the confidence of the fuzzy rule can be computed as  1    m p qq Aq p p pAq cq cA C x x xclass     8 where 12 12   min        Aq Aq Aq Aqn p pp pn xxxx    Assuming that the N-dimensional pattern uses 5 linguistic variable for each dimension and including donít careî condition, the total number of combinations for a rule becomes 5 1 n  In order to reduce the search space, a process of pre-screening would be carried out. The two criteria \(support and confidence\sed in th e pre-screening process 3.2 Generation of fuzzy rules base  Step 1: generation of initial population. To generate initial generation, each fuzzy rule should be coded as a string. At first there are 6 symbols which are used for denoting the five linguistic values and donít careî, 1=îsmallî, 2=îsmall medium 3=îmediumî, 4=îmedium largeî, 5=îlargeî, and donít care  12 3 4  If is small and is don't care and is medium large and is don't care, then  Class  j Ri x x x xC    is coded as ì1#4#î, which is a chromosome organized 
916 


with some genes, each gene present a fuzzy set of membership function  Calculate the compatibility grade of each training pattern 12   p p ppn xxx x  with the fuzzy if-then rule R j by the product operation as  1      1,2 n Rj p Aji p Ajn p xx xpm      9 where  Rj  015  is the membership function of ji A  Calculate the relative sum of compatibility grades of the training patterns for each class as follows       1 2  10  class h j Rj p class h xp class h RxNhc       where we use relative sum of compatibility grades instead of traditional heuristic method of Ishibuchi and Nakashima \(1999\e of the classes are very similar to each other. Moreover the number of training patterns for each of the classes is significantly different Find the consequent class C j that has the maximum value of  classh j R  among the c classes max   11 1 RRR class cj j class j class c j     When a single class has the maximum value in the above equation\(11\at class is used as the consequent class of the fuzzy if-then rule j R note if two or more classes have the same maximum value the consequent class cannot be uniquely specified. In this case, we assign empty class to C j and the zero certainty grade to CF j When the consequent class C j   is determined by equation \(8\e certainty grade CF j   is specified as 1   c classh j h classcj j j CF R R    12 where  1 class h j hcj Rc   012  the consequent class and the certainty grade for any combination of antecedent fuzzy sets can be specified by the proposed heuristic procedure Step 2: evaluate each of the fuzzy rules in the population. In the paper a novel fitness function is presented as follows   Rj xp j xp classcj j f itness R NCP R    13 where 1   0  0 j p Rj p R x x else      and  j NCP R is the number of correctly classified training patterns by R j This fitness means the power of a single rule which can classify a pattern correctly when we do not consider the existence of other rules. A pair of fuzzy if-then rules is selected from the current population to generate new fuzzy if-then rules for the next generation, each chromosome is in the current population is selected by the following selection probability mi n mi n    k Rk s P fitness R fitness s fitness R fitness s     14 where min  f itness s is the minimum fitness value of the fuzzy if-then rules in the rule set S. This procedure is iterated until a pre-specified number of pairs of fuzzy if-then rules are selected Step 3: crossover and mutation operation from each of the selected pairs of fuzzy if-then rules, two fuzzy if-then rules are generated by the uniform crossover for the antecedent fuzzy set of the generated fuzzy if-then rules .The uniform crossover and the random mutation method are presented Step 4: replacement. A certain percentage of fuzzy if-then rules in the current population are replaced with new fuzzy if-then rules generated by the crossover and mutation operations. The process of genetic fuzzy rules iterative learning can be stop with the total number of generation as a stopping criterion When a rule set S is generated, an input pattern 12   p p ppn xxx x  is classified by a single winner rule R j in S, which is determined as follows[7   max   cj p j j p j j xCF xCFRS   15 4. Experiments 
917 


The KDD Cup 99 dataset includes a set of 41 features derived from each connection and a label which speci  es the status of connection records as either normal or speci  c attack type. These features had all forms of continuous, discrete, and symbolic with signi  cantly varying ranges falling in four categories[17 KDD d a taset is d i v i d e d in to tr ain i n g  and testing record sets. Total number of connection records in the training dataset is about 5 million records. This is too large for our purpose; as such only concise training dataset of KDD, known as 10 training dataset, was employed here In computer simulation, we have to consider the following assumption: population size N pop 100 replacement percentage P rep 20%. Crossover rate P c 80%, Mutation rate P m 5%, Termination condition Maximum generation=100\Table 1 compare the different algorithm performances, just as shown in Table 1, the total performance of our algorithm is better than other algorithm Table 1. Different algorithms performances Algorithm Detection rate False alarm rate Our alghrithm 97.29 0.27 Ishibuchi[13 95 02  0.2 4  EFRID[14 98 15  7.0  5. Conclusions  In this work, a genetic fuzzy rule-based classifier has been designed by using fuzzy rule iterative learning algorithm in instruction detection system. To reduce the search space of fuzzy rule candidate, the population is initialized with the individuals randomly chosen among the pre-screened rules. The pre-screening process is completed by the usage of support and confidence. A new fitness function based on NC s pres e n t e d i n  t h i s paper, In f act t h e  performance of the final classification which was constructed according to the new fitness function was compared to several classi fication algorithms. Result showed that the presented algorithm was capable of increasing the detection rate and decreasing the false alarm rate simultaneously Acknowledgments This work was supported by the FuJian Province Department of Education \(JA05300 References  1 Mur a li.A R a o.M 2 0 0 5 A sur v ey on intr us i on de te c t i o n  approaches. In: First International Conference on Information and Communication Technologies .pp.233-240  Non g  Y   Qi a n g C  B o rro r C  M   2 0 0 4  E W M A f o recast  of normal system activity for computer intrusion Detection IEEE Trans, Reliab. 53\(4\557-566  A x elsso n  S   20 00  In tr u s ion d e tectio n sy ste m s: a su rve y  and taxonomy. Technical report no. 99-15, Department of Computer Engineering . Chalmers University of Technology Sewden 4 T i a n J F Fu.Y   W a ng J L  20 05. I n tr us ion de te c tio n  combining multiple decision trees by fuzzy logic. In: Sixth International Conference on Parallel and Distributed Computing. Application an Technologies,5-8 December 2005. pp.256-258 5 L  A  Za de l, Role of so f t c o m puting a nd f u z z y log i c in the  conception design and development of information intelligent systems, Lecture Notes in Computer Science 695\(1998\1-9 6 M.s  A b a d e h  J  H a bi bi, C  L u c a s  I n tr us i o n de te c tio n us i n g  a fuzzy genetics-based learning algorithm. Journal of Network and Computer Applications\(2005  7 J  E.D i c k e r s on, J  J u s lin O  K ouk ous oula  J  A  D i c k e r s on Fuzzy intrusion detection, in: Proceedings of IFSA World Congress and 20 th North American Fuzzy Information Processing Society Conference, NAFIPS2001, Vancouver British Columbia,July2001,pp.1506ñ1510  H Ish i b u c hi  T  Nakash i m a  T  M u rat a  A f u zz y cl assi f i er  system that generates fuzzy if-then rules for pattern classification problems, In: Proceedings of 2nd IEEE International Conference on Evolutionary Computation 
918 


Perth, Australia, 29 November-1 December 1995, IEEE vol.2, 1995, pp. 759-764 9 J  L i u, J  K w ok  A n e x te nde d g e ne tic r u le induc ti on  algorithm, In:Proceedings of the Congress on Evolutionary Computation Conference, 16-19 July 2000, La Jolla, CA USA, vol. 1, 2000,pp. 458-463 10 J.H  H o lla n d A d a p ta tio n in N a tura l a n d A r ti  cial Systems, University of Michigan Press, Ann Arbor, 1975 1 J G o m e z D Dasgu p t a E v o l vi n g f u zz y cl as si  ers for intrusion detection, in: Proceedings of IEEE Works hop on Information Assurance, United States Military Academy West Point, New York, June 2001, pp. 68ñ75  12 D  S ong M.I  H e y w ood, A  N. Zinc ir H e y w ood, T r a i ning  genetic programming on half a million patterns: an example from anomaly detection, IEEE Transactions on Evolutionary Computation 9\(3\ \(2005\ 225ñ239, doi: 10.1109/TEVC. 2004 841683 13 G  Flor e z  S  M.B r i d g e s  R  B  Va ug hn, A n im pr ov e d  algorithm for fuzzy data mining for intrusion detection, in Proceedings of North American Fuzzy Information Processing Society Conference, NAFIPS 2000, New Orleans LA, June 2002, pp. 457ñ462 14 Y  J i n  W  v on Se e l e n B  Se ndh of f  O n g e ne r a ting FC 3 fuzzy rule systems with data using evolution strategies, IEEE Trans. Syst. Man Cybern.óPart B: Cybernetics 29 \(6\ \(1999 829ñ845 15 Y  J i n Fuz z y  m ode ling of hig h d im e n s i ona l s y s t em s   complexity reduction and interpretability improvement, IEEE Trans. Fuzzy Syst. 8 \(2\ \(2000\ 212ñ221 1 H Rou b o s  M  S e t n es Co m p act an d t r an sp aren t f u zz y models and classi  ers through interactive complexity reduction, IEEE Trans. Fuzzy Syst. 9 \(4\ \(2001\ 516ñ524 1 KDD Cu p  1 9 9 9 I n t r u s i o n  d e t ect i o n d a t a set   http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html  
919 


                                                                                                                 
456 


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efﬁcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efﬁciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efﬁciency When the clustering model is available it is a signiﬁcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques Sufﬁcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efﬁcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efﬁcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conﬁdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conﬁdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conﬁdence The sufﬁcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efﬁcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efﬁciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufﬁciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70–81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207–216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487–499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145–154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146–153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9–15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597–600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern Classiﬁcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155–162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512–521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1–12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265–278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283–304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476–482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559–563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220–231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13–24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10–17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549–550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188–201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259–283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909–921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305–345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432–444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conﬁdence Intelligent Data Analysis  9\(4\:381–395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49–73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407–419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1–12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki Efﬁciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642–658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483–490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194–203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344–353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efﬁcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103–114 1996 
618 
618 


