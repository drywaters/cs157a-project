A distributed algorithm of density-based subspace frequent closed itemset mining Huaiguo Fu M  021che  al  O Foghl  u Telecommunications Software  Systems Group Waterford Institute of Technology Waterford Ireland f hfu mofoghlu g tssg.org Abstract Large dense-packed and high-dimensional data mining is one challenge of frequent closed itemset mining for association analysis although frequent closed itemset mining is an ef\002cient approach to reduce the complexity of mining frequent itemsets This paper proposes a distributed algorithm to address the challenge of discovering frequent closed itemsets in large dense-packed and high-dimensional data The algorithm partitions the search space of frequent closed itemsets into independent nonoverlapping subspaces that can be extracted independently to generate frequent closed itemsets The algorithm can generate frequent closed itemsets according to dense priority the closed itemset more dense or more frequent will be generated preferentially The experimental results show the algorithm is ef\002cient to extract frequent closed itemsets in large data Keywords Frequent closed itemset mining Association analysis Concept lattice Partition Distributed algorithm 1 Introduction The large dense-packed and high-dimensional data mining is one challenge of frequent closed itemset mining for association analysis although frequent closed itemset mining is an ef\002cient approach to reduce the complexity of mining frequent itemsets and some ef\002cient algorithms of frequent closed itemset mining have been proposed When the data is very large dense-packed and high-dimensional and the minimum support is small it is still a hard problem to mine frequent closed itemsets In recent years association analysis has attracted a lot of attention for research and applications Frequent itemset mining is one sub-problem and the key task of association analysis  Many research works focus on ming frequent itemsets as it is a hard problem when data is large Some techniques are proposed to reduce the complexity of mining frequent itemsets The well-known techniques are mining maximal frequent itemsets 11 and mining frequent closed itemsets The problem of 002nding frequent itemsets from data for association rules can be reduced to 002nding frequent closed itemsets with closed itemset lattice 8 10 And it s possible to prune the number of rules produced without information loss using closed itemset lattice Closed itemset lattice is based on concept lattice Theoretical foundation of concept lattice is derived from the mathematical lattice theory 2 that is a popular mathematical structure for modeling conceptual hierarchies Concept lattice can be used to analyze and mine the complex data for such as classi\002cation association rule mining clustering etc Furthermore concept lattice also pro vides an effective tool of knowledge visualization In recent years extensive studies have proposed some ef\002cient algorithms for mining frequent closed itemsets such as CLOSE A-close CLOSET 10 CHARM[13 and CLOSET free-sets 4 etc These algorithms ha v e good performance for sparse data However when the data is dense or the size of items is large the algorithms suffer from the challenge of mining large and high-dimensional data One ef\002cient solution is to reduce the complexity by partitioning the mining space In this paper we propose a distributed algorithm to discover frequent closed itemsets in large and highdimensional data The algorithm is based on the density of items and closed itemsets and the hierarchical order between the closed itemsets We propose a simple approach to determine the frequent closed itemsets The algorithm partitions the search space of frequent closed itemsets into independent nonoverlapping subspaces that can be extracted independently to generate frequent closed itemsets The algorithm can generate frequent closed itemsets according to dense priority the closed itemset more dense or more frequent will be generated preferentially The rest of this paper is organized as follows The basic concepts of mining frequent closed itemsets are presented 
The 10th IEEE International Conference on High Performance Computing and Communications 978-0-7695-3352-0/08 $25.00 © 2008 IEEE DOI 10.1109/HPCC.2008.147 750 
The 10th IEEE International Conference on High Performance Computing and Communications 978-0-7695-3352-0/08 $25.00 © 2008 IEEE DOI 10.1109/HPCC.2008.147 750 
The 10th IEEE International Conference on High Performance Computing and Communications 978-0-7695-3352-0/08 $25.00 © 2008 IEEE DOI 10.1109/HPCC.2008.147 750 


a 1 a 2 a 3 a 4 a 5 a 6 a 7 a 8 1 002 002 002 2 002 002 002 002 3 002 002 002 002 002 4 002 002 002 002 5 002 002 002 002 6 002 002 002 002 002 7 002 002 002 002 8 002 002 002 002 Figure 1 Example of formal context in the next section Section 3 analyzes the search space of frequent closed itemsets The distributed algorithm of mining frequent closed itemsets is introduced in section 4 We show the experimental results of the algorithm in section 5 The paper ends with a short conclusion in section 6 2 Closed itemset De\002nition 2.1 Formal context is de\002ned by a triple  O A R   where O and A are two sets and R is a relation between O and A  The elements of O are called objects or transactions while the elements of A are called items or attributes For example Figure 1 represents a formal context  O A R   De\002nition 2.2 Two closure operators are de\002ned as O 1  O 00 1 for set O and A 1  A 00 1 for set A  O 0 1  f a 2 A j oRa for all o 2 O 1 g A 0 1  f o 2 O j oRa for all a 2 A 1 g These two operators are called the Galois connection for  O A R   These operators are used to determine a formal concept De\002nition 2.3 A formal concept of  O A R  is a pair  O 1  A 1  with O 1 022 O  A 1 022 A O 1  A 0 1 and A 1  O 0 1  O 1 is called extent A 1 is called intent De\002nition 2.4 We say that there is a hierarchical order between two formal concepts  O 1  A 1  and  O 2  A 2   if O 1 022 O 2  or A 2 022 A 1   All formal concepts with the hierarchical order of concepts form a complete lattice called concept lattice  De\002nition 2.5 An itemset C 022 A is a closed itemset iff C 00  C  a 1 a 2 a 3 a 4 a 5 a 6 a 7 a 8 a 1 a 3 a 4 a 5 a 1 a 2 a 3 a 4 a 6 a 1 a 3 a 4 a 6 a 1 a 2 a 4 a 6 a 1 a 4 a 6 a 1 a 3 a 4 a 1 a 2 a 3 a 1 a 2 a 3 a 7 a 8 a 1 a 2 a 7 a 8 a 1 a 2 a 7 a 1 a 3 a 7 a 8 a 1 a 7 a 8 a 1 a 7 a 1 a 3 a 1 a 4 a 1 a 2 a 1 b b b b b b b b b b b b b b b b b b J J J 012 012 012 J J J 012 012 012 H H H H H H H H H H H H H H H H H H 012 012 012 J J J J J J J J 012 012 012 012 012 012 012 012 H H H H H H 012 012 012 J J 012 012 012 010 010 010 010 010 010 010 010 010 010 010 010 010 010 010 010 010 010 J J J 010 010 010 010 010 010 J J J J J J 012 012 012 010 010 010 010 010 010 H H H H H H 012 012 012 Figure 2 Example of closed itemset lattice De\002nition 2.6 If C 1 and C 2 are closed itemsets C 1 022 C 2  then we say that there is a hierarchical order between C 1 and C 2  And C 2 is called the sub-closed itemset of C 1  or C 1 is called the super-closed itemset of C 2  if there is no closed itemset C 3  C 1 022 C 3 022 C 2  All closed itemsets with the hierarchical order of closed itemsets form of a complete lattice called closed itemset lattice  The closed itemset lattice of the formal context of Figure 1 is presented in Figure 2 De\002nition 2.7 Given a formal context  O A R   C is an itemset the support or density of C  denoted as support  C   is the number of the transactions of C  Proposition 2.1 Let an itemset C 022 A be a closed itemset the support of C is support  C   jj C 0 jj 3 Analysis of search space for frequent closed itemsets In this section we propose an approach to determine a closed itemset in a formal context and then study the hierarchical order between each closed itemset and its sub-closed itemsets to analyze how to partition the search space of frequent closed itemsets 3.1 How to determine a closed itemset De\002nition 3.1 Given a context  O A R   an item a i is called maximal item  if a i 2 A  for all a j 2 A  i 6  j and f a i g 0 6\032 f a j g 0  It is easy to infer the following proposition from the definition 3.1 Proposition 3.1 Given a context  O A R   if item a i 2 A and a i is a maximal item and for all a j 2 A  i 6  j and f a i g 0 6  f a j g 0  then f a i g is a closed itemset 
751 
751 
751 


From the proposition 3.1 we have a simple approach to determine a closed itemset in the formal context  O A R   017 Merge the items that contain the same transactions as a single item 017 Find the maximal items in the merged context 017 Each maximal item must be a closed itemset 3.2 How to partition the search space There are different methods to partition the search space of frequent closed itemsets We will explore our method by the following objectives 017 The subspaces are nonoverlapping 017 The closed itemsets more dense or more frequent can be generated preferentially 017 The approach to determine a closed itemset can be used in subspaces Proposition 3.2 Given a closed itemset C i of context  O A R  and it's sub-closed itemset C ij where j  1  2  3    C ij 000 C i is closed itemset in sub-context  C i 0  A 000 C i  R   Proof  C ij is a sub-closed itemset of closed itemset C i  then we have C ij 000 C i   C ij 000 C i  00 in the sub-context  C i 0  A 000 C i  R   Thus C ij 000 C i is a closed itemset in the sub-context  C i 0  A 000 C i  R   Proposition 3.3 All sub-closed itemsets of C i can be generated from the sub-context  C i 0  A 000 C i  R   Proof  Given a closed itemset C j of the sub-context  C i 0  A 000 C i  R    C j  C i  is a sub-closed itemset of C i  Thus all sub-closed itemsets can be generated by the closed itemsets of the sub-context  C i 0  A 000 C i  R   From above propositions we can consider sub-contexts as the subspaces of closed itemsets The sub-context can be  C i 0  A 000 C i  R   C i is one super-closed itemset In each sub-context or subspace the maximal items or items with high density or support generate frequent closed itemsets For example see 002gure 3 a 2  a 3  a 4 and a 7 are maximal items in the sub-context of a 1 and they can generate the closed itemsets a 1 a 2  a 1 a 3  a 1 a 4 and a 1 a 7  By this approach some same closed itemsets will be generated in different sub-contexts as they have different super-closed itemsets The the sub-contexts are overlapping We can generate the same closed itemsets in only one sub-context and reduce the sub-contexts to nonoverlapping sub-contexts by the following proposition a 2 a 3 a 4 a 5 a 6 a 7 a 8 1 002 002 2 002 002 002 3 002 002 002 002 4 002 002 002 5 002 002 002 6 002 002 002 002 7 002 002 002 8 002 002 002 010 010 010 010 010 010 012 012 012 J J J H H H H H H a 1 a 1 a 7 a 1 a 3 a 1 a 2 a 1 a 4 u u u u u Figure 3 Example of sub-context and subclosed itemsets of a 1 Proposition 3.4 Given two super-closed itemsets C i and C j  C i 6\022 C j and C j 6\022 C i  All sub-closed itemsets of C i and C j can be generated from the nonoverlapping subcontexts  C i 0  A 000 C i  R  and  C j 0  A 000 C j 000 C i 022  R   C i 022 is noted by all items that is included by C i in the sub-context  C j 0  A 000 C j  R   De\002nition 3.2 Given an itemset a i of context  O A R    f a i g 0  A 000f a i g 00  R  is called projective sub-context of a i  We analyze the projective sub-contexts of a 1 a 2 and a 1 a 3 see 002gure 4 In such sub-contexts of a 1 a 2 and a 1 a 3  a 1 a 2 a 3 is a common closed itemset for two sub-contexts To reduce the redundant closed itemsets we do not change the projective sub-context of a 1 a 2  but remove a 2 022  a 2 from the projective sub-contexts of a 1 a 3 to reduce the subcontext of a 1 a 3 see 002gure 5 Thus all the closed itemsets that include a 2 in the sub-context of a 1 a 3 will not be generated The summary of the main idea of partitioning the search space is the reduced projective sub-contexts of high dense closed itemsets are the subspaces of frequent closed itemsets 4 Distributed algorithm of discovering frequent closed itemsets Given a formal context and minimum of support we propose a distributed algorithm to mine frequent closed itemsets by following steps 1 First of all the algorithm needs to generate the ordered frequent context 
752 
752 
752 


sub-context of a 1 a 2   f a 1 a 2 g 0  A 000 f a 1 a 2 g  R  a 3 a 4 a 5 a 6 a 7 a 8 1 002 2 002 002 3 002 002 002 5 002 002 6 002 002 002 010 010 010 010 010 010 012 012 012 J J J H H H H H H 010 010 010 010 010 010 012 012 012 H H H H H H a 1 a 1 a 7 a 1 a 3 a 1 a 2 a 1 a 4 a 1 a 2 a 7 a 1 a 2 a 3 a 1 a 2 a 4 a 6 u u u u u u u u sub-context of a 1 a 3   f a 1 a 3 g 0  A 000 f a 1 a 3 g  R  a 2 a 4 a 5 a 6 a 7 a 8 3 002 002 002 4 002 002 6 002 002 002 7 002 002 8 002 002 010 010 010 010 010 010 012 012 012 J J J H H H H H H 010 010 010 010 010 010 J J J H H H H H H a 1 a 1 a 7 a 1 a 3 a 1 a 2 a 1 a 4 a 1 a 3 a 7 a 8 a 1 a 2 a 3 a 1 a 3 a 4 u u u u u u u u Figure 4 Example of sub-context and subclosed itemsets of a 1 a 2 and a 1 a 3 reduced sub-context of a 1 a 3   f a 1 a 3 g 0  A 000 f a 1 a 3 g 000 f a 2 g  R  a 4 a 5 a 6 a 7 a 8 3 002 002 4 002 002 6 002 002 7 002 002 8 002 002 010 010 010 010 010 010 012 012 012 J J J H H H H H H 010 010 010 010 010 010 H H H H H H a 1 a 1 a 7 a 1 a 3 a 1 a 2 a 1 a 4 a 1 a 3 a 7 a 8 a 1 a 3 a 4 u u u u u u u Figure 5 Example of reduced sub-context and sub-closed itemsets of a 1 a 3 De\002nition 4.1 Given the minimum support m  a formal context is called ordered frequent context if we only choose the items which number of transactions is not less than m  and order these items of formal context by number of transactions of each item from the smallest to the biggest one and the items with the same objects are merged as one item We note ordered frequent context  O A C  R  m of the formal context  O A R   Ordered frequent context can count the density or support of each item and facilitate the generation of maximal items and reduced sub-contexts Ordered frequent context allows to generate frequent closed itemsets according to dense priority the closed itemset more dense or more frequent will be generated preferentially 2 The second step from the ordered frequent context every maximal item forms a frequent closed itemset of the 002rst level We have the following proposition by the de\002nition 4.1 and the proposition 3.1 Proposition 4.1 Each maximal item of the ordered data context  O A C  R  m is closed itemset For example the frequent closed itemset in 002rst level for the formal context of 002gure 1 is a 1  The density or the support of a 1 is 8 This step is the core of the algorithm For any subcontext we only see about the maximal items that can form the frequent closed itemset 3 The third step determine the number of partitions according to the size and dimension of data or the user's requirement We can generate more partitions if data is large or high-dimensional The partitions are independent for distributed mining The partition is the reduced sub-context of frequent closed itemset The partitions can be generated from the frequent closed itemsets of 002rst level From the projective sub-contexts of the frequent closed itemsets of 002rst level we can generate more partitions if we need 4 fourth step in each partition if the support of the frequent closed itemset is m  this itemset ends to generate itemsets of next level because the itemsets of next level will not be frequent Otherwise we will generate the frequent closed itemsets from the reduced sub-context of this itemset We generate the frequent closed itemsets from a context to all sub-contexts until the closed itemset is not frequent 5 At the end we can get all frequent closed itemsets For example given the minimum of support 2 the algorithm generates all frequent closed itemsets see 002gure 6 from the formal context of 002gure 1 4.1 An example We search for frequent closed itemsets from the context of 002gure 1 to illustrate the principle of new algorithm see 
753 
753 
753 


transactional data  generate maximal frequent closed itemsets a 1  a 1 a 2  a 1 a 3  a 1 a 4  a 1 a 7 from contexte  O A C  R  2   4 distributed partitions of mining frequent closed itemsets 020 020 020 020 020 020 020 020 020 020 020 020 020 000 000 000 000 000\011     R P P P P P P P P P P P P Pq partition 1  a 1 a 7  FCIs of partition a 8 partition 2  a 1 a 3  FCIs of partition a 7 a 8  a 4  a 4 a 6 partition 3  a 1 a 2  FCIs of partition a 3  a 7  a 7 a 8  a 4 a 6 partition 4  a 1 a 4  FCIs of partition a 6 Figure 7 Extracting the frequent closed itemsets from the context in 002gure 1 010 010 010 010 010 010 012 012 012 J J J H H H H H H 012 012 J J J 010 010 010 010 010 010 J J J H H H H H H 010 010 010 010 010 010 012 012 012 H H H H H H 012 012 012 J J J J J 023 023 J J J S S 012 012 012 012 012 J J S\(8 S\(4 S\(5 S\(5 S\(4 S\(3 S\(3 S\(2 S\(3 S\(2 S\(3 S\(2 S\(2 S\(2 a 1 a 1 a 7 a 1 a 3 a 1 a 2 a 1 a 4 a 1 a 7 a 8 a 1 a 4 a 6 a 1 a 3 a 7 a 8 a 1 a 2 a 7 a 1 a 2 a 3 a 1 a 3 a 4 a 1 a 2 a 4 a 6 a 1 a 2 a 7 a 8 a 1 a 3 a 4 a 6 u u u u u u u u u u u u u u Figure 6 An example all frequent closed itemsets minimum of support is 2 for each node S means support 002gure 7 Assuming the minimum support 2 we generate the ordered frequent context The support of a 5 is less than minimum support so a 5 is deleted from the context We will partition the search space of frequent closed itemset FCI into some subspaces a 1 is maximal item in the ordered frequent context so f a 1 g is frequent closed itemset All sub-closed itemsets a 1 a 2  a 1 a 3  a 1 a 4 and a 1 a 7  can be generated from the sub-context in 002gure 3 The subcontexts  f a 1 a 2 g 0  A 000f a 1 a 2 g  R    f a 1 a 3 g 0  A 000f a 1 a 3 g\000 f a 2 g  R    f a 1 a 4 g 0  A 000 f a 1 a 4 g 000 f a 2 g 000 f a 3 g  R  and  f a 1 a 7 g 0  A 000 f a 1 a 7 g 000 f a 2 g 000 f a 3 g  R  are 4 subspaces of mining frequent closed itemsets The 4 subspaces are independent We can generate all frequent closed itemsets FCIs in each subspace For exmaple in sub-contexts  f a 1 a 2 g 0  A 000 f a 1 a 2 g  R   we can 002nd FCIs a 3  a 7  a 7 a 8  a 4 a 6  In whole context we need to add f a 1 a 2 g to each FCI in sub-context to form the FCI of whole context Therefore a 1 a 2 a 3  a 1 a 2 a 7  a 1 a 2 a 7 a 8  a 1 a 2 a 4 a 6 are FCIs of the whole context As we mine the frequent closed itemsets from the reduced sub-contexts 7 redundant frequent closed itemsets are avoided to be repeatedly mined in different subcontexts In each subspace the item which support is less than minimum support should be deleted DataSet ID Objects Attributes Closed itemsets soybean-small d1 47 79 3253 car d2 1728 21 7999 breast-cancer d3 699 110 9860 house-votes-84 d4 435 18 10642 audiology d5 26 110 30401 tic-tac-toe d6 958 29 59503 nursery d7 12960 31 147577 lung-cancer d8 32 228 186092 agaricuslepiota d9 8124 124 227594 promoters d10 106 228 304385 soybean-large d11 307 133 806030 dermatogogy d12 366 130 1484088 Table 1 The datasets of real data for experiment 
754 
754 
754 


5 Experimental results We have implemented the algorithm in Java to generate frequent closed itemsets We test the algorithm in some real data and simulation data We compare the partitioning algorithm with some subspaces and non-partitioning algorithm without subspaces The preliminary experimental results in 002gure 8 show the ef\002ciency of the algorithm In the the experimental results the run time of partitioning algorithm is the total time of all subspaces mining The experimental results show the partitioning algorithm is much faster than non-partitioning algorithm The subspaces mining with the partitioning algorithm are independent We can develop the algorithm in distributed version Real data see table 1 for our experiment comes from machine learning benchmarks UCI repository Figure 8 Experimental comparison of partitioning algorithm and non-partitioning algorithm minimum support is 90 6 Conclusion and further work One challenge of frequent closed itemset mining is large dense-packed and high-dimensional data mining The proposed algorithm is based on the density of items and closed itemsets and the hierarchical order between the closed itemsets in the closed itemset lattice structure The algorithm can partition searching space of frequent closed itemsets into independent reduced subspaces and then mine frequent closed itemsets in each subspace The algorithm is scalable to extract frequent closed itemsets from large and high-dimensional data The experimental results show the algorithm is ef\002cient to extract frequent closed itemsets in large data The future work will focus on comparison the performance of the algorithm and other frequent itemset mining algorithms and development of more ef\002cient techniques to analyze huge and heterogeneous distributed data Acknowledgements This work is supported by the PRTLI project of Higher Education Authority HEA Ireland References  R Agra w al T  Imielinski and A N S w ami Mining association rules between sets of items in large databases In Proceedings of the 1993 ACM SIGMOD  pages 207–216 Washington D.C 26-28 1993  G Birkhof f Lattice Theory  American Mathematical Society Providence RI 3rd edition 1967  C Blak e E K eogh and C Merz UCI repository of machine learning databases 1998 http://www.ics.uci.edu 030 mlearn/MLRepository.html  J Boulicaut A Byk o wski and C Rigotti Freesets A condensed representation of boolean data for the approximation of frequency queries Data Mining and Knowledge Discovery  7\(1 2003  D Burdick M Calimlim and J E Gehrk e Ma\002a A maximal frequent itemset algorithm for transactional databases In Proceedings of the 17th International Conference on Data Engineering  April 2001  B Ganter and R W ille Formal Concept Analysis Mathematical Foundations  Springer 1999  E Mephu Nguifo M Liquiere and V  Duquenne JETAI Special Issue on Concept Lattice for KDD  Taylor and Francis 2002  N P asquier  Y  Bastide R T aouil and L Lakhal Disco v er ing frequent closed itemsets for association rules In Proc of 7th Intl Conf on DataBase Theory ICDT  pages 398–416 Jerusalem Israel January 1999  N P asquier  Y  Bastide R T aouil and L Lakhal  Ef 002cient mining of association rules using closed itemsets lattices Journal of Information Systems  24\(1 1999  J Pei J Han and R Mao CLOSET An ef 002cient algorithm for mining frequent closed itemsets In ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery  pages 21–30 2000  J Roberto J Bayardo Ef 002ciently mining long patterns from databases SIGMOD Rec  27\(2 1998  J W ang J Han and J Pei Closet Searching for the best strategies for mining frequent closed itemsets In In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD'03  Washington DC USA 2003  M J Zaki and C Hsiao CHARM An ef 002cient algorithm for closed itemset mining Technical Report 99-10 Rensselaer Polytechnic Institute 1999 
755 
755 
755 


Since the attribute determination algorithm has determined that the attribute Sno in Table 0, the attribute Cno in Table 1, and the attributes <Sno Cno> in Table 2 embrace the double-connective association rule student\(Sno 010 1 course\(Cno 010 2 study\(Sno, Cno\he connective determination algorithm make the relational matrix shown in Fig. 4 according to the binary relationship table of Table 2   C1 C2 C3 C4 S1   T  T  F  F S2   T  F  T  F S3   T  F  F  F S4   F  T  F  F S5   T  F  F  T   Fig. 4 The relational matrix made from Table 2  Fig. 4 is made like this: Table 2 has the tuple S1, C1>, then at the cross of the row S1 and the column C1, a T is filled; Table 2 does not have tuple S1, C3>, then at the cross of the row S1 and the column C3, a F is filled Suppose the cardinality of student\(Sno\s M, in this example 5, i.e. S1 to S5; the cardinality of course\(Cno\n this example 4, i.e. C1 to C4 The algorithms for DCAR1 through DCAR6 are as follows The algorithm for DCAR1 If in Fig. 4 there is M*cf 1 rows, N*cf 2 columns submatrix, in which all elements are Ts, then DCAR1 holds The algorithm for DCAR2 If in Fig. 4 there is at least one column, in which there are at least M*cf 1 Ts, then DCAR2 holds The algorithm for DCAR3 If in Fig. 4 at least M*cf 1 rows have Ts, then DCAR3 holds The algorithm for DCAR4 If in Fig. 4 there is at least one row, in which there are at least N*cf 2 Ts, then DCAR4 holds The algorithm for DCAR5 If in Fig. 4 at least N*cf 2 columns have Ts, then DCAR5 holds The algorithm for DCAR6    DCAR6   DCAR3  DCAR5     DCAR2  DCAR4   DCAR1 Fig. 5 The complement lattice formed by DCAR1 through DCAR6 
277 
277 


000\003 000\\000L\000J\000\021\000\031\000\003\000\003\000&\000R\000Q\000Q\000H\000F\000W\000L\000Y\000H\000\003\000G\000H\000W\000H\000U\000P\000L\000Q\000D\000W\000L\000R\000Q\000\003\000D\000O\000J\000R\000U\000L\000W\000K\000P\000\003 Start Call DCAR1 000D\000O\000J\000R\000U\000L\000W\000K\000P  DCAR1 holds 002  Call DCAR2 000D\000O\000J\000R\000U\000L\000W\000K\000P  Output DCAR1,2,3,4,5,6 End DCAR2 holds 002  Output DCAR2,3,6 Call DCAR3 000D\000O\000J\000R\000U\000L\000W\000K\000P  DCAR3 holds 002  Output DCAR3,6 Call DCAR4 000D\000O\000J\000R\000U\000L\000W\000K\000P  DCAR4 holds 002  Call DCAR5 000D\000O\000J\000R\000U\000L\000W\000K\000P  Output DCAR4,5,6 End DCAR5 holds 002  Call DCAR6 000D\000O\000J\000R\000U\000L\000W\000K\000P  Output DCAR5,6 End DCAR6 holds 002  Output DCAR6 End Error Y N N Y Y N N Y Y N N Y 
278 
278 


If in Fig. 4 there is at least one T, then DCAR6 holds DCAR1 through DCAR6 forms a complement lattice shown in Fig. 5 In Fig. 5, the lower rule implies the upper rule That is, if DCARj is reachable from DCARi via an ascending path, and DCARi holds, then DCARj holds Because DCAR1 through DCAR6 satisfies Fig 5, their algorithms can be merged into one algorithm called connective determination algorithm, shown in Fig. 6 Suppose cf 1 80%, cf 2 75%. In Fig. 4, for the column of C1, there are M*cf 1 5*80%=4 elements whose values are T \(namely, S1, S2, S3, S5 Therefore, DCAR2: course\(Cno 004 1  student\(Sno 003 1  study\(Sno, Cno\olds. From Fig. 5, we know that DCAR3 and DCAR6 also hold. In Fig. 4, there are at least N*cf 2 4*75%=3 columns which have value T \(namely, in the column of C1 there is S1, in the column of C2 there is S1, in the column of C3 there is S2, in the column of C4 there is S5 therefore DCAR5: course\(Cno 003 1  student\(Sno 004 1  study\(Sno, Cno  VI. CONCLUDING REMARKS 1\ Double-connective association rule mining is different from single-connective association rule mining. The former mines the association among the primary keys of the two entity tables and the primary key of the binary relationship table. The latter mines the association between frequent item sets 2\. 4 is different from data cubes in data warehouses. The elements in Fig. 4 are T or F. The elements in the data cubes are data 3\The differences between double-connective association rule and database query are that, first, the query information in databases are predeterminate while the information to be mined by double-connective association rule is not predeterminate, it is implied. Secondly, database query needs to write SQL statements, while double-connective association rule mining is automatic. Thirdly, the information obtained by database query is quantitative, while the information obtained by double-connective association rule mining is qualitative such as “for many”, “there are some  REFERENCES 1 Ji a w ei H a n   M i ch eli n e K a m b er   D a t a  M i n i n g C onc ep t s  a nd Techniques, Higher Education Press, Beijing, 2001, Morgan Kaufmann Publishers, 2000 2 A  G  Ha m i lt on  L o gi c for M a th em a t i c ia ns R evi s ed E d i t i o n   Cambridge University Press, 1988, Tsinghua University Press Beijing, 2003 3 X unw e i Z h o u   Br ie f I ntr o du c t io n  to  Mu t u al l y I nve r s is tic Logic”, 1999 European Summer Meeting of the Association for Symbolic Logic, Utrecht, The Netherlands, August 1-6 1999 4 u n w ei Zh ou F i r s t leve l exp l i c i t m u lt ip le i ndu ct i v e composition”, 2005 Spring Meeting of the Association for Symbolic Logic, The Westin St. Francis Hotel, San Francisco CA. USA, March 25-26, 2005 5 A b rah a m S i lb ers c ha t z  Hen r y  F  Kort h  S S u da rs ha n Dat a b a s e  System Concepts \(Fourth Edition\, Higher Education Press Beijing, 2002, McGraw-Hill Companies, 2002  
279 
279 


support pruning gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis Biosystems vol 85 computationally challenging As training set sizes increase it is likely that these difficulties will also increase VI RELATED WORK While operating on a microarray dataset current CAR 1 2 3 4 and other pattern/rule 20 21 mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets Hence they are generally quite computationally expensive for datasets containing many training samples or genes as the case may be BSTC is explicitly related to CAR-based classifiers but requires no expensive CAR mining BSTC is also related to decision tree-based classifiers such as random forest 19 and C4.5 family 9 methods It is possible to represent any consistent set of boolean association rules as a decision tree and vice versa However it is generally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data 1 2 6 7 11 BSTC is explicitly related to and motivated by CAR-based methods To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here Perhaps the work closest to utilizing 100 BARs is the TOPRULES 22 miner TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci Hence TOP-RULES discovers all 100 confident CARs in a dataset However the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER 23 and so generally isn't polynomial time Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules 24 25 Once obtained fuzzy rules can be used for classification in a manner analogous to CARs However the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM 25 VII CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with preclassification CAR mining see Tables IV and VI we developed a novel method which considers a larger subset of CAR-related boolean association rules BARs These rules can be compactly captured in a Boolean Structure Table BST which can then be used to produce a BST classifier called BSTC Comparison to the current leading CAR classifier RCBT on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining see Section VB Hence BSTC extends generalized CAR based methods to larger datasets then previously practical Furthermore unlike other association rule-based classifiers BSTC easily generalizes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse then CAR-based methods after all exponential time CAR mining is completed O SlS CGl versus O Si CGi As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists ACKNOWLEDGM[ENTS We thank Anthony K.H Tung and Xin Xu for sending us their discretized microarray data files and Top-k/RCBT executables This research was supported in part by NSF grant DMS-0510203 NIH grant I-U54-DA021519-OlAf and by the Michigan Technology Tri-Corridor grant GR687 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies REFERENCES 1 G Cong K L Tan A K H Tung and X Xu Mining top-k covering rule Mining SDM 2002 5 R Agrawal T Imielinski and A Swami Mining associations between sets of items Y Ma Integrating classification and association rule mining KDD 1998 11 T McIntosh and S Chawla On discovery of maximal confident rules without pp 43-52 1999 24 S Vinterbo E Kim and L Ohno-Machado Small fuzzy and interpretable pp 165-176 2006 1071 pp 207-216 1993 6 G Dong pp 273-297 t995 9 pp 5-32 2001 20 W Li J R Quinlan Bagging boosting and c4.5 AAAI vol 1 V Vapnik Support-vector networks the best strategies for mining frequent closed itemsets KDD 2003 4 M Zaki and C Hsiao Charm L Wong Identifying good diagnostic genes or gene expression data SIGMOD 2005 2 G Cong A K H Tung X Xu F Pan and J Yang Farmer Finding interesting rule gene expression data by using the gene expression based classifiers BioiiiJcrmatics vol 21 l and Inrelligent Systenis IFIS 1993 16 Available at http://sdmc.i2r.a-star.edu.sg/rp 17 The dprep package http:/cran r-project org/doclpackages dprep pdfI 18 C Chang and C Lin Libsvm a library for support vector machines 2007 Online Available www.csie.ntu.edu.tw cjlin/papers/libsvm.pdf 19 L Breiimnan Random forests Maclh Learn vol 45 no 1 M Chen and H L Huang Interpretable X Zhang 7 J Li and pp 725-734 2002 8 C Cortes and Mac hine Learming vol 20 no 3 in microarray data SIGKDD Worikshop on Dtra Mining in Bioinfrrnatics BIOKDD 2005 12 R Agrawal and R Srikant Fast algorithms for mining association rules VLDB pp 1964-1970 2005 25 L Wong and J Li Caep Classification by aggregating emerging patterns Proc 2nd Iat Coif Discovery Scieice DS 1999 gene groups from pp 487-499 t994 13 Available ot http://www-personal umich edu/o markiwen 14 R Motwani and P Raghavan Randomized Algoriitlms Caim-bridge University Press 1995 15 S Sudarsky Fuzzy satisfiability Intl Conf on Industrial Fuzzy Contri J Han and J Pei Cmar Accurate and efficient classification based on multiple class-association rules ICDM 2001 21 F Rioult J F Boulicaut B Cremilleux and J Besson Using groups for groups in microarray datasets SIGMOD 2004 3 concept of emerging patterns BioinformJotics vol 18 transposition for pattern discovery from microarray data DMKD pp 73-79 2003 22 J Li X Zhang G Dong K Ramamohanarao and Q Sun Efficient mining of high confidence association rules without S Y Ho C H Hsieh H pp 725-730 1996 10 B Liu W Hsu and support thresholds Principles f Drata Mining aind Knowledge Discovery PKDD pp 406 411 1999 23 G Dong and J Li Efficient mining of emerging patterns discovering trends and differences KDD J Wang J Han and J Pei Closet Searching for An efficient algorithm for closed association rule mining Proc oJ the 2nd SIAM Int Con on Data in large databases SIGMOD 


