A Study on the Granularity of User Modeling for Tag Prediction   E. FrÌas-Martinez M. Cebri·n A. Jaimes Data Mining and User Modeling Group, Telefonica Research Emilio Vargas 6, 28043, Madrid, Spain efm, manuelc, aj aimes}@tid.es   Abstract  One of the characteristics of tag prediction mechanisms is that, typically, all user models are constructed with the same granularity. In this paper we hypothesize and empirically demonstrate that in order to increase tag prediction accuracy, the granularity of each user model has to be adapted to the level of usage of each particular user.  We have constructed user models for tag prediction using association rules in Bibsonomy, a popular social bookmark and publication  sharing system, at three granularity levels: \(1\ canonical, \(2\tereotypical and \(3 individual. Our experiments show that prediction accuracy improves if the level of granularity matches the level of partici pation of the user in the community i.e., amount of tagging in Bibsonomy  1. Introduction  A user model \(UM\as been traditionally defined as a set of information structures designed to represent user preferences h e ren t l y th e defin i tion of  u s er  model does not imply a given granularity of the model i.e., how much information is used to construct the model and how it is represented in the model\ terms of granularity, three main types of user models can be differen 1  canonical user model where the model is the same for all users; \(2 stereotypical  which classifies users into clusters and creates a model for all users within each cluster, and \(3 individual in which a model is constructed per user. The individual  UM has the highest granularity, while the canonical  has the lowest Traditionally, adaptive \(i.e. automatic created using the same granularity for all users. This approach, extensively used in the literature [3 o es  not capture the fact that the more that a user has utilized a system, the better service the user expects 4 p o ssib l e so lu tio n  is to create UM s w h o s e  granularity is adapted to the level of usage of the system. In Bibsonomy h e s o ci al bookm ar k an d  publication sharing system used in this paper, the previous idea implies that the more the user has participated in tagging resources, the better the prediction rate should be In this paper we empirically demonstrate that a match between the level of usage and the granularity of the user model increases prediction rates \(if there is a match the accuracy increases, and if there is a mismatch accuracy decreases Related Work The problem of UM granularity has been raised in the area of information retrieval, and the general agreement is that different tasks require user models of different granularity o w e v e r, t a g prediction mechanisms similar to the one used in this paper [7  a v e n o t s p eci f i c a l l y s t u d i e d t h e i m pact of matching usage and UM granularity levels on tag prediction accuracy. Studies in web interaction have focused on implementing specialized services according to different levels of usag t in  m o s t  cases the levels were explicitly stated by experts. In contrast, in our approach, the level of usage is given by the amount of information available for a particular user, i.e., the process of determining the level of usage is also adaptive  2. Bibsonomy Tagging Behavior  Bibsonomy s a s o ci al bookm ar k i ng  sy s t e m i n  which users describe the resources added to their shared personal library using tags. The data considered for this study is freely available at [10   We use the TAS file from [1 w h i c h c o nt a i n s  816,197 entries. Each entry consists of a user ID, one tag, and a resource \(bookmark or publication\ tagged by that user. The file was manually filtered in order to include only non-spammers We define the level of usage as the total number of tags introduced by a particular user, and a tagging session as the set of tags that a particular user has used to describe a given resource 
2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology 978-0-7695-3496-1/08 $25.00 © 2008 IEEE DOI 10.1109/WIIAT.2008.67 824 
2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology 978-0-7695-3496-1/08 $25.00 © 2008 IEEE DOI 10.1109/WIIAT.2008.67 828 


The dataset of 816,197 entries contains 2,467 unique users who use 69,902 unique tags on 268,692 resources. As shown in Figure 1, the distribution of tags per user follows a power law distribution, with 1.0458 k    k  user per  tags Pr   1  i.e., a small number of users tag very frequently while the majority of users participate in the tagging community with a reduced number of tags. Similar statistical behavior has been shown in other tagging communities [11 2    10 1 10 2 10 3 10 4 10 5 10 5 10 4 10 3 10 2 10 1 10 0 k Pr\(tags per user > k  Figure 1. Distribution of tags per user in TAS  3. UM Granularity for Tag Prediction  This section describes the implementation of three prediction mechanisms, one for each granularity level considered. All thr ee prediction mechanisms have been tested using 3-fold cross validation \(67%-33% for training and testing  3.1. Tag Prediction Architecture  We use association rules \(AR\ construct user models. Association Rules are obtained using the Apriori algorithm [13  T h e prediction architecture is independent of the granularity of the UM and consists of an off-line and an on-line component. The off-line component generates, from a training set, a UM used for prediction. UMs are defined by the set of rules that predict a tag \(consequent of the rule\ given the set of the previous tags used \(antecedent of the rule AR has a confidence and a support value associated The support f a rule is defined as the fraction of strings in the set of transactions of where the rule successfully applies and the confidence a rule is defined as the fraction of times for which if the antecedent X is satisfied, the consequent Y is also true In the terminology of a tagging community, the set of itemsî is the set of tags used, and the set of transactionsî is the set of tagging sessions. ARs were obtained using the Association Rule Induction Tool   The on-line component of the architecture, for each one of the tagging sessions of the testing set, considers each individual tag, and uses that tag to fire the association rules. From all the ARs fired, the consequents of the three rules with the highest confidence form the predicted set of tags. If one of those predicted tags is included in the rest of the tagging session the prediction is considered correct Figure 2 presents this generic architecture The results of the evaluation of the prediction mechanisms are expressed in terms of CP \(number of Correct Tags predicted\ RF \(total number of rules fired\d T \(total number of tags of the testing set where CP/RF expresses the percentage of the total number of correctly predicted tags with respect to the total number of rules fired,  CP/T the percentage of the correct predictions per tag \(similar to recall, but here we divide by the total number of tags in order to evaluate the impact of the prediction system in the final user\d RF/T the number of rules fired per tag which indicates the computational load  Figure 2. Prediction Engine Architecture  3.2. Canonical User Modeling  We generated four different canonical UMs for different values of support 1%, 0.1%\ and confidence 50 80%\The results obtained from 3-fold cross-validation are presented in Table 1 Canonical UMs can be used for any user of the tagging community but only capture knowledge that is common to the entire community \(i.e., all of the users whose tags are included in the training set, with the rules defined by and ur hypothesis is that canonical UMs perform well at predicting tags for users with a low level of participation in the tagging community \(new users or users with a low level of usage\, but higher granularity models \(i.e   Tag  Prediction Engine     Predicted Tags   Use r  Model\(s   Apriori Algorithm   Training Tagging Sessions  T esting  Tagging Sessions  
825 
829 


stereotypical, individual\ better for users that have participated more in tagging activities  Table 1. Evaluation of the canonical user model  CP/RF CP/T RF/T 0.1-50 22 2 0.08 1-50 24 2 0.08 0.1-80 26 3 0.09 1-80 27 3 0.09  It is important to note that in the canonical user model few rules fire per tagóthis is to be expected because since the majority of users participate with a small number of tags, relatively ìfewî of the tags will lead to rules and those ARs constructed are not likely to fire  3.3. Stereotypical User Modeling  This level of granularity constructs UMs from clusters of users. Ideally, such clusters should correspond to groups of users that share common tagging interests. First, we identify the tagging interests of every user by building a ìtagging interest modelî. Next, we find clusters of users with similar interests, and finally, we build UMs for each of the clusters found We defined the tagging interests of each user as the set of tags that the user has assigned at least 20 times This eliminates from consideration any users that have not used any tags 20 times or more, reducing the candidate users to 355 \(out of the original 2,467 unique users\, each one with a different number of tags in their user model The next step consists in finding clusters of users using the tagging interest model \(i.e., clustering the 355 users based on the tags they have used more than 19 times\he first option is to represent the tagging interest model as a vector in which each dimension corresponds to one of the tags used more than 19 times. Given the large number of tags, however, this approach produces a sparse matrix which implies, from a clustering algorithm perspective, the curse of dimensionality problem. In order to avoid this problem, each user is represented by a vector containing the number of tags that the user has in common with each one of the remaining 354 users Users were clustered using agglomerative hierarchical clustering using Euclidean distance. Based on the clustering obtained \(Figure 3\we manually identified 4 clusters for the construction of stereotypical UMs It is interesting to note that the clusters correspond not only to similar interests but also coincide with the level of usage \(see Table 2\ while cluster 2 groups 314 users where the average number of tags introduced in the system amounts to 1,843, cluster 4 groups only 7 users but where each one is responsible, in average, for introducing more than 44,000 tags. The fact that there is a kernel of users that are responsible for the majority of the interaction was already shown in Figure 1 Figure 3. Clusters identified by hierarchical clustering  Table 2. Cluster Characteristics Cluster of users Average # of tags introduced C1 26 7,680 C2 314 1,843 C3 5 12,823 C4 7 44,032  A stereotypical UM has been generated for each one of the 4 clusters. We have generated association rules for 1% and 80%.  Table 3 presents the CP/RF CP/T and RF/T rates for each one of the set of association rules generated for each cluster \(C1, C2 C3, C4  Table 3. Evaluation of the set of stereotypical UMs Cluster CP/RF CP/T RF/T C1 43% 12% 0.12 C2 30% 5% 0.15 C3 46% 20% 0.25 C4 47% 22% 0.25  Table 3 shows that if the level of usage matches the granularity of the user model there is an increase in the correct prediction rate. Clusters C1, C3 and C4 group users that, when compared to C2, have a higher level of usage which causes an increase in the CP/RF and CP/T ratios  3.3. Individual User Modeling  Individual UMs can be constructed for any user of the tagging system. However, for new users or users with very little activity, there will usually not be enough information to construct a useful UM \(the cold start problem\dividual UMs can be very powerful however, because in tagging communities tags tend to 
826 
830 


be very individual and unique for each us especially for individuals with a high level of participation. Because C4 users have the highest participation in the community they are the best candidates to implement individual UMs. We selected from C4, the first five users with the highest participation in the community, and for each one of these users an individual UM was constructed 1 and 80%\able 4 presents the results for each one of the users selected. All five users have CP/RF ratios higher than 52% and CP/T values of at least 19  Table 4. Evaluation of the set of individual UMs User CP/RF CP/T RF/T 244 53% 19 0.32 41 60% 22 0.3 45 57% 22% 0.29 467 55% 25 0.33 337 61% 24 0.3  4. Comparative Analysis  The results presented verify the hypothesis presented in this paper: matching granularity in UM with level of usage is a key factor in providing good predictions. For comparison purposes, Table 5 presents the results for the canonical, stereotypical and individual models, where the canonical model presents the results for 1%-80%, and the stereotypical and individual models the average values for the cases considered in Table 3 and Table 4. By observing the CP/RF ratio, the results show that the higher the level of usage the higher the level of granularity should be  Table 5. Comparison of the 3 levels of granularity CP/RF CP/T RF/T Canonical 27 3 0.09 Stereotype 41 14 0.19 Individual 57 22 0.3  For a new user, or for a user that has a low participation in the tagging community, the canonical UM can be used. For users that have an intermediate level of usage, a stereotypical UM should be used. In this case, if a canonical model is used the mismatch will imply a reduction in the correct prediction rate Finally, for users with a high level of usage, an individual UM should be constructed  5. Conclusions  The hypothesis that this paper tested was that the matching between the level of usage with the correct granularity of the UM improves the prediction rate i.e., that the level of granularity should not be the same for all users but has to be adapted to the level of usage of each individual user The hypothesis was tested with a tagging prediction mechanism based on Association Rules. The results showed that in order to increase the prediction rate, the higher the level of usage the higher the level of granularity of the UM should be Future work includes investigating the following: \(1 how to identify the scope of information used in the construction of the models \(i.e., size and shape of clusters in the stereotypical case\d \(2\ow and when UMs evolve from one granularity to the next Also it would be interesting to study the relation between level of usage and level of expertise in order to extend the conclusions of the study  6. References   A  Ko b s a  G en eri c User  M o d e l i n g S y st e m s  User Modeling and User-Adapted Interaction 11, 2001, pp. 49-63 2 D  H a r t m u t, U  Ma li now s k i, T  K hm e  M Sc hne i d e r Hufschmidt State of the Art in Adaptive User Interfaces  Siemens Corporate Research and Development 3 E Fr ia s Ma r tine z  S  C h e n a nd S  L i u S ur v e y of D a t a  Mining Approaches to User Modeling for Adaptive Hypermedia IEEE Tran. SMC-C 36\(6\, 2006, pp. 734-749  E  F r i a s-M a rt i n ez S  Ch en   S  L i u  R M acread i e  T h e  Role of Human Factors in Stereotyping Behaviour and Perception of Digital Library Users: A Robust Clustering Approach UMUAI 17\(3\, 2006, 305-337 5 R  J  s c hk e  A  H o th o, C  Sc hm itz a nd G  Stum m e   Analysis of the Publication Sharing Behaviour in BibSonomy Proc. Conceptual Structures: Knowledge Architectures for Smart Applications Springer, 2006 6 J. L u J. Ca lla n U se r m ode ling f o r f u ll-te x t  f e de ra te d  search in peer-to-peer networksî, SIGIR 2006, pp. 332ñ339 7 G  Mis hne  A C o lla bor a t i v e  A ppr oa c h  to A u tom a te d ta g  Assignment for Weblog Posts WWW 2006 pp. 953-954 8  P  He y m a n D. Ra m a g e H G a r c ia Molina   S oc ia l T a g Predictionî, SIGIR 2008, pp. 531-538  A  L azan d e r an d H B i eman s Di f f e ren ces b e t w een  novice and experienced users in searching information on the World Wide Web JASIST  51 6\, 2000, pp. 576-581 10 htt p w ww  k de c s.uni-k a sse l  de w s/rsdc 08  11 R. A n g e lo v a M L i p czak  E. Milio s an d  P   P r alat  Characterizing a social bookmarking and tagging network 18th European Conf. Artificial Intelligence 2008  B  Si gur bj r ns s o n a n d R  va n Zw ol  F lic ke r Ta g  Recommendation based on Collective Knowledge WWW 2008 pp. 327-336 13 R  A g g r a w a l T  I m ie linsk i, A  S w a m i M ining  Association Rules between Set of items in large Databases Proc. ACM SIGMOD, 1993, pp. 207-216 14 C. B o rg e lt, A ssoc i a tion Rule In duc ti on T ools   http://www.borgelt.net/software.html, Ver. 1.10, 2007 
827 
831 


4 Completeness theorems In this section we introduce an axiomatic system for our logic and prove completeness theorems First we introduce deduction rules and a notion of a proof from a set T of FAIs Second we prove that A  B is provable from a set T of FAIs iff A  B semantically follows from T in degree 1 ordinary completeness Third we introduce a concept of a degree j A  B j  T of provability of A  B from a fuzzy set T of FAIs and show that j A  B j  T  jj A  B jj  T graded completeness 4.1 Deduction rules Our axiomatic system consists of the following deduction rules  Ax infer A  A  B  DCut from A  B and B  C  D infer A  C  D  Sh from A  B infer c 003  A  c 003  B for each A B C D 2 L Y  and c 2 L  Rules Sh are to be understood as usual deduction rules having FAIs which are of the form of FAIs in the input part the part preceding infer of a rule a rule allows us to infer in one step the corresponding FAI in the output part the part following infer of a rule Ax is a nullary rule axiom which says that each A  A  B  A B 2 L Y  is inferred in one step Rules Sh resemble Armstrong rules from database theory Remark 1 If 003 is globalization Sh can be omitted Indeed for c  1  we have c 003  1 and Sh becomes from A  B infer A  B  which is a trivial rule for c  1  we have c 003  0 and Sh becomes from A  B infer Y  Y  which can be omitted since Y  Y can be inferred by Ax A  B is called provable from a set T of FAIs using a set R of deduction rules written T  R A  B  if there is a sequence  1       n of fuzzy attribute implications such that  n is A  B and for each  i we either have  i 2 T or  i is inferred in one step from some of the preceding formulas i.e  1       i 000 1  using some deduction rule from R  If R consists of Sh we say just provable     instead of provable    using R  and write just T  A  B instead of T  R A  B  A deduction rule from  1       n infer     i   are fuzzy attribute implications is said to be derivable from a set R of deduction rules if f  1       n g  R   Again if R consists of Sh we omit R  We omit the proof of the next lemma Lemma 16 The following deduction rules are derivable from Ax and DCut Ref infer A  A  Wea from A  B infer A  C  B  Add from A  B and A  C infer A  B  C  Pro from A  B  C infer A  B  Tra from A  B and B  C infer A  C  for each A B C D 2 L Y  4.2 Ordinary completeness In this section we show that deduction rules Sh are sound and we prove their completeness A deduction rule from  1       n infer   is said to be sound if for each M 2 Mod f  1       n g  we have M 2 Mod f  g   i.e each model of all of  1       n is also a model of   The following lemmas are needed to prove our completeness theorems The proofs are technically involved and we omit them due to lack of space Lemma 17 Each of the deduction rules Sh is sound A set T of FAIs is called semantically closed if jj A  B jj  T  1 iff A  B 2 T  i.e if T  f A  B j jj A  B jj  T  1 g  syntactically closed if T  A  B iff A  B 2 T  i.e if T  f A  B j T  A  B g  The following lemma is almost immediate Lemma 18 A set T of fuzzy attribute implications is syntactically closed iff we have Ax A  A  B 2 T  DCut if A  B 2 T and B  C  D 2 T then A  C  D 2 T  Sh if A  B 2 T then c 003  A  c 003  B 2 T for each A B C D 2 L Y  and c 2 L  Lemma 19 Let T be a set of fuzzy attribute implications If T is semantically closed then T is syntactically closed Lemma 20 Let T be a set of fuzzy attribute implications let both Y and L be 002nite If T is syntactically closed then T is semantically closed Corollary 21 If L and Y are 002nite a set T of FAI T is syntactically closed iff T is semantically closed Theorem 22 ordinary completeness Let L and Y be 002nite Let T be a set of fuzzy attribute implications Then T  A  B iff jj A  B jj  T  1  
250 


Proof Sketch Denote by syn  T  the least syntactically closed set of fuzzy attribute implications which contains T  It can be shown that syn  T   f A  B j T  A  B g  Furthermore denote by sem  T  the least semantically closed set of fuzzy attribute implications which contains T  It can be shown that sem  T   f A  B j jj A  B jj  T  1 g  To prove the claim we need to show syn  T   sem  T   As syn  T  is syntactically closed it is also semantically closed by Corollary 21 which means sem  syn  T  022 syn  T   Therefore by T 022 syn  T  we get sem  T  022 sem  syn  T  022 syn  T   In a similar manner we get syn  T  022 sem  T   showing syn  T   sem  T   The proof is complete 003 4.3 Graded completeness We now turn to a graded version of the completeness theorem Note that Theorem 22 can be read as providing a syntactic characterization of entailment in degree 1  i.e of jj A  B jj  T  1  However entailment comes in degrees in general jj A  B jj  T is a degree not necessarily equal to 0 or 1  Our aim is to capture jj A  B jj  T syntactically For this purpose we introduce a notion of a degree j A  B j  T of provability of A  B from a fuzzy set T of FAIs Then we show that j A  B j  T  jj A  B jj  T  which can be understood as a graded completeness completeness in degrees Note that graded completeness was introduced by Pavelka see e.g 7 for detailed information For a fuzzy set T of FAIs and for A  B we de\002ne a degree j A  B j  T 2 L to which A  B is provable from T by j A  B j T  W f c 2 L j c T   A  c 012 B g  17 where c T  is de\002ned as in Lemma 5 This makes use of the reduction of general entailment from fuzzy sets of FAIs to entailment in degree 1 from crisp sets of FAIs established in Corollary 7 The we get Theorem 23 graded completeness Let L and Y be 002nite Then for every fuzzy set T of fuzzy attribute implications and A  B we have j A  B j  T  jj A  B jj  T  5 Further issues We provided several results regarding a logic of attribute containment for graded attributes Main results established in the paper are a description of non-redundant bases of data tables with fuzzy attributes and completeness theorems for such logic We omitted several issues which we plan to include in the full version Most importantly model-theoretic results related to an ef\002cient checking of entailment computation of non-redundant bases and illustrative as well as real-world examples References  S Abiteboul et al  The Lowell database research self-assessment Communications of ACM  48 pp 111ñ118 2005  R Belohla v ek Fuzzy Relational Systems Foundations and Principles  New York Kluwer 2002  R Belohla v ek V  Vychodil Fuzzy attrib ute implications computing non-redundant bases using maximal independent sets in AI 2005 Arti\002cial Intelligence  S Zhang and R Jarvis Eds LNAI  3809 Berlin Springer-Verlag pp 1126ñ1129 2005  R Belohla v ek V  Vychodil  Attrib ute implications in a fuzzy setting in ICFCA 2006 International Conference on Formal Concept Analysis  R Missaoui and J Schmid Eds LNAI  3874 Berlin Springer-Verlag pp 45ñ60 2006  B Ganter  R W ille Formal Concept Analysis Mathematical Foundations  Berlin Springer 1999  G Geor gescu A Popescu Non-dual fuzzy connections Archive for Mathematical Logic  43 pp 1009 1039 2004  G Gerla Fuzzy Logic Mathematical Tools for Approximate Reasoning  Dordrecht Kluwer 2001  J A Goguen The logic of ine xact concepts Synthese  18 pp 325ñ373 1968  J.-L Guigues V  Duquenne F amilles minimales d'implications informatives resultant d'un tableau de donn  ees binaires Math Sci Humaines  95 pp 5ñ18 1986  P  H  ajek Metamathematics of Fuzzy Logic  Dordrecht Kluwer 1998  P  H  ajek T Havr  anek Mechanizing Hypotheses Formation Mathematical Foundations for a General Theory  Berlin Springer 1978  D Maier  The Theory of Relational Databases  Rockville Computer Science Press 1983  J Rauch Logic of association rules Applied Intelligence  22 pp 9ñ28 2005  C Zhang C S Zhang Association Rule Mining Models and Algorithms  Berlin Springer-Verlag 2002 
251 


                                                                                                                 
456 


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efﬁcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efﬁciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efﬁciency When the clustering model is available it is a signiﬁcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques Sufﬁcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efﬁcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efﬁcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conﬁdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conﬁdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conﬁdence The sufﬁcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efﬁcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efﬁciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufﬁciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70–81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207–216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487–499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145–154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146–153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9–15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597–600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern Classiﬁcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155–162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512–521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1–12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265–278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283–304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476–482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559–563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220–231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13–24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10–17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549–550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188–201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259–283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909–921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305–345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432–444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conﬁdence Intelligent Data Analysis  9\(4\:381–395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49–73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407–419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1–12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki Efﬁciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642–658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483–490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194–203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344–353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efﬁcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103–114 1996 
618 
618 


