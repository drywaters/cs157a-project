Finding Correlated item pairs through e fficient pruning with a given threshold Bo Wang  Liang Su  Aiping L i   Peng Zou  School of Computer, National University of Defense Technology, China bowang_s.x@163.com slnudt@gmail.com apli1974@gmail.com zpeng@nudt.edu.cn   Abstract  Given a minimum threshold in a massive market- basket data set, an item pair whose correlation above the threshold is  considered correlated. In this paper, we provide a randomized  algorithm SERIT 002 a Searching-corrElated-pair Randomized algorithm for dIfferent Thresholds 002 to find all correlated pairs effectively, which adopts the Pearson’s correlation coefficient [11  as the measure criterion. In their CIKM’06 paper [2   Z h an g et al  a ddr ess t h e s a me  pr obl e m  by taking the relation of Pearson’s coefficient and Jaccard distance into account. However it is inefficient when the threshold is small. We propose a new probability function to prune uncorrelated item pairs based on [2 002 which can cover the shortage of the former one. Experimental results with synthetic and real data sets reveal that with a given threshold, even if it is small, SERIT algorithm can prune the item pairs unwanted efficiently and save large computational resources  1. Introduction  Finding correlated attribute pairs plays an important role in many applications, such as market-basket analysis climate studies, public health and bioinformatics [3 e  focus on market-basket itemset, which is a data model for many datasets if they involve two concepts: a set of items and a set of relations \(baskets Furthe rm ore, ite m s can  always be transformed to binary attributes. In such a market-basket model, finding correlated attribute pairs is to find correlated item pairs, which is a quintessential and one of the most fundamental problems In a massive data set, we may face a new problem of finding all correlated item pa irs effectively. When an appropriate measure criterion is identified,  a traditional brute-force approach needs to compute all item pairs and select the pair whose correla tion is above a user-specified threshold. The approach is easy to implement, but not applicable. Since n items will combine to 2 1 2 n nn C 000\020 000 pairs the computation complexity is at least 2  On Thus, we need a more efficient method especially when the main-memory capacity is limited. A natural thought is to mine the essential character of the measure criterion, which may speed up the pruning process In this work, we propose a new algorithm SERIT \(A Searching-corrElated-pair Random algorithm for dIfferent Thresholds whi c h t a kes Pears o n  s c o ef fi ci en t 000I 1 as c o r r e l a t i on m easure Acc o r d i n g t o a  new  probability function, uncorrelated item pairs are pruned and the remains are added to the candidate set. This the first step of SERIT called searching step. In the second refinement step, the correlation is comput ed exactly for each candidate pair to generate the final results. As demonstrated by our experiments on both real and synthetic data sets, with different thresholds, SERIT can prune the unwanted item pairs effectively bounded with a small false-negative tolerance. Moreover, our algorithm also deals with the shortage of the algorithm mentioned in [2 e efficien cy is too low with a small threshold 1.1. Related work  There already exist some methods to identify correlation association\ patterns in market-basket itemsets. Two measurements are often considered when evaluating a method: one is the power of pruning which can be reflected through execution time. The more powerful the pruning rule the smaller number of candidate pairs need to exactly compute  and the more execution time can be saved. The other is the accuracy. Some method may cut off the item pair truly correlated leading to false negative. Hence, the selection of methods is the result of balancing between pruning power and accuracy   This paper mainly relates with two kinds of literatures one category is about statisti cal correlation measure. The other is association-rule mining In the framework of data mining, association-rule mining has been an active research field. There are many algorithms for efficient mining under kinds of constraints [4 6, 10, 13 Th e su ppo r t  of  one asso ciatio n ru le is of ten u s ed  to judge the rule’s importance. It can be interpreted as the fraction of baskets contained a item set with the total baskets Most of the former work in identifying correlation association\statistically is primarily about the measures of correlation [5, 8, 9, 12, 15, 16  H o w e v e r  how t o f i nd t h e correlated pairs efficiently ha s not been concerned deeply which is often encountered in massive-data-set tasks Previously, in paper [1 Xion g et al propose TAPER algorithm to find strongly correlated item pairs. It chooses Pearson’s coefficient \(the 000I correlation coe fficient is the computation form for binary variables\e correlation measure. Then an upper bound of 000I  is provided, which can be computed as a function of the support of individual item Furthermore, the good monotone property of this upper bound allows the elimination of many item pairs directly TAPER is a two-pass algorith m and can gain the set of correlated item pairs exactly Xiong again exploits a 2-D monotone property of the upper bound of 000I  in  Its m a in 
The Ninth International Conference on Web-Age Information Management 978-0-7695-3185-4/08 $25.00 © 2008 IEEE DOI 10.1109/WAIM.2008.84 419 
The Ninth International Conference on Web-Age Information Management 978-0-7695-3185-4/08 $25.00 © 2008 IEEE DOI 10.1109/WAIM.2008.84 413 


contribution is the development of TOP-COP query algorithm to find the top-k co rrelated item pairs without a given threshold, which may be difficult for users to decide The 2-D monotone property decides the next item pair to be considered in a diagonal traversal direction. These two algorithms both need to sort the items by support values in non-increasing order. Moreover, TOP-COP also needs to construct a correlation matrix In order to enhance the pruning efficiency and save more execution time, Zhang et al introduce a randomized method adapted from TAPER in 2 ro ugh analysis of th e r e lation b e t w een Jaccar d  distance and the upper bound of 000I according to the probability property showed by a min-hash function [7  the algorithm is more effectiv e than TAPER. Because of the randomicity, it only selects the correlated pair in high probability and may filter out some truly wanted ones However, the false negative can be controlled. The shortage of this algorithm, just as menti oned in h at the efficienc y  lows down when the threshold is small, is also obvious. That is to say, it is more suitable to find the strongly correlated item pairs   We can draw a conclusion from the above analysis, that is, there are two key considera tions: \(1\e choice of the beginning point which searching starts from. The beginning point can be chosen randomly, or following some property For example, [1 ses th e ite m h a v i ng t h e b i g g e st suppo r t  value. \(2\he choice of sear ching strategy or searching direction. We can choose the next point sequently such as the sequence of support values in non-increasing order, or according to some special probability distribution 1.2. Overview of the paper   The rest of the paper is organized as follows: in section 2 we introduce the basic concepts, and the theories of TAPER and the algorithm in [2 Fo r sim p lic ity we call the  algorithm in [2 Big THresho l d n th is p a p e r In section 3, our SERIT algorithm is presented, with proposal of the new probability function. In section 4, we provide experimental results comparing with TAPER and BTH. The results show that our algorith m is efficient and scalable Section 5 summarizes the whole paper and shows some possible future work  2. Basic concepts   To simplify the discussion, table 1 firstly summarizes the notations that we will use throughout this paper T ABLE 1 NOTATIONS  T A relation table 002 namely the market-basket data set. A column in T represents an item 002 and a row represents a shopping basket. If basket i contains item j 002 it is recorded as  T  i,j   I the set of items, such as 000 000     abcde f gh  n the number of items, namely the number of columns in T  m the number of baskets, namely the number of rows in T  S  i  the set of baskets containing item i namely the set of rows satisfying 000 000\013 000\014 000 1 rmTri 000d 000  sp  i  the support value of item i  000T  a given threshld 002 if the correlation is bigger than 000T the item pair is correlated  For an item pair 000\013 000\014  ab if we know 000\013\000\014 s pa and 000\013 000\014 s pb the 000I coefficient \(namely Pearson corr elation coefficient\n be expressed in Equation 1 000\013\000\014 000\013 000\014\000\013\000\014\000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014   11 sp a b sp a sp b ab s paspb spa spb 000I 000\020 000 000\020\000\020 1 Hence the problem of finding all correlated item pairs can be described as follows: For a given threshold 000T it needs to find all 000\013 000\014  ab satisfying 000\013\000\014  ab 000I 000T 000t    Without loss of generality, let 000\013 000\014 000\013 000\014 s pa spb 000t Xiong et al reveal a upper bound of 000\014 000\013  ab 000I in [1 w e sho w in  Equation 2. Specifically, they also prove the monotone property of this upper bound. In this paper, we use this property as Lemma 1 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 1  1 s pb spa a b upper a b s pa spb 000I\000I 000\020 000d\000 000\020 2 LEMMA 1 Given an item pair 000\013\000\014  ab without loss of generality, let 000\013 000\014 000\013 000\014 s pa spb 000t If we fix item a and change b  the upper bound of 000\014 000\013  ab 000I is monotone decreasing with the decrease of support value of item b Moreover 000\013 000\014 000\013 000\014 0,1 upper a b 000I 000\037 000d is satisfied   The TAPER algorithm is based on Lemma 1. Firstly, it sorts all the items by support values in non-increasing order like 000 000 12   m ii i If there is the fact of 000\013 000\014 000\013 000\014 1  lk upper i i 000I 000T 000\037  and 000\013 000\014 000\013 000\014 12 kk s pi spi 000t according to Lemma 1, we can get 000\013 000\014 000\013 000\014 000\013 000\014 000\013 000\014 21  lk lk upper i i upper i i 000I 000I\000T 000\037 000\037 Hence, the pair 000\013 000\014 2  lk ii  can be removed. After the first pass, TAPER gets pairs whose upper bound are no smaller than 000T namely candidate pairs. Whether they are truly correlated needs further computation with Equation 1. This is the work during the second refinement step The main contribution of TAPER algorithm is to propose a method of computing the upper bound of 000I with individual item. Although it can remove many uncorrelated pairs, there is great potential for improvement. Firstly 000I coefficient is relevant with 000\013 000\014  s pab  this fact can be easily derived from Equation 1\ other words, this correlation measure considers the frequency of co-occurrence of the two items statistically, while TAPER only takes the frequency of individual item into account For instance, for item pair 000\014 000\013  ab satisfying 000\013 000\014 000\013 000\014 s pa spb 000 we get 000\013 000\014 000\013 000\014 1 upper a b 000I 000  with Equation 2. That is to say, if item a and b have close support values, pair 000\014 000\013  ab  will be chosen into candidate set 
420 
414 


In addition a s d i scov ered t h at item pairs rem o v e d b y  TAPER coincide with this feature: one item has small support value, and the other has a relatively big one. Those both having small \(or big\ values are remained as candidate pairs. Hence, Zhang et al design a randomized algorithm BTH, which reflects the frequency of co-occurrence of item a and b by introducing Jaccard distance. The Jaccard distance of 000\014 000\013  ab equals 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 1 Sa Sb Sa Sb 000 000\020 000 where 000x is the number of elements in a set. Let 000\013 000\014 000\013 000\014  R upper a b 000I 000 BTH reveals that Jaccard distance holds an inequa lity \(shown in Lemma 2\ with R and 000T  LEMMA 2 For any given item pair 000\013\000\014  ab and threshold 000T we have the inequality 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 1 Sa Sb Sa Sb R R 000T 000T 000 000t 000 000\016\000\020 What’s more, on the premise of that R is no smaller than 000T   1 R 000T 000t\000t  000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 Sa Sb Sa Sb 000 000  achieves its minimum value of 2 000T  when R 000T 000  Hence 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 Sa Sb Sa Sb 000 000 is bigger than 2 000T when 000\013\000\014 000\013\000\014  upper a b 000I is above 000T In this case, pair 000\013\000\014  ab can be removed if 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 2 Sa Sb Sa Sb 000T 000 000\037 000 2 ad d ition   Zh ang et al  discover that the min-hash function has the following property 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 min min Sa Sb hahb Sa Sb 000 0005\000 \000 000 BTH defines an equivalence relation 000\021 When 000\013\000\014 ab x 0005\000 000\021 According to Lemma 2, if 2 x 000T 000 we can get 000\013 000\014 000\013 000\014  upper a b 000I 000T 000 H repeats the process t times. Each time a set of k min-hash functions is chosen and pair 000\013\000\014  ab is mapped. Finally, the probability that item pair 000\013\000\014  ab satisfying ab 000\021 t into the candidate set is 000\013\000\014 11 t k x 000\020\000\020 The algorithm expects to select the pair having 2 x 000T 000 Experime show that BTH is more efficient than TAPER under a small false-negative tolerance, by judging parameter k and t  3. The algorithm of SERIT 3.1. The description of SERIT Figure 1 shows the probability distribution of 000\013 000\014 000\013 000\014 000\013\000\014 000\013\000\014 Sa Sb x Sa Sb 000 000 000 Given a threshold 000T if pair 000\013 000\014  ab has 000\013 000\014 000\013 000\014 000\013\000\014 000\013\000\014 2 Sa Sb Sa Sb 000T 000 000 000 as mentioned above, we can get 000\013 000\014 000\013 000\014  upper a b 000I 000T 000  000\013\000\014  ab  can be added to candidate set. As we know, the probability of a pair 000\013 000\014  ab  added to candidate set in BTH is 000\013 000\014 11 t k x 000\020\000\020 Note that with a relatively big threshold 000T the probability value of a candidate pair centres in the “northeast” area of the “S” shaped curve, that means an item pair who has 000\013\000\014 000\013\000\014  upper a b 000I 000T 000 would be more likely chosen as candidate pair. This is exactly the consequence BTH expects. But when 000T is small, some candidate item pairs locate in th e middle of the curve. These candidate pairs might be misse d because of low probability inducing the increase of false negatives. To bound the false negative with a small tolera nce, BTH needs to raise kt 000  which may low down efficiency. From the experimental results in false-ne gative tolerance equals 0.005 kt 000  needs to be adjusted to 740 when 0.3 000T 000 while only 94 with 0.5 000T 000  To resolve this problem, we firstly analyze the probability function 000\013 000\014 f x If only one min-hash function is used 000\013 000\014 f x  has the form of x illustrated in Figure 2\(a\ously this distribution won’t meet our expectation, which would be like the curve in Figu re 2\(b\We expect that the item pair selected must have 000\013 000\014 000\013 000\014 000\013\000\014 000\013\000\014 2 Sa Sb Sa Sb 000T 000 000 000 namely 000\013 000\014 000\013 000\014  upper a b 000I 000T 000 To close to the ideal probability distribution w ith different 000T we give the definition of ab 000z  0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Threshold small 2 Threshold big 2 1-\(1-x k  t x  Fig. 1 Shape of function 000\013\000\014  1 1 t k f xx 000 \000\020 000\020  0  2 0  4 0  6 0  8 1 0  2 0  4 0  6 0  8 1  2 000T  Fig. 2\(a\ Shape of function  f xx 000 Fig. 2\(b\robability distribution  DEFINITION 1 For two items a and b  ab 000z if and only if 000\013 000\014 000\013 000\014 min min hahb 000z  is satisfied for all of the k min-hash functions 
421 
415 


Still, when 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 min min Sa Sb hahb x Sa Sb 000 0005\000 \000 000 000 with one min-hash function 000\013\000\014 1 ab x 0005\000z\000 \000\020 then with k independent functions 000\013\000\014\000\013\000\014 1 k ab x 0005\000z\000 \000\020 The process is repeated t times and each time we require k independent min-hash functions As a result, the probability that 000\013\000\014  ab satisfying ab 000z put into the candidate set is 000\013\000\014 000\013\000\014 111 t k gx x 000 \000\020\000\020\000\020 Figure 3 describes the shape of g  x different k and t According to the feature of g  x we can select item pairs that satisfied 2 x 000T 000\037 with high probability, th en the remain requires further computation as candidate item pairs. In this way, the main process of SERIT is: We repeat t times, and each time item pairs satisfying ab 000z are selected and put into item set 0006 The final candidate set is I I 000u\000\020\0006 We can prove that kt 000  is reasonable with different 000T no matter it is big or small, by experimental results. If there is an item pair 000\013\000\014  ab having 000\013 000\014 000\013 000\014 000\013\000\014 000\013\000\014 2 Sa Sb Sa Sb 000T 000 000 000 the probability that it is removed and never added into candidate set is 000\013\000\014 000\013 000\014 22 111 t k g 000T 000T\000W 000 \000\020 000\020 000\020 000 While the probability that other pairs having 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 2 Sa Sb Sa Sb 000T 000 000 000  are removed would be smaller than 000W With the fixed values of t and false-negative tolerance 000W  000\013\000\014 000\013 000\014 2 1 11 t klog 000T 000W 000\020 000 \000\020\000\020 can guarantee the probability that a candidate item pair missed is no bigger than 000W  T ABLE 2 THE COLUMN VALUES MAPPED BY H 1  1 000Q 2 000Q 1 k 000Q 000\020 k 000Q  0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.2 0.4 0.6 0.8 1.0 K:10,T:2 K:10,T:5 K:30,T:5 K:8,T:7 X 1-\(1-\(1-x k  t  Fig. 3 Shape of function 000\013\000\014 000\013\000\014 111 t k gx x 000 \000\020\000\020\000\020   Here, how to single out item pairs we wanted also need considerations. The brute-force alternative is to pass through all the item pairs and judge whether it satisfies ab 000z This method is definitely slow and costly fin d s t h e  equivalence class by a hash bucket while both the time and space complexity are 000\013 000\014 on But this data structure is not suitable for our SERIT any more. We design a hash structure as the bitmap shown in Figure 4. Furthermore, we can receive the set 000\013 000\014 000\013 000\014 000 000   ab ab doesntsatisya b 000z directly without computation of 0006    After the first pass of data set, we can get kt 000 min-hash function values for each item. There is nkt 000<\000 vakues totally which can be devided into t groups of size k We choose a hash function H. For each item, H maps one of its kt 000  min-hash function values as a key into a lattice in the bitmap The i th 1 ik 000d 000d in-hash function value will be mapped into the lattice in i-th row. Suppose there are p columns More importantly, each lattice needs to maintain a linked list recording the items whose min-hash function value is mapped into the lattice. We consider the case that the i-th min-hash value of item x is mapped into lattice  ij P in i-th row and j-th column\this is the first time that  ij P is mapped, we label  ij P with “1” and add x into the corresponding linked list  ij L Otherwise, some other item\(s has been mapped into  ij P before and  ij P is already “1 What we need is to add x into the linked list  ij L To simplify the discussions  ij L also stands for the item sets contained in the linke d list. Clearly, the i th min-hash function of each item in  ij L  has the same value. We give an example that shows how the candidate set is generated. In round 1, we choose 1 H as the hash function for the min-hash function values contained in Group 1. Table 2 shows the values of columns mapped by 1 H  for each item where 000 000 123  k vv v v represents the k min-hash function c\illust rate the bitmap after the mapping of item a  b and c respectively. Par ticularly, we can get Set 000^\000 000^\000  1  k cij ij i LccL 000 0007 000 \000\020\000\000 000 000 000  ab 000 000 000 b 000 000 000   ab 000‰\000‡\000‰\000 000  Note that with each item in c 0007 item c has at least one equal min-hash value. The pair generated by c and an item from c 0007  is exactly the pair we want ed, because it doesn’t satisfy ab 000z As a result, item pairs  ac and  bc are added into the candidate set. By the same token, other items \(such as a  b  c 0007 In round 2, a new hash function 2 H  is introduced. At the end of round t we can get the final candidate set and start the refinement step The whole algorithm is described in Algorithm 1 
422 
416 


 Fig4\(a\. The bitmap after a is mapped by H1   Fig4\(b\. The bitmap after a,b are mapped by H1          Fig4\(c\ The bitmap after a,b and c are mapped by H1 Algorithm 1 000 SERIT  finding correlated pairs in massive itemset for a given threshold Variable description S   the set of correlated pairs C the candidate set Ti   the item that has at least one same min-hash value as item i is added to set Ti  000 000 000 000 min hil the l th min-hash value of item i  000 000 000 000 Lj u   the set linked to the lattice in j th row and u th column in the bitmap table v H  v H  1 vt 000d\000d to map 000 000 000 000 min hil to a lattice in l th row in Round v  Input data set T   kt 000 min-hash functions Output  S  Compute kt 000 min-hash values for each item i and u from 1 to kt 000 do  000 000 000 000 min hil 000m 000f  end for for each item i and each j in S  i  for u from 1 to kt 000 do if 000 000 000 000 000\013 000\014 min u hilhj 000 then  000 000 000 000 000\013 000\014 min u hil hj 000m  end if end for end for  Generate candidate set for i from 1 to t do      set the linked list to each lattice to 000I  for each item j from 1 to n do n is the Num of items for l from 1 to k do 000 000 000 000 000\013 000\014 imin p Hh jl 000 map to p th column in row l 000 000 000 000 min hjl  if 000 000 000 000 Ll p 000I 000z then  000 000 000 000 Ti Ti L l p 000 \000  end if  000 000 000 000 000 000 000 000 000 000 L lp Llp j 000m\000 add item j to the set end for Ti of item j is ready  000\013 000\014 000 000  CC jxxT 000m 000‰\000 update C  Ti 000I 000m  end for          // consider the next item end for Refine to find the truly correlated pairs for each pair 000\013 000\014  ab in C do  000\013\000\014 000\013\000\014 000\013\000\014\000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014 000\013\000\014   11 sp a b sp a sp b ab s paspb spa spb 000I 000\020 000m 000\020\000\020  if 000\013 000\014  ab 000I 000T 000t then  000\013 000\014  SSab 000m\000  end if end for output S  3.2. The complexity of candidate-generation method in  algorithm SERIT First we analyze the time complexity. From that the nkt 000<\000  min-hash function values can be computed in 000\013 000\014 On time. After that, for n items and t rounds, we need map k min-hash values into the bitmap and union at most k  subsets. That can be done in  Ok t n On 000x\000x 000 time Because k and t are two constants determined only by the threshold 000T  and the false-negative tolerance 000W om the experimental results, bounded with a small false-negative tolerance, BTH needs to c hoose a reasonlessly big t when 000T  is relatively small. However our SERIT has reasonable k  and t to reduce computing time distinctly Then we consider the space complexity. For each item we store kt 000  min-hash values. The bitmap totally has kp 000  lattices. And the length of a linked list is at most n Note that in the i th row, an item can only be mapped into one lattice 
423 
417 


In other words, an item would only appear at most one time in the i th row during a single round. In this way, we have few linked lists including n items if the hash function H is sufficiently uniform. The total memory required is again 000\013\000\014 On  In summary, the time an d space complexity of the candidate-generation process in our SERIT are asymptotically the same as BTH’s 4. experiment results In this section, we report the results of testing our SERIT on several data sets. Compar ing with algorithm TAPER and BTH, SERIT is proved to be effective. Bounded with a small false-negative tolerance 000W as 0.005, SERIT can not only prune the uncorrelated pairs efficiently to generate a small candidate set and save computi ng resources; but also cover the shortcoming of BTH, still achieving small running time when 000T is relatively small The experiment bases on both synthetic and real data sets We choose the famous “retail” data set, which contains data from a retail store and can be downloaded from FIMI website. The tool mentioned in [2 is also use d h e re  to generate three synthetic data sets S 1 S 2 and S 3 Table 3 lists the characteristics of these data sets. And table 4 shows the values of parameter k and t in algorithm SERIT and BTH for different 000T   T ABLE 3 THE CHARACTERISTICS OF DATA SETS  Data Set Num of Items Num of Records retail 16470 88163 S 1 20589 51316 S 2 33052 51292 S 3 42522 51337 All the experiments were performed on a personal computer, with a 2.0 GHz CPU and 512 Mbytes of memory running the windows XP operating system In section 5.1, we give th e executing time of the three algorithms, including candidate-generation running time and overall running time. We then show in section 5.2 the scalability of SERIT T ABLE 4   THE VALUES OF PARAMETER K AND T   Algorithm SERIT Algorithm BTH 000T  k t k t 0.8 7 8 2 10 0.7 11 6 2 18 0.6 14 6 2 38 0.5 17 6 2 47 0.4 28 4 2 204 0.3 30 4 2 370 0.2 110 4 2 2940 0.1 410 2 2 50000 4.1. The executing time  All the three algorith ms need two steps: 1\ generation of the candidate set 2\refinement. As a result, there are two types of executing time, candidate-generation running time and overall running time, which may present different properties. Although an algorithm executes efficiently during candidate-generation stage, it may produce a large candidate set inducing the incr ease of overall executing time The coefficient thresholds are sp lit to two parts with 0.4 as a dividing point. Figure 5 illust rates the candid ate-generation time, while Figure 6 shows the overall executin g time, both based on “retail” data set The corresponding executing time on S 1 is presented in Figure 7 and 8. Note that when 0.1 000T 000  we find that the executing time including the two types\ of BTH is too long, much longer than that of other algorithms Moreover, it is also much longer than the time when 0.2 000T 000  For the rationality of experiment and brevity to present, we don’t illustrate the execu ting time of BTH with 0.1 000T 000  From Figure 5~8\(a\s small executing time when 0.4 000T 000d And the advantage becomes more obvious when 000T  is smaller. Oppositely, the running time of BTH increases sharply with the decrease of 000T This is mainly due to the very large numbers of min-hash values caused by large t. Although BTH merely chooses two items in the same equivalence class to generate a candidate pair and the method in SERIT seem s a little complex, SERIT is still faster due to the r eason mentioned above When 0.5 000T 000t BTH computes less min-hash values, and consequentially it has smaller running time. SERIT is slightly slower than BTH in most cases, but always faster than TAPER. This is determin ed by the property of the probability function used in SERIT: With a big 000T the item pair whose correlation coefficient is smaller than 000T would be chosen into the candidate set in a relatively high probability \(compared with BTH\In this way, SERIT may have a slightly bigger candidate set than BTH From further analysis, the other two is much powerful than TAPER to remove pairs unwanted. As a result of the large candidate set generated in TAPER, in the refinement step more time is required. As shown in Figure 7 \(b\d 8 b\ the execution of TAPER can be one order of magnitude slower Moreover, comparing the experimental results on “retail with S 1 we can conclude that the size of item set has greater influence on TAPER than on SERIT and BTH. The running time increases obviously with the growth of item set 
424 
418 


0.10 0.15 0.20 0.25 0.30 0.35 0.40 0 100 200 300 400 500 600 BTH TAPER SERIT Execution Time \(sec Threshold  Fig 5\(a\ candidate-generati on time on “retail” with 0.4 000T 000d  0.50 0.55 0.60 0.65 0.70 0.75 0.80 5 10 15 20 25 30 Execution Time \(sec Threshold BTH TAPER SERIT  Fig 5\(b\ candidate-generati on time on “retail” with 0.5 000T 000t  0.10 0.15 0.20 0.25 0.30 0.35 0.40 0 100 200 300 400 500 600 BTH TAPER SERIT Execution Time \(sec Threshold  Fig 6\(a\ overall executing time on “retail” with 0.4 000T 000d  0.50 0.55 0.60 0.65 0.70 0.75 0.80 20 40 60 80 100 120 140 160 180 200 Execution Time \(sec Threshold BTH TAPER SERIT  Fig 6\(b\ overall executing time on “retail” with 0.5 000T 000t  0.10 0.15 0.20 0.25 0.30 0.35 0.40 0 100 200 300 400 500 600 BTH TAPER SERIT Execution Time \(sec Threshold  Fig 7\(a\ candidate-generation time on S 1 with 0.4 000T 000d  0.50 0.55 0.60 0.65 0.70 0.75 0.80 5 10 15 20 25 30 35 40 45 Execution Time \(sec Threshold BTH TAPER SERIT  Fig 7\(b\ candidate-generation time on S 1 with 0.5 000T 000t  0.10 0.15 0.20 0.25 0.30 0.35 0.40 0 100 200 300 400 500 600 700 800 900 BTH TAPER SERIT Execution Time \(sec Threshold  Fig 8\(a\ overall executing time on S 1 with 0.4 000T 000d  0.50 0.55 0.60 0.65 0.70 0.75 0.80 50 100 150 200 250 300 350 400 450 Execution Time \(sec Threshold BTH TAPER SERIT  Fig 8\(b\ overall executing time on S 1 with 0.5 000T 000t  4.2. The scalability of algorithm SERIT We use synthesized data sets list in Table 3 to examine the scalability of SERIT from the vi ew point of ex ecution time They have different sizes of item set. Figure 9 shows that the execution time increases with the number of items. This conclusion is consistent with th e complexity analysis shown in section 3 
425 
419 


2.0 2.5 3.0 3.5 4.0 4.5 40 60 80 100 120 140 160 Execution Time \(sec Num of items              X 10 4 Thre0.1 Thre0.3 Thre0.5 Thre0.7  Fig 9. scalability of algorithm SERIT 5. Conclusion and future work In this paper, given a coefficient threshold, we design and implement the algorithm SERIT which can find the correlated item pairs through efficient pruning in a massive market-basket data set. The method proposed by Zhang et al   o re suitable when 000T is relatively big. Hence, a new problem is generated when 000T is small. Our SERIT introduces a new probability functio n to select candidate pairs Experimental results show that the space and time complexity are asymptotically the same as BTH’s. In ad dition, with a relatively small 000T  0.4 000d eds a much smaller and more reasonable kt 000  than BTH. Theref ore, SERIT achieves much larger saving of computational resources  There are several potential dir ections in future research First, we will examine whether it is more efficient to combine SERIT and TAPER. However, the two different pruning methods need different pre-proces sings. In particular, we have to consider the trade-off between a more complex pre-processing and efficiency Second, we only considered pairs of items in this paper How about a correlated item set Also, there exist several other cr iterions to measure correlation association\, which may have peculiar properties  A CKNOWLEDGMENT  This work is supported by the National High-Tech Research and Development Plan of China \("863" plan\nder Grant No  2006AA01Z451 and No. 2007AA01Z474 6. References   Hui Xiong, Shashi Shekhar, P. N. Tan, and Vipin Kumar Exploiting a support-based upper bound of Pearson’s correlation coefficient for efficiently identi fying strongly correlated pairs KDD’04, pp. 334–343, August 22-25, 2004, USA   J i an Zh ang and J o an F e ig e nbaum, “Findind highly correlated pairs efficiently with powerfu l pruning”, CIKM’06, pp. 152–161 November 5-11,2006, USA  Hui Xiong, Mark Brodie a nd Sheng Ma, “TOP-COP: mining TOP-K strongly correlated pairs in large database”, ICDM’06, pp 1162–1166  R.Agrawal  T. Im i e linski   a nd A. S w am i   M ining associa tion rules between sets of items in large databases SIGMOD’93, pp 207-216  C. Jerm aine, “ Th e com putational complexity of high-dimensional correlation s earch”, ICDM’01, pp. 249-256  C.Bucila, J Gehrke, D  Kif e r and W. M. White, “ Dualminer: a dual-pruning algorithm for itemsets with constrains”, SIGKDD’02 pp. 241-272  E. Cohen   S ize-estim at i on fram e work with appl ic atio ns to transitive closure and reachability Journal of Computer and System Sciences, 1997, pp. 441-453  C. Jermaine, “Play i ng hide-a nd-seek with correlations SIGKDD’03, pp. 559-564  S  Brin R  M o tw ani and C S ilvers t ein   B e y ond m a rket bas k ets   Generalizing association rules to correlations”, SIGMOD’97, pp 265-276  Yu Li Miroslav, and  Kubat, “Searching for high-su pport itemsets in itemset trees”, Intellig ent Data Analysis, , March, 2006 Volume 10, Issue 2, pp. 105-120  H. T. R e ynolds The an aly s is of cross-classifications”, The Free Press, New York, 1977   Toon Cad e rs, B a rt Goeth a ls and S. Jaroszewicz, “Mining rank-correlated sets of numerical attributes KDD’06, August 20-23 2006, USA, pp. 96-105  C. K. S Leung, and Qu amru l l. Khan, “Efficient mining of constrained frequent patterns fro m Streams”, IDEAS’06, pp. 61-68  Motwani  E Cohe n, M Da ta r, S. Fujiwa re A Gionis P Indyk, J. Ullman, and C.Yang, “Finding interesting associations without support pruning”, IEEE Transactions on Knowledge and Data Engineering \(special issue  W. DuMouchel and D. Preg ibon Empirical bayes screening for multi-item associations”, KDD’01, pp. 67-76  S  J a ros zew icz and D a n A  Simovici, “ Inte restingness of frequent itemsets using Bayesi an networks as background knowledge”, SIGKDD’04, USA, pp. 178-186  
426 
420 


TESTS IN SECOND t INDICATES nl WAS LOWERED TO 2 Training BSTC Top-k RCBT 7 OC Holdout Validation Results RCBT outperforms BSTC on the single test it could finish by more then 7 although it should be kept in mind that RCBT's results for the 24 unfinished tests could vary widely Note that BSTC's mean accuracy increases monotonically with training set size as expected At 60 training BSTC's accuracy behaves almost identically to RCBT's 40 training accuracy see Figure 6 4 Ovarian Cancer OC Experiment For the Ovarian Cancer dataset which is the largest dataset in this collection the Top-k mining method that is used by RCBT also runs into long computational times Although Top-k is an exceptiounally fast CAR group upper bound miner it still depends on performing a pruned exponential search over the training sample subset space Thus as the number of training samples increases Top-k quickly becomes computationally challenging to tune/use Table VI contains four average classification test run times in seconds for each Ovarian Cancer\(OC training size As before the second column run times each give the average time required to build both class 0/1 BSTs and then use them to classify all test's samples with BSTC Note that BSTC was able to complete each OC classification test in about 1 minute In contrast RCBT again failed to complete processing most classification tests within 2 hours Table VI's third column gives the average times required for Top-k to mine the top 10 covering rule groups upper bouhnds for each training set test with the same 2 hour cutoff procedure as used for PC testing The fourth column gives the average run times of RCBT on the tests for which Topk finished mining rules also with a 2 hour cutoff Finally the  RCBT DNF column gives the number of tests that RCBT was unable to finish classifying in  2 hours each THE OC TESTS THAT RCBT FINISHED Training BSTC RCBT 40 92.05 97.66 60 95.75 96.73 80 94 12 98.04 1-133/077 9380 96.12 1070 cJ CZ C 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 BSTC RCBT d Median Median  Mean 260 Near outliers  Far outliers 40 Training 60 Training 0.90.80.70.6BSTC RCBT a 80 Training 1-52/0-50 Training 0.9DNFI 0.80.70.6BSTC RCBT b 1 u0.9DNFI 0.80.70.6BSTC RCBT  RCBT DNF 40 30.89 0.6186 273.37 0/25 60 61.28 41.21  5554.37 19/25 80 71.84  1421.80  7205.43 t 21/22 TIMES FOR THE OC 9 Mean 0 Near outliers  Far outliers 1.01 11 01 1.0 d Fig 6 PC Holdout Validation Results BSTC RCBT a Fig 0.80.8 0.8BSTC RCBT BSTC RCBT b c c i DNF cJ CZ C 40 Training 60 Training 80 Training 1-133/0-77 Training 0.95 DNF DNF DNF 0.9 0.90.90.90.85 0.8 BSTC RCBT TABLE VI AVERAGE RUN 1 133/0-77 70.38  1045.65  6362.86 t 20/23 over the number of tests for which Top-k finished Because RCBT couldn't finish any 80 or 1-133/0-77 tests within 2 hours with nl  20 we lowered nl to 2 Classification Accuracy Figure 7 contains boxplots for BSTC on all four OC classification test sets Boxplots were not generated for RCBT with 60 80 or 1-133/0-77 training since it was unable to finish all 25 tests for all these training set sizes in  2 hours each Table VII lists the mean accuracies of BSTC and RCBT over the tests on which RCBT was able to produce results Hence Table VII's 40 row consists of averages over 25 results Meanwhile Table VII's 60 row results are from 6 tests 80 contains a single test's result and 1-133/0-77 results from 3 tests RCBT has better mean accuracy on the 40 training size but the results are closer on the remaining sizes   4 difference over RCBT's completed tests Again RCBT's accuracy could vary widely on its uncompleted tests CAR Mining Parameter Tuning and Scalability We attempted to run Top-k to completion on the 3 OC 80 training and 2 OC 1-133/0-77 training tests However it could not finish mining rules within the 2 hour cutoff Top-k finished two of the three 80 training tests in 775 min 43.6 sec and 185 min 3.3 sec However the third test ran for over 16,000 mnm  11 days without finishing Likewise Top-k finished one of the two 1-133/0-77 tests in 126 min 45.2 sec but couldn't finish the other in 16,000 min  11 days After increasing Top-k's support cutoff from 0.7 to 0.9 it was able to finish the two unfinished 80 and 1-133/0-77 training tests in 5 min 13.8 sec and 35 min 36.9 sec respectively However RCBT with nl 2 then wasn't able to finish lower bound rule mining for either of these two tests within 1,500 min Clearly CAR-mining and parameter tuning on large training sets is TABLE VII MEAN AcCU1ACIES FOR 


support pruning gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis Biosystems vol 85 computationally challenging As training set sizes increase it is likely that these difficulties will also increase VI RELATED WORK While operating on a microarray dataset current CAR 1 2 3 4 and other pattern/rule 20 21 mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets Hence they are generally quite computationally expensive for datasets containing many training samples or genes as the case may be BSTC is explicitly related to CAR-based classifiers but requires no expensive CAR mining BSTC is also related to decision tree-based classifiers such as random forest 19 and C4.5 family 9 methods It is possible to represent any consistent set of boolean association rules as a decision tree and vice versa However it is generally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data 1 2 6 7 11 BSTC is explicitly related to and motivated by CAR-based methods To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here Perhaps the work closest to utilizing 100 BARs is the TOPRULES 22 miner TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci Hence TOP-RULES discovers all 100 confident CARs in a dataset However the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER 23 and so generally isn't polynomial time Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules 24 25 Once obtained fuzzy rules can be used for classification in a manner analogous to CARs However the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM 25 VII CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with preclassification CAR mining see Tables IV and VI we developed a novel method which considers a larger subset of CAR-related boolean association rules BARs These rules can be compactly captured in a Boolean Structure Table BST which can then be used to produce a BST classifier called BSTC Comparison to the current leading CAR classifier RCBT on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining see Section VB Hence BSTC extends generalized CAR based methods to larger datasets then previously practical Furthermore unlike other association rule-based classifiers BSTC easily generalizes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse then CAR-based methods after all exponential time CAR mining is completed O SlS CGl versus O Si CGi As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists ACKNOWLEDGM[ENTS We thank Anthony K.H Tung and Xin Xu for sending us their discretized microarray data files and Top-k/RCBT executables This research was supported in part by NSF grant DMS-0510203 NIH grant I-U54-DA021519-OlAf and by the Michigan Technology Tri-Corridor grant GR687 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies REFERENCES 1 G Cong K L Tan A K H Tung and X Xu Mining top-k covering rule Mining SDM 2002 5 R Agrawal T Imielinski and A Swami Mining associations between sets of items Y Ma Integrating classification and association rule mining KDD 1998 11 T McIntosh and S Chawla On discovery of maximal confident rules without pp 43-52 1999 24 S Vinterbo E Kim and L Ohno-Machado Small fuzzy and interpretable pp 165-176 2006 1071 pp 207-216 1993 6 G Dong pp 273-297 t995 9 pp 5-32 2001 20 W Li J R Quinlan Bagging boosting and c4.5 AAAI vol 1 V Vapnik Support-vector networks the best strategies for mining frequent closed itemsets KDD 2003 4 M Zaki and C Hsiao Charm L Wong Identifying good diagnostic genes or gene expression data SIGMOD 2005 2 G Cong A K H Tung X Xu F Pan and J Yang Farmer Finding interesting rule gene expression data by using the gene expression based classifiers BioiiiJcrmatics vol 21 l and Inrelligent Systenis IFIS 1993 16 Available at http://sdmc.i2r.a-star.edu.sg/rp 17 The dprep package http:/cran r-project org/doclpackages dprep pdfI 18 C Chang and C Lin Libsvm a library for support vector machines 2007 Online Available www.csie.ntu.edu.tw cjlin/papers/libsvm.pdf 19 L Breiimnan Random forests Maclh Learn vol 45 no 1 M Chen and H L Huang Interpretable X Zhang 7 J Li and pp 725-734 2002 8 C Cortes and Mac hine Learming vol 20 no 3 in microarray data SIGKDD Worikshop on Dtra Mining in Bioinfrrnatics BIOKDD 2005 12 R Agrawal and R Srikant Fast algorithms for mining association rules VLDB pp 1964-1970 2005 25 L Wong and J Li Caep Classification by aggregating emerging patterns Proc 2nd Iat Coif Discovery Scieice DS 1999 gene groups from pp 487-499 t994 13 Available ot http://www-personal umich edu/o markiwen 14 R Motwani and P Raghavan Randomized Algoriitlms Caim-bridge University Press 1995 15 S Sudarsky Fuzzy satisfiability Intl Conf on Industrial Fuzzy Contri J Han and J Pei Cmar Accurate and efficient classification based on multiple class-association rules ICDM 2001 21 F Rioult J F Boulicaut B Cremilleux and J Besson Using groups for groups in microarray datasets SIGMOD 2004 3 concept of emerging patterns BioinformJotics vol 18 transposition for pattern discovery from microarray data DMKD pp 73-79 2003 22 J Li X Zhang G Dong K Ramamohanarao and Q Sun Efficient mining of high confidence association rules without S Y Ho C H Hsieh H pp 725-730 1996 10 B Liu W Hsu and support thresholds Principles f Drata Mining aind Knowledge Discovery PKDD pp 406 411 1999 23 G Dong and J Li Efficient mining of emerging patterns discovering trends and differences KDD J Wang J Han and J Pei Closet Searching for An efficient algorithm for closed association rule mining Proc oJ the 2nd SIAM Int Con on Data in large databases SIGMOD 


