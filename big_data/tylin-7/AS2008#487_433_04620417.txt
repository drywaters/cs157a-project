  Proceedings of the Sev e nth Inter n ational Conference on Ma c h ine Lear ning and Cyber n etics  K u nming, 12-15 J u ly 2008  MINING FREQUENT CLOSED P A TTERNS WITH ITEM CONSTRAINTS IN DA T A STREAMS  WEI-CHENG HU 1 BEN-NIAN W ANG  1 Z H UAN-LIU  CHENG  1,2  1 Depart m e nt of C o m put er Sci e nce T ongl i ng C o l l e ge T ongl i ng 244000, C h i n a  2 C o l l e ge of C o m put er Sci e nce, Hefei  T echnol ogy Uni v ersi t y Hefei 230009,C h i n a E-M A IL huwc@t l u edu.cn, wangbn@t l u edu.cn,czl 9612@t l u edu.cn Ab s t ract I n ord e r to eff i cien 
tl y filt er th e u s efu l as s o cia t ion ru les  thr o ugh a large numbe r of mine d r u le s, some ite m  c o nstr aints that ar e boole a n e x pr e sssions ar e inte gr ate d into the associations discovery alg o rithm. The set of fr equent  clos ed p a tte rn s u n i q u e ly d e te r m in es th e com p lete s e t of al l fr e que nt patte r n s and it c a n be or de r s of magnitude  s m aller th an t h e latter  Ac co rd ing to the f e atur es of data  str e ams, a new algorithm ca ll DSCFCI, is pr oposed for  mining fr e que nt c l ose d patte r ns w i th ite m  constr aints in  data str e ams  The data str e am is divide d into a se t of s e gmen ts an d a n e w d a ta s t r u ctu r e cal l ed 
D S CFCI tr ee is  u s ed to s t or e th e p o ten t ial fr eq u e n t clos ed p a ttern s  w i th  item con s train t s d y n a mical l y  W i th th e ar riva l of each b a tch  of data, the algo rithm builds a corr esponding local DSCF CI-tr e e fir s tly  the n update s and pr un e s the global DS CFCI tr ee effect ive l y to min e th e fr eq u e n t clos ed  p a ttern s  w i th  it em con s train t s  i n th e en tir e d a t a s t r e am Th e  experim e nts and analy s is show that the algorithm has goo d p erforman ce Key words Data mining; d a ta str e ams; as soc i ation r u le  fr e que nt  clos ed items e ts  1.   Intr oducti 
on A data str eam is an un bou nd ed sequ en ce o f  d a ta arrivi ng at hi gh s p eed an d ch ang i ng  un ceasin g l y along  wi t h t h e t i m e. Freq ue nt pat t erns m i ni ng i n dat a st ream s has bee n st udi ed e x tensi v ely in recent yea r s, wit h m a ny al go ri t h m s 14  p r op ose d a n d i m pl em ent e d succe ssf ul l y  Ho we ver  fr eq uent pat t e r n s m i ni ng oft e n gene rat e s a  hu ge n u m b er of  fre q u e nt i t e m s et s an d t h ei r co rr esp ond ing asso ciation 
rul e s  i n cl udi ng m a n y  red u nda nt  u s el ess o n es w h i c h a r e di f f i c ul t t o  com p rehe nd a n d m a ni pul at e  T h ere i s an i n t e rest i n g  al t e rnat i v e p r o pos ed by Pas q ui er et al. [5 m i ning only fre que nt cl ose d item s ets and their corresponding rules whi c h has t h e sam e powe r  as associ at i o n m i ni ng b u t  subst a nt i a l l y reduces t h e num ber of rul e s t o be present e d More ove r  in practice, use r s are often in terested on ly  i n a s 
ubset  of a ssoci at i o n  ru le s that contain s o m e specific ite m s Su ch rules m a y b e called asso ciatio n  ru les with  i t e m const r ai nt s Whi l e i t e m const r ai nt s can  be a p pl i e d as  a post-processi ng step, i n tegr at i ng t h em i n t o t h e m i ni ng algorithm can dram atically reduce the e x ec ution tim e and space c o m p le xity and i n c r ease bot h ef ficiency and ef fectiveness of m i ning In t h i s  pa per  we p r o p o se a new al go r i t h m for m i ni ng f r e que nt cl ose d  pat t e rns wi t h i t e m  
con s t r ai nt s i n  data stream s A DSCFCI-tre e structure is use d to store  th e po ten tial frequ e n t clo s ed p a ttern s wh ich satisfy th e ite m co n s train t s. Th e d a ta strea m is p a rtitio n e d i n to a set of s e gm ent s  W i t h t h e a rri v a l of eac h bat c h of dat a t h e algorithm builds a corres p o ndi ng l o cal DSC F C I t ree  f i r s tly th en upd ates an d pr unes th e g l ob al D S CFCIt r e e ef fectiv ely to  min e th e frequ en t clo s ed  pattern s in th e entire data stream s  The rem a i n i n g of t h e pa per i s or 
ga ni zed as fol l o ws  In sect i o n 2  t h e  basi c c once p t s o f f r e que nt cl ose d  pat t e rns m i ni n g are i n t r o d u c e d. Sect i o n 3 prese n t s t h e alg o rith m fo r min i n g  frequ en t clo s ed ite msets with ite m con s t r ai nt s i n  dat a st ream s. A pe rf orm a nce st u d y  o f t h e al go ri t h m i s re po rt ed i n sect i o n  4 a n d  we  concl ude  o u r st udy i n sect i on 5   978-1-4244-2096-4/08/$25.00 \2512008 IEEE 274 


 X  an item s et. A  d a ta strea m   is a set of transactio n s  The num b er o f tran saction s in  cont ai ni n g i t e m s et DS     2 1 N T T T DS X is calle d th e support of X denot ed as  sup  X Defi ni t i on 2 Gi ve n a dat a st ream a supp or t t h res hol d  and an er ro r p a r a meter  DS s 001 212 002 An ite m s et X is a p o t en tial frequen t item s et, if an d o n l y i f  N X 001  sup   An ite m s et X  is an i n freq u e nt ite m s et, if an d on ly if N X 001 003 sup   Defi n itio n 3 An item s et X  is a clo s ed item s et if th ere ex ists no ite m s et Y  s u ch that \(1 Y  i s a pr op er su per s et of X and 2 sact i o n co nt ai ni ng  X  also c ontains Y  A clo s ed item s e t  X  i s f r eq uent i f  N s X   sup  001 212 002  A clo s ed item s e t  X is po tential frequent i f  N X 001  sup   Defi ni t i on 4 An i t e m  const r ai nt  B  is a boolean ex pr essi o n over  I  W e assu m e with ou t lo ss of g e n e rality th at  B  is in d i sjun ctiv e norm a l fo rm DNF That is  B  is of th e form  where  each dis j unct  is o f  th e form   and each is eith er or  fo r so m e   Here   m eans cont ai ni ng i t e m wh ile me a n s  not c ont ai ni n g i t e m  and can b e o m itted  An item s e t  n B B B 006 X B k  X i s a no n em pt y subset of  I A  tran saction  is a 2-t uple where tid is a  transaction-i d and    X tid T 004 004 004  2 1 i B im i i a a a 005 005 005  2 1 j i a i x i x 254 I x i 006 i x i x i x 254 i x X satisfies B  if th ere exists a certain such that    2  1   n k k 007 Defi n itio n 5 A  DSC F CI-t ree is a tree stru cture defi ned bel o w   1 nsists o f  on e roo t lab e led as \223nu ll\224, a set of ite m prefix s u btrees which contain t h e com p lete  in fo rm atio n of th e po ten tial frequ e n t clo s ed ite m s ets  sat i s fy i ng gi ve n i t e m const r a i nt s as t h e chi l dre n  of  the root, and a potential-fre quent-item header table  2  Each node in th e ite mpr ef i x sub t r e e im p lies an  ite m s et rep r esen ted b y th e p a t h  fro m th e d i rect ch ild no de o f t h e r o ot t o t h e c u r r en t n o d e It co nsists of 5 fields  name item  i N i i N i N   i N  002 i N i 1 s e alg o rith m is called to m i n e all th e  p o t en tial frequ en t clo s ed ite m s ets fro m  and th e ite m s ets sat i sfyin g ite m co n s train t s i N B are  obt ai ne d  fro m th em    275 212 p ort sup    where   reg i sters wh ich ite m th is n o d e rep r esen ts isfci update nextnode name item 212 p ort sup  reg i sters th e supp ort o f  th e item s e t  im pl i e d by t h e n ode  repres ents whet her the  ite m s et is closed, i f the val u e is 1, t h e ite m s et is  closed  re pre s ents whethe r the s u pport of th e ite m s et is u p d a ted  links to th e n e x t  no de in t h e DSCFCI tre e carry in g the sam e  i t e m nam e or nul l i f t h ere i s none isfci update nextnode 3 t r y in t h e po ten tial- freq u e n t ite m h ead er t a bl e co nsi s t s o f  3 fi e l ds   name item 212 p ort sup and  a po inter po in ting t o  the fir s t n o d e in the D S C F CI-tree car ry ing t h e i t e mn a me    nextnode 3.   DSCFCI Algorithm De sign and Implementation 3.1.   General Idea Th e d a ta strea m is p a rtitio n e d in t o  b a tch e s The size of t he num b e r o f  tran saction s in    2  1    Proceedings of the Sev e nth Inter n ational Conference on Ma c h ine Lear ning and Cyber n etics  K u nming, 12-15 J u ly 2008  2.   Pr obl e m S t atement Defi n itio n 1 Let be a s e t  of ite m s An  item s e t      2 1 m x x x I 001 1 or 001 1 n where  is a po sitiv e in teg e r   Here, for t h e sak e  o f th e  descri pt i on of DSC F C I al gori t h m    n   i N 001 1  Wh en all th e tran saction s  for ha ve a rri ved   DSCFCI al g o rith m calls th e ex isting freq u e n t clo s ed p a ttern s al g o ri th m FPclo se [6 to min e all th e po ten tial fre que nt cl ose d i t e m s et s 1 N 001 Let d e not e  th e cu rren t leng th  of t h e stream i.e., th e num b e r o f tup l es seen so fa r  An item s et N X  is a frequ e n t item s e t if an d onl y i f  N s X   sup  001 frequ en t clo s ed ite m s ets fr om   firstly th en  o b t ai n s th e ite m s ets satisfyin g ite m co n s train t s 1 N B  and us es them to create correspondi ng DSC F C I-t ree  All th e remain in g  b a tch e s are  processed as fol l o ws  2  


001  d s o rt t h em i n s u p p o rt  des cendi ng o r d e r   Th e sorted p o t en tial frequ en t ite m li st form s   list f _ 2. Sort each transaction t of  accordi n g to  an d th en in sert it in to an FP-tree i N list f _ 3  Call FPclose algo rith m to m i n e all n is a newl y produced cl osed i t e m s et   Procedure Updat e _DSC FC It ree I npu t the glo b al DSCFC I-tr ee o f  the first  bat c hes i t e m const r ai nt s tree oldDSCFCI   Proceedings of the Sev e nth Inter n ational Conference on Ma c h ine Lear ning and Cyber n etics  K u nming, 12-15 J u ly 2008  2 e item s ets ob tain ed  fro m step 1  d th ei r subsets satisfy ing B  are com p ress ed int o t h e l o cal DSCFCI-tree of  i N 3 h e l o cal DSCFCI-tree of an d t h e gl obal  DSCFCI tree of t h e first bat c hes   are m e r g ed int o t h e global  DSCFCI-tree of the first bat c hes  i N 1 212 i N N N i 4  h e gl obal DSCFC I-tree of t h e fir s t batch e s is p r un ed an d th e erro r of sup port is g u a ran t eed no t to exceed i 001 and  of  the first bat c hes  i f   is em pty list oldf _ list f _ 1 001 fre que nt  cl osed i t e m s ets p ot ent i a l f r e que nt cl ose d  i t e m s et s  from the FP-tree 4. C r eate an DSCFCI-tree w ith  o n l y th e roo t nod e, i f    h en  s o rt each pote n tial fr eque nt clos ed itemset sa tisfyin g 1 b t n 2 1 X X then 2 1 X X 001 If ne eded DSCFC I algorithm could be as ked to produ ce all th e freq u e n t closed item s ets sa tisfyin g item const r ai nt s B  of the first  b a tch e s at th is tim e i 3.2   Construction of the Local DSCFCI-tr ee Fo r th e first b a tch  the l o cal DSCFCI-tree only sto r es th e p o t en tial frequ e n t clo s ed ite m s ets satisfyin g ite m co n s traints 1 N B B u t for th e rem a in in g b a tch e s i n  or der t o t e st  whet her t h e r e a r e ne w  clo s ed item s e t s co m i n g up  n o t  on ly th e po ten tial fre que nt cl ose d i t e m s et s but al so t h ei r s ubs et s sat i s fy i n g   2  212 i     1 2 1 002 i N i B  are stored in the local DSCFCI-tree Procedure B u i l d_i DSC F C I t r ee In p u t  An i n co m i ng bat c h  ite m co n s train t s i N B an er ro r p a r a m e te r  212 i 1  i list oldf _ Ou t p u t: Th e lo cal DSCFCI-tree of a n d   of the first bat c hes i N list newf _ list f _ i M e t hod  1. Scan  o n c e to fi n d all th e lo cal po ten tial  freq u e n t item s ite m s who s e sup p o r t is n o  less th an  i N   i N  i B acc o r d i n g t o  a nd in sert it in t o  th e DS CFC I-t r e e  else so r t ea c h  p o t en tia l fr equ e n t clo s ed itemset an d its sub s ets sa tisfyin g  list f _ B ac cor d i n g t o  and insert  th em in t o t h e DS CFC I-tr e e  an d f o r th e f i nal nod e of  each cl ose d ite m s e t in DSCFCI-tree, m a ke its value of fi el d  be 1 a n d  t h e val u e of  fi el d  list f _ isfci p ort sup  be t h e i t e m s et 222 s support i n t h e FP-t r ee 5. m e r g e  and into   list oldf _ list f _ list newf _ 3.3.   Incr emental u pda te o f t h e Gl ob al  DSCFCI-tr ee T h eo r e m 1 If an ite ms e t  X  satisfies t h e fo llowing  co nd itio ns sim u ltan e ou sly  1 It is a clo s ed item s e t in a certain  b a tch o f d a ta stream, \(2 h a s no t b e en  prun ed yet, \(3 u e n t in d a ta stream seen so far  th en it is a frequent closed item s et in data stream seen so far   Pr oof Supp o s e X  is a clo s ed ite m s et in  b a tch  an d has  s u p p o rt  If any p r ope r s u perset  of  i N a X  doe s not e x ist in othe r batc he s, according to condition \(3 X  is a fre que nt closed item s et in data strea m seen so far ev id en tly; o t h e rwise, i f  Y  is a pr op er su p e r s et of  X and Y  com e s up  i n ot he r  ba t c hes o f dat a st ream  with su ppo rt th en th e sup port of b X is at least   and it m u st b e lar g er t h an th e supp ort o f  b a  Y  X is a frequ en t clo s ed item s et to o. Th u s  we h a v e th e th eo rem   W h en m e r g ing two DSCFCI-tr ees i n to a n e w  on e we shou ld consid er two co nditio n s fo r each ite m s et X  in th e orig i n al DS CFCI trees  1  f  X  i s cl os ed i n  one  of t h e ori g i n a l DSC F C I t ree s  X is still clo s ed after mer g ing and t h e supp or t of X in each DSCFCI-tree  sho u l d be a dde d t o get h er 2   X  is no t closed in b o t h of the DSC F CI-trees  we s hould c o m pute t h e cl osure of X  in each DSC F CI-tree a n d label them as 1 X  2 X  sep a rately If 212 1 212 i B  the local DSC F CI-tree o f bat c h tree DSCFCI 212   276 


  Proceedings of the Sev e nth Inter n ational Conference on Ma c h ine Lear ning and Cyber n etics  K u nming, 12-15 J u ly 2008  i N   of the first bat c hes list f _ i Ou t p u t  t he gl o b al  DSCFCI-tree of the first bat c hes tree newDSCFCI   sup  sup  6. For each node  update  isfci   sup  sup   isfci n  X If    sup   isfci  isfci 212  port port  port 013 in if   tree DSCFCI 013 in  tree DSCFCI b t  X and   X  does n o t exi st i n  and  tree newDSCFCI f  subsum es X  3  Am o n g all th e item s ets foun d in step 2, th e o n e  cont ai ni ng t h e l east num ber o f i t e m s i s t h e cl osu r e of  X  in the DSCFCI-tree Procedure Prune_DSC FC It ree Input: Item const r aints B  the gl obal DSC F C I t ree of t h e fi rst  bat c hes tree DSCFCI f in  bot t o m up search tree DSCFCI f if th e left item s e t in th e b r an ch do e s  no s a tis f y  B th en d e lete th e branch The o rem 2 Th ro u g h cal l i ng pr oce d u r e  Pru n e _ D S C F C I t r ee t o p r une t h e gl obal  DS C F C I t ree o f  d a ta stream th e esti m a ted su p port of a clos ed ite m s et is    277 212 i M e t hod  1. If    h en  is  ex it 1  i tree newDSCFCI 212 tree DSCFCI 212 2. C r eate  wi t h  a ro ot no d e  an d a po ten tial-frequ e n t item h ead er tab l e co nstru c ted acco rd ing to  o f  the  first  bat c hes tree newDSCFCI 212 list f _ i 3. Sca n  d ep thf i r s t s e ar ch   For ea ch pot ential freque n t closed itemset tree oldDSCFCI 212 X  encount ered 4     C a l l p r oced u r e FC I t o c o m put e t h e cl os ure o f  X lab e led  as in and  g a in th e su ppo r t  of   then for t h e final node of   X FCI tree DSCFCI 212   X FCI a X in m a k e its valu e o f field be 1 tree DSCFCI 212 update 5   So rt  X a ccording t o   an d in sert it  in to   up dat e t h e s u p p o rt  of list f _ tree newDSCFCI 212 X as  a X X 212 0  212 8  I f  1  212   X FCI   X FCI b X  doe s not exist in  sort it accor d i ng t o   a n d insert it in to  u p dat e i t s  co rr espo nd ing fin a l nod e tree newDSCFCI 212 list f _ tree newDSCFCI 212 212   X FCI tree DSCFCI 212    X FCI c      X FCI X FCI 212  X sa tisfies B sort it a c co r d ing to an d in sert it in t o   u pda te its co rr espo nd ing final n o d e  list f _ tree newDSCFCI 212 212 tree oldDSCFCI 212 tree DSCFCI 212 Procedure FC I In p u t: An  item s et X a global or local DSCFCI-tree Out put   The cl osure of X  M e t hod  1. Sort  X according to o f  DSCFCI-tree list f _ 2  Su ppo se t h at th e last ite m o f so rted  X is x loo k  up  x  in th e po ten tial-frequ e n t item h ead er tab l e an d fo llow its po in ter to  search bra n c h es of D S C F C I-t ree f o r  n ode  nextnode 212 i Out put  Pruned DSC F C I-t ree M e t hod  1. For eac h node 212 2   1 sup  sup  013   7    Gai n  item s e t  X  re prese n t e d by t h e pa t h fr om  t h e di rect chi l d no de of  t h e r oot  t o  013  Ca ll p r o c ed ur e FCI t o com pute th e closur e of  X in      an d g a i n t h e su p port of   If close d itemset tree oldDSCFCI f as  b X port f and  1  f   9  e l s e  Call pr oce d ur e FCI to computer the clos ur e  of  X in    and i n    sepa ra tely ga i n t h eir su ppo rt a nd d  l abel  as  tree oldDSCFCI f as  d c port f and  1  f   10. Call procedure Prune _ DSCFCItree to p r un e d e lete  and  tree newDSCFCI f  suc h t h at \(1  1  f  and \(2  m s et Y rep r ese n t e d by  t h e  pat h  fr om t h e di rect chi l d  no de of t h e r oot t o  f f  3  I f  0 sup  f  Delete 


  Proceedings of the Sev e nth Inter n ational Conference on Ma c h ine Lear ning and Cyber n etics  K u nming, 12-15 J u ly 2008  less th an th e t r u e sup port by at m o st N f  and i t s support   3.4.   T h e Al gori t hm Algorithm DSCFCI Inpu t: Data strea m  ite m co n s train t s DS B a sup por t  t h reshol d an error param e ter s 001 where   den o t e s t h e c u rre nt l e n g t h  of t h e st ream i  e., t h e num ber  of transactions seen so far    N Proof: Suppos e that the r e are  bat c he s o f dat a  i n  stream seen s o fa r  For a cl os ed item s et i X its suppo r t  i s re duc ed  by   altoget h er  According t o procedure  Prun e_ DSCFC I tree, it is ev iden t th at  Com b ined with t h e fact t h at the c u rr en t leng th  of strea m is and the lengt h of each batc h is k i k 003 N  001 003  Procedure Print_FCI I npu t the gl obal DSCFCI-tree of the first  b a tch e s r t thresho l d an error param e ter tree DSCFCI 212 i s 001  Out put  A l i s t  of  fre que nt cl o s ed i t e m s et s al on g wi t h t h ei r  est i m at ed support   M e t hod  1. Sca n   bo tto m u p sear ch  for each node tree DSCFCI 212 001  Ou tp u t A list o f freq u e n t clo s ed item s ets satisfyin g  B   M e t hod  1. Di vi de dat a  st ream  i n to  b a tc h e s  with   DS  2  1   i N i   i N 001 was set to  Item  const r aints 1 2 3  4 r e pr esen ted fou r item s w h ich wer e selected random l y from t h e 30 i t e m s cam e up fi rst   s 1  0  4 3   2 1  001 1 we in fer that  N i i 001 001 001  327 327 003 1 Therefore N k f  2. If 1   isfci f and   sup  001 f 212 002 s port 327 327 i  001 1   th en  ou tpu t th e item s et rep r esen ted b y th e pat h f r o m  t h e di rect chi l d  n ode of t h e r o ot t o  001 1 or 001 1 n  where  is a p o s itiv e in teg e r    n 278 2. For each batch  i N 3  Call procedure Buil d_i DSCFCIt r ee to construct  the local DSCFCI-tree of  i N 4  C a l l pr oce d u r e U p dat e _ D S C FC It ree t o  construct t h e global DSCFCI-tree of t h e first  bat c h i 5   C a l l  pr o cedu r e Pr une _DSCFCIt ree to prune the  gl obal DSC F C I-t ree 6     I f nee d e d cal l pr oce d u r e Pri n t _ FC I t o pr od uce a  list o f freq u e n t clo s ed item s ets satisfyin g  B  Notice When t h e le ngt h of ea ch batch is  001 1 n in pr oce d u r e P r une _ D SC FC It r ee, t h e val u e of fi el d  p ort sup  of each node is reduced by  i n st ead of 1  n 4.   E x peri mental  E val uati on 4.1.   T e st E n vi r o nment and Datasets Our algo rith m was written i n VC 6  0   All of ou r expe ri m e nt s were pe rf orm e d on a PC usi ng a 1 7 GHz  Pent i u m IV p r ocess o r   38 4M B of R A M   T h e o p er at i ng sy st em i n use was W i ndows XP  Th r e e sep a rate d a ta sets T 1 5 I 10D 100 0k  T15 I 7 D 10 00k T7I 4 D1 000 k  w e r e g e n e rated b y the I B M  sy nt het i c dat a gene rat o r   Here  T  rep r esent e d t h e  avera g e le ngt h of transactions  I r e p r es e n ted th e av er a g e len g t h o f  p o t en tial frequ en t i t e m sets, an d D represen ted  th e to tal n u m b e r of transactio n s i n d a ta set s Th ere were 1K distinct item s in each dat a sets, and the  defa ult value  fo r al l ot her p a ram e t e rs of t h e sy nt het i c d a t a gene rat o r  were used The stream s were broken int o batc hes of siz e 50,000 tran saction s  Th e su ppo rt thresh o l d  was va ried \(as to be desc ri be d b e l o w  a n d t h e err o r  param e t e r s 005 004 005  B 4.2.   E x peri mental  Resul t s    time \(s S=0.001 S=0.0015 S=0.002 num ber of dat a st ream segm ent s  Fi gure 1 The execut i on t i m e of  T15I7D1000k 


  Proceedings of the Sev e nth Inter n ational Conference on Ma c h ine Lear ning and Cyber n etics  K u nming, 12-15 J u ly 2008          In fi g u re 1, t h e y ax is represen ts t h e tim e spen t in  readi ng i n a bat c h of st r e am const r uc t i ng i t s l o cal  DSC F C I t ree  up dat i n g t h e gl o b al DSC F C I-t ree  an d pr o duci n g f r e que nt cl o s ed i t e m s et s sati sfy i ng i t e m  constraints B The y axis i n figure 2 re presents the  me m o ry capacity used for storing the gl obal  DSCFCI-tree As ca n be see n fr om t h e t w o f i gu res whe n t h e val u e of s u pp o r t   i s fi xe d, t h e t i m e and space re qui rem e nt s of t h e algorithm grow as th e stream progre sses, but tend to sta b ilize. For e x am ple, the m e m o ry re qui red by the  global DSC F CI-tree with su ppo r t 0  00 15  in cr eases relativ ely q u i ck ly as th e first sev e n  b a tch e s o f st ream arriv e and tend s to stab ilize at ro ugh ly 3  8MB with sm a l l  bum ps. T h is is because the num b er of pote ntial fre que nt clo s ed item s e t s satisfyin g ite m co n s trai n t s g o es up  relativ ely q u i ck ly at first, and b e co m e s relativ ely stead y  later th ro ugh in tr odu cing p r un ing tech no log y Th e stab ility resu lts are qu ite n i ce as th ey p r o v id e ev id en ce th at th e algorith m can  h a n d l e lon g  d a ta strea m s. As t h e v a lu e o f suppo r t   dec r eases the e x ecution tim e in fig u re 1 a n d t h e m e m o ry requi red by  DS CFCI-tree in  figure 2 inc r ea ses quickly  This is because t h ere are m o re p ot ent i a l  f r e que nt cl ose d i t e m s et s sati sfy i ng i t e m  const r ai nt s wi t h sl ower support    s s Fi gu re 1 an d F i gu re 2 sh o w t h e e x peri m e nt al res u l t s  o f  d a ta sets T1 5I7 D 1 000k  Th e ex per i m e n t al r e su lts of  o t h e rs two d a ta sets are sim ilar to th ese fig u r es                           Fig u re 3 shows th e av erag e t i m e sp en t i n  up d a ting  t h e gl obal DS C F C I-t ree o n c e  with three dif f eren t ite m con s t r ai nt s c onst r ai nt  0 n o c onst r ai nt s co nst r ai nt  1  co nst r ai nt 2   e  can dra w a c o nclusi on that w ith t h e reinforcem en t o f th e constraints 222  degree the num b er of fre que nt close d  ite m s ets satis fyin g co rrespo nd ing item co n s t r ain t s d ecreases, and th e tim e sp en t i n  up d a ti ng  th e g l o b a l DSCFCI-tree dec r eases t o o. M o re ove r  the space  requirem ent of t h e global DSCFCI-tree go es d o wn  sim u l t a neousl y The r ef ore  i n t e g r at i ng i t e m const r ai nt s in to t h e m i n i n g algorith m can  reduce t h e e x ecution tim e  and space com p lexity of the algorithm    4 3   2 1  005 005 005 5.   Concl u si ons In this pa per   we propose a n ef ficient algorith m for m i ni ng fre q u e n t cl ose d i t e m s et s i n dat a st ream s. The co n t r i bu tio n s of ou r stud y in clu d e 1 pro posin g a no v e l  dat a  st r u ct ure  DSC F C I t re e, fo r st ori n g pot e n t i a l   fre que nt cl ose d i t e m s et s, and de vel o pi n g  a new m e t hod  for increm ental updating DSCFCI-t ree ef ficiently 2 appl y i n g a  pr u n i n g t e c hni que f o r ef fi ci ent  p r u n i n g gl obal  DSCFCI-tree  to reduce the s p ace re quire m e nt of t h e  DSCFCI-tree an d th e tim e sp en t in trav ersi n g the DSCFCI-tree dram atica l l y 3  teg r ating ite m con s t r ai nt s i n t o t h e m i ni ng a l go ri t h m and t hus  red u ci n g  furthe r t h e e x ecution tim e and sp ace co m p lex ity of th e const r ai nt 0 S=0.001 T ime\(s S=0.0015  const r ai nt 1 const r ai nt 2 S=0.002  N umber of data stream segments Fi gu re 3 c o m p ari s on of e x ecut i o n  t i m e wi t h di f f erent const r ai nt s\(s=0.0015 Figure 2.The m e m o ry usage of DSCFCI-tree   279 005 004 005 4 3 2 1 


  Proceedings of the Sev e nth Inter n ational Conference on Ma c h ine Lear ning and Cyber n etics  K u nming, 12-15 J u ly 2008  al gori t h m   Our performance st udy shows  that DSCFCI  algorithm is ef ficient and ef fective  Refer e nces 1] C Gia nnel l a, J Ha n, J  Pei, et al. Mi ning fre quent p a ttern s i n  d a t a stream s at mu ltip le tim e g r an u l arities  G]. In  H Kar g up ta, A Jo sh i, K Siv a ku m a r  et al, ed s Next  Ge nerat i on Dat a M i ni ng C a m b ri dg e, M a ss  M I T Press, 2003  2 G S Mank u, R Mo t w an i. App r ox im a t e f r e q u e n c y cou n t s ove r st ream i ng dat a  C    The 28 th Int 222 l Confere n ce on V e ry Lar g e Data Bases \(VL D B 2002 HongKong, 2002 3  Aras u A M a nk u G S  A p pr oxi m a t e co unt s a n d  q u a n tiles ov er slid in g wi n d o w s. In Pro c eed ing s  o f  th e 23 rd ACM S I G M OD SI G AC T S I G A R T  Sym posi u m on Pri n ci pl es o f  Dat a base Sy st e m s. Pari s France  AC M Press, 2004 4 Datar M, Gio n i s A, In d y k P   Mo t w an i  R. Main tain in g str eam stat is tics o v e r slid in g  w i ndow s In  Proceedi ngs of the 13 th A nnu al ACMSIA M  Sy m p o s iu m on  Discrete Al g o rith m s San Fran cisco   USA  AC M Press, 2002 5 N Pas quier  Y Bastide  R T a ouil, et al Discoveri n g freq u e n t clo s ed item s e t s fo r asso ciatio n ru les[C]. In   Beeri C, et al, eds  Proc of the 17 th I n t 222 l Co nf  on Dat a base Theory B e rl i n  Spri nger V e rl ag, 1999  6  W a n g Jian yon g. Clo s et sear ch ing fo r t h e b e st st rat e gi es f o r m i ni ng f r e que nt cl ose d i t e m s et s. In  Pr oc. N i n t h A C M SIGK DD In t\222 1 Co nf  on  K now ledg e D i scov er y an d  D a t a Min i n g  W a shi ngt on,DC Aug.2003    280 


association mining The Likelihood Ratio Test fails to extract features also belonging to common vocabulary and it makes the extraction dependent on the feature position in the sentence leading to low recall The dBNP and bBNP based methods yield low recall due to the fact that the product features do not occur with the article the in front of them very often The Association Mining approach returns all frequent nouns which decreases precision Our results suggest that the choice of algorithm to use depends on the targeted dataset If it consists of mainly on-topic content the results of Table 10 indicate that the Association Mining algorithm is better suited for this task due to its high recall If the dataset consists of a mixture of onand off-topic content our results suggest that the Likelihood Ratio Test based algorithm would perform better due to its ability to distinguish and 002lter out the off-topic features For future work we plan to extend the Likelihood Ratio Test methods especially the dBNP based approach by other determiners such as a or this  which should increase the recall of this method Another possibility which we will investigate regards the BNP patterns The current Likelihood Ratio Test approach is not capable of dealing with discontinuous feature phrases for example in 5 the quality of the pictures is great the feature would be picture quality  This problem could be addressed by introducing wildcards in the BNP patterns We will also investigate whether there are any methods in order to calculate an optimal threshold for the candidate feature extraction in order to increase the recall of the Likelihood Ratio Test based algorithm We plan to investigate whether a deeper linguistic analysis e.g with a dependency parser can improve the feature extraction Acknowledgements The project was funded by means of the German Federal Ministry of Economy and Technology under the promotional reference 01MQ07012 The authors take the responsibility for the contents The information in this document is proprietary to the following Theseus Texo consortium members Technische Universit  at Darmstadt The information in this document is provided as is and no guarantee or warranty is given that the information is 002t for any particular purpose The above referenced consortium members shall have no liability for damages of any kind including without limitation direct special indirect or consequential damages that may result from the use of these materials subject to any liability which is mandatory due to applicable law Copyright 2008 by Technische Universit  at Darmstadt References  R Agra w al and R Srikant F ast algorithms for mining association rules Proc 20th Int Conf Very Large Data Bases VLDB  1215:487–499 1994  K Bloom N Gar g and S Ar g amon Extracting a ppraisal expressions In HLT-NAACL 2007  pages 308–315 2007  R Bruce and J W iebe Recognizing subjecti vity a case study in manual tagging Natural Language Engineering  5\(02 1999  K Da v e S La wrence and D Pennock Mi ning the peanut gallery opinion extraction and semantic classi\002cation of product reviews In Proceedings of the 12th International Conference on World Wide Web  pages 519–528 New York NY USA 2003 ACM  T  Dunning Accurate methods for the statistics of surprise and coincidence Computational Linguistics  19\(1 1993  O Feiguina and G Lapalme Query-based summ arization of customer reviews In Canadian Conference on AI  pages 452–463 2007  C Fellbaum Wordnet An Electronic Lexical Database  MIT Press 1998  A Ferraresi Building a v ery lar ge corpus of english obtained by web crawling ukwac Master's thesis University of Bologna Italy 2007  M Gamon A Aue S Corston-Oli v er  and E Ringger  Pulse Mining customer opinions from free text In Proceedings of the 6th International Symposium on Intelligent Data Analysis IDA-2006  Springer-Verlag 2005  N Glance M Hurst K Nig am M Sie gler  R Stockton and T Tomokiyo Deriving marketing intelligence from online discussion In Proceedings of the 11th ACM SIGKDD International Conference on Knowledge Discovery in Data Mining  pages 419–428 New York USA 2005 ACM  M Hu and B Liu Mining opinion features in customer reviews In Proceedings of 9th National Conference on Arti\002cial Intelligence  2004  N K obayashi K Inui K T atei shi and T  Fukushima Collecting evaluative expressions for opinion extraction In Proceedings of IJCNLP 2004  pages 596–605 2004  S Morinag a K Y amanishi K T ateishi and T  Fukushima Mining product reputations on the Web In Proceedings of KDD-02 8th ACM International Conference on Knowledge Discovery and Data Mining  pages 341–349 Edmonton CA 2002 ACM Press  A.-M Popescu and O Etzioni Extracting product features and opinions from reviews In Proceedings of HLT-EMNLP-05 the Human Language Technology Conference/Conference on Empirical Methods in Natural Language Processing  pages 339–346 Vancouver CA 2005  H Schmid T reetagger a language independent part-ofspeech tagger Institut fur Maschinelle Sprachverarbeitung Universitat Stuttgart  1995  J W iebe R Bruce and T  O'Hara De v elopment and use of a gold-standard data set for subjectivity classi\002cations In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics  pages 246–253 Association for Computational Linguistics Morristown NJ USA 1999  J Y i T  Nasuka w a R Bunescu and W  Niblack Sentiment analyzer Extracting sentiments about a given topic using natural language processing techniques In Proceeding of ICDM-03 the 3ird IEEE International Conference on Data Mining  pages 427–434 Melbourne US 2003 IEEE Computer Society 
160 
151 


Figure 4 Expected and real number of extracted patterns using two promoter sequence datasets Horizontal axis minimum support vertical axis number of patterns 
85 
85 


a frequency constraint and according to the structure of the dataset These proposals are all based on a global analytical model i.e an interesting approach that needs however to develop complex and speci\036c models As a result they cannot be easily extended to handle complex conjunctions of constraints to incorporate different symbol distributions or different semantics for pattern occurrences To the best of our knowledge no method has been proposed to estimate the number of patterns satisfying a constraint while avoiding to develop a global analytical model Our approach requires only to know how to compute for a given pattern its probability to satisfy the constraint this can be obtained in many situations and it remains ef\036cient in practice by adopting a pattern space sampling scheme 6 Conclusion Using constraints to specify subjective interestingness issues and to support actionable pattern discovery has become popular Constraint-based mining techniques are now well studied for many pattern domains but one of the bottlenecks for using them within Knowledge Discovery processes is the extraction parameter tuning This is especially true in the context of differential mining where domain knowledge is used to provide different datasets to support the search of truly interesting patterns From a user perspective a simple approach would be to get graphics that depict the extraction landscape i.e the number of extracted patterns for many points in the parameter space We developed an ef\036cient technique based on pattern space sampling that provides an estimate on the number of extracted patterns This has been applied to non trivial substring pattern mining tasks and we demonstrated by means of many experiments that the technique is effective It provides reasonable estimates given execution times that enable to probe a large number of points in the parameter space Notice that domain knowledge is also exploited here when selecting the distribution model Future directions of work include to adapt the approach to other pattern domains and to different constraints Another interesting aspect to investigate is the use of more sophisticated sampling schemes e.g that could b e incorporated in the approach when more complex syntactical constraints are handled e.g a grammar to specify the shape of the patterns Acknowledgments This work is partly funded by EU contract IQ FP6-516169 Inductive Queries for Mining Patterns and Models and by the French contract ANR-MDCO14 Bingo2 Knowledge Discovery For and By Inductive Queries We thank Dr Olivier Gandrillon from the Center for Molecular and Cellular Genetics CNRS UMR 5534 who provided the DNA promoter sequences References  J F  Boulicaut L De Raedt and H  M annila e ditors Constraint-Based Mining and Inductive Databases  volume 3848 of LNCS  Springer 2005  C  B resson C K e ime C F a ure Y  Letrillard M  B arbado S San\036lippo N Benhra O Gandrillon and S GoninGiraud Large-scale analysis by SAGE revealed new mechanisms of v-erba oncogene action BMC Genomics  8\(390 2007  L  C ao and C  Z hang Domain-dri v e n actionable kno wledge discovery in the real world In Proceedings PAKDDÕ06 volume 3918 of LNCS  pages 821–830 Springer 2006  G  D ong and J  L i Ef 036cient mining of emer ging patterns discovering trends and differences In Proceedings ACM SIGKDDÕ99  pages 43–52 1999  F  Geerts B  G oethals and J  V  d en Bussche T ight upper bounds on the number of candidate patterns ACM Trans on Database Systems  30\(2 2005  U  K eich and P  A  P e vzner  S ubtle motifs de\036ning the limits of motif 036nding algorithms Bioinformatics  18\(10 2002  S  K ramer  L De Raedt and C  Helma M olecular f eature mining in HIV data In Proceedings KDDÕ01  pages 136 143 2001  L  L hote F  Rioult and A  S oulet A v e rage number of frequent closed patterns in bernouilli and markovian databases In Proceedings IEEE ICDMÕ05  pages 713–716 2005  I  M itasiunaite a nd J.-F  B oulicaut Looking for monotonicity properties of a similarity constraint on sequences In Proceedings of ACM SACÕ06 Data Mining  pages 546–552 2006  I Mitasiunaite and J F  Boulicaut Introducing s oftness i nto inductive queries on string databases In Databases and Information Systems IV  pages 117–132 IOS Press 2007  I Mitasiunaite C Rigotti S Schicklin L  M e yniel J F  Boulicaut and O Gandrillon Extracting signature motifs from promoter sets of differentially expressed genes Technical report LIRIS CNRS UMR 5205 INSA Lyon France 2008 23 pages Submitted  G Ramesh W  M aniatty  a nd M J Zaki F easible itemset distributions in data mining theory and application In Proceedings ACM PODSÕ03  pages 284–295 2003  F  Zelezn  y Ef\036cient sampling in relational feature spaces In Proceedings ILPÕ05  volume 3625 of LNCS  pages 397 413 Springer 2005 
86 
86 


