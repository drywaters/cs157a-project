Software Cost Estimation using Fuzzy Decision Trees Andreas S. Andreou Department of Computer Science University of Cyprus Nicosia, Cyprus aandreou@cs.ucy.ac.cy Efi Papatheocharous Department of Computer Science University of Cyprus Nicosia, Cyprus efi.papatheocharous@cs.ucy.ac.cy Abstract 227 This paper addresses the issue of software cost estimation through fuzzy decision trees, aiming at acquiring accurate and reliable effort estimates for project resource allocation and control. Two algorithms, namely CHAID and CART, are applied on empirical software cost data recorded in the ISBSG repository. Approximately 1000 project data records 
are selected for analysis and experimentation, with fuzzy decision trees instances being generated and evaluated based on prediction accuracy. The set of association rules extracted is used for providing mean effort value ranges. The experimental results suggest that the proposed approach may provide accurate cost predictions in terms of effort. In addition, there is strong evidence that the fuzzy transformation of cost drivers contribute to enhancing the estimation process Keywords- Cost estimation, fuzzy decsion trees, association rules I I NTRODUCTION Effort estimation is considered as a promising method for approaching the issue of reliable software cost estimation [12   
The dynamic nature of the software development process often hinders robust effort estimates, while the process remains highly prone to human errors and biases  T h e c o nt i n uo us l y  evolving software development environment relates to the rapid change of technologies used, tasks performed, people involved, unique internal social networks and environment of each organization, as well as specifications and conditions of the systems under production. These result in major volatility regarding a software project\222s progress, productivity rates resource allocation, schedules and costs In this work, we approximate the issue of software cost estimation by classifying information describing a plethora of 
past projects, deriving association rules for cost drivers associated with effort; these rules are then used to successfully predict the actual development effort. To this end, a large publicly available database is used containing empirical samples of past project data, namely the ISBSG Rel. 9 [7   T h e  main assumption is that historical data of completed projects can be representative of real world values and can be used for predicting software development effort of current or future projects. More specifically, we attempt to build an automated fuzzy software cost estimation model combining fuzzy logic theory [10  an d  d eci s i o n  tr ees  8   P ar t i cu l ar l y w e a p p l y th e  Chi-squared Automatic Interaction Detection \(CHAID\ [6  a n d  
Classification and Regression Trees \(CART l g o r i t h m s for  deriving a finite set of association rules. Our proposition indicates that we can device instances of Fuzzy Decision Trees FDT\that yield robust rules, which can then be used for estimating effort values. Finally, once the automated tool is validated and relatively accurate and reliable software cost estimates are obtained, then this tool may be used for assisting the development of optimized software estimators in an increasingly competitive and complex environment The rest of the paper is organized as follows: In the following section we briefly mention customary techniques used in software cost estimation and discuss the recent 
emergence of fuzzy methods. In the third section, we describe the proposed approach of FDT using specific pre-processed cost attributes, while in the fourth section we describe the experiments conducted and the results obtained. Finally, in the last section, we present our conclusions along with some suggestions for improving our approach and future research steps II B RIEF L ITERATURE R EVIEW Over the last fifteen years software cost estimation research has shown a growing interest in the use of combinations of statistical prediction and machine learning techniques as these were proved able to detect composite relationships. Classic 
approaches like algorithmic models [14, 2 i n vol ve  a n a l y t i c a l  or statistical equations relating the software project costs to a number of input parameters called cost drivers. Usually machine learning models utilize neural networks [1, 5, 13  fuzzy logic [1, 17 a n d e vol u t i o n a r y a l g o r i t h m s  4  9 for  performing improved software cost estimations Moreover, research studies propose various data mining techniques to extract knowledge and produce useful rules Particularly [11 u t ili z es cla s s i f i c a ti o n  tr ees co m b in e d w ith  regression-based models applied on historical datasets COCOMO and Kemerer\ to obtain software cost estimates. In addition, techniques using notions of fuzzy logic have gained 
much interest among researchers and were soon adopted into models to act as universal approximators. In fact, an effort to embed risk assessment into a fuzzy decision tree approach for software cost estimation [16 s h o w ed  th at s u c h s tr u ct u r es  ev en  if they are hard to interpret\ can be better depicted and explained if fuzzification of the values is applied. The fuzzification empowered the characteristics of FDTs offering 371 
978-1-4244-2188-6/08/$25.00 \2512008 IEEE 


them significant advantages compared to other machine learning techniques regarding their ability to produce accurate predictive tools and extract self-descriptive rules in a way that is easier to interpret by humans III T HE F UZZY D ECISION T REE FDT A PPROACH The proposed approach attempts to provide a tool for automatic software cost estimation by generating Fuzzy Decision Tree \(FDT\ instances that produce  association rules having the following scope: \(i\lassify project data and provide conditional probabilities, \(ii\promote significant cost drivers from a wide pool of characteristics, \(iii\nhance two common decision tree algorithms, namely CHAID and CART iv\ search for interesting relationships holding among cost attributes and provide insights into the influence degrees, and finally, \(v\ extract useful rules. The novelty lies within the attempt to determine a plausible way to derive a hierarchy of significant attributes and also provide an insight of the most noteworthy association rules holding in the dataset. The basic steps of the approach are presented in the subsequent sections A  7 Description of Cost Drivers The ISBSG dataset [7 u s e d  in  th e ex p e r im e n t s co n s i s t s o f  various software project cost records coming from a broad cross section of the industry and range in size, effort, platform language and development technique data. It contains 92 variables from which a reduced number of project attributes was selected, as these are described in Table I The attributes were initially used in their pure form but they were considered difficult to manipulate as they contained highly heterogeneous, dispersed, or even biased samples Close examination of the probability distribution of each cost driver on the corresponding histograms confirmed that an ordinal representation of each variable by a trapezoidal membership function may be generated and exploited thus transforming actual numerical values to their fuzzy counterparts. Therefore, the numerical values of the attributes whose values ranges were relatively large \(i.e. of the order of thousands\ere separated into five ordinal intervals of equal size, whereas those that had small value ranges \(i.e. of the order of hundreds\ were separated into three ordinal intervals of equal size TABLE I S OFTWARE C OST D RIVERS D ESCRIPTION ABBR. NAME DESCRIPTION EFF Full Cycle Work Effort Total effort \(in hours\recorded against the project AFP Adjusted Function Points Functional size of the project at the final count PET Project Elapsed Time Total elapsed time for the project \(in calendar months PDRU Project PDR ufp Project delivery rate \(in hours per function point equals to the quotient of effort and functional size PIT Project Inactive Time The number of calendar months in which no activity occurred ATS Average Team Size The average number of people that worked on the project B Creating the FDT The FDT structure built contained flow-chart-like nodes with the top-most node called the root, the terminal nodes called leaves, the internal nodes representing attributes and each branch representing an outcome of the test performed on the cost attribute. The FDTs formed the basis to discover interrelations among vital project variables and attempted to predict the dependent variable \(Effort - EFF\, which is placed at the root of the tree \(as shown in the example of Fig. 1\. By applying various criteria the project data is split into groups and with the greedy local search method the optimal FDT structures are iteratively identified until the algorithm stops The FDT structure is interpreted by rules from the root node to the terminal node of the form: \223If \(condition 1 and condition 2 and \205 condition N hen Z 224, where the conditions are extracted from the nodes and Z is the root. Additionally, the FDT yields probability degrees which can be used to evaluate the appropriateness of the structure according to the problem being modeled. The degree of confidence and significance are used as evaluation metrics to indicate the probability of correct classification. A new project can be classified by starting at the root node of the FDT and sliding down selecting a branch based on the project\222s attribute values and until a terminal node is reached. At the terminal node the effort is calculated by using the rules extracted and the mean effort value of the ranges associated with the linguistic value in the inference part of the rule Figure 1 An example of a FDT and a rule extracted 372 


IV E XPERIMENTS AND R ESULTS In this section the experiments conducted and results obtained are presented. Firstly, we created trees with the raw data and extracted the splitting sets of ranges for each attribute classes identified by the trees\. Then, we used the preprocessed data \(i.e., after normalization and transformation into ordinal values according to the ranges\ to create FDTs, from which we extracted a hierarchy of the significant project attributes. The FDTs constructed with SPSS v.15.0 were tested with both algorithms CHAID and CART, for which the optimal parameter settings were found in empirically manner the minimum number of cases per node was set to 2. For CHAID, the significance level value was set to 0.05 and the chi-squared test statistic used was the Pearson. For CART, the minimum change in improvement was set to 0.0001 and both Twoing and Gini splitting methods were tested. According to the results obtained by both algorithms, the FDTs managed to locate the most significant project attributes, that is, those factors that seemed to influence more the value of the work effort attribute, among the thirteen attributes under investigation. These appeared in tree level and hence significance descending order as follows: first AFP, then PET followed by PDRU, ID, DT and LT, and last PIT and PPL. We also observed that the projects were classified in each node according to distinct classes of intervals, meaning five distinct ranges for AFP and three for the rest of the attributes, thus confirming our value ranges used for the fuzzification Secondly, in the rest of the experiments we used only the ordinal attributes \(see Table I\ excluding all categorical attributes. The hypothesis was that the ordinal attributes are more qualified for producing cost estimates and represent measurable data values. The reduced dataset was randomly separated into training and testing subsets, the former for constructing the trees and the latter for evaluating cost estimation ability by validating the rules extracted and providing an initial mean effort value prediction based on the intervals Table II lists indicative significant rules in terms of degree of confidence \(marked as CONF.\. Each rule was evaluated according to this degree, the number of times it appeared in each experiment and whether it appeared in the results of both algorithms or not. The threshold for selecting the best rules was set to CONF. > 50%. Thus, the output of this process was actually a condensed set of common \(best\ rules based on which a mean effort estimation was obtained. The rules were evaluated with the testing subset and performance was assessed with the Mean Relative Error MRE the Normalized Root Mean Squared Error NRMSE and the Correlation Coefficient  CC metrics TABLE II I NDICATIVE R ULES O BTAINED FROM FDT S IF-RULE THEN CONF AFP = "LOW" or "MEDIUM" or "VERY HIGH or "HIGH"\  and  \(PET = "" or "MEDIUM" or HIGH EFF VERY HIGH 79 AFP = "VERY LOW"\ and  \(PDRU = "LOW and  \(PET = "MEDIUM" or "HIGH EFF HIGH 66 AFP = "VERY LOW" \ AND \(PET = "MEDIUM or "HIGH EFF HIGH 66 The three different validation cost driver schemes that we used consisted of different attributes. In the first scheme the five most significant attributes, i.e. those that were placed at the top-most levels of the FDT,were taken into consideration and yielded MRE 0.13 NRMSE 1.04 and CC 0.53. The second scheme included four out of five most significant attributes \(i.e AFP, PET, PIT and ATS\ and yielded MRE 0.11 NRMSE 0.87 and CC 0.65 which is the most optimal prediction obtained. The third scheme included three out of the five most significant attributes and particularly those attributes that are available and can be measured from the early stages of the project development \(i.e. AFP, PDRU and ATS\nd yielded MRE 0.17 NRMSE 1.32 and CC 0.45. The effort estimations obtained may be considered quite successful with MRE and NRMSE being confined to quite low levels and the 223early available attributes\224 scheme, which may be considered more valuable in practice, exhibiting small performance accuracy decrease V  4 C ONCLUSIONS This work attempted to propose the use of automatic generation and evaluation of decision trees instances enhanced with fuzzy logic for software cost estimation. The approach of Fuzzy Decision Trees \(FDT\ extracts sets of association rules to express relationships located among project attributes under investigation and effort. These relationships are stated in a comprehensible manner using gradual linguistic terms that are easier to grasp by project managers. The simple structure of decision trees is enhanced from capable classifiers to automatic generators of sets of rules. Finally, the approach provides a hierarchy of the important project attributes grouped into three validation schemes and used to obtain preliminary effort predictions The experiments conducted showed that sufficiently accurate cost predictions, close to the actual development costs can be achieved. The rules extracted promote the linguistic representation of the attributes\222 associations and provide added value to the whole estimation process with optimized accuracy in relation to other approaches. The results indicated the applicability of the approach and suggested that the generation FDT may be conceived as a fairly good solution in classifying projects and extracting rules describing the nature of the software development environment. Future work will utilize these rules into other structures or software cost models Additional investigation will include comparison of other classification algorithms and further improvement on the structure and on the algorithms employed in the experiments VI 5 R EFERENCES 1 A  I d r i  T   M   K hos hgof t aar  and  A   A b r a n 223C a n N e ur al N e t w o r k s  be Easily Interpreted in Software Cost Estimation?\224, Proceedings of the 2002 IEEE World Congress on Computational Intelligence, IEEE Computer Press, Washington D.C., 2002, pp. 1162-1167 2 B  W  B o e h m C   A b t s  and  S  C h ul ani  223 S of t w ar e D e ve l opm en t C o s t  Estimation Approaches \226 A Survey\224, Annals of Software Engineering Vol. 10, No. 1, Springer, Netherlands, 2000, pp. 177-205 3 B r e i m a n L J   Fr i e d m a n  R  O s h l e n a n d C   St o n e  C l a s s i f i c a t i o n a n d  Regression Trees, Wadsworth International Group, 1984 373 


4 C  J  B u r g es s  a n d M  L e f t l e y 223C a n G e ne i c  P r og r a mm i n g I m p r ove Software Effort Estimation? A Comparative Evaluation\224, Information and Software Technology, Vol. 43, No. 14, Elsevier, Amsterdam, 2001 pp. 863-873 5 E  S  J u n  and J   K  L e e  223Q uas i o p t i m al C a s e s e l ec t i ve N e ur al N e t w o r k  Model for Software Effort Estimation\224, Expert Systems with Applications, Vol. 21, No. 1, Elsevier, New York, 2001, pp. 1-14 6 G  V   K a s s  223 A n E x pl or a t or y T e c hn i qu e f or I n ve s t i ga t i ng L a r g e  Quantities of Categorical Data\224, Applied Statistics, Vol. 20, No. 2 1980 pp. 119-127 7 I nt er na t i ona l S o f t w a r e B e n c h m a r k i n g St a n da r d s G r ou p T h e B e n c h m a r k  Release 9, 2005 HU http://www.isbsg.org U H  8 J  R  Q u i n l a n I n du ct i o n o f D e ci s i o n T r e e s  M a ch i n e L e a r ni n g   V o l  1   1986, pp. 81-106 9 J  J  D o l a do  223O n t h e P r ob l e m  of t h e Sof t w a r e C o s t  F unc t i on 224  Information and Software Technology, Vol. 43, No. 1, Elsevier Amsterdam, 2001, pp. 61-72 1 L   A  Z a de h 223F uzzy Se t 224  I n f o r m at i o n and C o nt r o l   V o l  8 1965 pp   338-353 11 L C Bria n d   V  R  Ba s ili a n d  W M  T h o m a s  223 A P a t t e r n R e c o g n it io n  Approach for Software Engineering Data Analysis\224, IEEE Transactions on Software Engineering, Vol. 18, 1992, pp. 931-942 1 M  J 370 r g e n s e n  and  M  S h ep pe r d  223 A  S y s t em at i c R e v i ew  of S o f t w a r e  Development Cost Estimation Studies\224, IEEE Transactions on Software Engineering, Vol. 33, No. 1, IEEE Computer Press, Washington D.C 2007, pp. 33-53 13 N  T a da y o n 223 N eu r a l N e t w or k  A p pr oa c h  f o r S o f t w a r e C o s t  E s t i ma t i o n 224   Proceedings of the International Conference on Information Technology Coding and Computing, IEEE Computer Press, Washington D.C., 2005 pp. 815-818 1 P u t n a m L  H   and W  M y e r s   M e a s ur es  f o r  E x ce l l enc e  R e l i a b l e Software on Time, Within Budget, Yourdan Press, New Jersey, 1992 15 R   V a le rd i  223 C o g n i t i v e Li m i ts o f S o f t w a re Co s t Es t i m a ti o n 224  F i rs t International Symposium on Empirical Software Engineering and Measurement \(ESEM\, 2007, pp. 117-125 16 S J H u a n g C  Y Li n a n d N  H C h iu 223 F u z z y D e c i s i o n T r e e  Apporoach for Embedding Risk Assessment Information into Software Cost Estimation Model\224, Software Engineering and Software, Vol. 22 2006, pp. 297-313 1 Z  X u   and  T  M  K h os hgof t aar   223I d e n t i f i c at i o n of F u zzy M o de l s of  Software Cost Estimation\224, Fuzzy Sets and Systems, Vol. 145, No. 1 Elsevier, New York, 2004, pp. 141-163 374 


                                                                                     2400 2008 Chinese Control and Decision Conference CCDC 2008 


B II*L\E#9I #&M\J+I%B  r r!0\n\n\rB\n\n\n   r  n    r"\n    n   n	\r\n	\n	\r  r"\r\r	\r!0\n r 0 r  n\r   r\r2\r r  r  0\n  n!"\r\r\n\r\n\n\r	BC n&*	70\r\r2\r7\n0\r n\r\r\n\n\r\n 7$\r"!\r	\r!%!\n\n    r r	\r$\r7\n   r"\r r$\r7\n   r"\r	7\n r\r%\n"7\n\n\r\n 2M  E	B n0\r\r\n"2\r0\r n\n\r  r  r    r 0\n2\r$\n\r\r 2M 2\r0 r r\n!\n\n r6\r"\r	\r r               B B  B      B B  C     B     B     B&B   B B&B   B&B    n\r\n\n0\r\r\r!27\n!1\n r\r\n	0	0\r\r\n\r  r r& \r$\r\r\n!2\n7\r\n r6\r"\r	%\r\n!27\n D  n  7\n  r\r  n  r\r2\r\n  r\r   r  n\r n!\r%\r\r\r6\r\r r r2\r\n0\r\r\n!&#"\r	\r  1\r\n\n\r\n"1\r r r 4\r&C&#\n\r\n2\r\r\r\r\n\r6\r r	\n\r!2\r$\r7 n\r\n7\r"!\r\n4\r\r\r\r  2\r7\n&#\n\n"7\n\r\r\r6\r r		\r!"!\n2\r\r\r\r r  r\n     2    r  r  7\n  n  0	0\r\n\r"!\r&907%7  r r r2\r\r\r	7\n"\r n\r\n%&&\r!2\r$\r7\n\n  r!2\r$\r7\n\n0&\n  r\n6"!\r\r7\r"!\r\n&C n!"7!\r\r\n!\r!$2\r    n\n2      r  r    77       r    r  n    r n&#\n"!\r\n\r\n!!	\n\r n r\r-"	\r!&#\n02 r:!"22;\n!\r\r!2 r$\r7\n&-\n	\r	\r\n\n  r0\n\r2\r\r	\r   J   n\r\n    r  6"!\r\n  7  r\n\n   2\r	\r!\n\r\r2\r\r"7\r\r\n	\r r\n\r	\n\r\r\n r\r\r\n2\n2/\r\r\n r!	!\r	\n""\r\r 2\n\n\r\r\n0\r n 7    n n\n  r\n  r    2    r r\r\n\r I6"!\r\n\r\n2\r	\r!\n\r2$\r\r n0\r\r\r\r2\r\r\n"\n\n20\r\r6\r  r	7	\n\r\n&907%\r  r\n\n2\r$\n\n\r\r!\n  r\n\r\r\r$\r  7\n&-\n\r0\n\n7\r6"!\r\n%\r r7\n\n!\r"\n\n\r	$\r\n"!\r\n r!"!\r\n\n7!$	7"\r!\n\r\n r\r6!\r\n7$1$\r\r r r\n\r\n0\r7\n\r"!\r\n\r 7	\r\r$\r 276"!\r\n\r5$\n\r	!\r n\r@J7\n\r\n\rA0\n\n&\r\n"\n\n2\r\r r\r\n\n\r2\r\r\r"\r\r\n\n 26"\r6"!\r\n 7%\r\n""\r\r\r r4\n\r\r	\n  IEI\(I I    0    n1    0    n\n r  n 2\r0\n\r\n\r!\nJ	*\r2\n\n  n\r\r\n   r\n  n\r\r\r  n  r\r  n    BB B       r  9\r\n  r  I\r"\n    r    r\r"3GG\n!&\r\r	\r&!G0\nG\rG%BB%\nV C B%&\r    1       r7  r  r\r  n  J2\r	!\r$\n\n  n\r\r n\r     r\n n\r\r\r n  n/\r\r  n!\r0  1 E	B3#"$2\r0\r\n\r#\n"!\r n\n\r6\r"\r	\r                  n n        n       r     


J+R-? ?-R3-J -#\\E J+#I\(L I#9\**-#- JI-L   n n\r%R&J!%W&S?:"\r  n\n\r 1	\n	17!\n  r+\n-\r\n*2 B%BB C       71    E2   n    r  n\r\r\n!\r	#\n1\n  n\r\r\n\r  3  r	\r9\r	\n*\r2\n\n0\r r n+\n	8\n\n#6\r!\r n\r\r\n r445	*'+,*$-\r\n\n\r\r\r\n*\r\r\n   BBB  I\n\r%9&R	%W&%S&S:-*\n\r n-	\r n7	 \n\r\nJ	"\r*\r2\n\n0\r n n\r\r\n\r r\n+\r\n	\n\r\r\r\n.\n/\r\r  n!\r0     E  R      I\n\r  E  E\n  9  r n\r	;\r\r"3GG000&\n&\n&GX\n\rG""\nGI    95  E      1    60    r\n  2\r r7\r  0.13  Y\n\n\n  B""&%3CB  R!2%&\(\n\n  r 8\r\n\r n\n29 r	\n\r\r  r\n-\r\n*\n\r  WZ\n%"\r 3BB  R!2%W& \n\r  r-8\r\n\r: \n\n29 r7\r  n    r  n\n    r!\r       W  Z  n  r  CBC  3 CCC B J%#&J	%,&J0:\r 3-10	2 n\r	\r r      n\r\r  n  r       r\n\n\r\r\r\n.\n/\r\r\n!\r0   BB    1    60  r\r  n  2\r\n 3BB  I&\(!%9&*:*\r 	&2!\n \r n   r\n\r\r	\n\r\r\n-:\r\r  MB&%*!2B C   71        9  L        J1    n\r\r  n  r    n\r\r\r  n r0\r\r\r  R&, :\r\n\r\r\n n\n n\r\r#6\r  n\r\r\n\r'\r\n	\n\r\r\r\n  r# \r\\n\r  W  22  n\n\r  n    r  n\n2  n     IA  J0  I2  n\n\r\n    3 CCC%B""%""BC    1  7  r\n  E    J1    n\r\r\n\r'\r\n\n'!\r0\r\r*\r\n  r  n\n  pp 467-472     1    r\r      J1    r   n 2!\n n\r\r\n\r'!\r0*\r\n'\r\n    n\r0\n  


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efﬁcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efﬁciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efﬁciency When the clustering model is available it is a signiﬁcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques Sufﬁcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efﬁcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efﬁcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conﬁdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conﬁdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conﬁdence The sufﬁcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efﬁcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efﬁciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufﬁciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70–81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207–216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487–499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145–154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146–153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9–15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597–600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern Classiﬁcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155–162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512–521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1–12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265–278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283–304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476–482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559–563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220–231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13–24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10–17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549–550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188–201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259–283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909–921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305–345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432–444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conﬁdence Intelligent Data Analysis  9\(4\:381–395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49–73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407–419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1–12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki Efﬁciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642–658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483–490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194–203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344–353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efﬁcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103–114 1996 
618 
618 


