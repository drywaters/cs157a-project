A New Associative Classifier for Text Categorization Zhitong Su, Wei Song, Dan Meng, Jinhong Li College of Information Engineering, North China University of Technology, Beijing 100144 China E-mail: suzhitong@ncut.edu.cn Abstract Text categorization has become one of the key techniques for handling and organizing text data. In practical text classification tasks, the ability to interpret the classification result is as important as the ability to classify exactly Associative classifiers have many favorable characteristics such as rapid training good classification accuracy, and excellent interpretation. In this paper, Closed-AC, which is a new associative classifier for text categorization, is proposed. Firstly, rough set is used to dimension reduction. Then, only generic rules composed of closed itemsets are used for cla ssification. Experimental results show benefits of the proposed associative classifier 1. Introduction Text categorization refers to the task of automatically assigning documents into one or more predefined there has been an increasing number of statistical and machine learning techniques that automatically generate text categoriza tion knowledge based on training examples. The goal of te xt categorization is to classify documents into a certain number of predefined categories Recently, a new classification technique, called associative classification, is proposed to combine the advantages of association rule mining and classification [2]. Given a tr aining data set, the task of an associative classification algorithm is to discover the classification rules which satisfy the user specified constraints denoted respectively by minimum support and minimum confidence thresholds. The classifier is built by choosing a subset of the generated classification rules that could be of use to classify new instances. Many studies have shown that associative classification often achieves better accuracy than do traditional classifica tion techniques [2, 3 To apply an associative classifier to the text classification problem in the real world, however, we need to remove several obstacles encountered during the training and testing phase. One of those is a high dimensional feature space. Dataset in the area of text classification, in many cases, has a very large number of features that are distinct lexical words. In associative classification, however, we consider all subsets of those words. Therefore, the effective number of features grows exponentially, and we cannot take into account all of them due to computational intractability. To overcome this problem we adopt the attribute reduction based on rough set [4 Another obstacle in associative text classification is the large number of classification rules that are produced in the training phase. Since using all of them becomes both inefficient computationally and ineffective in classifying we should select a part of those rules that have high quality. Liu et al proposed a pruning by database coverage, which is a kind of validation process using the training set for the purpose of choosing the best classi fication rules among  ned the concept of the database coverage. In addition, they proposed two other pruning methods. One is to prune low-ranked rules in terms of the confidence and support of the rules. The other is to prune the rules in which the correlation between the pattern and the class variables is weak. In this paper we adopted the problem of numerous redundant rules by using generic rules com posed of closed itemsets only, rather than the total set of strong association rules In Section 2 we introduce the general aspects of the associative classification. In Section 3 we explain the overall architecture of our text classification system using association rules and address the issues such as dimensionality reduction, and associative classifier based on generic rules composed of closed itemsets Experimental results and analys es of text classification ___________________________________ 978-1-4244-2197-8/08/$25.00 ©2008 IEEE Proceedings of 2008 3rd International Conference on Intelligent System and Knowledge Engineering 


using a large dataset are presented in Section 4, and we conclude our works in Section 5 2. Associative classification 2.1 Association rule mining An association rule is a kind of co-occurrence information on item I  i 1  i 2  i M  be a finite set of items and D be a dataset containing N transactions, where each transaction D is a list of distinct items. A set X I is called an itemset An itemset with k items is called a k itemset. The support of an itemset X denoted as sup  X  number of transactions in which X occurs as a subset For a given D let min_sup be the threshold minimum support value specified by user. If sup  X  min_sup  itemset X is called a frequent itemset. The set of all frequent k itemsets is denoted by F k  An association rule is an implication of the form A B where A I  B I and A B  Besides the above-mentioned support, the rule A B has confidence, denoted by conf  A B percentage of transactions in D containing A which also contain B  Rules that satisfy bot h a minimum support threshold  min_sup and a minimum confidence threshold  min_conf of association rule mining is to discover all strong rules 2.2 Associative classifier Consider the association rule in the view of a classification rule. Let A  A 1  A n be a set of attribute domains, and a data object obj  a 1  a n  a sequence of attribute values, i.e a j A j 1 j n Given a pattern P  a i 1  a ik where a ij A ij for 1 j k and i j i j for j j a data object obj is said to match pattern P if and only if, for 1 j k  obj has value a ij in attribute A ij  Definition 1 Associative classifier C  c 1  c m be a set of class labels. An associative classifier is the mapping R from the set of attribute values to a set of class labels R  A 1  A 2  A n  C 1 According to Eq. \(1 obj  a 1  a n a class label c C  Let a pattern variable be P and a class variable c If we rewrite the rule in the form of R  P c and have a training set T  P i  c i  induce the rule set R for which the element has the sup  P c  min_sup and conf  P c  min_conf The procedure of associative classifica tion rule mining is not much different from that of general association rule mining. One difference is that, in associative classification rule mining for text categorization, the information of the distribution of word patterns matching each class is additionally maintained Now that we have a classification system, it requires a decision on which cl ass to assign a new test document. First, we search for th e rules in which the pattern matches the document. Next, from these rules we perform a prediction based on some predefined decision criterion 3. Associative classifier for text categorization 3.1 Overall architecture The overall system architecture for associative classification is shown in Fig. 1. The left-hand side of the figure denotes the trai ning process and the righthand side the testing process  Training document Preprocessing Generic classification rules exaction R 1  R k Decide the class Associative classifier R 1  p 1 c 1  R n  p n c n Pattern matching New document Preprocessing Fig 1 Overall architecture of associative classier for text categorization First, raw data for training is processed to fit to an appropriate form for training. This is called Preprocessing. The approach begins with a standard practice in information retriev al \(IR to encode Proceedings of 2008 3rd International Conference on Intelligent System and Knowledge Engineering 


documents with vectors, in which each component corresponds to a different word, and the value of the component reflects the frequency of word occurrence in the document From the perspective of association rule, a vector of document can be treated as a transaction, and each word in the vector can be viewed as an item. Thus, we can mine classification rules. Because the initial number of rules is very large, we only generic rules composed of closed itemsets. Finally, we construct a classification rule database with these selected rules When a new document comes in to be classified, we convert it into a pattern of words and search the database for matching rules With the rules matched we decide which class the te st document is assigned to 3.2 Dimensionality reduction by rough set In practice, the resulting dimensionality of the space is often tremendously huge, since the number of dimensions is determined by the number of distinct indexed terms in the corpus. As a result, techniques for controlling the dimensionality of the vector space are required The theory of rough a jor m a them atical tool for managing uncertai n that arises from granularity in the domain of discourse, that is from the indiscernibility between objects in a set. The intention is to approximate a rough imprecise domain of discourse by a pai r of exact concepts, called lower and upper approximation. These exact concepts are determined by an indiscernibility relation on the domain, which, in turn, ma y be induced by a given set of attributes ascribed to the objects of the domain. The lower approximation is the set of objects definitely belonging to the vague concept, whereas the upper approximation is the set of objects possibly belonging to the same. These approximations are used to define the notions of discernibility matrices which play a fundamental role in the reduction of knowledge Attribute reduction techniques eliminate superfluous attributes and create a minimal sufficient subset of attributes for a deci sion table. Such minimal sufficient subset of attributes, called a reduct is an essential part of the decision table which can discern all examples discernible by the original table and cannot be reduced any more. A subset B of a set of attributes C is a reduct of C with respect to D if and only if 1 POS B  D  POS C  D  2 POS B  a   D  POS C  D  a B  Where is the positive region of D with respect to B A set C of condition attributes may contain more than one reduct. The set of attributes common to all reducts of C is called the core of C The core contains all indispensable attributes of a decision table and can be defined as  B POS D CORE C  D  c C  POS C  c   D  POS C  D  2 Our algorithm for computing a reduct is outlined as follows Step 1 Compute CORE C  D  attribute in C remove it from C and check whether it changes the positive region. Let CORE C  D  of all condition attributes whose removal does not change the positive region Step 2 Check whether CORE C  D is a reduct of the rule set. If yes, stop and CORE C  D  3.3 Extracting and storing generic rules As the number of items grows linearly, the number of the antecedents in the left-hand side of association rule grows exponentially. Though we can reduce the size of the subset of patterns by the two parameters min_sup and min_conf the search often becomes computationally intractable when we use naive methods. To solve this problem, we use generic rules composed of closed item total set of strong association rules The concept of closed itemset is based on the two following functions f and g  f  X  i I  t X  i t  g  Y  t T  i Y  i t  Function f associates with X the items common to all objects t X and g associates with Y the objects related to all items i Y  Definition 2 An itemset X is said to be closed if and only if c  X  f  g  X  fg  X  X Definition 3 An itemset g I is a generator of a closed itemset X iff c  g  X and g  I with g  g such that c  g  X  Definition 4 Min-max association rules Let AR be the set of association rules extracted. An association rule R  l 1 l 2 AR is a min-max association rule iff R  l 1  l 2  AR with sup  R  sup  R  conf  R  conf  R  l 1  l 1 and l 2 l 2   that we can only m i ne m i n-m ax association rules, and all other rules can be deduced from these rules. Furthermore, the rule with the form R  g c where g  is a generator c is a closed itemset, and g c  is a min-max association rule To efficiently discover the mi n-max association rule we propose the following method based on subsume   Proceedings of 2008 3rd International Conference on Intelligent System and Knowledge Engineering 


Definition 5 The subsume index of item i is subsume  i  j I  j i g  i  g  j  Furthermore, the following properties are used for pruning Theorem 1 Let X be an itemset X subsume  X  a closed item Theorem 2 Let G be the set of generators of dataset D Then for all A G every proper subset B of A is a generator, i.e B G and for all X G every proper superset Y of X is not a generator, i.e Y G   Algorithm 1 Discover generic classi fication rules Input dataset D  min_sup  min_conf Output associative classifier C 1 D once. Delete infrequent items 2 for each frequent item i do 3 C  i subsume  i  i  4 for  k 2 F k 1  k  do 5 G k Candidate-Gen F k 1  6 for each gen G k do 7 C  gen subsume  gen  gen  8  gen  subsume  gen  k  Procedure back gen  subsume  level  9 for  k 1 k  level  k  do 10 for each g G k and g subsume  gen  do 11 if g subsume  gen  g pass min_sup and min_conf then 12 C  g subsume  gen  g  3.4 Performing classification After a set of rules is selected for classification Closed-AC is ready to classify new objects. Some methods the support-confidence order to classify a new object However, the confidence measure selection could be misleading, since it may identify a rule A B as an interesting one even though, the occurrence of A does not imply the occurrence of B  fact the confidence can be deceiving since it is only an estimate of the conditional probability of itemset B given an itemset A and does not measure the actual strength of the implication between A and B  To avoid the lacuna of using only confidence metric we adopt the conviction  parameter of conviction is defined as Conv  A B  P  A  P  B  P  A  B  3 Unlike confidence, conviction factors in both P  A  and P  B and always has a value of 1 when the relevant items are completely unrelated. Generally conviction is truly a measure of implication because it is directional, it is maximal for perfect implications and it properly takes into account both P  A  P  B  Given two rules R 1 and R 2  R 1 is said to precede R 2  denoted R 1  R 2 if the followed condition is fulfilled Conv  R 1  Conv  R 2  Conv  R 1  Conv  R 2  Conf  R 1  Conf  R 2  Conv  R 1  Conv  R 2  Conf  R 1  Conf  R 2  and sup  R 1  sup  R 2  4. Experimental results We compare the accuracy of the proposed ClosedAC with other  biol ogical abstracts database which is created by Shanghai Information Center of Life Sciences, Chinese Academy of Sciences. The database contains documents on the biological research in China, including that on general biology cytology genetics, biophysics, molecul ar biology, etc. All abstract have been categorized by experts and researchers of relevant fields Fig 2 Comparison of accuracies on Chinese biological abstracts database Comparing the results in Fig.2, we observe that Closed-AC can achieve a better accuracy than CBA and CMAR on most experi ments with different number of the abstracts. Due to the usage of all the strong rules, the accuracy of CBA and CMAR heavily depends on the number of instances and decreases dramatically when the number of the abstracts increasing 5. Conclusions Associative classification is a new method in the area of document classifica tion. The expression of the Proceedings of 2008 3rd International Conference on Intelligent System and Knowledge Engineering 


classification rule is easy and human-readable. To solve the problems of existing associative classification for text categorization, Closed-AC is proposed in this paper. The advantages of Closed-AC are as follows. On the one hand, rough set is used to dimension reduction. On the other hand, only generic rules composed of closed itemsets are used for classification. Experiment al results show the proposed associative classifier is effective Acknowledgements This work is supported by Beijing Municipal Education Commission \(KM200710009006 Key Youth Research Project of North China University of Technology References  S. M. Weiss, N. Indurkhy a T Zhang and F Damerau Text Mining Springer, Berlin, 2005  B. Liu, W. Hsu and Y. Ma Integrating classification and association rule mining Proceedings of International Conference on Knowledge Discovery and Data Mining 1998 80-86  R. Rak, L. A. Kurgan and M Reformat A treeprojection-based algorithm for multi-label recurrent-item associative-classification rule generation Data Knowledge Engineering 2008, 64\(1  L. Polkowski Rough Sets: Mathematical Foundations  Springer, Berlin, 2002   W Li J Pei and J. Han, “CMAR: accurate and efficient classification based on multiple class-association rules Proceedings of the 2001 IEEE International Conference on Data Mining 2001: 369–376  m e and L. Lakhal, “Generating a condensed representation for association rules Journal of Intelligent Information Systems 2005, 24\(1  R Agrawal and R. Srikant, “Fast algorithm s for mining association rules Proceedings of the 20th International Conf erence on Very Large Data Bases  1994: 487-499   C D Manning, P. Raghavan and H. Schütze Introduction to Information Retrieval Cambridge University Press, Cambridge, UK, 2008  W  Song, B. R. Yang, and Z Y Xu IndexMaxMiner: a new maximal frequent itemset mining algorithm International Journal on Artificial Intelligence Tools 2008, 17\(2  Ham ilton Interestingness measures for data mining: A survey ACM Computing Surveys 2006, 38 \(3  an and S Tsur Dynamic itemset counting and implication rules for market basket data Proceedings of the 1997 ACM SIGMOD International C onference on Management of Data 1997: 255-264 Proceedings of 2008 3rd International Conference on Intelligent System and Knowledge Engineering 


  m otif function 3D structure interaction Gene Ontology molecular function, Cellular component, and biological process and Gene-related chemical compound and chemical reaction as illustrated in Figure 5 As a result, these association rules are used to get protein identifiers/gene/chemical compounds which might be proposed as drug targets for breast cancer   For example one of the association rules indicated that Biological Processes regulation of transcription DNA-dependent is associated with Cellular Components  nucleus when minimum support equals 4 and minimum  confidence equals 0.5 This association rule suggests that proteins having these characteristics are proposed to be drug targets for breast cancer, which are ESR2_HUMAN, Q15636_HUMAN Q9H2M1_HUMAN, Q9H2M2_HUMAN \(Swissprot protein ids A similar procedure is applied for chemical compounds  5. Discussion  Applying DruTiMine on several tissues for cancer has proven the validity of the system where text mining and association rule mining are used based on microarray experiments to identify drug targets for the specified disease and tissue type. DruTiMine can help pharmaceuticals to identify drug targets of proteins/genes/chemical compounds in silicon instead of going through major steps of laboratory tests in vitro This does not mean that laboratory tests will not be needed, but the need for systems like DruTiMine is becoming a must for providing guidance to experts before conducting laboratory experiments Microarray experiments have proven through DruTiMine, to be very useful for deriving drug targets of proteins/genes/chemical compounds. The system is capable of relating proteins identified in Medline database abstracts to differentially expressed genes in microarray experiments and provided insight on possible associations among a variety of related biological information such as gene molecular functions gene biological process and gene cellular components which is definitely of value in speeding up the drug target discovery phase of the drug discovery process      6. C onclusions and future work  DruTiMine has been introduced as a new system for identifying drug targets based on microarray experiments  Given a user-selected disease and associated criteria as mentioned above the user can identify drug targets related to this disease starting with microarray experiments.  We have explained the architecture process and usage scenario of how to discover drug targets through the integration between text mining and association rule mining We have tested our system to cancer with eight different tissues breast kidney ovarian prostate pancreatic liver skin, and lung.  In our future work, we will develop a protein-protein interaction network describing the type of interaction we can extract from related publications in order to understand the type of interaction affecting diseases Finally regarding our microarray experiments\222 retrieval we intend to consider other microarray databases such as ArrayExpress and Stanford microarray database  7. Acknowledgments  This work is part of an R&D project conducted at IBM Center for Advanced Studies in Cairo  8. References  Fig ure 4: Part of Multi basket for Breast Cancer  1   Fig ure 5: Example of Generated Association Rules  Biological Process  protein amino acid ph osphorylation regulation of transcription, DNAdependent, estrogen receptor signaling pathway, cell-cell signaling, estrogen receptor signaling pathway, negative regulation of cell growth, cell cycle checkpoint intracellular signaling cascade Cellular Component N ucleus, integral to membrane chromatin remodeling complex mitochondrion intracellular membrane  http://www.ncbi.nlm.nih.gov/geo 


l Parkinson, H., Sarkans, U., Shojatalab, M A beygunawardena, N., Contrino, S., Coulson, R Farne, A., Garcia Lara, G., Holloway, E Kapushesky, M., Lilja, P., Mukherjee, G., Oezcimen A., Rayner, T., Rocca-Serra, P., Sharma, A Sansone, S., and A. Brazma, \223ArrayExpress- A public repository for Microarray gene expression data at the EBI,\224 Nucleic Acids Research, Vol. 33 Database issue D553\226D555, \(2005 11 Bryan, J, Pollard, KS, and van der Laan, MJ, \223Paired and unpaired comparison and clustering with gene expression data,\224 Statist Sinica 12: 87\226110, 2002 15 http://dip.doe-mbi.ucla.edu Zaki, M. J. \223Scalable algorithms for association m ining,\224 IEEE Transactions on Knowledge and Data Engineering, 12\(3\:372-390, May-June 2000 21                          1 9 Mack, R., et al., \223Text Analytics for Life Science using the Unstructured Information Management Architecture,\224 IBM System Journal, Vol.43, No. 3 pp.490-515, \(2004 5 Ferrucci, D. and Lally, A. \223Building an Example Application with the Unstructured Information Management Architecture,\224 IBM Systems Journal 43, No. 3, pp.455\226475, \(2004 6 Haas, L. M., Shwarz, P. M., Kodali, P., Koltar, E Rice, J. E., and Swope, W. C., \223Discovery Link: A System for Integrated Access to Life Sciences Data Sources,\224 IBM System Journal, Vol. 40, No. 2 2001 7 Barrett, T., Suzek, T., Troup, D., Wilhite, S., Ngau W., Ledoux, P., Rudnev, D., Lash, A., Fujibuchi, W and Ron Edgar, \223NCBI GEO: mining millions of expression profiles database and tools,\224 Nucleic Acids Research, Jan 1; 33 \(Database issue\D562D566, \(2005 8 http://www.ncbi.nlm.nih.gov/geo http://genome-www5.stanford.edu 12 Brazma, A., Hingamp, P., Quackenbush, J., Sherlock G., Spellman, P., Stoeckert, C., Aach, J., Ansorge W., Ball, C., Causton, H., Gaasterland, T., Glenisson P., Holstege, F., Kim I., Markowitz, V., Matese, J Parkinson, H., Robinson, A., Sarkans, U., SchulzeKremer, S., Stewart, J., Taylor, R., Vilo1, J., an Martin Vingron, \223Minimum information about a microarray experiment \(MIAME\oward standards for microarray data,\224 Nature Genetics, 365 - 371 2001 13 http://www.mged.org/Workgroups/MAGE/mage.htm Chen, G, Jaradat, SA, Banerjee, N, Tanaka, TS, Ko MSH, and Zhang, MQ, \223Evaluation and comparison of clustering algorithms in analyzing ES cell gene expression data,\224 Statist Sinica 12: 241\226262, 2002 16 Golub, TR, Slonim, DK, Tamayo, P, Huard, C Gaasenbeek, M, Mesirov, JP, Coller, H, Loh, ML Downing, JR, Caligiuri, MA, et al.: \223Molecular classification of cancer: class discovery and class prediction by gene expression monitoring,\224 Science 1999, 286:531-537 17 Cho, RJ, Campbell, MJ, Winzeler, EA, Steinmetz, L Conway, A, Wodicka, L, Wolfsberg, TG, Gabrielian AE, Landsman, D, Lockhart, DJ, et al.:, \223A genomewide transcriptional analysis of the mitotic cell cycle,\224 Mol Cell 1998, 2:65-73 18 http://www.rcsb.org/pdb Shenoy, P, et al., \223Turbo-charging vertical mining of large databases,\224 In Intl. Conf. Management of Data May 2000 22 Zaki, M. J. and Hsiao, C.-J. , \223CHARM: An efficient algorithm for closed itemset mining,\224 2nd SIAM Int'l Conf. on Data Mining, April 2002 23 El-Hajj, M., Z\344iane, O., \223Yet Another Frequent Itemset Mining Algorithm,\224 Journal of Digital Information Management,\224 Vol.3, Number 4 pp.243-248, December 2005  1 0  9  2   1 4 http://www.genome.ad.jp/kegg Uramoto, N., et al., \223A Text-mining System for K nowledge Discovery from Biomedical Documents,\224 IBM System Journal, Vol.43, No. 3, pp 516-533, \(2004 4 http://www.ebi.ac.uk  2 0  3  


 all longest intervals for each cycle and periodic time intervals Transition Z 1  represents the filtration process of t he time stamped database The process of data transformation is represented by transition Z 2  and the data s egmentation by transition Z 3 Transitions Z 4 and Z 5 show r espectively the Process Switching Mechanism \(PSM\ and Interval Validation Process IVP Transition Z 1  has the f ollowing form  Z 1 l 1 l 2 l 4 l 3 l 4 r 1    l 3  l 4  r 1  l 1  false True  l 2  false True  l 4  W 4_3  W 4_4  W 4_3 223The process of data filtration has finished\224 W 4_4 254 W 4_3  P lace l 1  corresponds to the entrance point for the s tamped database in the GN It is represented by one 002  token which enters the input place with initial characteristic 223a stamped database\224  In place l 2  enters 003  t oken with initial characteristic 223time period \(TP\\224   The 002  token simultaneously with the 003 1  token passes t hrough transition Z 1  and enter place l 4  This place c orresponds to the process of filtration The two tokens merge and form one new token with initial characteristic the result of the united characteristics of the tokens On each transition activation the new token passes through the transition and enters again place l 4  extending its c haracteristic with the current results of the filtration process. After the process finishes the final token moves to place l 3 obtaining as a characteristic 223 filtered database\224 Transition Z 2 has the following form  Z 2  l 3  l 5  l 7  l 6  l 7  r 2    l 6  l 7  r 2  l 3  f  alse True   l 5  f  alse  True  l 7  W 7_6  W 7_7  W 7_6 223The process of data transformation has finished\224 W 7_6 254 W 7_7   T he description of transition Z 2 functioning is similar to t he one of transition Z 1 As a joint place between transitions Z 1  and Z 2  place l 3  do not need any addition explanations T he tokens from place l 3  pass through the transition and e nter place l 7  which corresponds to the process of data t ransformation Place l 5  is an entrance point for the p arameter giving the sequential pattern SP which is represented by one 004  token 004  token enters the net with initial characteristic 223sequential pattern\224 The tokens from places l 3 and l 5 pass simultaneously to place l 7 merge and o btain join characteristic. This characteristic is extended on every transition activation by the current results of the transformation process. After the process finishes the final token moves to place l 6 obtaining as characteristic 223 sequence database\224 The process of data segmentation consists of three consecutive stages splitting into user defined cycles division into user interested granules and granulation according to user defined sequence duration parameter. The form of transition Z 3 is the following  Z 3 l 6 l 8 l 9 l 10 l 12 l 13 l 14 l 11 l 12 l 13 l 14 r 3    l 11  l 12  l 13  l 14   l 6  false true false false  l 8  false true false false r 3  l 9  false false true false  l 10  false false false true  l 12  false W 12_12  W 12_13  false  l 13  false false W 13_13  W 13_14   l 14  W 14_11  false false W 14_14  W12_12  223The process of database division into user defined cycles has not finished\224 W12_13  254 W12_12 W13_13  223The process of database granulation has not finished\224, W13_14 = \254 W13_13, W14_11 = \223The process of database segmentation has finished\224 W14_14  254 W14_11  In places l 8  l 9  and l 10  enter respectively one 005   006  and 007  token with initial characteristics 223cyclicity interval  \(CY\\224  223granularity interval GR\\224 and 223sequence duration SD\\224   At the first activation of the transition the two tokens from places l 6 and l 8 pass simultaneously through it, merge i nto one new token and enter place l 12 At the beginning the n ewly created token obtains composite characteristic It extends its characteristic on every pass through the transition and entering into place l 12  with the current state o f the process of database division into user defined cycles After this process finishes the resulting token simultaneously with the 006 token from place l 9 pass through t ransition Z 3 and enter into place l 13 In this process the two t okens merge into one token with initial characteristic the composition of the characteristics of the parent tokens. Like in the previous case the resulting token enters place l 13  e xtending its characteristic with the current state of the process until a granulated database is obtained. On the next step the obtained token enters place l 14 simultaneously with t he 007 token from place l 10 The two tokens unite each other m erging their characteristics The newly created token passes through transition Z 3 and enters place l 14 until a final g ranulated database is obtained On each entrance into place l 14 the token extends its characteristic with the current s tate of the process of database final granulation. At the end of this process the token moves to place l 11  with a c haracteristic 223segmented database\224 Transition Z 4 has the f ollowing form  Z 3 l 6 l 8 l 9 l 10 l 12 l 13 l 14 l 11 l 12 l 13 l 14 r 3    l 11  l 12  l 13  l 14   l 6  false true false false  l 8  false true false false r 3  l 9  false false true false  l 10  false false false true  l 12  false W 12_12  W 12_13  false  l 13  false false W 13_13  W 13_14   l 14  W 14_11  false false W 14_14  


 W 12_12   223The process of database division into user d efined cycles has not finished\224 W 12_13 254 W 12_12  W 13_13  223 The process of database granulation has not finished\224 W 13_14   254 W 13_13  W 14_11   223The process of database s egmentation has finished\224 W 14_14 254 W 14_11   I n places l 8  l 9  and l 10  enter respectively one 005   006  and 007  token with initial characteristics 223cyclicity interval  \(CY\\224  223granularity interval GR\\224 and 223sequence duration SD\\224   At the first activation of the transition the two tokens from places l 6 and l 8 pass simultaneously through it, merge i nto one new token and enter place l 12 At the beginning the n ewly created token obtains composite characteristic It extends its characteristic on every pass through the transition and entering into place l 12  with the current state o f the process of database division into user defined cycles After this process finishes the resulting token simultaneously with the 006 token from place l 9 pass through t ransition Z 3 and enter into place l 13 In this process the two t okens merge into one token with initial characteristic the composition of the characteristics of the parent tokens. Like in the previous case the resulting token enters place l 13  e xtending its characteristic with the current state of the process until a granulated database is obtained. On the next step the obtained token enters place l 14 simultaneously with t he 007 token from place l 10 The two tokens unite each other m erging their characteristics The newly created token passes through transition Z 3 and enters place l 14 until a final g ranulated database is obtained On each entrance into place l 14 the token extends its characteristic with the current s tate of the process of database final granulation. At the end of this process the token moves to place l 11  with a c haracteristic 223segmented database\224 Transition Z 4 has the f ollowing form  Z 4  l 11  l 17  l 18  l 19  l 15  l 16  l 17  l 18  r 4    l 15  l 16  l 17  l 18    l 11  t  rue false  False  false   r 4  l 17  f  alse  W 17_16  W 17_17  false    l 18  false false W 18_17  true   l 19  f  alse false  True  false   W 17_16   223The process of data segmentation has f inished\224 W 17_17   254 W 17_16  W 18_17   223The PSM has to j ump minimum length parameter steps\224  Initially in place l 18 enters one b  token with initial characteristic 223minimum interval length \(min_ilen\\224   The already segmented database from place l 11 pass i nterval ITVL by interval through transition Z 4  to place l 15 The token from place l 11 passes directly to place l 15 for v alidity check without obtaining any new characteristics The b token moves simultaneously with one of the tokens in place l 19  to place l 17  obtaining as characteristic t he current state of the process of the longest interval LI finding\224  At the end of this process the resulting tokens move to place l 16 and merge with the tokens from the same interval o btaining characteristic 223segments of the longest interval\224 The form of transition Z 5 is  Z 5  l 15  l 19  r 5    l 19   r 5  l 15  t  rue  VII  CONCLUSION  AND  FUTURE  WORK I n this paper the problem of finding the periodic time intervals for a given sequential pattern is addressed The approach presented in this paper divides the main problem into two sub-problems finding the longest intervals for each cycle and discovery of periodic time intervals by using the already discovered longest intervals of each cycle Almost all the data processing and complexity is covered during the process of finding the longest intervals Moreover the second problem is purely based on mining results from the first problem.  Therefore, in this paper we mainly focused on the problem to find all the longest intervals for each cycle To confront the above problem efficiently, we introduced two main search techniques IVP Interval Validation Process\ and PSM \(Process Switching Mechanism\.  In this paper we have presented an effective mining approach for finding all the periodic time intervals for a given sequential pattern. As a future work the process presented in this paper can be extended to find periodic time intervals for all the given sequential patterns in one database scan However to accomplish this task a more complex data-structure/memory management technique needs to be implemented REFERENCES 1  X  Chen and I. Petrounias, Mining Temporal Features in Association Rules, Proc. of PKDD\22299, Prague, Czech Republic, pp.295-300 2  Ozden B Ramaswamy S and Silberschatz A  Cyclic Association Rules in Proceedings of the 14 th   International C onference on Data Engineering 1998 Orlando Florida USA IEEE Computer Society 3  Han, J., Pei, J., and Yin, Y., Mining Segment-Wise Periodic Patterns in Time-Related Databases in Proceedings of the 4 th   International C onference on Knowledge Discovery and Data Mining, 1998: AAAI Press, Menlo Park 4  Yang J Wang W and Yu P.S Mining asynchronous periodic patterns in time series data in Proceedings of KDD 2000 p 275279 5  Huang K.-Y and Change C.-H Asynchronous periodic patterns mining in temporal databases in Proceedings of the International Conference on Databases and Applications DBA'04 2004 Innsbruck, Austria 6  Huang K.-Y and Change C.-H  Mining Periodic Patterns in Sequence Data In Proceedings of the 6th International Conference on Data Warehousing and Knowledge Discovery DaWaK 2004 Zaragoza, Spain: Springer 7  Yang J Wang W and Yu P.S Mining Surprising Periodic Patterns, in Data Mining and Knowledge Discovery, 2004 8 p. 253273 8  Lin W Orgun M.A and Williams G.J  An Overview of T emporal Data Mining in Proceedings of the 1st Australian Data Mining Workshop, 2002  9  C  hen X and Petrounias I  A Framework for Temporal Data Mining, in Proceedings of 9 th International Conference on Database a nd Expert Systems Applications DEXA'98 1998 Vienna Austria: Spring 


 Fig. 8. PSM \(Process Switching Mechanism Move to N ITVL Next Interval 1 2 3 4 5 Time Line Invalid ITVL   a\  SLSI discovery process \(forward mode continues  Jump to 8 th ITVL min_ilen=6 1 2 3 4 5 Time Line 6 7 8 b SLSI found \(process in jumping mode  M o v e  t o  N I T V L  t o  f i n d  n e x t S L S I  p ro c e s s  s w i t c h  t o f o r w a r d m o d e  1 2 3 4 5 T i m e  L i n e L a s t  S L S I f o u n d 6 7 8 9 I n v a l i d I T V L  c \  SLSI found but C ITVL is invalid   Backward Mode 1 2 3 4 5 Time Line LSI found 6 7 8 9 Valid ITVL d  LSI is found and process switches into backward mode   Backward Mode 1 2 3 4 5 Time Line LSI found 6 7 8 9   e\  LSI found- Seed Interval Discovery Process 1 2 3 4 5 Time Line 6 7 8 9 Invalid ITVL SLSI is assign new value of ITVL 7 and the p rocess of searching next LSI proceed again 12 11 10 13  f\  PSM encounters invalid ITVL during the search of SI  1 2 3 4 5 T ime L ine 6 7 8 9 SI Fo u n d D isco v ery p ro c ess o f L I p ro c eed s  g\  SI is found and PSM proceed for the discovery of LI   


