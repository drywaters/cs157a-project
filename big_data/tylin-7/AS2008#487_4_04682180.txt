An Efficient Frequent Patterns Mi ning Algorithm based on Apriori Algorithm and the FP-tree Structure   Bo Wu, Defu Zhang, Qihua Lan, Jiemin Zheng Department of Computer Science, Xi amen University, Xiamen 361005, China Longtop Group Post-doctoral Resear ch Center, Xiamen, 361005, China dfzhang@xmu.edu.cn   Abstract  Association rule mining is to find association relationships among large data sets. Mining frequent patterns is an important aspect in association rule mining. In this paper, an efficient algorithm named Apriori-Growth based on Apriori algorithm and the FP-tree structure is presented to mine frequent patterns. The advantage of the Apriori-Growth algorithm is that it doesnít need to generate conditional pattern bases and sub- conditional pattern tree recursively. Computational results show the Apriori-Growth algorithm performs faster than Apriori algorithm, and it is almost as fast as FP-Growth, but it needs smaller memory  1. Introduction  Data mining has recently attracted considerable attention from database practitioners and researchers because it has been applied to many fields such as market strategy, financial forecasts and decision support Many al g o ri t h ms h a v e been propos ed t o  obtain useful and invaluable information from huge databases n e of t h e m o st i m port a n t al g o ri t h m s i s  mining association rules, which was first introduced in    Association rule mining has many important applications in our life. An association rule is of the form X => Y. And each rule has two measurements support and confidence. The association rule mining problem is to find rules that satisfy user-specified minimum support and minimum confidence. It mainly includes two steps: first, find all frequent patterns second, generate association rules through frequent patterns Many algorithms for mining association rules from transactions database have been proposed [5, 6  since Apriori algorithm was first presented. However most algorithms were based on Apriori algorithm which generated and tested candidate itemsets iteratively. This may scan database many times, so the computational cost is high In order to overcome the disadvantages of Apriori algorithm and efficiently mine association rules without generating candidate itemsets, a frequentpattern-tree \(FP-Growth\tructure is proposed in  The FP-Growth was used to compress a database into a tree structure which shows a better performance than Apriori. However, FP-Growth consumes more memory and performs badly with long pattern data sets. In order to further improve FP-Growth algorithm, many authors developed some improved algorithms and obtained some promising results [10, 11, 12, 13  Due to Apriori algorithm and FP-Growth algorithm belong to batch mining. What is more, their minimum support is often predefined, it is very difficult to meet the applications of the real-world Recently, there are some growing interests in developing techniques for mining association patterns without a support constraint or with variable supports [14, 15  A ssociation rule mining among rare items is also discussed in o far, t h ere are very few pape rs  that discuss how to combine Apriori algorithm and FPGrowth to mine association rules In this paper, an efficient algorithm named Apriori-Growth based on Apriori algorithm and FP-Growth algorithm is proposed, this algorithm can efficiently combine the advantages of Apriori algorithm and FP-Growth algorithm. Computational results verify the good performance of the Apriori-Growth algorithm The organization of this paper is as follows. In Section 2, we will briefly review the Apriori method and FP-Growth method. Section 3 proposes an efficient Apriori-Growth algorithm that based on Apriori and the FP-tree structure. Experimental results will be presented in Section 4. Section 5 gives out the conclusions   
Third 2008 International Conference on Convergence and Hybrid Information Technology 978-0-7695-3407-7/08 $25.00 © 2008 IEEE DOI 10.1109/ICCIT.2008.109 1091 
Third 2008 International Conference on Convergence and Hybrid Information Technology 978-0-7695-3407-7/08 $25.00 © 2008 IEEE DOI 10.1109/ICCIT.2008.109 1099 


2. Two Classcical Mining Algorithms  2.1 Apriori Algorithm  In g ra w a l propos ed an al g o ri t h m cal l e d  Apriori to the problem of mining association rules first Apriori algorithm is a bottm-up, breadth-first approach The frequent itemsets are extended one item at a time Its main idea is to generate k th candidate itemsets from the k 1\-th frequent itemsets and to find the k th frequent itemsets from the k th candidate itemsets The algorithm terminates when frequent itemsets can not be extended any more. But it has to generate a large amount of candidate itemsets and scans the data set as many times as the length of the longest frequent itemsets. Apriori algorithm can be written by pseudocode as follows Procedure Apriori Input: data set D, minimum support minsup Output: frequent itemsets L  1 1 L find_frequent_1_itemsets\(D 2\or k 2 1 k L     k  3 4 k C Apriori_gen 1 k L  minsup 5 for each transactions t D 6 7  t C subset k C  t  8  for each candidate c  t C  9  c count 10 11 k L  c  k C  c count > minsup 12 13\return L  1 L  2 L  n L  In the above pseudocode k C means k th candidate itemsets and k L means k th frequent itemsets  2.2 FP-Growth Algorithm  In a n  Pei et a l propr os ed a dat a s t r u ct u r e  called FP-tree \(frequent pattern tree\-tree is a highly compact representation of all relevant frequency information in the data set. Every path of FP-tree represents a frequent itemset and the nodes in the path are stored in decreasing order of the frequency of the corresponding items. A great advantage of FP-tree is that overlapping itemsets share the same prefix path So the information of the data set is greatly compressed. It only needs to scan the data set twice and no candidate itemsets are required An FP-tree has a header table. The nodes in the header table link to the same nodes in its FP-tree Single items and their counts are stored in the header table by decreasing order of their counts.  Fig.1a shows an example of a data set while Fig.1b shows the FPtree constructed by that data set with minsup = 30  Fig.1a. A data set   Fig.1b. FP-tree Constructed by the above data set The disadvantage of FP-Growth is that it needs to work out conditional pattern bases and build conditional FP-tree recursively. It performs badly in data sets of long patterns  3. Apriori-Growth Algorithm  In this Section a new algorithm based on Apriori and the FP-tree structure is presented, which is called Apriori-Growth 
1092 
1100 


Fig.2a shows the data structure of the node of header table. Its tablelink points to the first node in FPtree which has the same name with it. And Fig.2b shows the data structure of the node of FP-tree. Its tablelink points to the next node in FP-tree which has the same name with it Name tablelink Count Fig.2a the data structure of the node of header table  Name tablelink parent child Count Fig.2b the data structure of the node of FP-tree  The Apriori-Growth mainly includes two steps First, the data set is scanned one time to find out the frequent 1 itemsets, and then the data set is scanned again to build an FP-tree as [9  At last, the built FP-tree is mined by AprioriGrowth instead of FP-Growth. The detailed AprioriGrowth algorithm is as follows Procedure Apriori-Growth Input: data set D, minimum support minsup Output: frequent itemsets L  1 1 L frequent 1 itemsets 2\or k 2 1 k L    k  3 4 k C Apriori_gen 1 k L  minsup 5 for each candidate c  t C  6 7  sup = FP-treeCalculate c  8  if \(sup > minsup 9  k L  k L   c  10 11 12\rn L  1 L  2 L  n L    Procedure FP-treeCalculate Input: candidate itemset c  Output: the support of candidate itemset c  1  sort the items of c by the decreasing order of header table 2  find the node p in the header table which has the same name with the first item of c  3  q  p tablelink 4  count = 0 5  while q is not null 6   7   if the items of the itemset c except first item all appear in the prefix path of q  8   count q count 9   q  q tablelink 10   11  return count / totalrecord  As we know, the item of bigger count must be the ancestor of the smaller one if several items appear together in a path. So the count of the path is equal to the count of the node which is closest to leaf. So to find the count of candidate itemsets is to find the sum of those nodes In this case, we can work out the support of candidate itemsets by traversing related nodes in FPtree and their prefix paths  4. Experimental Results  The content of our test data set are bank card transactions seized from bank of China. There are 27 different items and 50905 records in that data set In order to verify the performance of the AprioriGrowth algorithm, we compare Apriori-Growth with Apriori and FP-Growth. Three algorithms are performed on a computer with a 1.41GHz processor and 512MB memory. The program is developed by Visual C++ 6.0. The computational results of three algorithms are reported in Table 1 The clearer comparison of three algorithms is given in Fig.3  Table 1. The running time of three algorithms  Min_sup Apriori AprioriGrowth FP-Growth  10% 766ms 391ms 407ms 5% 1515ms 406ms 437ms 1% 6953ms 438ms 561ms 0.5% 13125ms 484ms 688ms 0.2% 31672ms 687ms 874ms 0.1% 60547ms 1093ms 1078ms  From Fig.3, we can make the following two statements. First, Apriori-Growth works much faster than Apriori. It uses a different method FPtreeCalcualte to calculate the support of candidate itemsets. Second, Apriori-Growth works almost as fast 
1093 
1101 


as FP-Growth. But it consumes less memory than FPGrowth because it doesnít need to generate conditional pattern bases and build sub-conditional pattern tree recursively   Fig.3  5. Conclusions and future work  In this paper, we have proposed the AprioriGrowth algorithm. This method only scans the data set twice and builds FP-tree once while it still needs to generate candidate itemsets The future work is to further improve the AprioriGrowth and test more and larger datasets  Acknowledgments  This work was supported by the National Nature Science Foundation of China \(Grant no. 60773126 and the Province Nature Science Foundation of Fujian Grant no. A0710023\ and academician start-up fund Grant No. X01109\d 985 information technology fund \(Grant No. 0000-X07204\ in Xiamen University  Reference   M  S  C h en  J Han   P  S  Yu   Dat a m i n i n g  an o v er vi e w  from a database perspective IEEE Transactions on Knowledge and Data Engineering 1996, 8, pp. 866-883  J Han M  Kam b er  Data Mining: Concepts and Techniques Morgan Kaufmann Publisher, San Francisco CA, USA, 2001 3 R A g r a w a l T  Im i e linsk i a nd A  Sw a m i M ining  association rules between sets of items in large databases in Proceedings of the Association for Computing Machinery, ACM-SIGMOD 1993, 5, pp.207-216 4 R. A g ra wa l, R. Srik a n t F a s t a l g o rithm s f o r m i ning association rules Proceedings of the 20th Very Large DataBases Conference \(VLDBí94 Santiago de Chile, Chile 1994, pp. 487-499 5 A g r a w a l R., Srik a n t, R V u Q   M ini n g a s s o c i a tion rules with item constraints In The third international conference on knowledge discovery in databases and data mining Newport Beach, California, 1997, pp. 67-73 6 J Ha n Y. F u  D isc o v e ry o f m u ltiple le v e l a ssoc i a tion rules from large databaseî, In The twenty-first international conference on very large data bases Zurich, Switzerland 1995, pp. 420-431 7 F uk u d a  T Morim o to, Y M o ris h ita S T o k u y a m a  T    Mining optimized association rules for numeric attributes In The ACM SIGACT-SIGMOD-SIGART symposium on principles of database systems 1996, pp. 182-191 8 a r k  J  S., Che n M. S  Y u P. S U s i ng a ha s h ba s e d  method with transaction trimming for mining association rules IEEE Transactions on Knowledge and Data Engineering 1997, 9\(5\, pp. 812-825 9 J H a n J  P e i a n d Y  Y i n   M ining f r e que nt pa tte rns  without candidate Generationî, in Proceeding of ACM SIGMOD International Conference Management of Data  2000, pp. 1-12 10 J  H a n, J  W a ng Y  L u a nd P.T z v e t k ov  M ining topk  frequent closed patterns without minimum supportî, in Preceeding of  International Conference  Data Mining  2002,12, pp. 211-218 11 G  L i u H  L u  J  X Y u  W  W e i a nd X Xi a o   A F O P T  A n  efficient implementation of pattern growth approachî, in IEEE ICDM Workshop Frequent Itemset Mining Implementations CEUR Workshop Proc., 2003, 80 1 J W a n g  J Han  and J P e i   C L O S ET+: search in g f o r th e  best strategies for mining frequent closed Itemsetsî, in Preceeding of International Conference, Knowledge Discovery and Data Mining 2003, 8, pp. 236-245 13 T z ung P e i H o ng C h unW e i L i n, Y u L ung W u   Incrementally fast updated frequent pattern trees Expert Systems with Applications 2008, 34, pp. 2424-2435 1 W a n g  Y He D C h eu n g  Y C h in M i n i n g confident rules without support requirementî, in Proceedings of ACM International Conference on Information and Knowledge Management CIKM, 2001, pp 89-96 15 H   Xi ong   P  T a n V K u m a r   M ini n g s t r ong a f f i nit y  association patterns in data sets with skewed support distributionî, in Proceedings of the Third IEEE International Conference on Data Mining ICDM, 2003, pp 387-394 1 Ya-Han H u  Yen L i an g Ch e n  M i n i n g asso ci at i o n r u l e s with multiple minimum supports: a new mining algorithm and a support tuning mechanism Decision Support Systems  2006, 42, pp. 1-24 17 J  D i ng  E f f ic ie nt a s s o c i a tion r u le m i ning am ong  infrequent items Ph.D. Thesis University of Illinois at Chicago, 2005 18 L i ng Zh ou Ste p he n Y a u  E f f ic ie nt a s s o c i a tion r u le  mining among both frequent and infrequent items Computers and Mathematics with Applications 2007, 54, pp 737-749 
1094 
1102 


process is repeated until a terminating condition is met The algorithm terminates when 1 the maximum number of epoch is reached or 2 the total change in error falls below a low threshold i.e  Q q 1  P q p 1  E C q,p  0  TABLE I L IST OF S YMBOLS U SED IN S UPERVISED E RROR R EDUCTION P HASE  Symbol Description m k  q,p  Modulatory weight between ICM k and C q,p u  n  k  q,p  Temporary value of m k  q,p for the n th training input-output pair U mod  Factor that updates temporary modulatory weights u  n  k  q,p N  No of training input-output pairs Z L3  Z 1 L3 Z  N  L3   Training input vector   Z L4    Z 1 L4    Z  N  L4   Training output vector Z  n  L3  Z  n  ICM 1 Z  n  ICM K   n th training input vector   Z  n  L4    Z  n  OCM 1    Z  n  OCM Q   n th training output vector  L4   OCM 1  OCM Q   Propagated output of layer 4  OCM q   C q 1  C q,P q   Propagated output of OCM q  C q,p  Activation level of C q,p E init C q,p  Initial squared-error for C q,p E new C q,p  New squared-error for C q,p  E C q,p  Squared-error change for C q,p begin S UPERVISED E RROR R EDUCTION A LGORITHM 1 initialize m k  q,p 1  k  K  q  Q  p  P q 2 while not stable do 3 for all training vectors n  1 N  do 4 u  n  k  q,p  m k  q,p  k  K  q  Q  p  P q 5 with all ICM  compute  6 E init C q,p   C q,p    Z  n  C q,p  2  q  Q  p  P q 7 for all ICM k  k  K do 8 with only ICM k  compute  9 E new C q,p   C q,p    Z  n  C q,p  2  q  Q  p  P q 10  E C q,p  E new C q,p E init C q,p  q  Q  p  P q 11 u  n  k  q,p  U mod  u  n  k  q,p  q  Q  p  P q 12 end for k  1 K  13 end for n  1 N  14 m k  q,p  1 N N  n 1  u  n  k  q,p   k  K  q  Q  p  P q 15 end while end S UPERVISED E RROR R EDUCTION A LGORITHM Algorithm 1 Supervised error reduction algorithm U mod as deìned in 12 determines the magnitude of change in u  n  k  q,p  If the error decreases when using only ICM k  it means that ICM k is important for output prediction and u  n  k  q,p is strengthened When the opposite occurs ICM k is insigniìcant and u  n  k  q,p should be reduced The magnitude of change in u  n  k  q,p also depends on the number of IM preceding ICM k  and the current epoch U mod  2 1+exp  d  ep   E C q,p  12 where    1 is a parameter for the initial learning rate d   1  if   0  1 1    otherwise    I 1 2  I  i 1   IM i  ICM k  and ep 1    max  2  where  and  max are current and maximum epochs IV R ECALLING P ROCESS In the recalling process Fig 4 based on the input presented to layer 1 of the encoded network input functions of layers 1 to 3 are performed Sections II-A II-B and IIC The memory recall process then occurs in the hybrid associative memory Finally the output functions of layers 4 to 6 are performed Sections II-D II-E and II-F and the estimated output is recalled Here we explain the signaling processes of neurons within the hybrid associative memory   Fig 4 The recalling process highlighted in gray In the hybrid associative memory i is a signal transmitting A k,l node from layer 3 and j is a C q,p node from layer 4 that receives inputs from N neurons in layer 3 Based on ring strength Z i  i transmits a signal deìned by a i   0 for d  min  Z i  d  min  k prop  Z i otherwise  13 where a i is the output of i  k prop  0  1 is a propagation factor to improve network stability and d  min  0  1 and d  min    1  0 are predeìned positive and negative thresholds Nodes j in layer 4 then receive these output signals and update their activation levels  j asin  j  N  i 1 v i  j  a i 14 where v i  j is the resultant weight between i and j  V D ATA P REDICTION E XPERIMENTS Four data prediction tasks were used to benchmark our architecture against other existing architectures They include three separate sets of data from Nakanishiês nonlinear estimation tasks and real-w orld dataset for predicting the density of highway trafìc 2008 International Joint Conference on Neural Networks IJCNN 2008 1519 


TABLE II B ENCHMARKING R ESULTS ON N ONLINEAR E STIMATION FOR THE N AKANISHI E STIMATION T ASKS  Architecture Example A Example B Example C MSE R MSE R MSE R FASCOM 0  181 0  929 8  170  10 3 0  999 13  930 0  953 Hebb-RR 0  185 0  911 2  423  10 4 0  998 15  138 0  947 SVM 0  258 0  876 2  423  10 5 0  993 29  510 0  925 RSPOP 0  383 0  856 2  124  10 5 0  983 24  859 0  922 DENFIS 0  411 0  805 5  240  10 4 0  995 69  824 0  810 POP-CRI 0  270 0  877 5  630  10 5 0  946 76  221 0  733 ANFIS 0  286 0  853 2  969  10 6 0  780 38  062 0  875 EFuNN 0  566 0  720 7  247  10 5 0  946 72  541 0  756 A Nakanishiês Nonlinear Estimation Tasks The Nakanishiês dataset consists of three e xamples of real-world nonlinear estimation tasks The tasks for data prediction are namely 1 a nonlinear system 2 the human operation of a chemical plant and 3 the daily stock price of a stock in a stock market Based on these three experiments FASCOMês performance based on data prediction accuracy was benchmarked against Hebb-RR SVM 26 RSPOP 8 DENFIS 28 POP-CRI ANFIS 30 31 and EFuNN 32 T w o performance measures are used for this evaluation namely the mean squared error MSE and the Pearson productmoment correlation coefìcient R 1 Example A Nonlinear System FASCOM was used to model a nonlinear system given by y 1 x  2 1  x 1  5 2  2 1  x 1 x 2  5 15 The dataset consists of four input variables  x 1  x 2  x 3  x 4  and one output variable  y  whereby only x 1  x 2 are useful and x 3  x 4 are irrelevant The mean of modulatory weights discovered was computed across the output feature space It was found that the input conjuncted maps ICM s representing x 3  x 4 and x 3 012 x 4 exerted no inîuence on the outcome of y and could be discarded This is similar to the results obtained by 24  F o r this e xample task F A SCOM w a s most accurate when compared to other benchmarked architectures 2 Example B Chemical Plant Operation This example involves the human operation of a chemical plant whereby ve inputs representing monomer concentration  x 1  charge of monomer concentration  x 2  monomer ow rate  x 3  and local temperatures inside the plant  x 4  x 5  are used to estimate the set point for monomer ow rate  y  Similar to F ASCOM discarded x 2  x 4 and x 5 As a comparison discarded x 1  x 2 and x 5  while discarded all but x 3  Also we deduce that the set point for monomer ow rate  y  depends mainly on the combination of monomer ow rate  x 3  and monomer concentration  x 1  i.e x 1 012 x 3  For this example with a MSE of 8  170  10 3 and near perfect R value FASCOMês performance was signiìcantly better than the state-of-the-art 3 Example C Stock Price Forecasting The prediction of the price of a stock y for this example is performed with the ten inputs which are the past and present moving averages over a middle period  x 1  x 2  past and present separation ratios with respect to moving average over both short and middle periods  x 3  x 4  x 8  x 10  present change of moving average over both short and long periods  x 5  x 9  and past and present price changes  x 6  x 7  The supervised error reduction algorithm signiìcantly reduced inputs corresponding to x 1  x 2  x 3  x 5  x 6  x 8 and x 9  discarded inputs x 1  x 2  x 3  x 6  x 7  x 9 and x 10  discarded x 1  x 2  x 3  x 6 and x 10  and discarded only x 2 and x 5  Interestingly all methods did not discard x 4  which means the present separation ratio with respect to moving average over a middle period  x 4  is a critical component to predict stock prices Similar to the previous two examples FASCOM outperformed all other benchmarked architectures 4 Discussion The three tasks were also used to analyze the effects of three different experimental initializations of the learning process 1 using all three phases in the learning process 2 omitting membership function initialization phase 1 3 omitting error reduction phase 3 For each task each initializationês MSE was computed and normalized with respect to the highest MSE amongst the three initializations For all three tasks the comparison between the three initializations indicates that the inclusion of both phases 1 and 3 produced a MSE that is signiìcantly lower than when either one was omitted Fig 5 This shows that both phases 1 and 3 are crucial in improving the accuracy of data prediction and their existence within the learning process is justiìed Task A Task B Task C 0 0.2 0.4 0.6 0.8 1 Normalized M S E With phase 1, 2 & 3 Omit phase 1 Omit phase 3 Fig 5 Comparison between the three different experimental initializations 1520 2008 International Joint Conference on Neural Networks IJCNN 2008 


B Highway Trafìc Density Prediction The raw trafìc data 33 w a s collected for three straight lanes and two exit lanes at site 29 located at exit 5 along the east bound direction of the Pan Island Expressway PIE in Singapore using loop detectors embedded beneath the road surface Fig 6 Data spanning a period of six days from September 5 to 10 1996 for the three straight lanes i.e lanes 1 to 3 were considered for this experiment The dataset has four input attributes representing the normalized time and the trafìc density of each of the three lanes           Fig 6 Photograph of site 29 where the trafìc data was collected The purpose of this experiment is to model the trafìc ow trend and subsequently produce predictions of the trafìc density of each lane at time t    where  5  15  30  45  60 min is the prediction time interval Three cross-validation groups CV1 CV2 and CV3 of training and test sets were used for evaluation purposes The mean squared error MSE and the Pearson product-moment correlation coefìcient R were computed for each predictions run From the example in Fig 7 we observe that the prediction accuracy decreases as the time interval  increases To evaluate the accuracy of prediction the Avg MSE indicator was computed by taking the average MSE across all 45 prediction runs 3 lanes 5 time intervals and 3 crossvalidation groups Also the Var indicator reîecting the consistency of predictions over different time intervals across the three lanes was computed by taking the change in the mean of R from  5 min to  60 min expressed as a percentage of the former This was then averaged across all three lanes to produce Avg Var The results of trafìc density prediction was compared to Hebb-RR SVM 26 RSPOP 8 POP-CRI 29 DENFIS GeSoFNN 34 and EFuNN 32 F ASCOM signiìcantly outperformed all other architectures based on the results as shown in Fig 8 with the best combination of Avg MSE  0  098  and Avg Var  19  0  as compared to other architectures The results indicates that the output prediction by FASCOM are both highly accurate and consistent over different time intervals This is desirable 0 100 200 300 400 500 600 700 800 0 1 2 3 4 5  Time instance Normalized density Actual  Predicted  R=0.904   MSE=0.082 a Prediction at  5 min 0 100 200 300 400 500 600 700 800 0 1 2 3 4 5  Time instance Normalized density Actual  Predicted  R=0.886   MSE=0.099 b Prediction at  15 min 0 100 200 300 400 500 600 700 800 0 1 2 3 4 5  Time instance Normalized density Actual  Predicted  R=0.873   MSE=0.108 c Prediction at  30 min 0 100 200 300 400 500 600 700 800 0 1 2 3 4 5  Time instance Normalized density Actual  Predicted  R=0.839   MSE=0.135 d Prediction at  45 min 0 100 200 300 400 500 600 700 800 0 1 2 3 4 5  Time instance Normalized density Actual  Predicted  R=0.816   MSE=0.152 e Prediction at  60 min Fig 7 Trafìc density prediction of lane 1 using CV1 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0.22 15 20 25 30 35 40 Avg MSE Avg Var  0  143  19  1 RSPOP POP CRI 0  189  33  2 EFuNN FA SCO M 0  098  19  0 Hebb RR 0  114  29  6 0  150  33  2 DENFIS SVM 0  120  39  6 GenSoFNN 0  164  17  4 0  167  21  9 Fig 8 FASCOM outperforms other benchmarked architectures for highway trafìc density prediction 2008 International Joint Conference on Neural Networks IJCNN 2008 1521 


VI C ONCLUSIONS This paper proposed the Fuzzy Associative Conjuncted Maps FASCOM fuzzy neural network which represents information using conjuncted fuzzy sets and learns associations between them through three consecutive unsupervised and supervised phases in the learning process During the rst phase of unsupervised membership function initialization neurons are allocated to optimize information density For the supervised learning phase the single pass Hebbian learning performed is based on the memory mechanisms of synaptic plasticity forgetting and long-term facilitation Finally the nal supervised phase involves ne-tuning the network using the supervised error reduction algorithm which uses an error comparison to determine the inîuence of an input dimension across output space In the series of experiments performed we showed that each of the phases contributed to the overall performance of the architecture We were also able to demonstrate FASCOMês effectiveness in performing nonlinear data prediction on a variety of real-world problems of different natures and consisting of noisy data such as nonlinear system modeling chemical plant operation stock price forecasting and trafìc density prediction For every experiment FASCOM produced the best data prediction accuracy when benchmarked against existing architectures R EFERENCES  L A Zadeh Fuzzy logic neural netw orks and soft computing  Communications of ACM  vol 37 no 3 pp 77-84 1994  E H Mamdani and S Assilian  A n e xperiment in linguistic synthesis with a fuzzy logic controller Intl Jnl of Man-Machine Studies  vol 7 no 1 pp 1-13 1975  T  T akagi and M Sugeno Deri v ation of fuzzy control rules from human operatorês control actions Proc IFAC Symp Fuzzy Information Knowledge Representation and Decision Analysis  pp 55-60 1983  T  T akagi and M Sugeno Fuzzy identiìcation of systems and its applications to modelling and control IEEE Trans Syst Man Cybern  vol 15 no 1 pp 116-132 1985  M Sugeno and G T  Kang Structure identiìcation of fuzzy model  Fuzzy Sets and Systems  vol 28 pp 15-33 1988  S Guillaume Designing fuzzy inference systems from data An interpretability-oriented review IEEE Trans Fuzzy Syst  vol 9 no 3 pp 426-443 2001  J Casillas O Cordn F  Herrera and L Magdalena Interpretability issues in fuzzy modeling  Berlin Springer-Verlag 2003  K K Ang and C Quek RSPOP Rough set-based pseudo outer product fuzzy rule identiìcation algorithm Neural Computation  vol 17 pp 205-243 2005  C T  Lin and C S Lee Neural-netw ork-based fuzzy logic control and decision system IEEE Trans on Computers  vol 40 no 12 pp 13201336 1991  I Hayashi H Nomura H Y amasaki and N W akami Construction of fuzzy inference rules by NDF and NDFL Intl Jnl of Approximate Reasoning  vol 6 no 2 pp 241-266 1992  R R Y a ger  Modeling and formulating fuzzy kno wledge bases using neural networks Neural Networks  vol 7 no 8 pp 1273-1283 1994  C Quek and W  L T ung  A no v e l approach to the deri v ation of fuzzy membership functions using the Falcon-MART architecture Pattern Recognition Letters  vol 22 no 9 pp 941-958 2001  M Lee S Y  Lee and C H P ark  A ne w neuro-fuzzy identiìcation model of nonlinear dynamic systems Intl Jnl of Approximate Reasoning  vol 10 pp 30-44 1994  H Ishib uchi H T anaka and H Okada Interpolation of fuzzy if-then rules by neural networks Intl Jnl of Approximate Reasoning  vol 10 pp 3-27 1994  E R Kandel J H Schw artz and T  M Jessell Principles of neural science  4th ed New York McGraw-Hill Health Professions Division 2000  D O Hebb The organization of behavior a neuropsychological theory  New York Wiley 1949  N V  Swindale Ho w dif ferent feature spaces may be represented in cortical maps Network Computation in Neural Systems  vol 15 pp 217-242 2004  J Ro v amo V  V irsu and R Nsnen Cortical magniìcation f actors predicts the photopic contrast sensitivity of peripheral vision Nature  vol 271 pp 54-6 1978  S W  K u f er  J  G  Nicholls and A R Martin From neuron to brain a cellular approach to the function of the nervous system  2nd ed Sunderland Mass Sinauer Associates 1984  N Sug a Cortical computation maps for auditory imaging  Neural Networks  vol 3 pp 3-21 1990  P  Azzopardi and A Co we y  The o v errepresentation of the fo v e a and adjacent retina in the striate cortex and dorsal lateral geniculate nucleus of the macaque monkey Neuroscience  vol 72 pp 627-639 1996  A Das Plasticity in adult sensory corte x a r e vie w   Network Computation in Neural Systems  pp R33-R76 1997  M D Plumble y  Do cortical maps adapt to optimize information density Network Computation in Neural Systems  vol 10 pp 4158 1999  H Nakanishi I B T u rksen and M Sugeno  A re vie w and comparison of six reasoning methods Fuzzy Sets and Systems  vol 57 no 3 pp 257-294 1993  G K T an Feasibility of predicting congestion states with neural networks Technical Report School of Civil and Environmental Engineering Nanyang Technological University Singapore 1997  V  V apnik The Nature of Statistical Learning Theory  Springer-Verlag 1995  F  Liu C Quek and G S Ng  A no v e l generic Hebbian orderingbased fuzzy rule base reduction approach to mamdani neuro-fuzzy system Neural Computation  vol 19 pp 1656-1680 2007  N Kasabo v and Q Song DENFIS Dynamic e v olving neural-fuzzy inference system and its application for time-series prediction IEEE Trans Fuzzy Syst  vol 10 no 2 pp 144-154 2002  K K Ang C Quek and M P asquier  POPFNN-CRI\(S Pseudo outer product-based fuzzy neural network using the compositional rule of inference and singleton fuzziìer IEEE Trans Syst Man Cybern B Cybern  vol 33 no 6 pp 838-849 2003  J S Jang  ANFIS Adapti v e-netw ork-based fuzzy inference system  IEEE Trans Syst Man and Cybern  vol 23 no 3 pp 665-685 1993  S L Chiu Fuzzy model identiìcation based on cluster estimation  Journal of Intelligent and Fuzzy Systems  vol 2 no 3 pp 267-278 1994  N Kasabo v  Ev olving fuzzy neural netw orks for supervised  unsupervised online IEEE Trans Syst Man and Cybern B Cybern  vol 31 no 6 pp 902-918 2001  C Quek M P asquier  and B Lim POP-TRAFFIC A N o v el Fuzzy Neural Approach to Road Trafìc Analysis and Prediction IEEE Trans Intell Transp Syst  vol 7 no 2 pp 133-146 2006  W  L T ung and C Quek GenSoFNN a generic self-or ganizing fuzzy neural network IEEE Trans Neural Netw  vol 13 no 5 pp 10751086 2002 1522 2008 International Joint Conference on Neural Networks IJCNN 2008 


If in Fig. 4 there is at least one T, then DCAR6 holds DCAR1 through DCAR6 forms a complement lattice shown in Fig. 5 In Fig. 5, the lower rule implies the upper rule That is, if DCARj is reachable from DCARi via an ascending path, and DCARi holds, then DCARj holds Because DCAR1 through DCAR6 satisfies Fig 5, their algorithms can be merged into one algorithm called connective determination algorithm, shown in Fig. 6 Suppose cf 1 80%, cf 2 75%. In Fig. 4, for the column of C1, there are M*cf 1 5*80%=4 elements whose values are T \(namely, S1, S2, S3, S5 Therefore, DCAR2: course\(Cno 004 1  student\(Sno 003 1  study\(Sno, Cno\olds. From Fig. 5, we know that DCAR3 and DCAR6 also hold. In Fig. 4, there are at least N*cf 2 4*75%=3 columns which have value T \(namely, in the column of C1 there is S1, in the column of C2 there is S1, in the column of C3 there is S2, in the column of C4 there is S5 therefore DCAR5: course\(Cno 003 1  student\(Sno 004 1  study\(Sno, Cno  VI. CONCLUDING REMARKS 1\ Double-connective association rule mining is different from single-connective association rule mining. The former mines the association among the primary keys of the two entity tables and the primary key of the binary relationship table. The latter mines the association between frequent item sets 2\. 4 is different from data cubes in data warehouses. The elements in Fig. 4 are T or F. The elements in the data cubes are data 3\The differences between double-connective association rule and database query are that, first, the query information in databases are predeterminate while the information to be mined by double-connective association rule is not predeterminate, it is implied. Secondly, database query needs to write SQL statements, while double-connective association rule mining is automatic. Thirdly, the information obtained by database query is quantitative, while the information obtained by double-connective association rule mining is qualitative such as ìfor manyî, ìthere are some  REFERENCES 1 Ji a w ei H a n   M i ch eli n e K a m b er   D a t a  M i n i n g C onc ep t s  a nd Techniques, Higher Education Press, Beijing, 2001, Morgan Kaufmann Publishers, 2000 2 A  G  Ha m i lt on  L o gi c for M a th em a t i c ia ns R evi s ed E d i t i o n   Cambridge University Press, 1988, Tsinghua University Press Beijing, 2003 3 X unw e i Z h o u   Br ie f I ntr o du c t io n  to  Mu t u al l y I nve r s is tic Logicî, 1999 European Summer Meeting of the Association for Symbolic Logic, Utrecht, The Netherlands, August 1-6 1999 4 u n w ei Zh ou F i r s t leve l exp l i c i t m u lt ip le i ndu ct i v e compositionî, 2005 Spring Meeting of the Association for Symbolic Logic, The Westin St. Francis Hotel, San Francisco CA. USA, March 25-26, 2005 5 A b rah a m S i lb ers c ha t z  Hen r y  F  Kort h  S S u da rs ha n Dat a b a s e  System Concepts \(Fourth Edition\, Higher Education Press Beijing, 2002, McGraw-Hill Companies, 2002  
279 
279 


support pruning gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis Biosystems vol 85 computationally challenging As training set sizes increase it is likely that these difficulties will also increase VI RELATED WORK While operating on a microarray dataset current CAR 1 2 3 4 and other pattern/rule 20 21 mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets Hence they are generally quite computationally expensive for datasets containing many training samples or genes as the case may be BSTC is explicitly related to CAR-based classifiers but requires no expensive CAR mining BSTC is also related to decision tree-based classifiers such as random forest 19 and C4.5 family 9 methods It is possible to represent any consistent set of boolean association rules as a decision tree and vice versa However it is generally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data 1 2 6 7 11 BSTC is explicitly related to and motivated by CAR-based methods To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here Perhaps the work closest to utilizing 100 BARs is the TOPRULES 22 miner TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci Hence TOP-RULES discovers all 100 confident CARs in a dataset However the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER 23 and so generally isn't polynomial time Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules 24 25 Once obtained fuzzy rules can be used for classification in a manner analogous to CARs However the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM 25 VII CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with preclassification CAR mining see Tables IV and VI we developed a novel method which considers a larger subset of CAR-related boolean association rules BARs These rules can be compactly captured in a Boolean Structure Table BST which can then be used to produce a BST classifier called BSTC Comparison to the current leading CAR classifier RCBT on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining see Section VB Hence BSTC extends generalized CAR based methods to larger datasets then previously practical Furthermore unlike other association rule-based classifiers BSTC easily generalizes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse then CAR-based methods after all exponential time CAR mining is completed O SlS CGl versus O Si CGi As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists ACKNOWLEDGM[ENTS We thank Anthony K.H Tung and Xin Xu for sending us their discretized microarray data files and Top-k/RCBT executables This research was supported in part by NSF grant DMS-0510203 NIH grant I-U54-DA021519-OlAf and by the Michigan Technology Tri-Corridor grant GR687 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies REFERENCES 1 G Cong K L Tan A K H Tung and X Xu Mining top-k covering rule Mining SDM 2002 5 R Agrawal T Imielinski and A Swami Mining associations between sets of items Y Ma Integrating classification and association rule mining KDD 1998 11 T McIntosh and S Chawla On discovery of maximal confident rules without pp 43-52 1999 24 S Vinterbo E Kim and L Ohno-Machado Small fuzzy and interpretable pp 165-176 2006 1071 pp 207-216 1993 6 G Dong pp 273-297 t995 9 pp 5-32 2001 20 W Li J R Quinlan Bagging boosting and c4.5 AAAI vol 1 V Vapnik Support-vector networks the best strategies for mining frequent closed itemsets KDD 2003 4 M Zaki and C Hsiao Charm L Wong Identifying good diagnostic genes or gene expression data SIGMOD 2005 2 G Cong A K H Tung X Xu F Pan and J Yang Farmer Finding interesting rule gene expression data by using the gene expression based classifiers BioiiiJcrmatics vol 21 l and Inrelligent Systenis IFIS 1993 16 Available at http://sdmc.i2r.a-star.edu.sg/rp 17 The dprep package http:/cran r-project org/doclpackages dprep pdfI 18 C Chang and C Lin Libsvm a library for support vector machines 2007 Online Available www.csie.ntu.edu.tw cjlin/papers/libsvm.pdf 19 L Breiimnan Random forests Maclh Learn vol 45 no 1 M Chen and H L Huang Interpretable X Zhang 7 J Li and pp 725-734 2002 8 C Cortes and Mac hine Learming vol 20 no 3 in microarray data SIGKDD Worikshop on Dtra Mining in Bioinfrrnatics BIOKDD 2005 12 R Agrawal and R Srikant Fast algorithms for mining association rules VLDB pp 1964-1970 2005 25 L Wong and J Li Caep Classification by aggregating emerging patterns Proc 2nd Iat Coif Discovery Scieice DS 1999 gene groups from pp 487-499 t994 13 Available ot http://www-personal umich edu/o markiwen 14 R Motwani and P Raghavan Randomized Algoriitlms Caim-bridge University Press 1995 15 S Sudarsky Fuzzy satisfiability Intl Conf on Industrial Fuzzy Contri J Han and J Pei Cmar Accurate and efficient classification based on multiple class-association rules ICDM 2001 21 F Rioult J F Boulicaut B Cremilleux and J Besson Using groups for groups in microarray datasets SIGMOD 2004 3 concept of emerging patterns BioinformJotics vol 18 transposition for pattern discovery from microarray data DMKD pp 73-79 2003 22 J Li X Zhang G Dong K Ramamohanarao and Q Sun Efficient mining of high confidence association rules without S Y Ho C H Hsieh H pp 725-730 1996 10 B Liu W Hsu and support thresholds Principles f Drata Mining aind Knowledge Discovery PKDD pp 406 411 1999 23 G Dong and J Li Efficient mining of emerging patterns discovering trends and differences KDD J Wang J Han and J Pei Closet Searching for An efficient algorithm for closed association rule mining Proc oJ the 2nd SIAM Int Con on Data in large databases SIGMOD 


