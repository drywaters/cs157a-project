 Abstract The problem of discovery association rules in large databases is considered An encoding method for converting large databases to small one is proposed Significant efficiency is obtained by applying some modified known algorithm on our proposed database layout In addition a new algorithm based on the proposed encoding method is introduced Using some properties of numbers our database converts itemset to numerical domain Our implementation indicates that the proposed layout made the size of database significantly smaller Also the time to find association rules is reduced I I NTRODUCTION HE amount of data stored in databases continues to grow fast Intuitively this large amount of stored data contains valuable hidden knowledge which could be used to 
improve the decision-making process of an organization For instance data about previous sales might contain interesting relationships between products and customers The discovery of such relationships can be very useful to increase the sales of a company Thus there is a clear need for semi-\atic methods for extracting knowledge from data Data mining specially discovery association rules try to find interesting patterns from databases that represent meaningful relation between products and customers or other relations in some other applications Let I be a set of items A set X i 1  i k  I is called itemset or a k-itemset if contain k items Each transaction T in 
the database that is denoted by T tid X is composed of two parts First part is transaction identifier and second subset of the itemset occurs in I. An association rule between a set of item X  I and another set Y  I and X  Y  is expressed as X=>Y and indicate that the presence of the itemset X in the transaction also indicate a strong possibility of presence of the itemset Y The measure used to evaluate Manuscript received December 13, 2007 Manuscript received December 14, 2007 Huimin HE is with the Department of computer science Handan College Hebei Handan 056005 CHINA He is an associate professor now and the 
research fields is Computer Structure and Embedded System phone 86-15930805817; e-mail Hehuimin1967@gmail.com   Haiyan DU is with the Department of computer science Handan College Hebei Handan 056005 CHINA She is a lecturer now and the research areas are Computer Networks Yongjin LIU is with the Department of computer science Handan College Hebei Handan 056005 CHINA He is an associate professor now and the research areas are Computer Application Fangping LI is with the Department of computer science Handan College Hebei Handan 056005 CHINA Her research areas are Multimedia Technology Yi XIE is with the Department of Elect rical and Electronics Engineering 
Nanyang Technological University Singapore His research areas are Computer Networks Optical Communications the level of presence of an itemset in the database is the support The support of itemset X is equal to the fraction of transaction containing X The measure that demonstrates the possibility of presence Y in the transaction when X occurs in it is confidence The confidence of the rule X=>Y is the fraction of transaction containing X which also contains Y In association rule mining we want to find all rules with the frequent and confidence above minimum threshold for frequent and confidence The problem of finding association rules first was introduced by Agrawal and his cooperators in The frequent itemset and association rule mining problem have received a 
great deal of attention and many algorithms have been proposed to solve this problem Discovering association rules in these algorithms usually done in two phase In the first phase frequent itemset generated and then interesting rules extract from frequent itemset The rules that have a support and confidence above minimum threshold are interest The task of discovering all frequent itemset is quite challenging especially in large databases The database could be massive containing million of transactions A fast algorithm called Apriori was proposed in which generates k+1 using joins over frequent k-itemset all subset of one itemset must be generated by the algorithm. Although many of these frequent itemset may be not useful and may not exploit for finding association rules because some of these frequent itemset haven 
t any interestingness antecedent or consequent in rules but we had to generate them to find superior frequent itemset Additionally size of database was main problem of this algorithm Some modified algorithm of Apriori algorithm proposed to solve this problem but those algorithms also have the database size problem For discovery association rules in short time some algorithms proposed such as sampling algorithm Sampling algorithm proposed by Toivonen is an attractive approach to find association rules in tiny time with lost a little accuracy In this paper we wish to propose new encoding database method and new algorithm to find only frequent itemset that useful and have interestingness antecedent or consequent and with higher probability generate association rules This paper is organized as follows In the next section we 
propose a method for encoding databases and converting it to new layout In section III we will discuss about key step in discovery association rules itemset mining and we ll propose new algorithm for generate frequent itemset We propose our strategy for finding interesting association rules in section 4 In section VI we discuss some optimization on the New Coding Method to Reduce the Database Size and Algorithm with Significant Efficiency in Association Rules Huimin HE, Haiyan DU, Yongjin LIU, Fangping LI, Yi XIE T 2323 978-1-4244-1823-7/08/$25.00 c  2008 IEEE 


algorithm and we propose some technique to solve some drawback of the algorithm In section 6 we present some empirical result while last section contains the conclusion and summery II DATABASE ENCODING Database presentation is important consideration in most algorithms The most commonly used layout is the horizontal database layout and vertical database layout In both database layout set of items in transaction is denoted by binary value The size of database in both presentations is large Encoding database layout to new presentation can reduce the database size and improve the efficiency of algorithms We want to convert large databases into smaller database layout which have all of property of prior layout Since instead of maintain large table for database transactions one table is created only with two columns First column is for transaction identifier and second column for entire items that occur in the transaction All items in one transaction are converting to only one number that has all property of those items In this new state our database is too smaller than previous database layout and bringing it to memory is very simpler and its cost is order of magnitude less than bringing prior layout to memory With this assumption any itemset as represented by only one number For convert any itemset to number we must define some measures First we define measure attribute A measure attribute MA is a numerical attribute associated with each item in each transaction in database layout A numerical attribute can have binary type Those items that are occurring in one transaction are depicted with 1 and other items represent with 0.The transaction measure value denoted as tmv\(I p T q  is the value of a measure attribute associated with an item I p in a transaction T q I f tmv\(I p T q  this means item I p not occur in transaction T q but if occur item I p in transaction T q tmv\(I p T q  to 1.For example in table 1 tmv\(D,T 1  equals to 1.Any item I p in set of items is encode to one prime number denoted as E\(I p  We use prime numbers for this property that prime numbers aren t divided by any number except 1 and themselves For any item Ip in transaction Tq we assign new measure that denoted as M\(I p  T q  to multiply tmv\(I p T q  it s Encoding number\(E\(I p  by equation 1.After this for all I p and T q that its M\(I p T q  equals to 0,M\(I p T q  converted to 1.This operation described in equation 2.Value M Tq for any transaction is equal to multiplication of all M\(I p  T q  I p T q      Pq Pq P M I T tmv I T E I  1    all  if M  0  1 Pq Pq Pq For IT IT MIT   2   Tq P q M MI T   3 For any itemset I that is represented in schema I=\(I p1 I p2  I pn  is one value denoted as M I is equal to multiplication of all E\(I p  its I p occur in I Value M I is show the number correspond to itemset I after this we can use this number instead of itemset I  p IP II M EI   4 With this encoding instead of maintain all tmv\(I p T q  every item and transaction we can store value M I for every transaction. You can see using this technique in table 1 T ABLE I C ONVERT VERTICAL DATABASE LAYOUT TID A B C D 1 2 3 4 a IP EIP A7 B5 C3 D2 b T ID M 114 22 310 5 46 c In table 1 our database has represented in a\e have 1 column for T id and 4 columns for items In b set of items in left column and prime numbers correspond to them in right column By applying equation 1 2 3 we have c that has only two columns and every transaction have M Tid instead of some binary numbers for itemset In this situation if we want checking for example is there itemset I B C in 3th transaction or not we can divide M Tid 3=105 to M I 5*3=15 In this condition 105 divided by 15 then we can say itemset I has occured in transaction Tid=3 means for verifying presence any itemset I p in transaction T q  dividing value M Tq to M Ip is sufficient Only one drawback may be occur in this encoding when the number of items in database be large then the last prime numbers is been high This caused the value M of itemset and transaction increased rapidly We propose a technique to solve this problem in section 5.using this encoding method and applying modified known algorithm such as apriori and apririTid and aprioriHybrid can significantly improve efficiency of algorithms III FREQUENT ITEMSET MINING Almost all of algorithms for itemset mining start finding frequent itemset with single itemset in first iteration and in next iteration with combining generated frequent itemset create 2-itemset candidate and continue to discover larger frequent itemset In these techniques some frequent itemset create that may have no significant value such as 1-itemset that has no value independently In other words in these algorithm finding frequent itemset is done in bottom up manner In our algorithm discovery frequent itemset is done in up to down style It means that first large frequent itemset are found and then all of their subsets that are certainly frequent are extracted This property of frequent itemset is described in Instead of computing small frequent sets and 2324 2008 IEEE Congress on Evolutionary Computation CEC 2008 


combining those to find larger frequent itemset in this algorithm frequent large itemset is discovered and we can extract all of subsets without more computing because if an itemset is frequent all of its subset could be frequent as well In this technique is supposed that any frequent itemset must be at least one time occurs in database lonely without any other item than not member of that itemset in transactions In other words if itemset A B C frequent we must have this itemset at least in one transaction without any other item That transaction in prior layout must be in this schema T ABLE II F REQUENT ITEMSET PRESENTATION ABC Tid 1 1 1 000000000 In this method for every transaction T q  greatest common divisors GCD between M Tq and M T correspond to other transactions computed and these greatest common divisor and frequent of them are stored in GCD set GCD set is candidate for frequent set If there is any GCD in GCD set that it s frequent above the required threshold it will be selected and added to frequent GCD\(FGCD itemset For every transaction is maintained a set denoted GCD Tid composed of GCDs and frequency of any GCD. For example if GCD set and frequency between first transaction and other transactions is equal to GCD1 21,2 15,4 105,1 required threshold for support is equal to 7 then set\(6,8 a frequency greater than 8,then 6 add to frequent GCD\(FGCD itemset mining phase of this algorithm is given in Algorithm 1 We use notation t o r epresent the value M f or ith transaction and for required threshold for support CF consists of Candidate frequent set GCDs and their frequency those GCDs have a frequency greater or equal to be adding to FGCD Algorithm 1:itemset mining M: array  aintain M of transaction FGCD For i: =1 to DBsize do Begin C F  For j: =1 to DBsize do Begin a:=gcd\(M[i],M[j if a in C F then inc\(a.f frequency of a else add a,1 F insert a with frequency=1 to C F end for if there is any a in C F that a.f>=then add a to FGCD end for IV ASSOCIATION RULE MINING Discovering association rules based on all frequent GCD which have been found in previous phase Measure M corresponds to any frequent itemset is maintained in FGCD Every measure M in FGCD is decomposed to multiplication of prime number and each prime number corresponds to one item The itemset that correspond to M is identical and frequent then any subset of it must be frequent Every M in FGCD decomposed into a candidate head Y and a body X=M/Y algorithm iteratively generates candidate heads C k+1 of size k+1 starting with k=0.If head and body are interesting and valuable the confidence c of the rule X Yis computed as the quotient of the supports for the itemset c=support support\(X computed by counting the number of M Tid that divided by X any rule haveacgr eater than or equal to given threshold for confidence that rule adds into association rules The association rule mining phase of this algorithm is given in Algorithm 2 We use notation t o r epresent the value M f or ith member of FGCD and  for required threshold for confidence Algorithm 2: association rule mining Association rules For i: =1 to FGCD-size do Begin Decom e num ber set M[i] = {t 1 t2 t n  K: =1 While k<n do Begin C k all subset with k mem For any t C k Begin X=1 For i: =1 to k X: =X*t i t i t Y=M[i]/X If X and Y are interesting and c\(X=>Y  Add X=>Y End for K End while End for V OPTIMIZATION Encoding database by using prime number to quantity number at least reduce the size of database to half size But in large database with large number of item encoding any item to one prime number may be caused some problems specially in computing the value M for nay transaction The M value for any transaction with number of item greater than 30 may be excess than number that can be shown with two byte To solve this problem can divide the database in smaller part vertically and put correlated items in one part Correspond to previous discovery of association rules 2008 IEEE Congress on Evolutionary Computation CEC 2008 2325 


example put book items in one part and food items to another part Then every part of database is encoded to one column In this situation frequent itemset mining is done independently for any part and association rules in every part are discovered After this encoding database has little columns and all algorithms for itemset mining can use it to achieve better performance and efficiency Another approach to solve problem of growing M is assigning smaller prime numbers to items with higher frequency VI EXPERIMENTAL RESULT Our implementation on some benchmarks such as T40I10D100K and mushroom and BMS-Webview-1 denoted that by applying our encoding method size of database was reduced at least half The sizes of databases have been shown in figure 1 in our encoding layout and prior layout Fig.1 Database size in prior and new encoding Also our implementation showed that the speed of our algorithm after this encoding increased especially in sparse databases Our improvement in speed of discovery association rules are at least 25%.The compromise between our algorithm and Apriori and FP-growth showed in figure 2 Fig.2 Required time for generate association rules in synthetic data set VII CONCLUSION In this paper an encoding method was proposed to reduce the database size and after it by applying new algorithm on new layout we can achieve significant efficiency in association rules discovery This efficiency in algorithm is achieved by lost 5%of association rules In situation that size of database is bottleneck of discovery frequent patterns in databases using encoding method can significantly improve the efficiency of known algorithms In some condition the efficiency of discovery of association rules mining is more important that accuracy we can use this algorithm to find association rules with consuming short time REFERENCES  R.agrawal T.Im ielinski A  S wam i  M ining A ssociation Rules between Sets of Items in Very Large Databases.ACM SIGMOD Conference Proceedings, pages 207-216  T.Fukuda Y.Morim o to, S.Moridhita, T.Tokuy am a.Mining Optim ized Association Rules for Numeric Attributes.ACM PODS Conference Proceedings, pages 182-191, 1996  T.Fukuda Y.Morim o to, S.Moridhit a, T.Tokuyama.Data Mining using Two-dimensional Optimized Association Rules for Numeric Attributes Schema Algorithmd Visualization.ACM SIGMOD Conference Proceedings, pages 13-23, 1996  R.Rastogi K Shim Mining optim i zed Association rules for categorical and numeric attributes.ICDE Conference Proceedings, pages 503-512 1998  R.Rastogi K Shim Mining op timized Support Rules for numeric attributes.ICDE Conference Proceedings, pages 126-135, 1999  R.Srikant, R.Agrwal.Mining G eneralized Association Rules.VLDB Conference Proceedings, pages 407-419, 1995 7 R.Srikant, R.Agrwal. Mining Q uantitative Association rules in large databases.ACM SIGMOD Conference Proceedings, pages 1-12, 1996  R.Agrawal R.Srikant.Fast algorithm for m i ning association rules.In J.B Bocca, M.Jarke, and C.Zaniolo, editors.Proceedings 20th International Conference on vary Larghe Databases,pages 487-499,1994  R.Agrawal,T.Im ielinski,R.Srikant,H.Toivonen,A.I.Verkam o.fast discovery of association rules.In U.M.Fayyad,G.Piatetsky Shapiro,P.Smyth,R.Uthursamy.editors,advances in knowledge discovery and data mining,pages 307-238.MIT press,1996  M.J.Zaki.Scalable algorithm s for association m ining.IEEE transactions of knowledge and data engineering, 12\(3\/June 2000  C.L.Blake and C.J.Merz. UCI Repository of m achine learning databases University of California, Irvine Dept. of Information and Computer Sciences http://www.ics.uci.edu/~mlearn/MLRepository.html  1998  R.Agrawal and R.Srikant.Quest Sy nthetic Data Generator.IBM Almaden Research Center,San Jose,California,http://www.almaden  ibm.com/cs/quest/syndata.html  R.Kohavi C.Brodley  B.Frasca L.Mason and Z.Zheng.KDDCup 2000 organizers report: Peeling the onion.SIGKDD Explorations,2\(2 98,2000.http://www.ecn.purdue.edu/KDDCUP  14 H.Toivonen.Sampling large databases for association rules.In T.M. Vijayaraman, A.P.Buchmann, C.Mohan and N.L.Sarda, editors, proceedings 22nd International Conference on Very Large Data Bases, pages 134 145 Morgan Kaufmann, 1996 2326 2008 IEEE Congress on Evolutionary Computation CEC 2008 


We made a set of comparing experiments between FPclose and GTCWFPMiner among min-sup number of patterns Max_L and execution time et. al. The difference between GTCWFPMiner and FPclose  results from weight constraints Figure 11 shows the trend of the execution time of GTCWFPMiner and FPclose with respect to different specified minimum support threshold min-sup based on Max_L=7 As shown in figure, as min-sup decreasing the average execution time increases. When the min-sup is lowered, the performance diffe rence becomes larger. In all case of min-sup  GTCWFPMiner outperforms algorithm FPclose. This is because GTCWFPMiner carries out the closed frequent pattern mining with weight constraints and can reduced effectively search space, but FPclose do the closed frequent pattern mining without weight constraints, its search space is larger than algorithm GTCWFPMiner Figure 12 shows the trend of the number of patterns & runtime w.r.t Max_L As shown in Fig 12\(a GTCWFPMiner generates fewer patterns than FPclose. The difference of the number of patterns between two algorithms becomes smaller as the Max_L increases. In Fig. 12\(b GTCWFPMiner is faster than FPclose. The performance difference becomes larger when Max_L becomes longer. Figure 13 shows GTCWFPMiner has much better scale-up properties than FPclose for the numbers of nodes & traversal transactions  min-sup 15%\n the figure 13\(a\although itself runtime also increases GTCWFPMiner runs faster than FPclose as the number nodes increases. In figure 13\(b GTCWFPMiner also has a better scalability in terns of number of traversal transactions and runs faster than FPclose. Our own experimental evaluation \(not presented in this paper\that WTMaxMiner is equally effective for other value of Max_L and min-sup as well  6. Conclusions and future works  This paper explores the problem of discovering closed frequent patterns from traversals on WDG A transformable model between EWDG and VWDG is proposed Based on the model, we present the mining algorithm named GTCWFPMiner This algorithm exploits a divideand-conquer approach in a bottom-up pattern-growth manner and incorporates the closure property with weight constrains to reduce effectiv ely search space and extracts succinct and lossless patterns from graph traversal TDB Experimental results show the algorithm is an effective and scalable algorithm for CWFP based on graph traversals. How to scale the model and algorithm proposed in this paper to a larger scale, e.g., Web-size scale and other large relation models etc.; can we deeply optimize the algorithm, and how to efficiently put it into practice are still be worthy doing further exploring for researches  7. References  1 M S  Che n   J S Pa rk a nd P S. Yu, “Efficient data mining for path traversal patterns IEEE Trans. on Knowledge and Data Engineering IEEE CS, Los Alamitos, CA, USA, 1998, pp. 209–221  A. Nanopoulos, Y. Manolopoul os, “Mining patterns from graph traversals Data & Knowledge Engineering Elsevier Science, Netherlands, 2001, pp. 243–266  R. Agrawal, R. Srikant F ast algorithm s for m i ning association rules Proceedings of the 20th Int'l Conference  of VLDB  Morgan Kaufmann, San Fr ancisco, USA, 1994   J. Han, J. Pei and Y. Yin Mining frequent patterns without candidate generation Proceedings of SIGMOD’00 ACM, NY USA, 2000   F. Bonchi, C. Lunnhese, “O n closed constrained frequent pattern mining Proceedings of ICDM’04 IEEE Computer Society, Los Alamitos, CA, USA, 2004  M  J Za ki C J Hsia o  Charm: an efficient algorithm for closed itemsets mining Proceedings of SIAM’02 Arlington USA, 2002  Wa ng J Pe i a nd J. Han, , “Closet+: searching for the best strategies for mining fre quent closed itemsets Proceeding of SIGKDD’03 ACM, NY. USA, 2003  Yun J J Le ggett, “WLPMiner: weighted frequent pattern mining with length-decreasing support constraints Proceedings of  PAKDD‘05 Springer, Berlin/Heidelberg, 2005  W. Wang, J. Yang and P S. Yu, “Efficient mining of Weighted Association Rules \(WAR Proceedings of SIGKDD’00 ACM, NY. USA, 2000  C.H. Cai, A.W.C. Fu and C.H. Cheng et. al., “Mining association rules with weighted items Proceedings of IDEAS’98  IEEE Computer Society, Washington, DC, USA,1998  G. Grahne, J. Zhu Fas t algorithms for frequent itemset mining using FP-trees”, IEEE Tr ansactions on Knowledge and Data Engineering, IEEE Computer Society, Los Alamitos, CA USA,2005,pp.1347–1362 5 6 7 8 9 10 0 10 20 30 40 50 60 70 80  Max length of traversal patterns Numbers of patterns   minimum support=15 FPclose  GTCWFPMiner 5 6 7 8 9 10 0 10 20 30 40 50 60 70  Average execution time\(sec Max length of traversal patterns   minimum=15 FPclose  GTCWFPMiner 10 15 20 25 30 10 15 20 25 30 35 40 45 50  Specified minmum support threshold Average execution time\(sec   Max-L=7 |T|=10,000 FPclose  GTCWFPMiner 100 200 300 400 500 0 10 20 30 40 50 60 70  Number of vertices Average execution time\(sec   Max-L=7,|T|=10,000,minimumsup=15 FPclose  GTCWFPMiner 4 6 8 10 12 0 5 10 15 20 25 30 35 40 45 50  Number of transactions Average execution time\(sec   Max-L=7,minimumsup=15 FPclose  GTCWFPMiner 0 0.2 0.4 0.6 0.8 1 0 5 10 15 20 25 30 Weight Frequency   Frequency 
408 
408 
403 


 Very few ows are high throughput Most ows are short lived Almost all ows are mice  Most ows have an average packet size medium Most ows are packet mice Almost all bulk ows are medium throughput Almost all bulk TCP ows are short-lived  Fig 5 Simple on-line linguistic summary of the CRAWDAD-Fall03 NetFlow collection truth values between brackets of rules to analyze In particular we disregarded those rules with a low support or with a low condence truth value Many interesting rules were found for the NetFlow records analyzed We list as examples a selection of them  Most DNS request ows occur both during the day and at night are mice and short lived with condence 0.970 in the WIDE-F-1-Aug collection  Most ows at night are mice with condence 0.890 and Most ows during the day are mice with condence 0.998 in the CAIDA-OC48-0-Apr collection  Most SSH trafc occurs during the day and consists of short lived mice ows with condence 0.892 in the CRAWDAD-Fall03 collection Linguistic summaries provide a novel method to describe qualitative relations in NetFlow collections using natural language Thus by using association rules mining to nd relevant summaries we have a suitable method for addressing a problem related to ow analysis nding invariants in trafc what is known as one the major goals of Internet Science VI C ONCLUSIONS We have addressed network trafc analysis at the ow level from the perspective of linguistic summaries Two approaches for summarizing NetFlow collections have been developed 1 on-line summarization via a predened and congurable set of potential interesting protoforms and 2 discovery of hidden relevant summaries by means of association rules mining A tool that implements both approaches has been developed Experimental results for a set of benchmark NetFlow collections conrm linguistic summaries as an alternative look into network ow statistics useful for both network users and practitioners The method presented is a novel technique to generate simple and human-interpretable reports but also provides a promising technique for nding invariants in network trafc and advancing Internet Science This can be seen as a rst step towards natural language based knowledge discovery for Internet Science A CKNOWLEDGEMENT We acknowledge the MAWI Working Group from the Wide Integrated Distributed Environment WIDE project for k indly p ro viding their  o w collections and support We are also indebted to the Cooperative Association for Internet Data Analysis CAIDA for providing their OC48 data collection  Support f or CAID A  s O C48 Traces Dataset is provided by the National Science Foundation the US Department of Homeland Security DARPA Digital Envoy and CAIDA Members We used the Dartmouth/campus data set from t he Community Resource for Archiving Wireless Data CRAWDAD Our work has beneted from the use of measurement data collected on the Abilene network as part of the Abilene Observatory Project http://abilene.internet2.edu/observatory R EFERENCES  C ooperati v e Association f or Internet D ata Analysis CAID A V i sualization Tools http://www.caida.org/tools/visualization  J  S ommers P  B arford a nd W  W illinger  SPLA T  A V i sualization Tool for Mining Internet Measurements in 7 t h Passive and Ac t ive Ne t work Measuremen t Workshop  Mar 2006 pp 31–40  C  E stan S  S a v age and G  V ar guese  Automatically Inferring P a tterns of Resource Consumption in Network Trafc in SI G C OMM 200 3  Karlsruhe Germany Aug 2003 pp 137–148  R  R  Y ager   A N e w Approach to the S ummarization o f D ata  I n f orma t ion S ciences  vol 28 pp 69–86 1982   Database D isco v e ry Using F uzzy Sets  I n t erna t ional Journal o fI n t elligen tSy s t ems  vol 11 1996  J  K acprzyk and R  R  Y ager   Linguistic Summaries of Data Using Fuzzy Logic I n t erna t ional Journal o f General Sy s t ems  vol 30 no 2 pp 133–1504 Jan 2001  J  K acprzyk and S  Z adro  zny Linguistic database summaries and their protoforms Towards natural language based knowledge discovery tools I n f orma t ion S ciences  vol 173 no 4 Mar 2005   Cisco I OS NetFlo w  h ttp://www cisco.com/en/US/products/ps6601 products ios protocol group home.html Nov 2007  B  C laise e t al  Specication of the IPFIX Protocol for the Exchange of IP Trafc Flow Information Internet Engineering Task Force IPFIX Working Group Revision 26 Sep 2007 Internet Draft  S Shaluno v a nd B T eitelbaum TCP Use a nd Performance on Internet2 in A C M SI G C OMM I n t erne t Measuremen t Workshop San Francisco USA 2001  L A Zadeh A Computational A pproach to Fuzzy Quantiers i n Natural Languages C ompu t ers and Ma t hema t ics wi t h Applica t ions  vol 9 pp 149–184 1983  R R Y a ger   O n O rdered W eighted A v eraging O perators in Multicriteria Decision Making IEEE Transac t ions on Sy s t ems Man and Cy berne t ics  vol 18 pp 183–190  1988  L A Zadeh  A P rototype-Centered Approach to Adding Deduction Capability to Search Engines-the Concept of Protoform in F irs t I n t erna t ional IEEE Sy mposium on I n t elligen tSy s t ems vol.1,Sep 2002 pp 2–3   The concept o f a linguistic v a riable and its application t o approximate reasoning I n f orma t ion S ciences  vol 8 no 3 pp 199 249 1975  A Broido Y  Hyun R Gao and k c claf fy   Their Share Di v e rsity and Disparity in IP Trafc in 5 t h Passive and Ac t ive Measuremen t Workshop  PAM   Antibes Juan-Les-Pins France 2004 pp 113–125  M Delgado N Mar  n D S  anchez and M.-A Vila Fuzzy Association Rules General Model and Applications IEEE Transac t ions on F u zzy Sy s t ems  vol 11 no 2 pp 214–225 Apr 2003  J Kacprzyk and S  Zadro  zny Linguistic Summarization of Data Sets Using Association Rules in IEEE I n t erna t ional C on f erence on F u zzy Sy s t ems FUZZ IEEE  St Louis USA May 2003 pp 702  707  R Agra w al H Mannila R Srikant H T o i v onen and A  V erkamo Advances in Knowledge Discover y and Da t a Mining  American Association for Articial Intelligence 1996 Fast Discovery of Association Rules pp 307–328  M Fullmer e t al  ow-tools http://www.splintered.net/sw/owtools Nov 2007  W i dely Inte grated Distrib u ted En vironment  WIDE P roject MAWI Working Group Packet traces from wide backbone http://tracer.csl.sony.co.jp/mawi 2006  CAID A O C48 T race Project CAID A OC48 T r aces 200304-24 collection http://imdc.datcat.org/collection/1-0018N=CAIDA+OC48+Traces  D K o tz T  Henderson and I  A byzo v   CRA WD AD data set dartmouth/campus v 2007-02-08 Downloaded from http://crawdad.cs.dartmouth.edu/dartmouth/campus Feb 2007 624 2008 IEEE I n t erna t ional C on f erence on F u zzy Sy s t ems FUZZ 2008 


Since the attribute determination algorithm has determined that the attribute Sno in Table 0, the attribute Cno in Table 1, and the attributes <Sno Cno> in Table 2 embrace the double-connective association rule student\(Sno 010 1 course\(Cno 010 2 study\(Sno, Cno\he connective determination algorithm make the relational matrix shown in Fig. 4 according to the binary relationship table of Table 2   C1 C2 C3 C4 S1   T  T  F  F S2   T  F  T  F S3   T  F  F  F S4   F  T  F  F S5   T  F  F  T   Fig. 4 The relational matrix made from Table 2  Fig. 4 is made like this: Table 2 has the tuple S1, C1>, then at the cross of the row S1 and the column C1, a T is filled; Table 2 does not have tuple S1, C3>, then at the cross of the row S1 and the column C3, a F is filled Suppose the cardinality of student\(Sno\s M, in this example 5, i.e. S1 to S5; the cardinality of course\(Cno\n this example 4, i.e. C1 to C4 The algorithms for DCAR1 through DCAR6 are as follows The algorithm for DCAR1 If in Fig. 4 there is M*cf 1 rows, N*cf 2 columns submatrix, in which all elements are Ts, then DCAR1 holds The algorithm for DCAR2 If in Fig. 4 there is at least one column, in which there are at least M*cf 1 Ts, then DCAR2 holds The algorithm for DCAR3 If in Fig. 4 at least M*cf 1 rows have Ts, then DCAR3 holds The algorithm for DCAR4 If in Fig. 4 there is at least one row, in which there are at least N*cf 2 Ts, then DCAR4 holds The algorithm for DCAR5 If in Fig. 4 at least N*cf 2 columns have Ts, then DCAR5 holds The algorithm for DCAR6    DCAR6   DCAR3  DCAR5     DCAR2  DCAR4   DCAR1 Fig. 5 The complement lattice formed by DCAR1 through DCAR6 
277 
277 


000\003 000\\000L\000J\000\021\000\031\000\003\000\003\000&\000R\000Q\000Q\000H\000F\000W\000L\000Y\000H\000\003\000G\000H\000W\000H\000U\000P\000L\000Q\000D\000W\000L\000R\000Q\000\003\000D\000O\000J\000R\000U\000L\000W\000K\000P\000\003 Start Call DCAR1 000D\000O\000J\000R\000U\000L\000W\000K\000P  DCAR1 holds 002  Call DCAR2 000D\000O\000J\000R\000U\000L\000W\000K\000P  Output DCAR1,2,3,4,5,6 End DCAR2 holds 002  Output DCAR2,3,6 Call DCAR3 000D\000O\000J\000R\000U\000L\000W\000K\000P  DCAR3 holds 002  Output DCAR3,6 Call DCAR4 000D\000O\000J\000R\000U\000L\000W\000K\000P  DCAR4 holds 002  Call DCAR5 000D\000O\000J\000R\000U\000L\000W\000K\000P  Output DCAR4,5,6 End DCAR5 holds 002  Call DCAR6 000D\000O\000J\000R\000U\000L\000W\000K\000P  Output DCAR5,6 End DCAR6 holds 002  Output DCAR6 End Error Y N N Y Y N N Y Y N N Y 
278 
278 


If in Fig. 4 there is at least one T, then DCAR6 holds DCAR1 through DCAR6 forms a complement lattice shown in Fig. 5 In Fig. 5, the lower rule implies the upper rule That is, if DCARj is reachable from DCARi via an ascending path, and DCARi holds, then DCARj holds Because DCAR1 through DCAR6 satisfies Fig 5, their algorithms can be merged into one algorithm called connective determination algorithm, shown in Fig. 6 Suppose cf 1 80%, cf 2 75%. In Fig. 4, for the column of C1, there are M*cf 1 5*80%=4 elements whose values are T \(namely, S1, S2, S3, S5 Therefore, DCAR2: course\(Cno 004 1  student\(Sno 003 1  study\(Sno, Cno\olds. From Fig. 5, we know that DCAR3 and DCAR6 also hold. In Fig. 4, there are at least N*cf 2 4*75%=3 columns which have value T \(namely, in the column of C1 there is S1, in the column of C2 there is S1, in the column of C3 there is S2, in the column of C4 there is S5 therefore DCAR5: course\(Cno 003 1  student\(Sno 004 1  study\(Sno, Cno  VI. CONCLUDING REMARKS 1\ Double-connective association rule mining is different from single-connective association rule mining. The former mines the association among the primary keys of the two entity tables and the primary key of the binary relationship table. The latter mines the association between frequent item sets 2\. 4 is different from data cubes in data warehouses. The elements in Fig. 4 are T or F. The elements in the data cubes are data 3\The differences between double-connective association rule and database query are that, first, the query information in databases are predeterminate while the information to be mined by double-connective association rule is not predeterminate, it is implied. Secondly, database query needs to write SQL statements, while double-connective association rule mining is automatic. Thirdly, the information obtained by database query is quantitative, while the information obtained by double-connective association rule mining is qualitative such as “for many”, “there are some  REFERENCES 1 Ji a w ei H a n   M i ch eli n e K a m b er   D a t a  M i n i n g C onc ep t s  a nd Techniques, Higher Education Press, Beijing, 2001, Morgan Kaufmann Publishers, 2000 2 A  G  Ha m i lt on  L o gi c for M a th em a t i c ia ns R evi s ed E d i t i o n   Cambridge University Press, 1988, Tsinghua University Press Beijing, 2003 3 X unw e i Z h o u   Br ie f I ntr o du c t io n  to  Mu t u al l y I nve r s is tic Logic”, 1999 European Summer Meeting of the Association for Symbolic Logic, Utrecht, The Netherlands, August 1-6 1999 4 u n w ei Zh ou F i r s t leve l exp l i c i t m u lt ip le i ndu ct i v e composition”, 2005 Spring Meeting of the Association for Symbolic Logic, The Westin St. Francis Hotel, San Francisco CA. USA, March 25-26, 2005 5 A b rah a m S i lb ers c ha t z  Hen r y  F  Kort h  S S u da rs ha n Dat a b a s e  System Concepts \(Fourth Edition\, Higher Education Press Beijing, 2002, McGraw-Hill Companies, 2002  
279 
279 


support pruning gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis Biosystems vol 85 computationally challenging As training set sizes increase it is likely that these difficulties will also increase VI RELATED WORK While operating on a microarray dataset current CAR 1 2 3 4 and other pattern/rule 20 21 mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets Hence they are generally quite computationally expensive for datasets containing many training samples or genes as the case may be BSTC is explicitly related to CAR-based classifiers but requires no expensive CAR mining BSTC is also related to decision tree-based classifiers such as random forest 19 and C4.5 family 9 methods It is possible to represent any consistent set of boolean association rules as a decision tree and vice versa However it is generally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data 1 2 6 7 11 BSTC is explicitly related to and motivated by CAR-based methods To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here Perhaps the work closest to utilizing 100 BARs is the TOPRULES 22 miner TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci Hence TOP-RULES discovers all 100 confident CARs in a dataset However the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER 23 and so generally isn't polynomial time Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules 24 25 Once obtained fuzzy rules can be used for classification in a manner analogous to CARs However the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM 25 VII CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with preclassification CAR mining see Tables IV and VI we developed a novel method which considers a larger subset of CAR-related boolean association rules BARs These rules can be compactly captured in a Boolean Structure Table BST which can then be used to produce a BST classifier called BSTC Comparison to the current leading CAR classifier RCBT on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining see Section VB Hence BSTC extends generalized CAR based methods to larger datasets then previously practical Furthermore unlike other association rule-based classifiers BSTC easily generalizes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse then CAR-based methods after all exponential time CAR mining is completed O SlS CGl versus O Si CGi As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists ACKNOWLEDGM[ENTS We thank Anthony K.H Tung and Xin Xu for sending us their discretized microarray data files and Top-k/RCBT executables This research was supported in part by NSF grant DMS-0510203 NIH grant I-U54-DA021519-OlAf and by the Michigan Technology Tri-Corridor grant GR687 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies REFERENCES 1 G Cong K L Tan A K H Tung and X Xu Mining top-k covering rule Mining SDM 2002 5 R Agrawal T Imielinski and A Swami Mining associations between sets of items Y Ma Integrating classification and association rule mining KDD 1998 11 T McIntosh and S Chawla On discovery of maximal confident rules without pp 43-52 1999 24 S Vinterbo E Kim and L Ohno-Machado Small fuzzy and interpretable pp 165-176 2006 1071 pp 207-216 1993 6 G Dong pp 273-297 t995 9 pp 5-32 2001 20 W Li J R Quinlan Bagging boosting and c4.5 AAAI vol 1 V Vapnik Support-vector networks the best strategies for mining frequent closed itemsets KDD 2003 4 M Zaki and C Hsiao Charm L Wong Identifying good diagnostic genes or gene expression data SIGMOD 2005 2 G Cong A K H Tung X Xu F Pan and J Yang Farmer Finding interesting rule gene expression data by using the gene expression based classifiers BioiiiJcrmatics vol 21 l and Inrelligent Systenis IFIS 1993 16 Available at http://sdmc.i2r.a-star.edu.sg/rp 17 The dprep package http:/cran r-project org/doclpackages dprep pdfI 18 C Chang and C Lin Libsvm a library for support vector machines 2007 Online Available www.csie.ntu.edu.tw cjlin/papers/libsvm.pdf 19 L Breiimnan Random forests Maclh Learn vol 45 no 1 M Chen and H L Huang Interpretable X Zhang 7 J Li and pp 725-734 2002 8 C Cortes and Mac hine Learming vol 20 no 3 in microarray data SIGKDD Worikshop on Dtra Mining in Bioinfrrnatics BIOKDD 2005 12 R Agrawal and R Srikant Fast algorithms for mining association rules VLDB pp 1964-1970 2005 25 L Wong and J Li Caep Classification by aggregating emerging patterns Proc 2nd Iat Coif Discovery Scieice DS 1999 gene groups from pp 487-499 t994 13 Available ot http://www-personal umich edu/o markiwen 14 R Motwani and P Raghavan Randomized Algoriitlms Caim-bridge University Press 1995 15 S Sudarsky Fuzzy satisfiability Intl Conf on Industrial Fuzzy Contri J Han and J Pei Cmar Accurate and efficient classification based on multiple class-association rules ICDM 2001 21 F Rioult J F Boulicaut B Cremilleux and J Besson Using groups for groups in microarray datasets SIGMOD 2004 3 concept of emerging patterns BioinformJotics vol 18 transposition for pattern discovery from microarray data DMKD pp 73-79 2003 22 J Li X Zhang G Dong K Ramamohanarao and Q Sun Efficient mining of high confidence association rules without S Y Ho C H Hsieh H pp 725-730 1996 10 B Liu W Hsu and support thresholds Principles f Drata Mining aind Knowledge Discovery PKDD pp 406 411 1999 23 G Dong and J Li Efficient mining of emerging patterns discovering trends and differences KDD J Wang J Han and J Pei Closet Searching for An efficient algorithm for closed association rule mining Proc oJ the 2nd SIAM Int Con on Data in large databases SIGMOD 


