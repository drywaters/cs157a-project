A Dynamic Mining Algorithm of Association Rules  for Alarm Correlation in Communication Networks  Wu Jian , Li Xing ming School of Communication and Information Engineering  University of Electronic Science and Technology of China Chengdu, 610054, China    Abstract Nowadays, communication network turns to be more complex, once there occurs a failure, there will result in multi-alarm-events which require relevant transactions. This paper describes the alarm correlation in communication networks based on data mining. The associ ation rules from self-adaptation for dynamic network resource and service that build new rule by 
fully utilizing and maintainin g rules formed before while transactions grow/delete or minimum support changes which enables the framework is easily updated and new discovery methods be readily incorporated within it. Both theoretical analysis and computer simulations illustrate outstanding performance of the proposed models, which can be further optimized by experiments for specific environment   I  I NTRODUCTION  A  Background Data mining is the science of extracting implicit, previously unknown, and potentially useful information from large data sets or databases, also known as knowledge discovery in databases \(KDD\network, we utilize mined information 
to position the root alarms in it which can help to improve performance of administration of networks B  Problem description One of the most studied data mining problems is mining for association rules which represent an important class of knowledge that can be discovered from database. Given a collection of items and a set of records \(also called transactions\which co ntain some number of items from the given collection, the association rules indicate affinities that exist among the collection of items  The task of mining association rules can be formally stated as follows : Let I = {i 1 i 2  
i m be a set of literals, called items. Let D be a set of transactions where each transaction T that has a unique identifier-TID is a set of items such that T I We say that a transaction T contains X, a set of some items in I if X T. An association rule is an implication of the form X Y, where X I, Y I and X Y The rule X=>Y has two properties, support and confidence. When s% of transactions in D contain X Y, the support of it is s%. If some of the transactions in D contain X, and c% also contain Y then the confidence in the rule is c%. In general, the confidence is expressed in the form confidence\(X=>Y 
support\(X Y\/support\(X\. The problem of mining association rules is to generate all association rules that have support and confidence no lesse r than the user-specified minimum support and minimum confidence respectively In all the different algorithms given in the literature, the problem of discovering association rules is decomposed into two sub-steps  Find all sets of items that have support above the minimum support. Itemsets with minimum support are called frequency itemsets  Use the frequency itemsets to generate the desired rules 
For every frequency itemset L the length of L is larger than 1\, find all non-empty subsets of L. For every such subset a, output a rule of th e form a =>L-a, if the ratio of support \(L\ support \(a\ is at least minconf In the rest of the paper, Apriori algorithm and Fast Update Algorithm \(FUP\are explained in Section Next, our new adaptive algorithm is introduced analyzed and compared in Section After that, Experimental results for algorithms mentioned above are showed in Section Finally, the conclusion and future work are described in Section  II  R ELATED 
W ORK  C  The Apriori algorithm The Apriori algorithm concen trates primarily on the discovery of frequent itemsets according to a user-defined minsup. The algorithm relies on the fact that an itemset could be frequent only when each of its subset is frequent. In the first pass, the Apriori algorithm constructs and counts all 1-itemsets.After it has found all frequent 1-itemsets, the algorithm joins the fre quent 1-itemsets with each other to form candidate 2-itemsets. Apriori scans the transaction database and counts the candidate 2-itemsets to determine which of the 2-itemsets are frequent. The other passes are made accordingly Frequent \(k-1\msets are joined to form k-itemsets whose first k-1 items are identical. If k 
3, Apriori prunes some of the k-itemsets; of these, \(k-1\-itemsets have at least one infrequent subset. All remaining k-itemsets constitute candidate k-itemsets. The process is reiterated until no more candidates can be generated D  The FUP algorithm The purpose of the incremental mining is not only to validate the existing rules, but also to find new rules both in increment and the updated database. We scan the increment to find which rules continue to prevail and which ones fail in the merged database. FUP is proposed for the maintenance of discovered association rules in large databases. It relies on 


Apriori and considers only these newly added transactions. In the first pass, FUP scans db to obtain the occurrence count of each 1-itemset. If some of them are in L 1 frequent in DB\ the total count of them can easily be calculated; else DB must be rescanned. By combining newly frequent itemsets with those remained frequent in L 1 the set of all frequent 1-itemsets  is generated. Similarly, in iteration k C  1 L k is generated by first using the join-based apriori-gen function on  1 k L and then pruning C k by removing those itemsets which are already present in L k The process is reiterated until all frequent itemsets have been found. In the worst case, FUP does not reduce the number of the original database must be scanned III  OUR NEW APPROACH  E  Definitions and Notations   In order to describe easily, we give the following definitions DB: the original database DB  the increment database DB the decrement database DB': the updated database L: the set of all frequent itemsets in the original database DB L  the set of all frequent itemsets in the increment database DB  L': the set of all frequent itemsets in the updated database DB s: the minimum support threshold d: the number of transactions in DB d d and d': denote the numbers of transactions in DB DB  and DB' respectively count X   and denote the support counts of itemset X in DB,DB count X count X  count X DB and DB' respectively  With the same minimum support threshold s, a frequent itemset X of DB remains a frequent itemset of DB' if and only if d+d  count X d s= d'*s. All the itemsets in DB' can be divided into four groups as shown in Table 1 Table 1  Four groups associated with an itemset in DB       The itemsets of group 4 are indifferent. The itemsets in group 1 and group 3 can be obtained easily for the support count of each frequent itemset X in the original database has been recorded. However, the itemsets of group 2 need further consideration for their support counts in DB are unknown in advance. Thus, how to efficiently generate the itemsets of group 2 is an important problem for the maintenance of association rules. In the following section, we propose efficient approaches to so lve the problem F  A variant algorithm: The IDARM algorithm In this paper, we developed a new Incremental Dynamic Association Rule Mining algorithm\(IDARM\r adaptive association rules mining. There are three innovative points of our approach  First of all, in each iteration of FUP, only candidate itemsets of one size are handled. Therefore the number of passes over DB equals the size of the highest order itemsets to be handled To reduce the passes over DB, we perform exactly one scan over DB to count all candidates generated from DB  While some excessive candidates may be counted, the number of passes over DB is guaranteed to be minimum. It is expected that the execution time saved in scanning DB should be much larger than the increased time in counting the extra candidates in DB  for d is much smaller than d at most situations The IDARM algorithm works as follows 1  Scan DB  by using Apriori to find all frequent itemsets in it, for each frequent itemset record its support count  count X 2  For each itemset X in L L  if   s*\(d+d count X count X ed to the set L' with an identifier 223S1\224\(represent Group1 3  Check all itemsets in L-L  L', by scanning DB  one time more to updated their counts. Similar, for each itemset X add it to the set L' with an identifier \223S3\224 \(represent Group3\if  s*\(d+d count X count X  4  Check all itemsets in L  L-L', by scanning DB once to updated their counts. After the counting is finished, new frequent itemsets that satisfied the s are identified and added to the set L' with an identifier \223S2\224 \(represent Group2 Secondly, our approaches store the itemsets that are not frequency at present but may become frequency after updating the database, so that the cost of processing the updated database can be reduced. We discuss the cases where the frequency itemsets can be obtained without scanning DB We define a bound parameter 0 s\which is used to control the number of itemsets whose supports are less than s but no less than \(s\call these itemsets the potential frequent itemsets. The potential frequent itemsets and their support counts are stored in the se t P. It is easy to see that the more the number of the potential frequent itemsets is, the less the cost of processing the original database will be  We will first discuss the insertions. When the increment database DB  is scanned, the support counts of the itemsets in L P are accumulated. Then L and P are updated according to the support count thresholds s*\(d+d  d \(s d+d   respectively. Let X be an itemset of DB  and X L P. By Theorem 1, if the support count of X in DB   is greater than d  s\, then X may be a frequent or potential frequent itemset of the updated database DB'. In the case, X is stored in set M. Let mx be the maximal value of the support counts of itemsets in M. By Theorem 2, if mx d  s+ d we need not scan DB. Namely, in this case all itemsets in M can not be frequent of the updated database Theorem 1 If X L P and  d count X  s X is not a frequent or potential frequent itemset of DB Proof    d*\(s  count X count X count X count X d*\(s\d  s d+d  s 


Therefore, X is not a frequent or potential frequent itemset of DB Theorem 2 If mx<d  s+d then we need not scan the database DB   Proof Since X L P, X count d*\(s\herefore, if mx d  s +d mx+d*\(sd  d\s, then all the itemsets X in M is not a frequent itemset in the updated database DB  One important thing refers to how to set the value of parament Since d  is much smaller than d at most situations if the minimum support threshold set in DB  is the same as set in DB, the number of frequent itemsets in DB  may become incredible. Here gives an example,d=1000,d  10 and s=0.1 respectively. It is easy to see every item in DB  is frequent which results in candidate 2-itemsets in DB  is 45 candidate 3-itemsets in DB 2 10 C  is up to 990, as it goes on the size of candidate k-itemsets is numerous. Hence, we propose a principle to solve this problem 2 45 C When d  100, we call DB  small database and set s*d  d. By Theorem 3, all frequent itemsets in DB  are included in L, we need not find set M of DB  On the contrary, when d  100, we set a quantity lower than s and run the algorithm Theorem 3 If s*d  d,we need not process the increment database DB   Proof All itemset X in DB+DB  be frequent must satisfy   s*\(d+d  count X count X count X 1+d  d\=\(s\d which means all frequent itemsets in DB' are included in L P of DB, so we need not process DB  and only scan it once to update of X L P  count X Although classical FUP only c onsiders the insertion cases the user may remove out-of-date data from the database. In the following, we discuss the deletions. When the decrement database DB  is scanned, the support count of the itemset in L P lessens if it appears in DB Then L and P are updated according to the support count thresholds s*\(d-d nd \(s d d ectively. Let X be an itemset of DB and X L P By Theorem 4,if d d s, then we need not process the updated database \(DB-DB If we do not want to process DB  we can set minimum support threshold of DB to s\(1\ere d d. By Theorem 5, we need not process DB  Theorem 4 If d d s, then we need not scan the updated database DB   Proof  X L P  since  and are available, we need not scan DB   count X count X count X count X count X X L P  d*\(s  d*s-d d*s-d*s*d  count X count X count X count X d=d'*s. X is not frequent in DB  Theorem 5 If s'= s\(1\inimum support threshold of DB, we need not process the decrement database DB  Proof All itemset X in DB-DB be frequent must satisfy  s*\(d-d  count X count X count X s*\(1-d d\d= s'*d, which means all frequent itemsets in DB are included in L of DB, so we need not process DB and only scan it once to update of X  count X L P When both insertions and de letions are exist, there is a special case if d  d and mx<d d  s-d s, we need not scan the updated database DB'\(DB+DB  DB milar to the above  Last but not least, we apply prune techniques described below to reduce the size of the original database as well as the updated database   At the first step, all the candidate sets which do not have enough support in DB  are stored in the set D. Later during the scan of DB, all items in D can be removed from all the transactions, because they will not appear in any frequent itemset   At any k-th iteration, during the scan in DB  while FUP is counting the support for sets in the candidate sets C, for each transaction T the prune\(DB  nction is called. It counts, for each I T the number of sets in C and L k  which contain I. This number gives an upper bound on the number of frequent k-items ets that contain I. If this number is smaller than k+1, then I cannot belong to any frequent \(k+l\itemset, and hence can be removed from all the transactions    All the steps in the outline above are described graphically in Figure 1 for easy understanding             Figure 1    process of IDARM algorithm IV  P ERFORMANCE S TUDY  G   Generation of synthetic data   In the experiments, we used synthetic databases to evaluate the performance of the algorithms. Topology of our network is showed in figure 2-pink nodes are center nodes while others are edge nodes. The alarm data is generated using the following principle   Alarms in lower level of arbitrary node cause corresponding alarms in upper level of the same node  Alarms of the edge node may be transmitted to the center node which it connect with a probability p  Alarms of the center node will be transmitted to one of the edge node which it connect randomly  Alarms of the center node will be transmitted to all center nodes which it connect    The way we insert the increment database DB  is to put the d  transactions in the rear of th e original database. And the way 


we delete the decrement database DB is to remove the first d  transactions from the original database          Figure 2    Topology of our communication network H  Experimental Results All the experiments were performed on a 1.5GHz Pentium IVPC with 512 MB of main memory, running under Windows XP professional. The algorithm was coded in Java 2 SDK  Two sets of experiments have been performed to analyze IDARM versus FUP for different supports as well as IDARM versus FUP for different incremental sizes. First, the IDARM algorithm and the FUP algorithm are tested for different s ranged from 0.01 to 0.1.The interval sizes for counting DB   and DB were fixed at 1000 and 10000, respectively. The result is shown in Figure 3. Note that, our algorithm has a steady advantage over FUP at different minimum support thresholds The difference between the speedup ratio of IDARM and that of FUP is more than 2.5 for all the supports tested  We then set different incremental size ranging from 1000 to 30000, with DB and s fixed at 10000 and 0.02, respectively The performance ratio is plotted in Figure 4. The IDARM algorithm clearly outperforms the FUP algorithm for all incremental sizes tested, especially when d  1000.We also see the trend that as the incremental size continues to increase, the performances of both the IDARM and the FUP decrease However, a gradually level off only appears when the increment size is about 3 times th e size of the original database The fact that IDARM still exhibits performance gain when the increment is much larg er than the original database shows that it is very efficient         Figure 3  IDARM and FUP         Figure 4  IDARM and FUP with different supports           for different incremental sizes The number of scans of original database was 0 or 1, only the incremental database is scanned ,also by using parameter and pruning, we expect to have better performance for our algorithm. To compare it with FUP more clearly we perform another experiment on the dataset with minimum support 1 The experiment run five insertions of increment databases and the relative execution time of each insertion is recorded for both algorithm. Fig.5 shows the relative execution times in the cases of 0.1% and 0.5%. The size of each increment database is 1% of the size of the original database. Fig.6 shows the deletion cases with the same parameters. It can be seen that IDARM algorithm takes much less time than FUP algorithm since the original database which size is much larger than that of the increment database and the decrement database does not be scanned         Figure 5 Relative execution time      Figure 6 Relative execution time  for Insert and FUP\(s=0.01      for Delete and FUP\(s=0.01 V  CONCLUSION  In the real world, databases are periodically and continually updated. As it grows, the discovered rules need to be verified and new rules need to be adde d to the knowledge base. Their primary aim is to avoid or mi nimize scans of the large older database by using the intermediate data constructed during the earlier mining. Experimental results show that our algorithms outperform other algorithms, esp ecially when the size of the original database is much larger than that of the increment database and the decrement database or there is no need to scan original database. Applying data mining to realize automatization and intelligence of network management can accurately diagnose and locate failures as well as shorten the recover times, therefore has a significant meaning. Another possible research topic is the incremental mining of association rules in a distributed environment. Besides, algorithms of mining weighted association rule  are also worth studying R EFERENCES     R Agrnwnl  antl R Srikanr. F a st algorithms for mining association rules In   Proceeding of International C onference on Very Lage Database, pages 487-499, Santiago, Chile,1994  Agrawal  T I m ielinski, an d A. S w a m i   Mining Association Rules between Sets of Items in Large Databases. In Proc.1993 ACM-SIGMOD Int.Conf. Management of Data, 207-216, May 1993  m ettin en, H Mannila P Ronkainen,H. Toivonen, and A. I Verkamo. Finding interesting rules from large sets of discovered association rules. In Proc. 3rd Int\222I Conf. on In formation and Know ledge Management pages 401-408,Gaithersburg, Maryland, Nov. 1994  Cheung D W Han, J Ng  V.T   Wong, C.Y.: Maintenance of Discovered Association Rules in Large Databases An Incremental Updating Technique Proc.the International Conference on Data Engineering, \(1996\ 106-114  Cheung D W   L e e S D Kao B A general Incremental Technique for Mining Discovered Association Rules Proc. International Conference on Database System for Advanced Applications, \(1997\ 185-194 6 W. Cheu ng, A W C. Fu a nd J. Han.Knowledge discovery in databases: A rule-based attributeoriented approach. In Proc.1994 Int\2221Symp.on Methodologies for Intellig ent Systems,pages 164-173, Charlotte North Carolina, October1994 7 K  L    Ef f icient GraphBa sed Algorithms for Discovering and Maintaining Knowledge in Large Data bases, NTHU Master Thesis, \(1997  N F Ay an  A. U T a nsel and E   Ar kun An efficient algorithm to update large itemsets with early pruning In Proc.5th ACM SIGKDD Intl.Conf.on Knowledge Discovery and Data Mining, San Diego pp.287-291, Aug. 1999 9 Ng and W La m   Updating of associati on rules dynamically In Proc. Intl. Symposium on Database Applications in Non-Traditional Environments, Kyoto, Japan, pp. 84-91, Nov. 1999 


In this paper, the test samples are composed of 15 large hail examples in all 699 images, 17 slight hail examples in all 459 images and 12 rainstorm examples in all 594 images, which are offered by weather bureau of a city. The result obtained by Association Rules is shown in Table.3            As shown in Table.3, 1 monomer and 2 monomers were alert failure respectively in hail examples Because hail melted when falling and can not sense on the ground, 3 monomers in 459 images won’t cause disaster The results show that hail echo ensemble forecast model based on association rules has higher accuracy compared with the PUP products. And the hail cloud detection based on association rules satisfies practical applications. It supports forecasters to make more accurate forecasts   6. Conclusion  1\ A novel method of image mining is proposed in the hail detection model. And a system that is concerned on the automatic hail echo detection based on mining rules was built. This system is applied to distinguish hail cloud to rainstorm cloud and super-refraction effectively  2\ The accuracy of the hail cloud detection is obviously higher by classifying the production rules than that by the PUP products  3\ The system can help forecasters to make accurate forecasts Theoretical analysis and experiment results show that the automatic hail echo detection based on mining rules is effective. This method is a whole new route and made great contribution for the automatic hailstone detection. This research of this paper is a challenging and developing work, and is worth in-depth studies in the future  References   u rl  M  C e t  a l   M i n i n g f o r Im a g e Co n t e n t    Systemics, Cybernetics and Informatics  Information Systems  Analysis and Synthesis \(SCI-ISAS99  Orlando, FL  Jul.1999  2 J i a w ei H a n  an d M i c h el in e K a m b er   Data Mining  Concepts and Techniques Morgan Kaufmann Publishers, Beijing , 2000   u j i n Z h a n g  Image Engineering Tsinghu University Press, Beijing, 2002    u j i n Z h a n g   Content-based Visual Information Retrieval Science Press, Beijing, 2003   5 Y a n  C a i an d D e s h an g F u  C la s s i f i ca t i o n o f B a s e d  on Satellite Data and Its Program Design Journal of Nanjing Institute of Meteorology Sept.1999, Vol.22 No.3, pp. 416-422   u l o n g H a o  B a oy i C h e ng Y i n F a n  a n d H o n g j un Zhang, “Auto-recognition of Typhoon Based on the Character of Layout of Texture Direction Journal of Image and Graphics Dec. 2002, Vol.7, No.12,pp 1319-1322   a n q i n g  T i a n  P i ng G u o  a n d P i ngqi ng L u   T e x t u r e  Feature Extraction of Multiband Remote Sensing Image Based on Gray Level Co-occurrence Matrix Computer Science 2004, Vol.31, No.12, pp. 162-163   8 P i n g Y u e X i a o y u n Li u  Li an g cai G u o an d  Changhong Lv, “Identification Geosynchronous Satellite Infrared Image and Detection Violent Convective Weather System in Flood Season by Textural Analysis Technique Arid Meteorology Jul 2005, Vol.23, No. 2, pp. 50-53     Rafael C. Gonzalez and Richard E. Woods   Digital Image Processing   Prentice Hall, 1993  10 P et er S t an c h ev  U s i n g I m a g e M i n i n g F o r I m ag e Retrieval IASTED Conf. Computer Science and Technolog Mexico, May 19-21, 2003, pp.214-218  11 O  R  Zai a n e an d J  WH an F in d i n g S p at ia l Associations in Images Proceeding of SPIE 14th Int Symposium Orlando, FL, 2000, pp. 138-147     D a t c u a nd K  S e i de l   I m a ge I n f o r m a t i o n Mining: Exploration of Image Content in Large Archives IEEE Conference on Aerospace 2000  Vol.3, pp.255-259  Tabl e 3   Results of different forecast models  699 images \(large hail examples 459 images \(slight hail  examples 74 images \(rainstorm  examples  Alert failure monomer false alarm monomer Alert failure monomer false alarm monomer Alert failure false alarm monomer Association Rules 1 455 3 108 0 1479 PUP production 2 1436 3 376 0 6123 Note: The hail and storm example with the same monomers 
43 


 Very few ows are high throughput Most ows are short lived Almost all ows are mice  Most ows have an average packet size medium Most ows are packet mice Almost all bulk ows are medium throughput Almost all bulk TCP ows are short-lived  Fig 5 Simple on-line linguistic summary of the CRAWDAD-Fall03 NetFlow collection truth values between brackets of rules to analyze In particular we disregarded those rules with a low support or with a low condence truth value Many interesting rules were found for the NetFlow records analyzed We list as examples a selection of them  Most DNS request ows occur both during the day and at night are mice and short lived with condence 0.970 in the WIDE-F-1-Aug collection  Most ows at night are mice with condence 0.890 and Most ows during the day are mice with condence 0.998 in the CAIDA-OC48-0-Apr collection  Most SSH trafc occurs during the day and consists of short lived mice ows with condence 0.892 in the CRAWDAD-Fall03 collection Linguistic summaries provide a novel method to describe qualitative relations in NetFlow collections using natural language Thus by using association rules mining to nd relevant summaries we have a suitable method for addressing a problem related to ow analysis nding invariants in trafc what is known as one the major goals of Internet Science VI C ONCLUSIONS We have addressed network trafc analysis at the ow level from the perspective of linguistic summaries Two approaches for summarizing NetFlow collections have been developed 1 on-line summarization via a predened and congurable set of potential interesting protoforms and 2 discovery of hidden relevant summaries by means of association rules mining A tool that implements both approaches has been developed Experimental results for a set of benchmark NetFlow collections conrm linguistic summaries as an alternative look into network ow statistics useful for both network users and practitioners The method presented is a novel technique to generate simple and human-interpretable reports but also provides a promising technique for nding invariants in network trafc and advancing Internet Science This can be seen as a rst step towards natural language based knowledge discovery for Internet Science A CKNOWLEDGEMENT We acknowledge the MAWI Working Group from the Wide Integrated Distributed Environment WIDE project for k indly p ro viding their  o w collections and support We are also indebted to the Cooperative Association for Internet Data Analysis CAIDA for providing their OC48 data collection  Support f or CAID A  s O C48 Traces Dataset is provided by the National Science Foundation the US Department of Homeland Security DARPA Digital Envoy and CAIDA Members We used the Dartmouth/campus data set from t he Community Resource for Archiving Wireless Data CRAWDAD Our work has beneted from the use of measurement data collected on the Abilene network as part of the Abilene Observatory Project http://abilene.internet2.edu/observatory R EFERENCES  C ooperati v e Association f or Internet D ata Analysis CAID A V i sualization Tools http://www.caida.org/tools/visualization  J  S ommers P  B arford a nd W  W illinger  SPLA T  A V i sualization Tool for Mining Internet Measurements in 7 t h Passive and Ac t ive Ne t work Measuremen t Workshop  Mar 2006 pp 31–40  C  E stan S  S a v age and G  V ar guese  Automatically Inferring P a tterns of Resource Consumption in Network Trafc in SI G C OMM 200 3  Karlsruhe Germany Aug 2003 pp 137–148  R  R  Y ager   A N e w Approach to the S ummarization o f D ata  I n f orma t ion S ciences  vol 28 pp 69–86 1982   Database D isco v e ry Using F uzzy Sets  I n t erna t ional Journal o fI n t elligen tSy s t ems  vol 11 1996  J  K acprzyk and R  R  Y ager   Linguistic Summaries of Data Using Fuzzy Logic I n t erna t ional Journal o f General Sy s t ems  vol 30 no 2 pp 133–1504 Jan 2001  J  K acprzyk and S  Z adro  zny Linguistic database summaries and their protoforms Towards natural language based knowledge discovery tools I n f orma t ion S ciences  vol 173 no 4 Mar 2005   Cisco I OS NetFlo w  h ttp://www cisco.com/en/US/products/ps6601 products ios protocol group home.html Nov 2007  B  C laise e t al  Specication of the IPFIX Protocol for the Exchange of IP Trafc Flow Information Internet Engineering Task Force IPFIX Working Group Revision 26 Sep 2007 Internet Draft  S Shaluno v a nd B T eitelbaum TCP Use a nd Performance on Internet2 in A C M SI G C OMM I n t erne t Measuremen t Workshop San Francisco USA 2001  L A Zadeh A Computational A pproach to Fuzzy Quantiers i n Natural Languages C ompu t ers and Ma t hema t ics wi t h Applica t ions  vol 9 pp 149–184 1983  R R Y a ger   O n O rdered W eighted A v eraging O perators in Multicriteria Decision Making IEEE Transac t ions on Sy s t ems Man and Cy berne t ics  vol 18 pp 183–190  1988  L A Zadeh  A P rototype-Centered Approach to Adding Deduction Capability to Search Engines-the Concept of Protoform in F irs t I n t erna t ional IEEE Sy mposium on I n t elligen tSy s t ems vol.1,Sep 2002 pp 2–3   The concept o f a linguistic v a riable and its application t o approximate reasoning I n f orma t ion S ciences  vol 8 no 3 pp 199 249 1975  A Broido Y  Hyun R Gao and k c claf fy   Their Share Di v e rsity and Disparity in IP Trafc in 5 t h Passive and Ac t ive Measuremen t Workshop  PAM   Antibes Juan-Les-Pins France 2004 pp 113–125  M Delgado N Mar  n D S  anchez and M.-A Vila Fuzzy Association Rules General Model and Applications IEEE Transac t ions on F u zzy Sy s t ems  vol 11 no 2 pp 214–225 Apr 2003  J Kacprzyk and S  Zadro  zny Linguistic Summarization of Data Sets Using Association Rules in IEEE I n t erna t ional C on f erence on F u zzy Sy s t ems FUZZ IEEE  St Louis USA May 2003 pp 702  707  R Agra w al H Mannila R Srikant H T o i v onen and A  V erkamo Advances in Knowledge Discover y and Da t a Mining  American Association for Articial Intelligence 1996 Fast Discovery of Association Rules pp 307–328  M Fullmer e t al  ow-tools http://www.splintered.net/sw/owtools Nov 2007  W i dely Inte grated Distrib u ted En vironment  WIDE P roject MAWI Working Group Packet traces from wide backbone http://tracer.csl.sony.co.jp/mawi 2006  CAID A O C48 T race Project CAID A OC48 T r aces 200304-24 collection http://imdc.datcat.org/collection/1-0018N=CAIDA+OC48+Traces  D K o tz T  Henderson and I  A byzo v   CRA WD AD data set dartmouth/campus v 2007-02-08 Downloaded from http://crawdad.cs.dartmouth.edu/dartmouth/campus Feb 2007 624 2008 IEEE I n t erna t ional C on f erence on F u zzy Sy s t ems FUZZ 2008 


Since the attribute determination algorithm has determined that the attribute Sno in Table 0, the attribute Cno in Table 1, and the attributes <Sno Cno> in Table 2 embrace the double-connective association rule student\(Sno 010 1 course\(Cno 010 2 study\(Sno, Cno\he connective determination algorithm make the relational matrix shown in Fig. 4 according to the binary relationship table of Table 2   C1 C2 C3 C4 S1   T  T  F  F S2   T  F  T  F S3   T  F  F  F S4   F  T  F  F S5   T  F  F  T   Fig. 4 The relational matrix made from Table 2  Fig. 4 is made like this: Table 2 has the tuple S1, C1>, then at the cross of the row S1 and the column C1, a T is filled; Table 2 does not have tuple S1, C3>, then at the cross of the row S1 and the column C3, a F is filled Suppose the cardinality of student\(Sno\s M, in this example 5, i.e. S1 to S5; the cardinality of course\(Cno\n this example 4, i.e. C1 to C4 The algorithms for DCAR1 through DCAR6 are as follows The algorithm for DCAR1 If in Fig. 4 there is M*cf 1 rows, N*cf 2 columns submatrix, in which all elements are Ts, then DCAR1 holds The algorithm for DCAR2 If in Fig. 4 there is at least one column, in which there are at least M*cf 1 Ts, then DCAR2 holds The algorithm for DCAR3 If in Fig. 4 at least M*cf 1 rows have Ts, then DCAR3 holds The algorithm for DCAR4 If in Fig. 4 there is at least one row, in which there are at least N*cf 2 Ts, then DCAR4 holds The algorithm for DCAR5 If in Fig. 4 at least N*cf 2 columns have Ts, then DCAR5 holds The algorithm for DCAR6    DCAR6   DCAR3  DCAR5     DCAR2  DCAR4   DCAR1 Fig. 5 The complement lattice formed by DCAR1 through DCAR6 
277 
277 


000\003 000\\000L\000J\000\021\000\031\000\003\000\003\000&\000R\000Q\000Q\000H\000F\000W\000L\000Y\000H\000\003\000G\000H\000W\000H\000U\000P\000L\000Q\000D\000W\000L\000R\000Q\000\003\000D\000O\000J\000R\000U\000L\000W\000K\000P\000\003 Start Call DCAR1 000D\000O\000J\000R\000U\000L\000W\000K\000P  DCAR1 holds 002  Call DCAR2 000D\000O\000J\000R\000U\000L\000W\000K\000P  Output DCAR1,2,3,4,5,6 End DCAR2 holds 002  Output DCAR2,3,6 Call DCAR3 000D\000O\000J\000R\000U\000L\000W\000K\000P  DCAR3 holds 002  Output DCAR3,6 Call DCAR4 000D\000O\000J\000R\000U\000L\000W\000K\000P  DCAR4 holds 002  Call DCAR5 000D\000O\000J\000R\000U\000L\000W\000K\000P  Output DCAR4,5,6 End DCAR5 holds 002  Call DCAR6 000D\000O\000J\000R\000U\000L\000W\000K\000P  Output DCAR5,6 End DCAR6 holds 002  Output DCAR6 End Error Y N N Y Y N N Y Y N N Y 
278 
278 


If in Fig. 4 there is at least one T, then DCAR6 holds DCAR1 through DCAR6 forms a complement lattice shown in Fig. 5 In Fig. 5, the lower rule implies the upper rule That is, if DCARj is reachable from DCARi via an ascending path, and DCARi holds, then DCARj holds Because DCAR1 through DCAR6 satisfies Fig 5, their algorithms can be merged into one algorithm called connective determination algorithm, shown in Fig. 6 Suppose cf 1 80%, cf 2 75%. In Fig. 4, for the column of C1, there are M*cf 1 5*80%=4 elements whose values are T \(namely, S1, S2, S3, S5 Therefore, DCAR2: course\(Cno 004 1  student\(Sno 003 1  study\(Sno, Cno\olds. From Fig. 5, we know that DCAR3 and DCAR6 also hold. In Fig. 4, there are at least N*cf 2 4*75%=3 columns which have value T \(namely, in the column of C1 there is S1, in the column of C2 there is S1, in the column of C3 there is S2, in the column of C4 there is S5 therefore DCAR5: course\(Cno 003 1  student\(Sno 004 1  study\(Sno, Cno  VI. CONCLUDING REMARKS 1\ Double-connective association rule mining is different from single-connective association rule mining. The former mines the association among the primary keys of the two entity tables and the primary key of the binary relationship table. The latter mines the association between frequent item sets 2\. 4 is different from data cubes in data warehouses. The elements in Fig. 4 are T or F. The elements in the data cubes are data 3\The differences between double-connective association rule and database query are that, first, the query information in databases are predeterminate while the information to be mined by double-connective association rule is not predeterminate, it is implied. Secondly, database query needs to write SQL statements, while double-connective association rule mining is automatic. Thirdly, the information obtained by database query is quantitative, while the information obtained by double-connective association rule mining is qualitative such as “for many”, “there are some  REFERENCES 1 Ji a w ei H a n   M i ch eli n e K a m b er   D a t a  M i n i n g C onc ep t s  a nd Techniques, Higher Education Press, Beijing, 2001, Morgan Kaufmann Publishers, 2000 2 A  G  Ha m i lt on  L o gi c for M a th em a t i c ia ns R evi s ed E d i t i o n   Cambridge University Press, 1988, Tsinghua University Press Beijing, 2003 3 X unw e i Z h o u   Br ie f I ntr o du c t io n  to  Mu t u al l y I nve r s is tic Logic”, 1999 European Summer Meeting of the Association for Symbolic Logic, Utrecht, The Netherlands, August 1-6 1999 4 u n w ei Zh ou F i r s t leve l exp l i c i t m u lt ip le i ndu ct i v e composition”, 2005 Spring Meeting of the Association for Symbolic Logic, The Westin St. Francis Hotel, San Francisco CA. USA, March 25-26, 2005 5 A b rah a m S i lb ers c ha t z  Hen r y  F  Kort h  S S u da rs ha n Dat a b a s e  System Concepts \(Fourth Edition\, Higher Education Press Beijing, 2002, McGraw-Hill Companies, 2002  
279 
279 


support pruning gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis Biosystems vol 85 computationally challenging As training set sizes increase it is likely that these difficulties will also increase VI RELATED WORK While operating on a microarray dataset current CAR 1 2 3 4 and other pattern/rule 20 21 mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets Hence they are generally quite computationally expensive for datasets containing many training samples or genes as the case may be BSTC is explicitly related to CAR-based classifiers but requires no expensive CAR mining BSTC is also related to decision tree-based classifiers such as random forest 19 and C4.5 family 9 methods It is possible to represent any consistent set of boolean association rules as a decision tree and vice versa However it is generally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data 1 2 6 7 11 BSTC is explicitly related to and motivated by CAR-based methods To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here Perhaps the work closest to utilizing 100 BARs is the TOPRULES 22 miner TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci Hence TOP-RULES discovers all 100 confident CARs in a dataset However the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER 23 and so generally isn't polynomial time Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules 24 25 Once obtained fuzzy rules can be used for classification in a manner analogous to CARs However the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM 25 VII CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with preclassification CAR mining see Tables IV and VI we developed a novel method which considers a larger subset of CAR-related boolean association rules BARs These rules can be compactly captured in a Boolean Structure Table BST which can then be used to produce a BST classifier called BSTC Comparison to the current leading CAR classifier RCBT on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining see Section VB Hence BSTC extends generalized CAR based methods to larger datasets then previously practical Furthermore unlike other association rule-based classifiers BSTC easily generalizes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse then CAR-based methods after all exponential time CAR mining is completed O SlS CGl versus O Si CGi As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists ACKNOWLEDGM[ENTS We thank Anthony K.H Tung and Xin Xu for sending us their discretized microarray data files and Top-k/RCBT executables This research was supported in part by NSF grant DMS-0510203 NIH grant I-U54-DA021519-OlAf and by the Michigan Technology Tri-Corridor grant GR687 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies REFERENCES 1 G Cong K L Tan A K H Tung and X Xu Mining top-k covering rule Mining SDM 2002 5 R Agrawal T Imielinski and A Swami Mining associations between sets of items Y Ma Integrating classification and association rule mining KDD 1998 11 T McIntosh and S Chawla On discovery of maximal confident rules without pp 43-52 1999 24 S Vinterbo E Kim and L Ohno-Machado Small fuzzy and interpretable pp 165-176 2006 1071 pp 207-216 1993 6 G Dong pp 273-297 t995 9 pp 5-32 2001 20 W Li J R Quinlan Bagging boosting and c4.5 AAAI vol 1 V Vapnik Support-vector networks the best strategies for mining frequent closed itemsets KDD 2003 4 M Zaki and C Hsiao Charm L Wong Identifying good diagnostic genes or gene expression data SIGMOD 2005 2 G Cong A K H Tung X Xu F Pan and J Yang Farmer Finding interesting rule gene expression data by using the gene expression based classifiers BioiiiJcrmatics vol 21 l and Inrelligent Systenis IFIS 1993 16 Available at http://sdmc.i2r.a-star.edu.sg/rp 17 The dprep package http:/cran r-project org/doclpackages dprep pdfI 18 C Chang and C Lin Libsvm a library for support vector machines 2007 Online Available www.csie.ntu.edu.tw cjlin/papers/libsvm.pdf 19 L Breiimnan Random forests Maclh Learn vol 45 no 1 M Chen and H L Huang Interpretable X Zhang 7 J Li and pp 725-734 2002 8 C Cortes and Mac hine Learming vol 20 no 3 in microarray data SIGKDD Worikshop on Dtra Mining in Bioinfrrnatics BIOKDD 2005 12 R Agrawal and R Srikant Fast algorithms for mining association rules VLDB pp 1964-1970 2005 25 L Wong and J Li Caep Classification by aggregating emerging patterns Proc 2nd Iat Coif Discovery Scieice DS 1999 gene groups from pp 487-499 t994 13 Available ot http://www-personal umich edu/o markiwen 14 R Motwani and P Raghavan Randomized Algoriitlms Caim-bridge University Press 1995 15 S Sudarsky Fuzzy satisfiability Intl Conf on Industrial Fuzzy Contri J Han and J Pei Cmar Accurate and efficient classification based on multiple class-association rules ICDM 2001 21 F Rioult J F Boulicaut B Cremilleux and J Besson Using groups for groups in microarray datasets SIGMOD 2004 3 concept of emerging patterns BioinformJotics vol 18 transposition for pattern discovery from microarray data DMKD pp 73-79 2003 22 J Li X Zhang G Dong K Ramamohanarao and Q Sun Efficient mining of high confidence association rules without S Y Ho C H Hsieh H pp 725-730 1996 10 B Liu W Hsu and support thresholds Principles f Drata Mining aind Knowledge Discovery PKDD pp 406 411 1999 23 G Dong and J Li Efficient mining of emerging patterns discovering trends and differences KDD J Wang J Han and J Pei Closet Searching for An efficient algorithm for closed association rule mining Proc oJ the 2nd SIAM Int Con on Data in large databases SIGMOD 


