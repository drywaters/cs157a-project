978-1-4244-1674-5/08 /$25.00 \2512008 IEEE  CIS 2008  Applying Multiple Time Series Data Mining  to Large-Scale Network Traffic Analysis  Weisong He,Guangmin Hu, Xingmiao Yao ,Guangyuan Kan and Hong Wang School of Communication and Information Engineering University of Electronic Science and Technology of China Chengdu, P.R. China weisonghe,hgm}@uestc.edu.cn Hongmei Xiang Chongqing College of Electronic Engineering Chongqing, P.R. China    Abstract 227Minimize false positive and false negative is one of the difficult problems of network traffic analysis. This paper propose a large-scale communications network traffic feature analysis method using multiple time series data mining, analyze multiple traffic feature time series as a whole, produce valid association rules of abnormal network traffic feature characterize the entire communication network security situation accurately. Experiment with Abilene network data verify this method Keywords\227 network traffic analysis, principal components analysis, time frequency analysis, symbolic time series analysis multiple time series data mining  I   I NTRODUCTION  Network traffic anomalies has the feature of erupting suddenly without known signs, which can bring great damage to network or computers in network in a short time. Therefore one of the prepositions to ensure a safe network is to detect network traffic anomalies fast and accurately, determine the reasons that causes them and make reasonable response to them in time. To decrease abnormal network traffic and reduce or eradicate attack of denial of service, large-scale communication network routing and switching equipment must possess abilities of detecting and analyzing network traffic behavior The features of large-scale communication network traffic are high speed and immense data while anomalous traffic is small and scattered in multiple links, which is hard to be detected among normal traffic. Moreover, parameters for analysis are limited. All these make anomaly identification very difficult Large-scale communication network traffic anomaly detection is mainly on three levels: packet level, flow level and network-wide level. The advantage of packet level traffic analysis is to provide elaborate information about user performance on the finest level of granularity and basic information about application layer, which in favor of description of anomalous features and fault diagnosis. For example, Snort[2   a s i g n at u r eb a s e d  in tr u s i o n d e t ect io n  system, summarize packet content in which special attack will appear as one attack feature in artificial ways, and this special attack can be determined if a packet has the same feature when intrusion detection system matches packet content. Ke Wang and Salvatore J.Stolfo  a d opt t h e w a y of u s i n g s t a t i s t i c a l  distribution of bytes ASCII code of packet to distinguish the content difference between normal packet and abnormal packet, then the normal connects and abnormal events. Masaki Ishiguro d i s t i ngu i s h e s  n e t w o r k w o r m s a t t a c k o r po rt s c a n through observing packet frequency to specific IP address by Bayesian classification method. But, because of the features of wide distribution, high speed and massive data possessed by large-scale communication network, capturing packet is hard to be implemented on large-scale network. Flow level traffic analysis is based on flow classification, collecting statistical information of each flow and providing performance information of users on medium granularity, which makes characterizing, detecting, diagnosing and restoring network convenient. The idea of flow level traffic analysis is to separate events, group abnormal types, search distributive model of anomaly and analyze anomaly pattern. Since Netflow is a good compromise for traffic analysis based on SNMP and packet in its performance and accuracy, the mainstream method of flow level traffic analysis is to be based on Netflow.  Network-wide traffic analysis uses the global traffic information, including path traffic, link traffic information, etc. for example, [5 deploys method of multi-way subspace to identify links with traffic anomaly Early network traffic analysis mainly focused on the laws that single traffic feature \(such as value of traffic, counts of bytes\ changes. But single traffic feature time series can not characterize large-scale communication network traffic completely and accurately, so problems like high false positives and false negatives can not be avoided. Lakhina [5 adopted method of multi-way subspace to detect and identify links with traffic feature anomaly. However, this study only used clustering analysis to obtain types of anomaly and did not study correlations among multiple traffic features Our contributions lie in: \(1\ aiming at traffic features of large-scale communication network, make every traffic feature simple time series; then take multiple traffic feature as a whole to analyze and study through multiple time series data mining 2\ search motif correlation pattern among anomalous segments of multiple time series within the same time interval by Multiple Time Series Data Mining\(MTSDM for short in the following\, analyze correlation patterns of multiple traffic feature anomaly and describe network security situation of large-scale network accurately and qualitatively 


  The rest of the paper is organized as the following. In Section II, we illustrate the process of network traffic analysis and three-level network traffic analysis frame. In section III we illustrate data preprocessing. In section IV we introduce multiple time series data mining method. In section V, we provide experiment and in section a conclusion II  T HE P ROCESS OF L ARGE SCALE C OMMUNICATION N ETWORK T RAFFIC A NALYSIS  A  Overview The process of large-scale communication network traffic analysis is shown in Fig. 1 Basically, in this paper, the process of large-scale communication network traffic analysis consists of the following five steps 001 occurring For example, the probability of seeing IP 129.173.192.0 is defined to be number of packets using IP 129.173.192.0 divided by the total number of packets in the given time interval A set of 6 aggregate features used in this paper are listed as Table I  is the probability of event X x i 200  Compute entropy of several flow level traffic features collected over each time bin 200  Apply Principal Component Analysis and subspace method to entropy time series 200  Apply time frequency analysis method and Piecewise Aggregate Approximation and Symbolic Aggregate approximation to anomaly time series 200  Apply association rule mining to symbolic sequence 200  Real-time monitoring with valid motif association pattern The first 1,2,3 step is data preprocessing stage, step 4 is data mining stage, step 5 is the outcome of the mining and network traffic monitoring with the valid association rules B  Level of Network Traffic Analysis The network traffic analysis consists of three levels, from bottom to top are packet level, flow level, network-wide level 1  Packet Level Traffic Analysis Each packet including time stamp,IP address or prefix, port number, protocol type bytes and content. IP header information includes traffic volume by IP addresses or protocol, burst of the stream of packets, packet properties \(e.g., sizes, out-of-order\. TCP header information includes traffic breakdown by application e.g., Web\, TCP congestion and flow control, number of bytes and packets per session. Application header information includes URLs, HTTP headers \(e.g., cacheable response DNS queries and responses, user key strokes and so on  2  Flow Level Traffic Analysis Basic information about the flow include source and destination IP address, port number, packet and byte counts, start and end times, ToS, TCP flags. Information related to routing includes next-hop IP address, source and destination AS  3  Network-wide Level Traffic Analysis Network-wide level traffic analysis combines traffic, topology, and state information. Network-wide level traffic analysis is mainly referred to traffic matrix analysis in this paper. Traffic Matrices \(TM\ reflect the traffic volume of OD \(origindestination\ flow in a large-scale communication network            Figure 1  The process of large-scale communication network traffic analysis TABLE I  D ESCRIPTION OF 6  F EATURE T IME S ERIES  Series Description H\(srcPort Entropy of source port distribution H\(dstPort Entropy of destination port distribution H\(srcIP Entropy of source IP address distribution H\(dstIP Entropy of destination IP address distribution H\(octets Entropy of octets distribution H\(prot Entropy of protocol type distribution Information on the size and locality of flows contained in traffic matrix is crucial for monitoring network and diagnosing problems III  D ATA P REPROCESSING  A  Information Entropy The data packets within each bin of five minutes are summarized by a set of aggregate features. A flow is formally defined as a 5-tuple: source address, destination address, source port, destination port, and type of protocol. We focus on six fields: source address \(sometimes called source IP and denoted srcIP\, destination address \(or destination IP, denoted dstIP source port \(srcPort\, destination port \(dstPort\, octets and type of protocol Entropy is a metric that captures the degree of dispersal or concentration of a distribution. A wide variety of anomalies will impact the distribution of one of the discussed IP features as in    212  n i i i x X P x X P X H 1   log     1   i x X P 


2.5 1.5 0.5 004  j appr j p  Let i 1 2 3 4 5 6 7 8 9   B  Principal Components Analysis and Subspace Method The subspace method is an effective approach to separate normal from anomalous network traffic. Principal components analysis \(PCA\ [6 is u s u a lly t h e m ai n me t h o d  ap p lie d  to e x e r t  such function We regard the data points as a cloud with n dimensions and the first main component 1 PC is the direction point with the greatest change 2 PC is the direction points with greatest change which stand on the orthogonal direction of 1 PC This process does not finish until all the main components are discovered, and n refers to the dimensions of data. Please make sure all the main components converge with one another orthogonally, and thus form an orthonormal basis. The data change most on the first number axis direction, while the one on the second axis change less than the first one, and so do the other components The subspace method uses these PCA mentioned above to define the normal subspace and the anomalous   subspace. For some m the normal subspace is the space spanned by 1 PC  through m PC and the abnormal subspace is similarly the space spanned by 1 003  0.67 0.25 0 -0.18 -0.32 -0.43 -0.52 4 003    0.84 0.43 0.18 0 0.14 0.25 5 003     0.97 0.57 0.32 0.14 0 6 003      1.07 0.67 0.43 0.25 7 003       1.15 0.76 0.52 8 003        1.22 0.84 9 003         1.28  004 denote the th i element of the alphabetize a b b a b c  Figure 2  A time series is dispersed by firstly obtaining a PAA approximation and then using predetermined breakpoints to map the PAA coefficients into SAX symbols Note that in this example the 3 symbols, \223a\224, \223b\224 and \223c\224 are approximately equiprobable as we desired. We call the series of symbols representing a subsequence a word A subsequence P with length N can be represented as a word n p p P  1 002  C  Time Series Representation and Time Frequency Analysis 1  Piecewise Aggregate Approximation The basic idea of Piecewise Aggregate Approximation \(PAA\ [7  8   9   i s  th at i t represents the time series as a sequence of rectangle basis functions. It is a dimensionality-reduction representation method in essential as in   005 001 is the basic wavelet function and   1   a t a t a  2 003 003 002 002 212 1 3 The PAA representation is only an intermediate step of obtaining the SAX 3  Wavelets Transform and Wavelet Packet Transform Suppose       2 t R L t x 2 1  212   i N n i N n j j i p n N p 1  1  1  n is the length of sequence N is the number of PAA segments i p is the average value of the th i segment 2  Symbolic Aggregate approximation The basic idea of Symbolic Aggregate approximation \(SAX   s t ha t i t  converts the time series into  an  discrete symbolic sequence Having transformed a time series data into the PAA, we can apply SAX to obtain a discrete symbolic representation. Since normalized time series have a Gaussian distribution, we can determine the \223breakpoints\224 that will produce c equal-sized areas under Gaussian distribution curve [10   The breakpoints will be found out by looking them up in Table II. Once the breakpoints have been obtained we can disperse time series into discrete symbolic series. We firstly obtain a PAA manner of the origin time series. All PAA coefficients that are below the smallest breakpoint are transformed to the symbol \223a\224, and all coefficients greater than or equal to the smallest breakpoint but less than the second smallest breakpoint are transformed to the symbol \223b\224, etc Fig.2 illustrates the idea TABLE II  A  L OOKUP T ABLE T HAT C ONTAINS T HE B REAKPOINTS   c  0 1 003  0.43 -0.67 -0.84 -0.97 -1.07 -1.15 -1.22 -1.28 2 003  0.43 0 -0.25 -0.43 -0.57 -0.67 -0.76 -0.84 3 003  3 4 5 6 7 8 9 10 1 10 0.5 004 Then the transformation from a PAA approximation appr P to a word P is obtained as in  iif p j i  004 and b  m PC through   n m PC n 006 005 005 006 212  is the shift and scale extension of the basic wavelet function as in   1 


  is a set of transaction. A transaction t contains itemset X iff, for all items, where i X i         k n n k n n k t y k g t y k t y k h t y  2    2    2    2   1 2 2 7 Function set 006 005 t  10      T t t X t X i i i   is the result of decomposition of k band on scale j WPT may consist of various orthogonal basis, wavelet basis is the typical case. Among all combination, the least entropy is the good basis. The decomposition of good basis can represent the timefrequency of signal which imply that the method is adaptive for signal 4  Choi-Williams Distribution In order to reduce the disturbed components of the Chio-William distribution[11   we should have a research on the factors which make the disturbed value minimum. The Choi-Williams is proposed to solve this problem as in     001 f  t Given an itemset  X we find all the rules Y X r Frequent itemsets is used to generate all frequent itemsets in a given database T Dislike general association rules mining, the time of data point should be focused on when we using time series association rules mining. Each feature elements have different value at each time so we apply association rule mining to different values of different network feature within the same time interval The abrupt changes of different time series at the same time bin usually have a certain motif association pattern. This rule lays the foundation for us to analyze anomalous behavior of large-scale network and predict the network security situation The demo of Multiple Time Series Association Rules Data Mining is showed in the following Fig.3    212  007         1    t t x dt a t t x a a WT a x 006 005 006 4 is called as the wavelet transform of   t x The basic idea of discrete wavelet transform \(DWT\ is to transform a discrete time signal into a discrete wavelet representation. Discrete wavelet transform converts an input series k x x x   1 0 into one high-pass wavelet coefficient series and one low-pass wavelet coefficient series \(of length  2 n each\ given by as in  \(5\  and  \(6  212  212 200  1 0 2   m k k k i i z s x H 5     t y n is called as wavelet packet which is the result of whole decomposition of all bands on various scale of origin signal. Let j n k 2  but the ones in a limited range. When studying the distribution shape of a certain time t\e should study the features of signals near the given time In a sense, it means to condense the cross-item of multivariable signal by adding some windows and deleting the non local components. Finally we change Wigner-Ville distribution into local distribution. Pseudo Wigner-Ville distribution characterizes [11 l oc a l be h a vi or of a s i gn a l  a s  the following formula, so it is convenient for us to mine the local features of the fault signals as in  212  212  212  n i In practice, such transformation will be used recursively on the low-pass series until the desired number of iterations is reached Wavelet Packet Transform \(WPT\ is a method which makes time frequency decomposition of signal. WPT is of self-adaptive of signal, which can effectively display the time frequency property of signal. Just by orthogonal mirror filter we can obtain WPT decomposition. Assume the signal   t y  we can obtain       212   007 212 212 n  n 212 n  n 212 8 Wigner-Ville distribution    f t C satisfies the edge conditions and shift characteristics but does not satisfy weak and limited support characteristics. However,when 212  then     2 t y t y k n j  is a set of items X is an itemset if it is a subset of I      1 n i i t t t T n n 212 212 007 212   2  2   2       9   t h  is the window function IV  M ULTIPLE T IME S ERIES A SSOCIATION R ULES D ATA M INING      2 1 k i i i I 212  212 200  1 0 2   m k k k i i z t x L 6 Where   z s k and    z t k are wavelet filters m is the length of the filter, and 1  2   1  0 r Confidence is a conditional probability that a transaction contains X as well as Y denoted as     X Sup Y X Sup 006 265 006 265 006 265 b\006 t 006 265 t b d d x x e e f t C t ft j  2   2  4    2 2 4   2 2 n 013 n 212 and n 006 212 t itemset All itemset  X in a transaction database T has a support, denote as   X Sup  13   s e e  as in  T X X Sup     r  denoted as   Y X Sup 001 is a t it would satisfy weak and limited support characteristics 5  Pseudo Wigner-Ville Distribution For a given time Wigner-Ville Distribution can describe the global distribution of a signal. In addition, for a given frequency, it can also equally measure all the frequencies either higher or lower than the given frequency. In fact, we are not able to study all the integrals between 013 with minimum support and confidence Support is a probability that a transaction contains Y X 006 006 006 006 b d e t x t x h f t PW f j x 


12/19 12 20 12 21 12 22 12 23 12 24 12 25 0 1 12/19 12 20 12 21 12 22 12 23 12 24 12 25 12/19 12 20 12 21 12 22 12 23 12 24 12 25 12/19 12 20 12 21 12 22 12 23 12 24 12 25 12/19 12 20 12 21 12 22 12 23 12 24 12 25 12/19 12 20 12 21 12 22 12 23 12 24 12 25 12/19 12 20 12 21 12 22 12 23 12 24 12 25 12/18 12/19 12 20 12 21 12 22 12 23 12 24 12 25     Octets H dstPort H s rcPort H s rcIP H  6    5           Figure 3  Multiple time series association rules data mining at the same time interval T 013 the curve doesn\222t increase and doesn\222t decrease\, the distribution of source port is concentrated, the distribution of octets is concentrated, and the distribution of protocol is dispersive is also concentrated, from which we can infer that the distribution of destination port is dispersive. Hence this rule can be used to identify whether the anomaly is worm or not. The experimental result is shown in Table III  12/26 0.1 12/26 0.2 12/26 0.1 12/26 0.2 12/26 0.2 12/26 0.1 12/26 12/26   conf  205\205 The rule EE AA AA AA EE 3 5  4  2  1 means that the distribution of source address is dispersive  E E   dstIP H p rot H 221A\222 denotes the lowest entropy value, \221B\222 denotes the lower entropy value, \221C\222 denotes the medium entropy value, \221D\222 denotes larger entropy value, \221E\222 denotes the largest entropy value. Then we apply association rules mining to alphabet sequence of eight days \(from Dec. 18 to Dec.25\ to get the association rules of the anomaly pattern in the backbone IPLSng router EE AA AA AA EE 3 5  4  2  1  100  11 sup 12/18 12/18 12/18 12/18 12/18 12/18 12/18 0 05 0 05  a Applying WPT  to H\(octets  b\            Applying CWD to H\(octets  c\           Applying PWVD to H\(octets Figure 4  Apply WT,CWD and PWVD methods to anomaly entropy time series By such method, we can obtain different motif pattern about various anomalous behaviors of large-scale communication network, which can be applied to network traffic analysis V  E XPERIMENT R ESULT  A  Data Sets The sampled flow data collected from the backbone networks: Abilene [1  Ab il e n e i s  th e I n t e r n et2 b a ck b o n e  network, connecting over 200 US universities and peering with research networks in Europe and Asia. It consists of 11 Points of Presence \(PoPs\, spanning the continental US. Sampling is periodic, at a rate of 1 out of 100 packets B  Simulation Test We collect data from 08:05 a.m. on December 18th, 2006 to 08:00 a.m. on December 26th, 2006.We firstly compute entropy value of each feature within each time bin. We analyze six time series of entropy value by PCA method, and obtain the time series of the anomalies\222 entropy value In order to locate the anomalies, we apply the wavelet packet transformation, Choi-Williams distribution and Pseudo Wigner-Ville distribution methods to anomaly entropy time series. The results are shown in Fig. 4 From the Fig.4, we can make sure some anomalies worms certainly exist in 20:40 Dec.18 In order to find out other anomalous time point, we should obtain the rule of the existing anomaly time point. Firstly, we must extract an anomalous time series segment between 146 point and 155 point \(from 20:10 to 20:55 on Dec. 18\ Then we apply PAA and SAX to the symbolic series segment. The outcome is shown by Fig. 5 Let  4    3    2    1   0 2 0 2 0 1 0 2 0 2 0 1 0 0 0 0 0 0 0 0 0.05 0.05 


 BBBBEEBBBB  BBBBEEBBBB 10   10 0.5 1.5 1 2 3 4 5 6 7 8 9 2 1.5 1 0.5 2.5 2 1 Time Points\(5 minutes interval H srcIP Time Points\(5 minutes interval H sr cP or t Time Points\(5 minutes interval H dstPort 0 5  DC  Figure 5  Apply SAX on 6 time series TABLE III  T HE T IME D ISTRIBUTION OF 223 EE AA AA AA EE 3 5  4  2  1  224 Day 18 19 19 19 19 19 19 19 19 21 25 Hour 20 05 05 06 08 20 20 20 20 18 16 Minutes 40 30 40 40 40 00 10 20 30 20 00  A CKNOWLEDGMENT  The authors would like to thank Abilene for the providing Netflow data and will thank reviewers for their helpful comments. This research supported by Chinese National Science Foundation under grant No.60572092, Program for New Century Excellent Talents in University and National Basic Research Program of China \(\223973 program\224\ under grant No.2007CB307100 R EFERENCES  1  http://abilene.internet2.edu 2  http://www.snort.org 3  Ke Wang, Salvatore J.Stolfo, \223Anomalous Payload-based Network Intrusion Detection\224, the 7th International Symposium on Recent Advances in Intrusion Detection 2004 4  Masaki Ishiguro, Hironobu Suzuki, Ichiro Murase, Hiroyuki, \223Internet Threat Detection System Using Bayesian Estimation,\224 the 16th Annul FIRST Conference on Computer Security Incident Handling 2004 5  Lakhina,A.,Crovella,M.,and Diot,C,\223Mining anomalies using traffic feature distributions\224. In ACM SIGCOMM Philadelphia, Pennsylvania USA, 2005\,pp.217\226228 6  Hotelling,H,\223Analysis of a complex of statistical variables into principal components,\224 J. Educ. Psy 1933\, 417\226441 7  Lin,J.,Keogh,E., Lonardi, S. & Chiu, B, \223A Symbolic Representation of Time Series, with Implications for Streaming Algorithms,\224 In proceedings of the 8th ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery San Diego, CA. June 13, 2003 8  Lin,J.,Keogh,E., Patel, P. & Lonardi, S, \223Finding Motifs in Time Series". In proceedings of the 2nd Workshop on Temporal Data Mining at the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining Edmonton, Alberta, Canada. July 23-26 2002 9  Eamonn J. Keogh ,Michael J. Pazzani, \223An Enhanced Representation of Time Series Which Allows Fast and Accurate Classification,Clustering and Relevance Feedback. In International Conference on Knowledge Discovery and Data Mining pages 239\226243, New York,NY, USA August 1998 10  R.J.Larsen and M.L.Marx An Introduction to Mathematical Statistics and Its Applications 2nd ed. Englewood,Cliffs,NJ:Prentice Hall.1986 11  L.Cohen Time-Frequency Analysis:Theory and Applications Prentice Hall.1998 12  R.Agrawal, T. Imielinski, A. Swami, \223Mining association rules between sets of items in large databases,\224 in Proc. ACM SIGMOD Int. Conf Management Data 1993, pp. 207\226216 13  Chengqi Zhang, Shichao Zhang Association Rule Mining:models and algorithms 2002   DDDDAADDDD  EDDEDCBBAA 2 5 4 6 8 Time Points\(5 minut es interval H sr cP or t  Worm 10 0 5 10 10 10 1 2 3 4 5 6 7 8 9 2 1.5 1 0.5 T ime Points\(5 minutes interval H\(octets 1 2 3 4 5 6 7 8 9 1 0 1 2 2 0 1 2 b 0 1 2 0 1 d 1 2 0 1 2 b 0 1 2 0 1 d 0 T im e Points 5 m inutes interval H prot 2 0 1 d 0 1 T ime Points\(5 minutes interval H\(dst I P 2 0 1 e b b b e e b b b b d d d a a d d d d b b b e e b b b b 1 2 3 4 5 6 7 8 9 2 1 5 1 0 5  AA  AA  AA 2 1 10 0 5 1 5 2 5 2 3 4 5 6 7 8 9 0 5 1 d d e d c b b a a 10 0.5 0.5 1.5 2.5 0  5 1.5 0.5 10 10 2 5 2 5 4 6 8 2 1 5 1 0 5 4 6 8 2 1 5 1 0 5 4 6 8 0.5 4 6 8 2 1.5 1 0.5 4 6 8 0.5 d d d a a d d d d d d d a a d d d d 10 0 5 1 5 Tim e Points 5 minutes  int erval H  sr cI P 0 5 1 5 2 5 0 5 0 5 1 5 2 5 2.5 1 2.5 1 1 2 3 4 5 6 7 8 9 0 1 2 T im e Points\(5 minutes interval H dst Por t 1.5 0.5  DDDDAADDDD  DDDDAADDDD   EE T im e Points\(5 minut es interval H\(dst IP    EE 0.5 0.5 Time Poin ts 5 minutes in terval H octets Time Poin ts 5 minutes in terval H\(prot  10 


Obviously in both 036gures the number of maximal inference channels is reduced when the value of 001 is increased The underlying reason is that increasing the value of 001 reduces the number of 001 frequent itemsets For example when 001/N increases from 10 to 25 in Figure 7 the number of 001 frequent itemsets shrinks from 574431 to 5545 This means that the amount of information exposed to attackers is reduced Consequently the number of maximal inference channels is also reduced 5.3 The e\001ect of anonymity threshold 10 15 20 25 30 35 40 45 50 0 5 10 15 20 25 30 k Time \(second MUSHROOM OICD PMICD Figure 9 Time for detecting maximal inference channels 10 15 20 25 30 35 40 45 50 0 5 10 15 20 25 30 35 40 45 50 k Time \(second CHESS OICD PMICD Figure 10 Time for detecting maximal inference channels In the second set of experiments we 036rst 036x the value of minimum support threshold 001 to perform frequent itemset mining on the datasets Then we run PMICD and OICD on the mining results with different values of anonymity threshold k  respectively The time for detecting all the maximal inference channels in MUSHROOM is shown in Figure 9 where we 036x 001/N  15  the number of closed frequent itemsets is 2261 and the number of maximal frequent itemsets is 321 We can see that the execution time of both PMICD and OICD is almost unchanged when varying the value of anonymity threshold k  The reason is that in both PMICD and OICD the anonymity threshold k only acts as a condition for judging if a set of frequent items forms an maximal inference channel or not it does not affect the execution time of both approaches In Figure 10 we mine CHESS with 001/N  80  There are 5083 closed frequent itemsets and 226 maximal frequent itemsets in the mining result When detecting maximal inference channels under different values of anonymity threshold k  we 036nd the same phenomenon as that in Figure 9 That is the execution time of both PMICD and OICD is almost unchanged We also observe that PMICD is more ef\036cient than OICD in both Figure 9 and Figure 10 The reason is again that the number of closed frequent itemsets is close to or even larger than the number of transactions in MUSHROOM or CHESS 10 15 20 25 30 35 40 45 50 0 200 400 600 800 1000 1200 1400 1600 1800 2000 k Number of Maximal Inference Channels MUSHROOM Figure 11 Number of maximal inference channels vs anonymity threshold k In Figure 11 and Figure 12 we plot the number of maximal inference channels when varying the value of anonymity threshold k  Because in all these experiments the maximal inference channels discovered by PMICD are the same as those by OICD which again demonstrates the correctness and completeness of PMICD We only plot the result of PMICD The results in Figure 11 and Figure 12 show that the number of maximal inference channels increases for both MUSHROOM and CHESS when we enlarge the value of anonymity threshold k  This phenomenon is easily understood from the de\036nition of inference channel When enlarging the value of k  the number of pattern p 002P  D 001  s.t 0 sup D  p  k are increased Consequently the 
345 
339 


10 15 20 25 30 35 40 45 50 0 500 1000 1500 2000 2500 3000 3500 4000 k Number of Maximal Inference Channels CHESS Figure 12 Number of maximal inference channels vs anonymity threshold k number of maximal inference channels will also increase 6 Conclusions Sharing the knowledge discovered by data mining without discrimination may pose threats to individual’s anonymity In this paper we present a new projectionbased approach for detecting the anonymity of patterns before sharing the result of frequent itemset mining In our approach maximal inference channels are detected by projecting the original database based on each maximal frequent itemset without the actual calculation on the supports of frequent itemsets We prove that our approach can discover the collection of all the maximal inference channels for nonk anonymous patterns The experimental results show that our projection-based approach is more ef\036cient than previous work especially when the number of closed frequent itemsets in the mining result is close to or larger than the number of transactions in a database For future research we will investigate the possibility of developing more effective and ef\036cient approaches for detecting anonymity of patterns and we will also extend our research to other forms of knowledge representation Acknowledgment This research was partly supported by the Shanghai Leading Academic Discipline Project No B114 the National Natural Science Foundation of China No 60673133 the National Grand Fundamental Research 973 Program of China No 2005CB321905 We would like to thank the anonymous reviewers for their helpful comments References  R  A gra w al and R  S rikant F ast a lgorithms for m ining a ssociation rules in large databases In VLDB  pages 487–499 Santiago Chile 1994  M  A tzori F  Bonchi F  G iannotti and D  P edreschi k anonymous patterns In PKDD  pages 10–21 Porto Portugal 2005  M  A tzori F  Bonchi F  G iannotti and D  P edreschi Anonymity preserving pattern discovery VLDB J  accepted for publication 2008  D  B urdick M  C alimlim J  F lannick J  Gehrk e and T  Y iu MAFIA A maximal frequent itemset algorithm IEEE Trans Knowl Data Eng  17\(11 2005  J W  Byun A Kamra E  B ertino and N  L i Ef 036cient k anonymization using clustering techniques In DASFAA  pages 188–200 Bangkok Thailand 2007  B  C  M  F ung K W a ng and P  S  Y u T op-do wn specialization for information and privacy preservation In ICDE  pages 205–216 Tokyo Japan 2005  J  Han and M  Kamber  Data Mining Concept and Techniques  Morgan Kaufmann Publishers San Francisco 2000  J  Han J  P ei a nd Y  Y i n Mining frequent patterns w ithout candidate generation In SIGMOD Conference  pages 1–12 Dallas Texas USA 2000  D  K nuth Fundamental Algorithms  Addison-Wesley Reading Massachusetts 1997  K LeFe vre D J DeW itt and R  R amakrishnan Mondrian multidimensional k-anonymity In ICDE  Atlanta GA USA 2006  S R M Oli v eira O  R  Z a  031ane and Y Saygin Secure association rule sharing In PAKDD  pages 74–85 Sydney Australia 2004  P  Samarati Protecting r espondents identities in m icrodata release IEEE Trans Knowl Data Eng  13\(6 2001  L Sweene y  k-anon ymity A model f or protecting privacy International Journal of Uncertainty Fuzziness and Knowledge-Based Systems  10\(5 2002  J W a ng J Han a nd J Pei CLOSET s earching f or the best strategies for mining frequent closed itemsets In KDD  pages 236–245 Washington DC USA 2003  Z W a ng W  W a ng and B  S hi B locking i nference channels in frequent pattern sharing In ICDE  pages 1425–1429 Istanbul Turkey 2007  Z W a ng W  W a ng B Shi and S  H  B oe y  Pri v ac ypreserving frequent pattern sharing In DASFAA  pages 225 236 Bangkok Thailand 2007  J Xu W  W ang J Pei X W a ng B Shi and A  W C Fu Utility-based anonymization using local recoding In KDD  pages 785–790 Philadelphia PA USA 2006 
346 
340 


applying optimization 2\(line 3.1 and 3.1.1 If Y is NULL and support of X is  S then any itemset which is a superset of IS has support  Sin W 002  therefore pop item from IS and continue to examine next itemset\(line 3.1.2 Similar is the case for line 4 If sum of support of X and Y\(i.e support of IS in W 002  is 002 S then we need to examine if their intersection say Z is a new CFI\(line 5 If Z contains done-set items then it should have been already examined earlier according to optimization 5 so all intersections of closures of supersets of IS contains done-set items Therefore pop item from IS and continue\(line 5.1 If Z is a subset of Y then Z is nonclosed in W/d-block but is closed in W 002 due to addition of d 002 block therefore add it as a new CFI to set CFI 002 and prune common todo-set items by dynamic reordering\(line 5.2 Suppose if Z  Y and support of Y is  S in W/d-block then Y\(already closed becomes frequent in W 002 due to addition of d 002 block therefore add it as a new CFI to set CFI 002 and prune common todo-set items by dynamic reordering\(line 5.3 If support of Y is 002 S in W/d-block then it is already present in LDIUW tree therefore just update it’s support by support of IS in d 002 block and prune common todo-set items by dynamic reordering\(line 5.4 Line 6 constructs IS’s conditional FP-tree F new w.r.t minimum support:=1 to explore further itemsets based on dynamic reordering and recursively calls the procedure Stream-Close on F new  Finally in line 6 of BEGIN block it merges CFI’s of CFI 002 and LDIUW tree to output the set of CFI 002 in W 002  Lemma 8 An itemset is a CFI in W 002 iff Stream-Close says Proof An itemset D is found as a CFI by Stream-Close when 1 D is frequent 2 there is no item which appears in every transaction in D-conditional database and 3 D is not a proper subset of any CFI already found To assert correctness of the lemma we show that there is no frequent closed itemset E which can be found later such that D is a subset of E Suppose we can 036nd such an itemset E then ED 011  0 must happen in every transaction of the D-conditional database This leads to a con\037ict with the fact that there is no item appearing in every transaction in the D-conditional database Thus we have the lemma The correctness of the algorithm has been reasoned stepby-step It generates the complete set of CFI’s in W 002 as shown in above lemmas 4 Empirical results We compare our algorithm with Moment w hich is the state-of-the-art algorithm to mine frequent closed item0 0.01 0.02 0.03 0.04 0.05 0.5 3 5.5 8 10.5 13 15.5 18 20.5  The Minimum Support Running time in Seconds   Moment Stream\025Close  Figure 7 Performance comparisons of Moment and Stream-close 0 0.01 0.02 0.03 0.04 0.05 0.4 0.9 1.4 1.9 2.4 2.9  The Minimum Support Memory usage in number of itemsets   Moment Stream\025Close  Figure 8 Memory usage results of Moment and Stream-close The order of number of itemsets is 10 7  sets in data streams For performance evaluation the synthetic dataset T10.I6.D100K is used The dataset is generated by the same procedure as described in where t he three numbers o f each dataset denote the average transaction size T the average maximal potential frequent itemset size I and the total number of transactions D respectively In our experiments the transactions of T10.I6.D100K dataset are looked up incrementally in blocks in sequence to simulate the environment of an online data stream In our experiments we keep sliding width parameter d  10 for Stream-Close window width:=1,00,000 for both Stream-Close and Moment and perform experiments over 100 sliding windows for Moment and 10 sliding windows for Stream-Close\(since d’:=10 and take the average processing time for every 10 transactions processed under different minimum supports for T10.I6.D100K data set as shown in 036gure 7 We can see from 036gure 7 that Stream-Close runs much faster than Moment when the support threshold is relatively low because the number of boundary nodes stored in the data structure of Moment increases when the support threshold decreases as the number of nodes to be processed 
524 
524 


and checked for node property increase execution time is increased When the support threshold is relatively high these two algorithms have comparable running time Moment runs a little bit faster than Stream-Close as the threshold increases This is because as the threshold is increased the number of the boundary nodes in Moment decreases while Stream-Close processes the same number of closed itemsets independent of support information This is advantageous when users have different speci\036ed support thresholds in their online queries Regarding memory usage Stream-Close maintains only CFI’s in LDIU-tree but whereas for Moment when the user de\036ned support threshold is small the number of nodes it maintains in the memory increases dramatically which consists of all the infrequent gateway nodes unpromising gateway nodes intermediate nodes and closed nodes as shown in 036gure 8 We performed all our experiments on 2 GB RAM 1.8 GHz AMD machine Our performance study shows that Stream-Close is very ef\036cient and the optimization techniques proposed in this paper are effective in improving the algorithm ef\036ciency 5 Conclusions In this paper we investigated the issues for cumulative mining of CFI’s in high speed data streams and addressed the inef\036ciency problem of mining the new window from scratch,sliding window by one transaction and also the problem associated with closedness checking of candidate itemsets in newly arrived block We proposed an algorithm Stream-Close by exploring several novel techniques to increase ef\036ciency and scalability It is a promising algorithm to mine CFI’s over high speed data streams In the future we plan to explore how to mine compressed top-k itemsets in datastreams and also how to adaptively vary sliding width according to speed of the data streams Also we plan to perform experiments on different datasets 6 Acknowledgements We thank Dr Yun Chi at the University of California for providing us the Moment algorithm source code References  J  Pei J  Han and R Mao Closet An e f 036cient algorithm for mining frequent closed itemsets ACM SIGMOD International Workshop on Data Mining and Knowledge Discovery May 2000  J  Pei J  Han and J W ang Closet Searching for the best strategies for mining frequent closed itemsets ACM SIGKDD Int’l Conf on Knowledge Discovery and Data Mining August 2003  M  J  Z aki and C J Hsiao Charm An ef 036cient algorithm for closed itemsets mining SIAM Int’l Conf on Data Mining April 2002  C  L ucchese S Orlando and R Pere go F a st and memory ef\036cient mining of frequent closed itemsets Knowledge and Data Engineering IEEE Transactions January 2006  J  H  Chang W  S Lee A Zhou Finding recent frequent itemsets adaptively over online data streams ACM SIGKDD Int’l Conf on Knowledge Discovery and Data Mining August 2003  S H L i S Lee and M Shan An ef 036cient algorithm for mining frequent itemsets over the entire history of data streams Int’l Workshop on Knowledge Discovery in Data Streams Sept 2004  G S  M anku R Motw ani Approximate frequenc y counts over data streams Int’l Conf on Very Large Databases 2002  J  H  Chang W  S Lee A s liding w indo w m ethod for 036nding recently frequent itemsets over online data streams Journal of Information Science and Engineering July 2004  C  Giannella J  Han J Pei X Y an P  S Y u  Mining frequent patterns in data streams at multiple time granularities Data Mining Next Generation Challenges and Future Directions AAAI/MIT 2003  C Lin D Chiu Y  W u A L P  Chen Mining frequent itemsets from data streams with a time-sensitive sliding window SIAM Int’l Conf on Data Mining April 2005  Y  Chi H W ang  P  S  Y u  R R Muntz Moment Maintaining closed frequent itemsets over a stream sliding window Int’l Conf on Data Mining November 2004  Nan J iang Le Gruenw ald CFI-Stream M ining Closed Frequent Itemsets in Data Streams KDD August 2006  Dong Xin Jia wei Han Xifeng Y a n and Hong Cheng On Compressing Frequent Patterns Knowledge and Data Engineering Special issue on Intelligent Data Mining 60\(1 5-29 2007  R Agra w a l R Srikant F ast a lgorithms for m ining association rules Int’l Conf on Very Large Databases September 1994  P  V a ltchef R  M issaoui and R Godin A F ramework for Incremental Generation of Frequent Closed ItemSets Proceedings of the 2nd SIAM Workshop on Data Mining Arlington VA April 2002  A v ailable a t h ttp://cl w eb csa.iisc.ernet.in/srirang a 
525 
525 


